{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdipourasl/Deep-Learning-1402/blob/main/DL5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQPNR08rglpt"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<h1>Deep Learning Project #5<h1>\n",
        "Amin Abdipour 401133011</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDdJFcJpglpw"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCzkV-6xVs6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5972437-16c6-4ad6-dd8e-e92e3493cdf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import os\n",
        "import os.path as op\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# from PIL import Image\n",
        "# from torch.utils.data import random_split, DataLoader\n",
        "# from sklearn.metrics import accuracy_score, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict With RNN (LSTM)"
      ],
      "metadata": {
        "id": "4qPS_VyOsgRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Data"
      ],
      "metadata": {
        "id": "lWu1IljJ9tjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have ConnectTimeout error for this section, so Run this Section in VS Code and Load Excel data"
      ],
      "metadata": {
        "id": "1CV-M-FtnZYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/Glyphack/pytse-client.git\n",
        "# import pytse_client as tse\n",
        "# tickers = tse.download(symbols=['فولاد'], write_to_csv=True, adjust=True , include_jdate=True)\n",
        "# display(tickers['فولاد '])\n",
        "\n",
        "# indices=tse.download_financial_indexes(symbols=['شاخص كل'],include_jdate=True,write_to_csv=True)\n",
        "# display(indices['شاخص كل'])\n",
        "\n",
        "# indices=tse.download_financial_indexes(symbols=['شاخص كل (هم وزن)'],include_jdate=True,write_to_csv=True)\n",
        "# display(indices['شاخص كل (هم وزن)'])"
      ],
      "metadata": {
        "id": "qcjYvr1ll_qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = op.join('/content/drive/My Drive/','DL','DL_HW05','فولاد-ت.csv')  # Path to the Data folder\n",
        "Foolad = pd.read_csv(dir)\n",
        "dir = op.join('/content/drive/My Drive/','DL','DL_HW05','شاخص كل.csv')  # Path to the Data folder\n",
        "All = pd.read_csv(dir)\n",
        "dir = op.join('/content/drive/My Drive/','DL','DL_HW05','شاخص كل (هم وزن).csv')  # Path to the Data folder\n",
        "HamVazn = pd.read_csv(dir)\n",
        "Foolad"
      ],
      "metadata": {
        "id": "m-YC9Jz3ss66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "252257eb-5bdb-4aa9-b18c-3323a9bd38bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            date    open    high     low  adjClose         value     volume  \\\n",
              "0     2007-03-11     9.0    10.0     9.0       9.0  889437216900  468077431   \n",
              "1     2007-03-12    10.0    10.0    10.0      10.0  193879458000  100041000   \n",
              "2     2007-03-13    10.0    10.0     9.0      10.0  249241504527  126270939   \n",
              "3     2007-03-14    10.0    10.0    10.0      10.0   51666379451   26705128   \n",
              "4     2007-03-17     9.0    10.0     9.0       9.0   28239006789   14877283   \n",
              "...          ...     ...     ...     ...       ...           ...        ...   \n",
              "3715  2023-12-30  6280.0  6330.0  6250.0    6280.0  413007029830   65778615   \n",
              "3716  2023-12-31  6300.0  6300.0  6180.0    6230.0  498910921170   80066614   \n",
              "3717  2024-01-01  6230.0  6230.0  6150.0    6200.0  417831898480   67436187   \n",
              "3718  2024-01-02  6180.0  6220.0  6100.0    6160.0  446092828520   72415054   \n",
              "3719  2024-01-03  6180.0  6230.0  6100.0    6160.0  510237384690   82800015   \n",
              "\n",
              "      count  yesterday   close       jdate  \n",
              "0      7736        9.0     9.0  1385-12-20  \n",
              "1      9214        9.0    10.0  1385-12-21  \n",
              "2      5862       10.0    10.0  1385-12-22  \n",
              "3      1901       10.0    10.0  1385-12-23  \n",
              "4      1514       10.0     9.0  1385-12-26  \n",
              "...     ...        ...     ...         ...  \n",
              "3715   3774     6220.0  6280.0  1402-10-09  \n",
              "3716   3884     6280.0  6190.0  1402-10-10  \n",
              "3717   3320     6230.0  6180.0  1402-10-11  \n",
              "3718   3753     6200.0  6150.0  1402-10-12  \n",
              "3719   3654     6160.0  6190.0  1402-10-13  \n",
              "\n",
              "[3720 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d55d1a20-4186-4edb-88a9-263dd678f550\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>adjClose</th>\n",
              "      <th>value</th>\n",
              "      <th>volume</th>\n",
              "      <th>count</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>close</th>\n",
              "      <th>jdate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007-03-11</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>889437216900</td>\n",
              "      <td>468077431</td>\n",
              "      <td>7736</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1385-12-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2007-03-12</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>193879458000</td>\n",
              "      <td>100041000</td>\n",
              "      <td>9214</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1385-12-21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2007-03-13</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>249241504527</td>\n",
              "      <td>126270939</td>\n",
              "      <td>5862</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1385-12-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2007-03-14</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>51666379451</td>\n",
              "      <td>26705128</td>\n",
              "      <td>1901</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1385-12-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2007-03-17</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>28239006789</td>\n",
              "      <td>14877283</td>\n",
              "      <td>1514</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1385-12-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3715</th>\n",
              "      <td>2023-12-30</td>\n",
              "      <td>6280.0</td>\n",
              "      <td>6330.0</td>\n",
              "      <td>6250.0</td>\n",
              "      <td>6280.0</td>\n",
              "      <td>413007029830</td>\n",
              "      <td>65778615</td>\n",
              "      <td>3774</td>\n",
              "      <td>6220.0</td>\n",
              "      <td>6280.0</td>\n",
              "      <td>1402-10-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3716</th>\n",
              "      <td>2023-12-31</td>\n",
              "      <td>6300.0</td>\n",
              "      <td>6300.0</td>\n",
              "      <td>6180.0</td>\n",
              "      <td>6230.0</td>\n",
              "      <td>498910921170</td>\n",
              "      <td>80066614</td>\n",
              "      <td>3884</td>\n",
              "      <td>6280.0</td>\n",
              "      <td>6190.0</td>\n",
              "      <td>1402-10-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3717</th>\n",
              "      <td>2024-01-01</td>\n",
              "      <td>6230.0</td>\n",
              "      <td>6230.0</td>\n",
              "      <td>6150.0</td>\n",
              "      <td>6200.0</td>\n",
              "      <td>417831898480</td>\n",
              "      <td>67436187</td>\n",
              "      <td>3320</td>\n",
              "      <td>6230.0</td>\n",
              "      <td>6180.0</td>\n",
              "      <td>1402-10-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3718</th>\n",
              "      <td>2024-01-02</td>\n",
              "      <td>6180.0</td>\n",
              "      <td>6220.0</td>\n",
              "      <td>6100.0</td>\n",
              "      <td>6160.0</td>\n",
              "      <td>446092828520</td>\n",
              "      <td>72415054</td>\n",
              "      <td>3753</td>\n",
              "      <td>6200.0</td>\n",
              "      <td>6150.0</td>\n",
              "      <td>1402-10-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3719</th>\n",
              "      <td>2024-01-03</td>\n",
              "      <td>6180.0</td>\n",
              "      <td>6230.0</td>\n",
              "      <td>6100.0</td>\n",
              "      <td>6160.0</td>\n",
              "      <td>510237384690</td>\n",
              "      <td>82800015</td>\n",
              "      <td>3654</td>\n",
              "      <td>6160.0</td>\n",
              "      <td>6190.0</td>\n",
              "      <td>1402-10-13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3720 rows × 11 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d55d1a20-4186-4edb-88a9-263dd678f550')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d55d1a20-4186-4edb-88a9-263dd678f550 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d55d1a20-4186-4edb-88a9-263dd678f550');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-afff112a-263c-4ae1-977c-aaa4ffc31b47\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-afff112a-263c-4ae1-977c-aaa4ffc31b47')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-afff112a-263c-4ae1-977c-aaa4ffc31b47 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Merge 3 data with Common Dates"
      ],
      "metadata": {
        "id": "vFcB5UXF9yzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the common dates\n",
        "common_dates = np.intersect1d(np.intersect1d(Foolad['jdate'].values, All['jdate'].values), HamVazn['jdate'].values)\n",
        "\n",
        "# Create empty matrices to store the merged data\n",
        "merged_Foolad = np.zeros(len(common_dates))\n",
        "merged_All = np.zeros(len(common_dates))\n",
        "merged_HamVazn = np.zeros(len(common_dates))\n",
        "\n",
        "# Find the indices of common dates in each matrix\n",
        "indices_Foolad = np.where(np.isin(Foolad['jdate'].values, common_dates))[0]\n",
        "indices_All = np.where(np.isin(All['jdate'].values, common_dates))[0]\n",
        "indices_HamVazn = np.where(np.isin(HamVazn['jdate'].values, common_dates))[0]\n",
        "\n",
        "mat1 = Foolad['adjClose'].values\n",
        "mat2 = All['close'].values\n",
        "mat3 = HamVazn['close'].values\n",
        "\n",
        "merged_Foolad = mat1[indices_Foolad]\n",
        "merged_All = mat2[indices_All]\n",
        "merged_HamVazn = mat3[indices_HamVazn]\n",
        "\n",
        "labels = np.zeros(2006)\n",
        "labels = np.where(merged_Foolad[1:] >= merged_Foolad[:-1], 1, 0)\n",
        "\n",
        "\n",
        "# concatenate them\n",
        "data = np.vstack((merged_Foolad[1:],merged_All[1:],merged_HamVazn[1:],common_dates[1:],labels[:]))\n",
        "\n",
        "print(data.shape)\n",
        "data"
      ],
      "metadata": {
        "id": "o1h_qh3CBYxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b26949-04ad-4775-a17d-69ad5139364d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 2006)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[70.0, 68.0, 68.0, ..., 6200.0, 6160.0, 6160.0],\n",
              "       [64526, 64052, 63951, ..., 2168327, 2158766, 2155235],\n",
              "       [9893, 9815, 9794, ..., 757994, 755945, 756758],\n",
              "       ['1393-12-05', '1393-12-06', '1393-12-09', ..., '1402-10-11',\n",
              "        '1402-10-12', '1402-10-13'],\n",
              "       [0, 0, 1, ..., 0, 0, 1]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x_values = np.arange(len(merged_Foolad))\n",
        "\n",
        "# Plotting the array\n",
        "plt.plot(x_values, merged_Foolad)\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Values')\n",
        "plt.title('ّFoolad Price Changes')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWbWJmub7rJB",
        "outputId": "8ac41dab-9c82-4bd6-8e5d-7f52b7b5ba78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h2>data[0,:] : Foolad<h2>\n",
        "data[1,:] : Shakhese Kol <h2>\n",
        "data[2,:] : Shakhese Ham Vazn <h2>\n",
        "data[3,:] : Common Dates <h2>\n",
        "data[4,:] : Labels <h2>\n"
      ],
      "metadata": {
        "id": "-Bqgbg8lKndT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Normalize Features"
      ],
      "metadata": {
        "id": "xoEI9n-BMJg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.T\n",
        "features = data[:, :3].astype(np.float64)  # Convert features to float64\n",
        "labels = data[:, 4].astype(np.int64)  # Convert labels to int64 for classification\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features = scaler.fit_transform(features)\n"
      ],
      "metadata": {
        "id": "9DDOWRNgMN9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Create data windows"
      ],
      "metadata": {
        "id": "EwAS7rX4MjIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_windows(data, window_size):\n",
        "    windows = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        window = data[i:i+window_size, :3]  # Extract only the first 3 columns (features)\n",
        "        target = data[i+window_size, -1]  # Extract the label from the last column\n",
        "        windows.append(window)\n",
        "        targets.append(target)\n",
        "    return torch.tensor(windows), torch.tensor(targets, dtype=torch.long)  # Use dtype=torch.long for classification labels\n"
      ],
      "metadata": {
        "id": "PKEEoe82MzDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Split data and adjust window size"
      ],
      "metadata": {
        "id": "HMCHvwUVM7zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 5\n",
        "X, y = create_windows(np.hstack((features, labels.reshape(-1, 1))), window_size)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_size = int(len(X) * 0.80)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ],
      "metadata": {
        "id": "GfckU7j-NFxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Define RNN Model"
      ],
      "metadata": {
        "id": "L2IRXvtoRpZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "tf9VcXvZRowM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 3  # 3 features\n",
        "hidden_size = 50\n",
        "output_size = 2  # Two classes\n",
        "model = RNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "p7oXk6QgR4jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training"
      ],
      "metadata": {
        "id": "nT14eQRNR80e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    outputs = model(X_train.float())\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        loss.item()\n",
        "        ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMtZNnlkcv6_",
        "outputId": "4e03bbbd-cdec-4aa4-fc75-ba60f0a15828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.684849\n",
            "Epoch: 2 \tTraining Loss: 0.683265\n",
            "Epoch: 3 \tTraining Loss: 0.681716\n",
            "Epoch: 4 \tTraining Loss: 0.680200\n",
            "Epoch: 5 \tTraining Loss: 0.678717\n",
            "Epoch: 6 \tTraining Loss: 0.677266\n",
            "Epoch: 7 \tTraining Loss: 0.675849\n",
            "Epoch: 8 \tTraining Loss: 0.674468\n",
            "Epoch: 9 \tTraining Loss: 0.673125\n",
            "Epoch: 10 \tTraining Loss: 0.671825\n",
            "Epoch: 11 \tTraining Loss: 0.670572\n",
            "Epoch: 12 \tTraining Loss: 0.669374\n",
            "Epoch: 13 \tTraining Loss: 0.668238\n",
            "Epoch: 14 \tTraining Loss: 0.667173\n",
            "Epoch: 15 \tTraining Loss: 0.666190\n",
            "Epoch: 16 \tTraining Loss: 0.665296\n",
            "Epoch: 17 \tTraining Loss: 0.664502\n",
            "Epoch: 18 \tTraining Loss: 0.663814\n",
            "Epoch: 19 \tTraining Loss: 0.663235\n",
            "Epoch: 20 \tTraining Loss: 0.662759\n",
            "Epoch: 21 \tTraining Loss: 0.662376\n",
            "Epoch: 22 \tTraining Loss: 0.662062\n",
            "Epoch: 23 \tTraining Loss: 0.661789\n",
            "Epoch: 24 \tTraining Loss: 0.661524\n",
            "Epoch: 25 \tTraining Loss: 0.661239\n",
            "Epoch: 26 \tTraining Loss: 0.660912\n",
            "Epoch: 27 \tTraining Loss: 0.660535\n",
            "Epoch: 28 \tTraining Loss: 0.660111\n",
            "Epoch: 29 \tTraining Loss: 0.659652\n",
            "Epoch: 30 \tTraining Loss: 0.659176\n",
            "Epoch: 31 \tTraining Loss: 0.658700\n",
            "Epoch: 32 \tTraining Loss: 0.658242\n",
            "Epoch: 33 \tTraining Loss: 0.657811\n",
            "Epoch: 34 \tTraining Loss: 0.657415\n",
            "Epoch: 35 \tTraining Loss: 0.657054\n",
            "Epoch: 36 \tTraining Loss: 0.656724\n",
            "Epoch: 37 \tTraining Loss: 0.656421\n",
            "Epoch: 38 \tTraining Loss: 0.656136\n",
            "Epoch: 39 \tTraining Loss: 0.655862\n",
            "Epoch: 40 \tTraining Loss: 0.655593\n",
            "Epoch: 41 \tTraining Loss: 0.655326\n",
            "Epoch: 42 \tTraining Loss: 0.655060\n",
            "Epoch: 43 \tTraining Loss: 0.654796\n",
            "Epoch: 44 \tTraining Loss: 0.654537\n",
            "Epoch: 45 \tTraining Loss: 0.654292\n",
            "Epoch: 46 \tTraining Loss: 0.654065\n",
            "Epoch: 47 \tTraining Loss: 0.653863\n",
            "Epoch: 48 \tTraining Loss: 0.653692\n",
            "Epoch: 49 \tTraining Loss: 0.653554\n",
            "Epoch: 50 \tTraining Loss: 0.653446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Test"
      ],
      "metadata": {
        "id": "MROj3PHKSHAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test.float())\n",
        "    predicted_labels = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "predicted_labels = np.where(predicted_labels == 0 ,-1,1)\n",
        "y_test = np.where(y_test == 0 ,-1,1)\n",
        "\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "id": "0eeOsJXfSHAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d6b728-e503-4775-e60d-bc97e12fb408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1\n",
            "  1  1  1 -1 -1 -1 -1 -1 -1 -1  1  1  1 -1 -1 -1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# a = np.array(*predicted_labels)\n",
        "test_accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f'Test Accuracy: {100 * test_accuracy:.2f}%')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
        "import seaborn as sns\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sglvjRmE9HxZ",
        "outputId": "d864ab9a-7d76-4187-f15d-5335cee2428f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 52.12%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict with CNN"
      ],
      "metadata": {
        "id": "_6L-agtZYO9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Split data and adjust window size"
      ],
      "metadata": {
        "id": "zJVkNSqcd09r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 10\n",
        "X, y = create_windows(np.hstack((features, labels.reshape(-1, 1))), window_size)\n",
        "\n",
        "# Reshape data for CNN\n",
        "X = X.unsqueeze(1)\n",
        "\n",
        "train_size = int(len(X) * 0.80)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ],
      "metadata": {
        "id": "bWjDPsF8eDXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define CNN Model"
      ],
      "metadata": {
        "id": "LUacQMhNd09s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channels, output_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "coQ1YQXEeMbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_channels = 1\n",
        "output_size = 2\n",
        "model = CNN(input_channels, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "lJlhisIVxQLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ],
      "metadata": {
        "id": "V9IZaKTdd09t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    outputs = model(X_train.float())\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        loss.item()\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5191e2-96ad-48a0-c4bc-b5711f52a466",
        "id": "kqMGpf5qd09t"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.687024\n",
            "Epoch: 2 \tTraining Loss: 0.682560\n",
            "Epoch: 3 \tTraining Loss: 0.679183\n",
            "Epoch: 4 \tTraining Loss: 0.676332\n",
            "Epoch: 5 \tTraining Loss: 0.673855\n",
            "Epoch: 6 \tTraining Loss: 0.671560\n",
            "Epoch: 7 \tTraining Loss: 0.669318\n",
            "Epoch: 8 \tTraining Loss: 0.667153\n",
            "Epoch: 9 \tTraining Loss: 0.665072\n",
            "Epoch: 10 \tTraining Loss: 0.663038\n",
            "Epoch: 11 \tTraining Loss: 0.660944\n",
            "Epoch: 12 \tTraining Loss: 0.658765\n",
            "Epoch: 13 \tTraining Loss: 0.656714\n",
            "Epoch: 14 \tTraining Loss: 0.655112\n",
            "Epoch: 15 \tTraining Loss: 0.654093\n",
            "Epoch: 16 \tTraining Loss: 0.653809\n",
            "Epoch: 17 \tTraining Loss: 0.654118\n",
            "Epoch: 18 \tTraining Loss: 0.654771\n",
            "Epoch: 19 \tTraining Loss: 0.655420\n",
            "Epoch: 20 \tTraining Loss: 0.655726\n",
            "Epoch: 21 \tTraining Loss: 0.655557\n",
            "Epoch: 22 \tTraining Loss: 0.655034\n",
            "Epoch: 23 \tTraining Loss: 0.654408\n",
            "Epoch: 24 \tTraining Loss: 0.653862\n",
            "Epoch: 25 \tTraining Loss: 0.653466\n",
            "Epoch: 26 \tTraining Loss: 0.653253\n",
            "Epoch: 27 \tTraining Loss: 0.653216\n",
            "Epoch: 28 \tTraining Loss: 0.653310\n",
            "Epoch: 29 \tTraining Loss: 0.653437\n",
            "Epoch: 30 \tTraining Loss: 0.653495\n",
            "Epoch: 31 \tTraining Loss: 0.653452\n",
            "Epoch: 32 \tTraining Loss: 0.653324\n",
            "Epoch: 33 \tTraining Loss: 0.653124\n",
            "Epoch: 34 \tTraining Loss: 0.652858\n",
            "Epoch: 35 \tTraining Loss: 0.652621\n",
            "Epoch: 36 \tTraining Loss: 0.652421\n",
            "Epoch: 37 \tTraining Loss: 0.652297\n",
            "Epoch: 38 \tTraining Loss: 0.652238\n",
            "Epoch: 39 \tTraining Loss: 0.652203\n",
            "Epoch: 40 \tTraining Loss: 0.652153\n",
            "Epoch: 41 \tTraining Loss: 0.652075\n",
            "Epoch: 42 \tTraining Loss: 0.651953\n",
            "Epoch: 43 \tTraining Loss: 0.651784\n",
            "Epoch: 44 \tTraining Loss: 0.651601\n",
            "Epoch: 45 \tTraining Loss: 0.651436\n",
            "Epoch: 46 \tTraining Loss: 0.651291\n",
            "Epoch: 47 \tTraining Loss: 0.651159\n",
            "Epoch: 48 \tTraining Loss: 0.651040\n",
            "Epoch: 49 \tTraining Loss: 0.650923\n",
            "Epoch: 50 \tTraining Loss: 0.650785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Test"
      ],
      "metadata": {
        "id": "PRJHOvbAd09t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test.float())\n",
        "    predicted_labels = torch.argmax(test_outputs, dim=1).numpy()\n",
        "predicted_labels = np.where(predicted_labels == 0 ,-1,1)\n",
        "y_test = np.where(y_test == 0 ,-1,1)\n",
        "\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "768c7459-3079-4644-c8da-3198312a63b9",
        "id": "BQ_zoXzDd09t"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# a = np.array(*predicted_labels)\n",
        "test_accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f'Test Accuracy: {100 * test_accuracy:.2f}%')\n",
        "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
        "import seaborn as sns\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "outputId": "9ad21c0d-ba16-41c1-e7aa-659d7d55d897",
        "id": "2FXXGon-d09t"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 53.25%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With Indicator"
      ],
      "metadata": {
        "id": "V-byM0CffWtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## without use شاخص کل & شاخص کل(هم وزن)"
      ],
      "metadata": {
        "id": "PyrowQNRyo3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Add indicators to Features"
      ],
      "metadata": {
        "id": "7UnI-c3Qyo3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas-ta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d240fafa-d041-4b66-a417-c2cbedb3530b",
        "id": "GOnoGjVcyo3P"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas-ta in /usr/local/lib/python3.10/dist-packages (0.3.14b0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pandas-ta) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->pandas-ta) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->pandas-ta) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = op.join('/content/drive/My Drive/','DL','DL_HW05','فولاد-ت.csv')  # Path to the Data folder\n",
        "df = pd.read_csv(dir)\n",
        "aroon = df.ta.aroon(inplace=True)\n",
        "aroon = aroon['AROONU_14'].values[indices_Foolad].astype(np.float64)\n",
        "macd = df.ta.macd(inplace=True)\n",
        "macd = macd['MACD_12_26_9'].values[indices_Foolad].astype(np.float64)\n",
        "rsi = df.ta.rsi(inplace=True)\n",
        "rsi = rsi[indices_Foolad].astype(np.float64)\n",
        "\n",
        "adjClose = df['adjClose'].values[indices_Foolad].astype(np.float64)\n",
        "\n",
        "features = np.vstack((adjClose[1:],aroon[1:],macd[1:],rsi[1:]))\n",
        "\n",
        "labels = np.zeros(2006)\n",
        "labels = np.where(merged_Foolad[1:] >= merged_Foolad[:-1], 1, 0)"
      ],
      "metadata": {
        "id": "9n3yFcFCyo3P"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Split data and adjust window size"
      ],
      "metadata": {
        "id": "geJlMRc8yo3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 5\n",
        "X, y = create_windows(np.hstack((features.T, labels.reshape(-1, 1))), window_size)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_size = int(len(X) * 0.80)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ],
      "metadata": {
        "id": "q_LvGxaoyo3P"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define RNN Model"
      ],
      "metadata": {
        "id": "P3EgEE_byo3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "KkcqEjGayo3Q"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 4  # 4 features\n",
        "hidden_size = 50\n",
        "output_size = 2  # Two classes\n",
        "model = RNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)"
      ],
      "metadata": {
        "id": "MfN3cG3lyo3Q"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Training"
      ],
      "metadata": {
        "id": "D7nugXpeyo3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    outputs = model(X_train.float())\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        loss.item()\n",
        "        ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc525e44-4e97-4526-fd14-ae2162a573de",
        "id": "sGAf61Epyo3Q"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.736806\n",
            "Epoch: 2 \tTraining Loss: 0.729267\n",
            "Epoch: 3 \tTraining Loss: 0.722360\n",
            "Epoch: 4 \tTraining Loss: 0.715723\n",
            "Epoch: 5 \tTraining Loss: 0.709395\n",
            "Epoch: 6 \tTraining Loss: 0.703476\n",
            "Epoch: 7 \tTraining Loss: 0.697850\n",
            "Epoch: 8 \tTraining Loss: 0.692505\n",
            "Epoch: 9 \tTraining Loss: 0.687477\n",
            "Epoch: 10 \tTraining Loss: 0.682809\n",
            "Epoch: 11 \tTraining Loss: 0.678426\n",
            "Epoch: 12 \tTraining Loss: 0.674473\n",
            "Epoch: 13 \tTraining Loss: 0.670813\n",
            "Epoch: 14 \tTraining Loss: 0.667597\n",
            "Epoch: 15 \tTraining Loss: 0.664583\n",
            "Epoch: 16 \tTraining Loss: 0.661750\n",
            "Epoch: 17 \tTraining Loss: 0.659149\n",
            "Epoch: 18 \tTraining Loss: 0.656753\n",
            "Epoch: 19 \tTraining Loss: 0.654613\n",
            "Epoch: 20 \tTraining Loss: 0.652773\n",
            "Epoch: 21 \tTraining Loss: 0.651262\n",
            "Epoch: 22 \tTraining Loss: 0.650039\n",
            "Epoch: 23 \tTraining Loss: 0.649042\n",
            "Epoch: 24 \tTraining Loss: 0.648295\n",
            "Epoch: 25 \tTraining Loss: 0.647794\n",
            "Epoch: 26 \tTraining Loss: 0.647521\n",
            "Epoch: 27 \tTraining Loss: 0.647398\n",
            "Epoch: 28 \tTraining Loss: 0.647382\n",
            "Epoch: 29 \tTraining Loss: 0.647427\n",
            "Epoch: 30 \tTraining Loss: 0.647435\n",
            "Epoch: 31 \tTraining Loss: 0.647326\n",
            "Epoch: 32 \tTraining Loss: 0.647301\n",
            "Epoch: 33 \tTraining Loss: 0.647139\n",
            "Epoch: 34 \tTraining Loss: 0.646974\n",
            "Epoch: 35 \tTraining Loss: 0.646805\n",
            "Epoch: 36 \tTraining Loss: 0.646581\n",
            "Epoch: 37 \tTraining Loss: 0.646320\n",
            "Epoch: 38 \tTraining Loss: 0.646022\n",
            "Epoch: 39 \tTraining Loss: 0.645716\n",
            "Epoch: 40 \tTraining Loss: 0.645386\n",
            "Epoch: 41 \tTraining Loss: 0.645067\n",
            "Epoch: 42 \tTraining Loss: 0.644751\n",
            "Epoch: 43 \tTraining Loss: 0.644396\n",
            "Epoch: 44 \tTraining Loss: 0.644021\n",
            "Epoch: 45 \tTraining Loss: 0.643665\n",
            "Epoch: 46 \tTraining Loss: 0.643305\n",
            "Epoch: 47 \tTraining Loss: 0.642931\n",
            "Epoch: 48 \tTraining Loss: 0.642559\n",
            "Epoch: 49 \tTraining Loss: 0.642201\n",
            "Epoch: 50 \tTraining Loss: 0.641835\n",
            "Epoch: 51 \tTraining Loss: 0.641457\n",
            "Epoch: 52 \tTraining Loss: 0.641084\n",
            "Epoch: 53 \tTraining Loss: 0.640775\n",
            "Epoch: 54 \tTraining Loss: 0.640488\n",
            "Epoch: 55 \tTraining Loss: 0.640225\n",
            "Epoch: 56 \tTraining Loss: 0.640091\n",
            "Epoch: 57 \tTraining Loss: 0.639873\n",
            "Epoch: 58 \tTraining Loss: 0.639619\n",
            "Epoch: 59 \tTraining Loss: 0.639370\n",
            "Epoch: 60 \tTraining Loss: 0.639196\n",
            "Epoch: 61 \tTraining Loss: 0.638959\n",
            "Epoch: 62 \tTraining Loss: 0.638701\n",
            "Epoch: 63 \tTraining Loss: 0.638486\n",
            "Epoch: 64 \tTraining Loss: 0.638359\n",
            "Epoch: 65 \tTraining Loss: 0.638191\n",
            "Epoch: 66 \tTraining Loss: 0.637977\n",
            "Epoch: 67 \tTraining Loss: 0.637791\n",
            "Epoch: 68 \tTraining Loss: 0.637531\n",
            "Epoch: 69 \tTraining Loss: 0.637291\n",
            "Epoch: 70 \tTraining Loss: 0.637195\n",
            "Epoch: 71 \tTraining Loss: 0.637096\n",
            "Epoch: 72 \tTraining Loss: 0.636961\n",
            "Epoch: 73 \tTraining Loss: 0.636801\n",
            "Epoch: 74 \tTraining Loss: 0.636700\n",
            "Epoch: 75 \tTraining Loss: 0.636569\n",
            "Epoch: 76 \tTraining Loss: 0.636417\n",
            "Epoch: 77 \tTraining Loss: 0.636358\n",
            "Epoch: 78 \tTraining Loss: 0.636274\n",
            "Epoch: 79 \tTraining Loss: 0.636214\n",
            "Epoch: 80 \tTraining Loss: 0.636101\n",
            "Epoch: 81 \tTraining Loss: 0.635946\n",
            "Epoch: 82 \tTraining Loss: 0.635812\n",
            "Epoch: 83 \tTraining Loss: 0.635782\n",
            "Epoch: 84 \tTraining Loss: 0.635694\n",
            "Epoch: 85 \tTraining Loss: 0.635568\n",
            "Epoch: 86 \tTraining Loss: 0.635452\n",
            "Epoch: 87 \tTraining Loss: 0.635327\n",
            "Epoch: 88 \tTraining Loss: 0.635224\n",
            "Epoch: 89 \tTraining Loss: 0.635128\n",
            "Epoch: 90 \tTraining Loss: 0.635031\n",
            "Epoch: 91 \tTraining Loss: 0.634911\n",
            "Epoch: 92 \tTraining Loss: 0.634784\n",
            "Epoch: 93 \tTraining Loss: 0.634689\n",
            "Epoch: 94 \tTraining Loss: 0.634572\n",
            "Epoch: 95 \tTraining Loss: 0.634499\n",
            "Epoch: 96 \tTraining Loss: 0.634400\n",
            "Epoch: 97 \tTraining Loss: 0.634295\n",
            "Epoch: 98 \tTraining Loss: 0.634186\n",
            "Epoch: 99 \tTraining Loss: 0.634101\n",
            "Epoch: 100 \tTraining Loss: 0.634001\n",
            "Epoch: 101 \tTraining Loss: 0.633872\n",
            "Epoch: 102 \tTraining Loss: 0.633723\n",
            "Epoch: 103 \tTraining Loss: 0.633648\n",
            "Epoch: 104 \tTraining Loss: 0.633603\n",
            "Epoch: 105 \tTraining Loss: 0.633487\n",
            "Epoch: 106 \tTraining Loss: 0.633355\n",
            "Epoch: 107 \tTraining Loss: 0.633269\n",
            "Epoch: 108 \tTraining Loss: 0.633193\n",
            "Epoch: 109 \tTraining Loss: 0.633083\n",
            "Epoch: 110 \tTraining Loss: 0.633003\n",
            "Epoch: 111 \tTraining Loss: 0.632842\n",
            "Epoch: 112 \tTraining Loss: 0.632765\n",
            "Epoch: 113 \tTraining Loss: 0.632662\n",
            "Epoch: 114 \tTraining Loss: 0.632538\n",
            "Epoch: 115 \tTraining Loss: 0.632442\n",
            "Epoch: 116 \tTraining Loss: 0.632350\n",
            "Epoch: 117 \tTraining Loss: 0.632243\n",
            "Epoch: 118 \tTraining Loss: 0.632126\n",
            "Epoch: 119 \tTraining Loss: 0.632015\n",
            "Epoch: 120 \tTraining Loss: 0.631905\n",
            "Epoch: 121 \tTraining Loss: 0.631791\n",
            "Epoch: 122 \tTraining Loss: 0.631673\n",
            "Epoch: 123 \tTraining Loss: 0.631554\n",
            "Epoch: 124 \tTraining Loss: 0.631431\n",
            "Epoch: 125 \tTraining Loss: 0.631300\n",
            "Epoch: 126 \tTraining Loss: 0.631182\n",
            "Epoch: 127 \tTraining Loss: 0.631057\n",
            "Epoch: 128 \tTraining Loss: 0.630933\n",
            "Epoch: 129 \tTraining Loss: 0.630829\n",
            "Epoch: 130 \tTraining Loss: 0.630706\n",
            "Epoch: 131 \tTraining Loss: 0.630574\n",
            "Epoch: 132 \tTraining Loss: 0.630422\n",
            "Epoch: 133 \tTraining Loss: 0.630260\n",
            "Epoch: 134 \tTraining Loss: 0.630094\n",
            "Epoch: 135 \tTraining Loss: 0.629924\n",
            "Epoch: 136 \tTraining Loss: 0.629784\n",
            "Epoch: 137 \tTraining Loss: 0.629660\n",
            "Epoch: 138 \tTraining Loss: 0.629526\n",
            "Epoch: 139 \tTraining Loss: 0.629398\n",
            "Epoch: 140 \tTraining Loss: 0.629265\n",
            "Epoch: 141 \tTraining Loss: 0.629124\n",
            "Epoch: 142 \tTraining Loss: 0.628984\n",
            "Epoch: 143 \tTraining Loss: 0.628859\n",
            "Epoch: 144 \tTraining Loss: 0.628734\n",
            "Epoch: 145 \tTraining Loss: 0.628606\n",
            "Epoch: 146 \tTraining Loss: 0.628476\n",
            "Epoch: 147 \tTraining Loss: 0.628348\n",
            "Epoch: 148 \tTraining Loss: 0.628211\n",
            "Epoch: 149 \tTraining Loss: 0.628077\n",
            "Epoch: 150 \tTraining Loss: 0.627939\n",
            "Epoch: 151 \tTraining Loss: 0.627796\n",
            "Epoch: 152 \tTraining Loss: 0.627646\n",
            "Epoch: 153 \tTraining Loss: 0.627492\n",
            "Epoch: 154 \tTraining Loss: 0.627335\n",
            "Epoch: 155 \tTraining Loss: 0.627149\n",
            "Epoch: 156 \tTraining Loss: 0.626962\n",
            "Epoch: 157 \tTraining Loss: 0.626829\n",
            "Epoch: 158 \tTraining Loss: 0.626670\n",
            "Epoch: 159 \tTraining Loss: 0.626526\n",
            "Epoch: 160 \tTraining Loss: 0.626377\n",
            "Epoch: 161 \tTraining Loss: 0.626230\n",
            "Epoch: 162 \tTraining Loss: 0.626091\n",
            "Epoch: 163 \tTraining Loss: 0.625944\n",
            "Epoch: 164 \tTraining Loss: 0.625796\n",
            "Epoch: 165 \tTraining Loss: 0.625644\n",
            "Epoch: 166 \tTraining Loss: 0.625497\n",
            "Epoch: 167 \tTraining Loss: 0.625354\n",
            "Epoch: 168 \tTraining Loss: 0.625208\n",
            "Epoch: 169 \tTraining Loss: 0.625041\n",
            "Epoch: 170 \tTraining Loss: 0.624888\n",
            "Epoch: 171 \tTraining Loss: 0.624746\n",
            "Epoch: 172 \tTraining Loss: 0.624574\n",
            "Epoch: 173 \tTraining Loss: 0.624416\n",
            "Epoch: 174 \tTraining Loss: 0.624270\n",
            "Epoch: 175 \tTraining Loss: 0.624104\n",
            "Epoch: 176 \tTraining Loss: 0.623937\n",
            "Epoch: 177 \tTraining Loss: 0.623777\n",
            "Epoch: 178 \tTraining Loss: 0.623616\n",
            "Epoch: 179 \tTraining Loss: 0.623451\n",
            "Epoch: 180 \tTraining Loss: 0.623276\n",
            "Epoch: 181 \tTraining Loss: 0.623058\n",
            "Epoch: 182 \tTraining Loss: 0.622648\n",
            "Epoch: 183 \tTraining Loss: 0.622915\n",
            "Epoch: 184 \tTraining Loss: 0.621811\n",
            "Epoch: 185 \tTraining Loss: 0.621892\n",
            "Epoch: 186 \tTraining Loss: 0.621601\n",
            "Epoch: 187 \tTraining Loss: 0.621763\n",
            "Epoch: 188 \tTraining Loss: 0.621638\n",
            "Epoch: 189 \tTraining Loss: 0.621569\n",
            "Epoch: 190 \tTraining Loss: 0.621382\n",
            "Epoch: 191 \tTraining Loss: 0.621121\n",
            "Epoch: 192 \tTraining Loss: 0.620877\n",
            "Epoch: 193 \tTraining Loss: 0.620552\n",
            "Epoch: 194 \tTraining Loss: 0.619988\n",
            "Epoch: 195 \tTraining Loss: 0.619803\n",
            "Epoch: 196 \tTraining Loss: 0.619952\n",
            "Epoch: 197 \tTraining Loss: 0.619834\n",
            "Epoch: 198 \tTraining Loss: 0.619650\n",
            "Epoch: 199 \tTraining Loss: 0.619444\n",
            "Epoch: 200 \tTraining Loss: 0.619239\n",
            "Epoch: 201 \tTraining Loss: 0.618709\n",
            "Epoch: 202 \tTraining Loss: 0.618912\n",
            "Epoch: 203 \tTraining Loss: 0.618131\n",
            "Epoch: 204 \tTraining Loss: 0.618053\n",
            "Epoch: 205 \tTraining Loss: 0.617880\n",
            "Epoch: 206 \tTraining Loss: 0.617620\n",
            "Epoch: 207 \tTraining Loss: 0.617658\n",
            "Epoch: 208 \tTraining Loss: 0.617539\n",
            "Epoch: 209 \tTraining Loss: 0.616893\n",
            "Epoch: 210 \tTraining Loss: 0.617070\n",
            "Epoch: 211 \tTraining Loss: 0.617003\n",
            "Epoch: 212 \tTraining Loss: 0.616170\n",
            "Epoch: 213 \tTraining Loss: 0.616686\n",
            "Epoch: 214 \tTraining Loss: 0.616697\n",
            "Epoch: 215 \tTraining Loss: 0.616499\n",
            "Epoch: 216 \tTraining Loss: 0.616329\n",
            "Epoch: 217 \tTraining Loss: 0.615615\n",
            "Epoch: 218 \tTraining Loss: 0.615203\n",
            "Epoch: 219 \tTraining Loss: 0.615442\n",
            "Epoch: 220 \tTraining Loss: 0.615325\n",
            "Epoch: 221 \tTraining Loss: 0.615191\n",
            "Epoch: 222 \tTraining Loss: 0.614966\n",
            "Epoch: 223 \tTraining Loss: 0.614720\n",
            "Epoch: 224 \tTraining Loss: 0.614509\n",
            "Epoch: 225 \tTraining Loss: 0.614221\n",
            "Epoch: 226 \tTraining Loss: 0.613751\n",
            "Epoch: 227 \tTraining Loss: 0.613335\n",
            "Epoch: 228 \tTraining Loss: 0.613272\n",
            "Epoch: 229 \tTraining Loss: 0.613210\n",
            "Epoch: 230 \tTraining Loss: 0.613562\n",
            "Epoch: 231 \tTraining Loss: 0.613796\n",
            "Epoch: 232 \tTraining Loss: 0.613752\n",
            "Epoch: 233 \tTraining Loss: 0.613486\n",
            "Epoch: 234 \tTraining Loss: 0.613226\n",
            "Epoch: 235 \tTraining Loss: 0.612886\n",
            "Epoch: 236 \tTraining Loss: 0.612433\n",
            "Epoch: 237 \tTraining Loss: 0.611671\n",
            "Epoch: 238 \tTraining Loss: 0.610988\n",
            "Epoch: 239 \tTraining Loss: 0.610815\n",
            "Epoch: 240 \tTraining Loss: 0.610558\n",
            "Epoch: 241 \tTraining Loss: 0.610719\n",
            "Epoch: 242 \tTraining Loss: 0.610438\n",
            "Epoch: 243 \tTraining Loss: 0.610199\n",
            "Epoch: 244 \tTraining Loss: 0.609709\n",
            "Epoch: 245 \tTraining Loss: 0.609180\n",
            "Epoch: 246 \tTraining Loss: 0.609133\n",
            "Epoch: 247 \tTraining Loss: 0.608739\n",
            "Epoch: 248 \tTraining Loss: 0.608596\n",
            "Epoch: 249 \tTraining Loss: 0.608369\n",
            "Epoch: 250 \tTraining Loss: 0.608086\n",
            "Epoch: 251 \tTraining Loss: 0.607911\n",
            "Epoch: 252 \tTraining Loss: 0.607669\n",
            "Epoch: 253 \tTraining Loss: 0.607400\n",
            "Epoch: 254 \tTraining Loss: 0.607174\n",
            "Epoch: 255 \tTraining Loss: 0.606908\n",
            "Epoch: 256 \tTraining Loss: 0.606597\n",
            "Epoch: 257 \tTraining Loss: 0.606332\n",
            "Epoch: 258 \tTraining Loss: 0.606090\n",
            "Epoch: 259 \tTraining Loss: 0.605844\n",
            "Epoch: 260 \tTraining Loss: 0.605580\n",
            "Epoch: 261 \tTraining Loss: 0.605169\n",
            "Epoch: 262 \tTraining Loss: 0.605044\n",
            "Epoch: 263 \tTraining Loss: 0.604894\n",
            "Epoch: 264 \tTraining Loss: 0.604694\n",
            "Epoch: 265 \tTraining Loss: 0.604319\n",
            "Epoch: 266 \tTraining Loss: 0.604005\n",
            "Epoch: 267 \tTraining Loss: 0.603774\n",
            "Epoch: 268 \tTraining Loss: 0.603434\n",
            "Epoch: 269 \tTraining Loss: 0.603224\n",
            "Epoch: 270 \tTraining Loss: 0.602968\n",
            "Epoch: 271 \tTraining Loss: 0.602452\n",
            "Epoch: 272 \tTraining Loss: 0.602373\n",
            "Epoch: 273 \tTraining Loss: 0.602136\n",
            "Epoch: 274 \tTraining Loss: 0.601611\n",
            "Epoch: 275 \tTraining Loss: 0.601415\n",
            "Epoch: 276 \tTraining Loss: 0.601194\n",
            "Epoch: 277 \tTraining Loss: 0.600815\n",
            "Epoch: 278 \tTraining Loss: 0.600511\n",
            "Epoch: 279 \tTraining Loss: 0.600243\n",
            "Epoch: 280 \tTraining Loss: 0.599880\n",
            "Epoch: 281 \tTraining Loss: 0.599626\n",
            "Epoch: 282 \tTraining Loss: 0.599338\n",
            "Epoch: 283 \tTraining Loss: 0.599004\n",
            "Epoch: 284 \tTraining Loss: 0.598652\n",
            "Epoch: 285 \tTraining Loss: 0.598332\n",
            "Epoch: 286 \tTraining Loss: 0.598018\n",
            "Epoch: 287 \tTraining Loss: 0.597699\n",
            "Epoch: 288 \tTraining Loss: 0.597387\n",
            "Epoch: 289 \tTraining Loss: 0.597088\n",
            "Epoch: 290 \tTraining Loss: 0.596733\n",
            "Epoch: 291 \tTraining Loss: 0.596361\n",
            "Epoch: 292 \tTraining Loss: 0.595979\n",
            "Epoch: 293 \tTraining Loss: 0.595620\n",
            "Epoch: 294 \tTraining Loss: 0.595288\n",
            "Epoch: 295 \tTraining Loss: 0.594985\n",
            "Epoch: 296 \tTraining Loss: 0.594674\n",
            "Epoch: 297 \tTraining Loss: 0.594376\n",
            "Epoch: 298 \tTraining Loss: 0.594066\n",
            "Epoch: 299 \tTraining Loss: 0.593714\n",
            "Epoch: 300 \tTraining Loss: 0.593338\n",
            "Epoch: 301 \tTraining Loss: 0.592983\n",
            "Epoch: 302 \tTraining Loss: 0.592673\n",
            "Epoch: 303 \tTraining Loss: 0.592382\n",
            "Epoch: 304 \tTraining Loss: 0.592111\n",
            "Epoch: 305 \tTraining Loss: 0.591833\n",
            "Epoch: 306 \tTraining Loss: 0.591504\n",
            "Epoch: 307 \tTraining Loss: 0.591189\n",
            "Epoch: 308 \tTraining Loss: 0.590849\n",
            "Epoch: 309 \tTraining Loss: 0.590459\n",
            "Epoch: 310 \tTraining Loss: 0.590105\n",
            "Epoch: 311 \tTraining Loss: 0.589845\n",
            "Epoch: 312 \tTraining Loss: 0.589570\n",
            "Epoch: 313 \tTraining Loss: 0.589273\n",
            "Epoch: 314 \tTraining Loss: 0.589006\n",
            "Epoch: 315 \tTraining Loss: 0.588681\n",
            "Epoch: 316 \tTraining Loss: 0.588301\n",
            "Epoch: 317 \tTraining Loss: 0.587958\n",
            "Epoch: 318 \tTraining Loss: 0.587622\n",
            "Epoch: 319 \tTraining Loss: 0.587319\n",
            "Epoch: 320 \tTraining Loss: 0.587057\n",
            "Epoch: 321 \tTraining Loss: 0.586784\n",
            "Epoch: 322 \tTraining Loss: 0.586526\n",
            "Epoch: 323 \tTraining Loss: 0.586262\n",
            "Epoch: 324 \tTraining Loss: 0.585940\n",
            "Epoch: 325 \tTraining Loss: 0.585574\n",
            "Epoch: 326 \tTraining Loss: 0.585194\n",
            "Epoch: 327 \tTraining Loss: 0.584843\n",
            "Epoch: 328 \tTraining Loss: 0.584557\n",
            "Epoch: 329 \tTraining Loss: 0.584295\n",
            "Epoch: 330 \tTraining Loss: 0.584050\n",
            "Epoch: 331 \tTraining Loss: 0.583823\n",
            "Epoch: 332 \tTraining Loss: 0.583546\n",
            "Epoch: 333 \tTraining Loss: 0.583208\n",
            "Epoch: 334 \tTraining Loss: 0.582808\n",
            "Epoch: 335 \tTraining Loss: 0.582422\n",
            "Epoch: 336 \tTraining Loss: 0.582098\n",
            "Epoch: 337 \tTraining Loss: 0.581784\n",
            "Epoch: 338 \tTraining Loss: 0.581566\n",
            "Epoch: 339 \tTraining Loss: 0.581318\n",
            "Epoch: 340 \tTraining Loss: 0.581039\n",
            "Epoch: 341 \tTraining Loss: 0.580783\n",
            "Epoch: 342 \tTraining Loss: 0.580410\n",
            "Epoch: 343 \tTraining Loss: 0.579998\n",
            "Epoch: 344 \tTraining Loss: 0.579657\n",
            "Epoch: 345 \tTraining Loss: 0.579390\n",
            "Epoch: 346 \tTraining Loss: 0.579111\n",
            "Epoch: 347 \tTraining Loss: 0.578873\n",
            "Epoch: 348 \tTraining Loss: 0.578675\n",
            "Epoch: 349 \tTraining Loss: 0.578469\n",
            "Epoch: 350 \tTraining Loss: 0.578191\n",
            "Epoch: 351 \tTraining Loss: 0.577834\n",
            "Epoch: 352 \tTraining Loss: 0.577405\n",
            "Epoch: 353 \tTraining Loss: 0.577026\n",
            "Epoch: 354 \tTraining Loss: 0.576747\n",
            "Epoch: 355 \tTraining Loss: 0.576512\n",
            "Epoch: 356 \tTraining Loss: 0.576274\n",
            "Epoch: 357 \tTraining Loss: 0.576012\n",
            "Epoch: 358 \tTraining Loss: 0.575684\n",
            "Epoch: 359 \tTraining Loss: 0.575168\n",
            "Epoch: 360 \tTraining Loss: 0.575037\n",
            "Epoch: 361 \tTraining Loss: 0.574727\n",
            "Epoch: 362 \tTraining Loss: 0.574457\n",
            "Epoch: 363 \tTraining Loss: 0.574054\n",
            "Epoch: 364 \tTraining Loss: 0.574046\n",
            "Epoch: 365 \tTraining Loss: 0.573840\n",
            "Epoch: 366 \tTraining Loss: 0.573505\n",
            "Epoch: 367 \tTraining Loss: 0.573105\n",
            "Epoch: 368 \tTraining Loss: 0.572767\n",
            "Epoch: 369 \tTraining Loss: 0.572526\n",
            "Epoch: 370 \tTraining Loss: 0.572334\n",
            "Epoch: 371 \tTraining Loss: 0.572128\n",
            "Epoch: 372 \tTraining Loss: 0.571858\n",
            "Epoch: 373 \tTraining Loss: 0.571517\n",
            "Epoch: 374 \tTraining Loss: 0.571164\n",
            "Epoch: 375 \tTraining Loss: 0.570859\n",
            "Epoch: 376 \tTraining Loss: 0.570591\n",
            "Epoch: 377 \tTraining Loss: 0.570350\n",
            "Epoch: 378 \tTraining Loss: 0.570116\n",
            "Epoch: 379 \tTraining Loss: 0.569886\n",
            "Epoch: 380 \tTraining Loss: 0.569636\n",
            "Epoch: 381 \tTraining Loss: 0.569344\n",
            "Epoch: 382 \tTraining Loss: 0.569027\n",
            "Epoch: 383 \tTraining Loss: 0.568693\n",
            "Epoch: 384 \tTraining Loss: 0.568355\n",
            "Epoch: 385 \tTraining Loss: 0.568017\n",
            "Epoch: 386 \tTraining Loss: 0.567682\n",
            "Epoch: 387 \tTraining Loss: 0.567341\n",
            "Epoch: 388 \tTraining Loss: 0.566922\n",
            "Epoch: 389 \tTraining Loss: 0.566632\n",
            "Epoch: 390 \tTraining Loss: 0.566382\n",
            "Epoch: 391 \tTraining Loss: 0.566240\n",
            "Epoch: 392 \tTraining Loss: 0.566259\n",
            "Epoch: 393 \tTraining Loss: 0.566181\n",
            "Epoch: 394 \tTraining Loss: 0.565780\n",
            "Epoch: 395 \tTraining Loss: 0.565141\n",
            "Epoch: 396 \tTraining Loss: 0.564550\n",
            "Epoch: 397 \tTraining Loss: 0.564340\n",
            "Epoch: 398 \tTraining Loss: 0.564329\n",
            "Epoch: 399 \tTraining Loss: 0.564107\n",
            "Epoch: 400 \tTraining Loss: 0.563752\n",
            "Epoch: 401 \tTraining Loss: 0.563229\n",
            "Epoch: 402 \tTraining Loss: 0.562985\n",
            "Epoch: 403 \tTraining Loss: 0.562893\n",
            "Epoch: 404 \tTraining Loss: 0.562562\n",
            "Epoch: 405 \tTraining Loss: 0.562161\n",
            "Epoch: 406 \tTraining Loss: 0.561733\n",
            "Epoch: 407 \tTraining Loss: 0.561638\n",
            "Epoch: 408 \tTraining Loss: 0.561472\n",
            "Epoch: 409 \tTraining Loss: 0.561085\n",
            "Epoch: 410 \tTraining Loss: 0.560647\n",
            "Epoch: 411 \tTraining Loss: 0.560489\n",
            "Epoch: 412 \tTraining Loss: 0.560281\n",
            "Epoch: 413 \tTraining Loss: 0.560054\n",
            "Epoch: 414 \tTraining Loss: 0.559627\n",
            "Epoch: 415 \tTraining Loss: 0.559329\n",
            "Epoch: 416 \tTraining Loss: 0.559373\n",
            "Epoch: 417 \tTraining Loss: 0.558894\n",
            "Epoch: 418 \tTraining Loss: 0.559163\n",
            "Epoch: 419 \tTraining Loss: 0.558224\n",
            "Epoch: 420 \tTraining Loss: 0.558628\n",
            "Epoch: 421 \tTraining Loss: 0.558636\n",
            "Epoch: 422 \tTraining Loss: 0.558343\n",
            "Epoch: 423 \tTraining Loss: 0.558048\n",
            "Epoch: 424 \tTraining Loss: 0.557419\n",
            "Epoch: 425 \tTraining Loss: 0.556553\n",
            "Epoch: 426 \tTraining Loss: 0.557042\n",
            "Epoch: 427 \tTraining Loss: 0.557050\n",
            "Epoch: 428 \tTraining Loss: 0.556698\n",
            "Epoch: 429 \tTraining Loss: 0.556177\n",
            "Epoch: 430 \tTraining Loss: 0.555349\n",
            "Epoch: 431 \tTraining Loss: 0.555541\n",
            "Epoch: 432 \tTraining Loss: 0.555665\n",
            "Epoch: 433 \tTraining Loss: 0.555403\n",
            "Epoch: 434 \tTraining Loss: 0.555517\n",
            "Epoch: 435 \tTraining Loss: 0.554874\n",
            "Epoch: 436 \tTraining Loss: 0.554563\n",
            "Epoch: 437 \tTraining Loss: 0.554160\n",
            "Epoch: 438 \tTraining Loss: 0.553634\n",
            "Epoch: 439 \tTraining Loss: 0.552737\n",
            "Epoch: 440 \tTraining Loss: 0.553199\n",
            "Epoch: 441 \tTraining Loss: 0.553421\n",
            "Epoch: 442 \tTraining Loss: 0.553237\n",
            "Epoch: 443 \tTraining Loss: 0.552917\n",
            "Epoch: 444 \tTraining Loss: 0.552589\n",
            "Epoch: 445 \tTraining Loss: 0.552281\n",
            "Epoch: 446 \tTraining Loss: 0.551907\n",
            "Epoch: 447 \tTraining Loss: 0.551211\n",
            "Epoch: 448 \tTraining Loss: 0.550336\n",
            "Epoch: 449 \tTraining Loss: 0.550521\n",
            "Epoch: 450 \tTraining Loss: 0.550697\n",
            "Epoch: 451 \tTraining Loss: 0.550686\n",
            "Epoch: 452 \tTraining Loss: 0.550952\n",
            "Epoch: 453 \tTraining Loss: 0.549991\n",
            "Epoch: 454 \tTraining Loss: 0.549552\n",
            "Epoch: 455 \tTraining Loss: 0.548982\n",
            "Epoch: 456 \tTraining Loss: 0.548193\n",
            "Epoch: 457 \tTraining Loss: 0.548086\n",
            "Epoch: 458 \tTraining Loss: 0.548035\n",
            "Epoch: 459 \tTraining Loss: 0.547851\n",
            "Epoch: 460 \tTraining Loss: 0.547540\n",
            "Epoch: 461 \tTraining Loss: 0.547300\n",
            "Epoch: 462 \tTraining Loss: 0.547008\n",
            "Epoch: 463 \tTraining Loss: 0.546348\n",
            "Epoch: 464 \tTraining Loss: 0.546067\n",
            "Epoch: 465 \tTraining Loss: 0.546141\n",
            "Epoch: 466 \tTraining Loss: 0.545893\n",
            "Epoch: 467 \tTraining Loss: 0.545348\n",
            "Epoch: 468 \tTraining Loss: 0.544738\n",
            "Epoch: 469 \tTraining Loss: 0.544535\n",
            "Epoch: 470 \tTraining Loss: 0.543932\n",
            "Epoch: 471 \tTraining Loss: 0.543746\n",
            "Epoch: 472 \tTraining Loss: 0.543556\n",
            "Epoch: 473 \tTraining Loss: 0.543029\n",
            "Epoch: 474 \tTraining Loss: 0.542981\n",
            "Epoch: 475 \tTraining Loss: 0.542912\n",
            "Epoch: 476 \tTraining Loss: 0.542440\n",
            "Epoch: 477 \tTraining Loss: 0.542524\n",
            "Epoch: 478 \tTraining Loss: 0.542395\n",
            "Epoch: 479 \tTraining Loss: 0.542095\n",
            "Epoch: 480 \tTraining Loss: 0.541883\n",
            "Epoch: 481 \tTraining Loss: 0.541285\n",
            "Epoch: 482 \tTraining Loss: 0.540567\n",
            "Epoch: 483 \tTraining Loss: 0.540272\n",
            "Epoch: 484 \tTraining Loss: 0.540098\n",
            "Epoch: 485 \tTraining Loss: 0.539997\n",
            "Epoch: 486 \tTraining Loss: 0.539827\n",
            "Epoch: 487 \tTraining Loss: 0.539505\n",
            "Epoch: 488 \tTraining Loss: 0.539066\n",
            "Epoch: 489 \tTraining Loss: 0.538550\n",
            "Epoch: 490 \tTraining Loss: 0.538314\n",
            "Epoch: 491 \tTraining Loss: 0.538018\n",
            "Epoch: 492 \tTraining Loss: 0.537937\n",
            "Epoch: 493 \tTraining Loss: 0.537751\n",
            "Epoch: 494 \tTraining Loss: 0.537483\n",
            "Epoch: 495 \tTraining Loss: 0.537125\n",
            "Epoch: 496 \tTraining Loss: 0.536739\n",
            "Epoch: 497 \tTraining Loss: 0.536355\n",
            "Epoch: 498 \tTraining Loss: 0.536004\n",
            "Epoch: 499 \tTraining Loss: 0.535744\n",
            "Epoch: 500 \tTraining Loss: 0.535523\n",
            "Epoch: 501 \tTraining Loss: 0.535347\n",
            "Epoch: 502 \tTraining Loss: 0.535170\n",
            "Epoch: 503 \tTraining Loss: 0.535067\n",
            "Epoch: 504 \tTraining Loss: 0.534961\n",
            "Epoch: 505 \tTraining Loss: 0.534882\n",
            "Epoch: 506 \tTraining Loss: 0.534677\n",
            "Epoch: 507 \tTraining Loss: 0.534258\n",
            "Epoch: 508 \tTraining Loss: 0.533676\n",
            "Epoch: 509 \tTraining Loss: 0.533145\n",
            "Epoch: 510 \tTraining Loss: 0.532817\n",
            "Epoch: 511 \tTraining Loss: 0.532654\n",
            "Epoch: 512 \tTraining Loss: 0.532582\n",
            "Epoch: 513 \tTraining Loss: 0.532494\n",
            "Epoch: 514 \tTraining Loss: 0.532339\n",
            "Epoch: 515 \tTraining Loss: 0.531855\n",
            "Epoch: 516 \tTraining Loss: 0.531337\n",
            "Epoch: 517 \tTraining Loss: 0.531053\n",
            "Epoch: 518 \tTraining Loss: 0.530728\n",
            "Epoch: 519 \tTraining Loss: 0.530659\n",
            "Epoch: 520 \tTraining Loss: 0.530480\n",
            "Epoch: 521 \tTraining Loss: 0.530287\n",
            "Epoch: 522 \tTraining Loss: 0.530024\n",
            "Epoch: 523 \tTraining Loss: 0.529738\n",
            "Epoch: 524 \tTraining Loss: 0.529361\n",
            "Epoch: 525 \tTraining Loss: 0.529003\n",
            "Epoch: 526 \tTraining Loss: 0.528587\n",
            "Epoch: 527 \tTraining Loss: 0.528293\n",
            "Epoch: 528 \tTraining Loss: 0.528026\n",
            "Epoch: 529 \tTraining Loss: 0.527819\n",
            "Epoch: 530 \tTraining Loss: 0.527621\n",
            "Epoch: 531 \tTraining Loss: 0.527456\n",
            "Epoch: 532 \tTraining Loss: 0.527360\n",
            "Epoch: 533 \tTraining Loss: 0.527397\n",
            "Epoch: 534 \tTraining Loss: 0.527505\n",
            "Epoch: 535 \tTraining Loss: 0.527629\n",
            "Epoch: 536 \tTraining Loss: 0.527274\n",
            "Epoch: 537 \tTraining Loss: 0.526489\n",
            "Epoch: 538 \tTraining Loss: 0.525599\n",
            "Epoch: 539 \tTraining Loss: 0.525218\n",
            "Epoch: 540 \tTraining Loss: 0.525330\n",
            "Epoch: 541 \tTraining Loss: 0.525460\n",
            "Epoch: 542 \tTraining Loss: 0.525179\n",
            "Epoch: 543 \tTraining Loss: 0.524518\n",
            "Epoch: 544 \tTraining Loss: 0.523963\n",
            "Epoch: 545 \tTraining Loss: 0.523838\n",
            "Epoch: 546 \tTraining Loss: 0.523880\n",
            "Epoch: 547 \tTraining Loss: 0.523742\n",
            "Epoch: 548 \tTraining Loss: 0.523299\n",
            "Epoch: 549 \tTraining Loss: 0.522814\n",
            "Epoch: 550 \tTraining Loss: 0.522519\n",
            "Epoch: 551 \tTraining Loss: 0.522402\n",
            "Epoch: 552 \tTraining Loss: 0.522311\n",
            "Epoch: 553 \tTraining Loss: 0.522110\n",
            "Epoch: 554 \tTraining Loss: 0.521800\n",
            "Epoch: 555 \tTraining Loss: 0.521426\n",
            "Epoch: 556 \tTraining Loss: 0.521077\n",
            "Epoch: 557 \tTraining Loss: 0.520805\n",
            "Epoch: 558 \tTraining Loss: 0.520607\n",
            "Epoch: 559 \tTraining Loss: 0.520463\n",
            "Epoch: 560 \tTraining Loss: 0.520326\n",
            "Epoch: 561 \tTraining Loss: 0.520151\n",
            "Epoch: 562 \tTraining Loss: 0.519925\n",
            "Epoch: 563 \tTraining Loss: 0.519668\n",
            "Epoch: 564 \tTraining Loss: 0.519398\n",
            "Epoch: 565 \tTraining Loss: 0.519104\n",
            "Epoch: 566 \tTraining Loss: 0.518771\n",
            "Epoch: 567 \tTraining Loss: 0.518414\n",
            "Epoch: 568 \tTraining Loss: 0.518155\n",
            "Epoch: 569 \tTraining Loss: 0.517843\n",
            "Epoch: 570 \tTraining Loss: 0.517570\n",
            "Epoch: 571 \tTraining Loss: 0.517309\n",
            "Epoch: 572 \tTraining Loss: 0.517092\n",
            "Epoch: 573 \tTraining Loss: 0.516835\n",
            "Epoch: 574 \tTraining Loss: 0.516594\n",
            "Epoch: 575 \tTraining Loss: 0.516369\n",
            "Epoch: 576 \tTraining Loss: 0.516150\n",
            "Epoch: 577 \tTraining Loss: 0.516004\n",
            "Epoch: 578 \tTraining Loss: 0.516005\n",
            "Epoch: 579 \tTraining Loss: 0.516407\n",
            "Epoch: 580 \tTraining Loss: 0.517651\n",
            "Epoch: 581 \tTraining Loss: 0.519672\n",
            "Epoch: 582 \tTraining Loss: 0.519636\n",
            "Epoch: 583 \tTraining Loss: 0.516238\n",
            "Epoch: 584 \tTraining Loss: 0.514492\n",
            "Epoch: 585 \tTraining Loss: 0.516470\n",
            "Epoch: 586 \tTraining Loss: 0.516294\n",
            "Epoch: 587 \tTraining Loss: 0.514014\n",
            "Epoch: 588 \tTraining Loss: 0.514617\n",
            "Epoch: 589 \tTraining Loss: 0.515414\n",
            "Epoch: 590 \tTraining Loss: 0.514033\n",
            "Epoch: 591 \tTraining Loss: 0.513563\n",
            "Epoch: 592 \tTraining Loss: 0.514686\n",
            "Epoch: 593 \tTraining Loss: 0.512917\n",
            "Epoch: 594 \tTraining Loss: 0.513182\n",
            "Epoch: 595 \tTraining Loss: 0.513335\n",
            "Epoch: 596 \tTraining Loss: 0.512644\n",
            "Epoch: 597 \tTraining Loss: 0.512356\n",
            "Epoch: 598 \tTraining Loss: 0.512797\n",
            "Epoch: 599 \tTraining Loss: 0.512553\n",
            "Epoch: 600 \tTraining Loss: 0.512217\n",
            "Epoch: 601 \tTraining Loss: 0.511759\n",
            "Epoch: 602 \tTraining Loss: 0.511756\n",
            "Epoch: 603 \tTraining Loss: 0.511093\n",
            "Epoch: 604 \tTraining Loss: 0.511023\n",
            "Epoch: 605 \tTraining Loss: 0.510707\n",
            "Epoch: 606 \tTraining Loss: 0.509952\n",
            "Epoch: 607 \tTraining Loss: 0.510705\n",
            "Epoch: 608 \tTraining Loss: 0.509627\n",
            "Epoch: 609 \tTraining Loss: 0.509723\n",
            "Epoch: 610 \tTraining Loss: 0.509511\n",
            "Epoch: 611 \tTraining Loss: 0.509139\n",
            "Epoch: 612 \tTraining Loss: 0.508821\n",
            "Epoch: 613 \tTraining Loss: 0.508578\n",
            "Epoch: 614 \tTraining Loss: 0.508579\n",
            "Epoch: 615 \tTraining Loss: 0.508128\n",
            "Epoch: 616 \tTraining Loss: 0.508121\n",
            "Epoch: 617 \tTraining Loss: 0.507968\n",
            "Epoch: 618 \tTraining Loss: 0.507537\n",
            "Epoch: 619 \tTraining Loss: 0.507376\n",
            "Epoch: 620 \tTraining Loss: 0.507042\n",
            "Epoch: 621 \tTraining Loss: 0.506899\n",
            "Epoch: 622 \tTraining Loss: 0.506674\n",
            "Epoch: 623 \tTraining Loss: 0.506455\n",
            "Epoch: 624 \tTraining Loss: 0.506155\n",
            "Epoch: 625 \tTraining Loss: 0.505987\n",
            "Epoch: 626 \tTraining Loss: 0.505864\n",
            "Epoch: 627 \tTraining Loss: 0.505692\n",
            "Epoch: 628 \tTraining Loss: 0.505349\n",
            "Epoch: 629 \tTraining Loss: 0.505263\n",
            "Epoch: 630 \tTraining Loss: 0.505000\n",
            "Epoch: 631 \tTraining Loss: 0.504949\n",
            "Epoch: 632 \tTraining Loss: 0.504561\n",
            "Epoch: 633 \tTraining Loss: 0.504762\n",
            "Epoch: 634 \tTraining Loss: 0.504118\n",
            "Epoch: 635 \tTraining Loss: 0.504049\n",
            "Epoch: 636 \tTraining Loss: 0.503796\n",
            "Epoch: 637 \tTraining Loss: 0.503518\n",
            "Epoch: 638 \tTraining Loss: 0.503392\n",
            "Epoch: 639 \tTraining Loss: 0.503254\n",
            "Epoch: 640 \tTraining Loss: 0.503078\n",
            "Epoch: 641 \tTraining Loss: 0.502689\n",
            "Epoch: 642 \tTraining Loss: 0.502780\n",
            "Epoch: 643 \tTraining Loss: 0.502398\n",
            "Epoch: 644 \tTraining Loss: 0.502368\n",
            "Epoch: 645 \tTraining Loss: 0.501824\n",
            "Epoch: 646 \tTraining Loss: 0.502242\n",
            "Epoch: 647 \tTraining Loss: 0.501436\n",
            "Epoch: 648 \tTraining Loss: 0.501467\n",
            "Epoch: 649 \tTraining Loss: 0.501183\n",
            "Epoch: 650 \tTraining Loss: 0.500966\n",
            "Epoch: 651 \tTraining Loss: 0.500860\n",
            "Epoch: 652 \tTraining Loss: 0.500869\n",
            "Epoch: 653 \tTraining Loss: 0.501105\n",
            "Epoch: 654 \tTraining Loss: 0.501315\n",
            "Epoch: 655 \tTraining Loss: 0.502313\n",
            "Epoch: 656 \tTraining Loss: 0.502275\n",
            "Epoch: 657 \tTraining Loss: 0.502086\n",
            "Epoch: 658 \tTraining Loss: 0.500318\n",
            "Epoch: 659 \tTraining Loss: 0.499640\n",
            "Epoch: 660 \tTraining Loss: 0.499566\n",
            "Epoch: 661 \tTraining Loss: 0.499795\n",
            "Epoch: 662 \tTraining Loss: 0.499355\n",
            "Epoch: 663 \tTraining Loss: 0.499063\n",
            "Epoch: 664 \tTraining Loss: 0.498423\n",
            "Epoch: 665 \tTraining Loss: 0.498040\n",
            "Epoch: 666 \tTraining Loss: 0.498059\n",
            "Epoch: 667 \tTraining Loss: 0.498131\n",
            "Epoch: 668 \tTraining Loss: 0.497533\n",
            "Epoch: 669 \tTraining Loss: 0.496898\n",
            "Epoch: 670 \tTraining Loss: 0.497037\n",
            "Epoch: 671 \tTraining Loss: 0.497018\n",
            "Epoch: 672 \tTraining Loss: 0.496623\n",
            "Epoch: 673 \tTraining Loss: 0.496322\n",
            "Epoch: 674 \tTraining Loss: 0.496070\n",
            "Epoch: 675 \tTraining Loss: 0.495786\n",
            "Epoch: 676 \tTraining Loss: 0.495627\n",
            "Epoch: 677 \tTraining Loss: 0.495611\n",
            "Epoch: 678 \tTraining Loss: 0.495359\n",
            "Epoch: 679 \tTraining Loss: 0.494926\n",
            "Epoch: 680 \tTraining Loss: 0.494659\n",
            "Epoch: 681 \tTraining Loss: 0.494524\n",
            "Epoch: 682 \tTraining Loss: 0.494394\n",
            "Epoch: 683 \tTraining Loss: 0.494191\n",
            "Epoch: 684 \tTraining Loss: 0.493938\n",
            "Epoch: 685 \tTraining Loss: 0.493782\n",
            "Epoch: 686 \tTraining Loss: 0.493561\n",
            "Epoch: 687 \tTraining Loss: 0.493269\n",
            "Epoch: 688 \tTraining Loss: 0.493004\n",
            "Epoch: 689 \tTraining Loss: 0.492770\n",
            "Epoch: 690 \tTraining Loss: 0.492629\n",
            "Epoch: 691 \tTraining Loss: 0.492457\n",
            "Epoch: 692 \tTraining Loss: 0.492243\n",
            "Epoch: 693 \tTraining Loss: 0.492029\n",
            "Epoch: 694 \tTraining Loss: 0.491859\n",
            "Epoch: 695 \tTraining Loss: 0.491721\n",
            "Epoch: 696 \tTraining Loss: 0.491599\n",
            "Epoch: 697 \tTraining Loss: 0.491477\n",
            "Epoch: 698 \tTraining Loss: 0.491359\n",
            "Epoch: 699 \tTraining Loss: 0.491304\n",
            "Epoch: 700 \tTraining Loss: 0.491295\n",
            "Epoch: 701 \tTraining Loss: 0.491365\n",
            "Epoch: 702 \tTraining Loss: 0.491373\n",
            "Epoch: 703 \tTraining Loss: 0.491136\n",
            "Epoch: 704 \tTraining Loss: 0.490584\n",
            "Epoch: 705 \tTraining Loss: 0.490012\n",
            "Epoch: 706 \tTraining Loss: 0.489521\n",
            "Epoch: 707 \tTraining Loss: 0.489209\n",
            "Epoch: 708 \tTraining Loss: 0.489044\n",
            "Epoch: 709 \tTraining Loss: 0.489006\n",
            "Epoch: 710 \tTraining Loss: 0.489031\n",
            "Epoch: 711 \tTraining Loss: 0.489001\n",
            "Epoch: 712 \tTraining Loss: 0.488848\n",
            "Epoch: 713 \tTraining Loss: 0.488484\n",
            "Epoch: 714 \tTraining Loss: 0.488035\n",
            "Epoch: 715 \tTraining Loss: 0.487630\n",
            "Epoch: 716 \tTraining Loss: 0.487376\n",
            "Epoch: 717 \tTraining Loss: 0.487274\n",
            "Epoch: 718 \tTraining Loss: 0.487220\n",
            "Epoch: 719 \tTraining Loss: 0.487176\n",
            "Epoch: 720 \tTraining Loss: 0.486998\n",
            "Epoch: 721 \tTraining Loss: 0.486745\n",
            "Epoch: 722 \tTraining Loss: 0.486402\n",
            "Epoch: 723 \tTraining Loss: 0.486081\n",
            "Epoch: 724 \tTraining Loss: 0.485812\n",
            "Epoch: 725 \tTraining Loss: 0.485619\n",
            "Epoch: 726 \tTraining Loss: 0.485509\n",
            "Epoch: 727 \tTraining Loss: 0.485427\n",
            "Epoch: 728 \tTraining Loss: 0.485352\n",
            "Epoch: 729 \tTraining Loss: 0.485071\n",
            "Epoch: 730 \tTraining Loss: 0.484695\n",
            "Epoch: 731 \tTraining Loss: 0.484398\n",
            "Epoch: 732 \tTraining Loss: 0.484258\n",
            "Epoch: 733 \tTraining Loss: 0.484239\n",
            "Epoch: 734 \tTraining Loss: 0.484132\n",
            "Epoch: 735 \tTraining Loss: 0.483937\n",
            "Epoch: 736 \tTraining Loss: 0.483552\n",
            "Epoch: 737 \tTraining Loss: 0.483233\n",
            "Epoch: 738 \tTraining Loss: 0.483213\n",
            "Epoch: 739 \tTraining Loss: 0.483303\n",
            "Epoch: 740 \tTraining Loss: 0.483580\n",
            "Epoch: 741 \tTraining Loss: 0.483141\n",
            "Epoch: 742 \tTraining Loss: 0.483297\n",
            "Epoch: 743 \tTraining Loss: 0.485159\n",
            "Epoch: 744 \tTraining Loss: 0.488285\n",
            "Epoch: 745 \tTraining Loss: 0.490644\n",
            "Epoch: 746 \tTraining Loss: 0.489562\n",
            "Epoch: 747 \tTraining Loss: 0.482545\n",
            "Epoch: 748 \tTraining Loss: 0.483558\n",
            "Epoch: 749 \tTraining Loss: 0.487300\n",
            "Epoch: 750 \tTraining Loss: 0.482943\n",
            "Epoch: 751 \tTraining Loss: 0.481676\n",
            "Epoch: 752 \tTraining Loss: 0.484475\n",
            "Epoch: 753 \tTraining Loss: 0.482072\n",
            "Epoch: 754 \tTraining Loss: 0.481145\n",
            "Epoch: 755 \tTraining Loss: 0.482831\n",
            "Epoch: 756 \tTraining Loss: 0.480975\n",
            "Epoch: 757 \tTraining Loss: 0.480476\n",
            "Epoch: 758 \tTraining Loss: 0.481796\n",
            "Epoch: 759 \tTraining Loss: 0.480099\n",
            "Epoch: 760 \tTraining Loss: 0.479794\n",
            "Epoch: 761 \tTraining Loss: 0.480676\n",
            "Epoch: 762 \tTraining Loss: 0.479295\n",
            "Epoch: 763 \tTraining Loss: 0.479364\n",
            "Epoch: 764 \tTraining Loss: 0.479713\n",
            "Epoch: 765 \tTraining Loss: 0.478527\n",
            "Epoch: 766 \tTraining Loss: 0.478711\n",
            "Epoch: 767 \tTraining Loss: 0.478791\n",
            "Epoch: 768 \tTraining Loss: 0.478094\n",
            "Epoch: 769 \tTraining Loss: 0.478099\n",
            "Epoch: 770 \tTraining Loss: 0.478336\n",
            "Epoch: 771 \tTraining Loss: 0.477685\n",
            "Epoch: 772 \tTraining Loss: 0.477463\n",
            "Epoch: 773 \tTraining Loss: 0.477561\n",
            "Epoch: 774 \tTraining Loss: 0.476982\n",
            "Epoch: 775 \tTraining Loss: 0.476867\n",
            "Epoch: 776 \tTraining Loss: 0.477082\n",
            "Epoch: 777 \tTraining Loss: 0.476986\n",
            "Epoch: 778 \tTraining Loss: 0.476579\n",
            "Epoch: 779 \tTraining Loss: 0.476273\n",
            "Epoch: 780 \tTraining Loss: 0.476099\n",
            "Epoch: 781 \tTraining Loss: 0.476173\n",
            "Epoch: 782 \tTraining Loss: 0.476538\n",
            "Epoch: 783 \tTraining Loss: 0.475551\n",
            "Epoch: 784 \tTraining Loss: 0.475524\n",
            "Epoch: 785 \tTraining Loss: 0.476105\n",
            "Epoch: 786 \tTraining Loss: 0.474960\n",
            "Epoch: 787 \tTraining Loss: 0.475174\n",
            "Epoch: 788 \tTraining Loss: 0.475923\n",
            "Epoch: 789 \tTraining Loss: 0.474407\n",
            "Epoch: 790 \tTraining Loss: 0.475343\n",
            "Epoch: 791 \tTraining Loss: 0.475779\n",
            "Epoch: 792 \tTraining Loss: 0.474041\n",
            "Epoch: 793 \tTraining Loss: 0.475628\n",
            "Epoch: 794 \tTraining Loss: 0.474214\n",
            "Epoch: 795 \tTraining Loss: 0.474143\n",
            "Epoch: 796 \tTraining Loss: 0.474336\n",
            "Epoch: 797 \tTraining Loss: 0.473453\n",
            "Epoch: 798 \tTraining Loss: 0.473806\n",
            "Epoch: 799 \tTraining Loss: 0.472927\n",
            "Epoch: 800 \tTraining Loss: 0.473345\n",
            "Epoch: 801 \tTraining Loss: 0.472717\n",
            "Epoch: 802 \tTraining Loss: 0.472640\n",
            "Epoch: 803 \tTraining Loss: 0.472517\n",
            "Epoch: 804 \tTraining Loss: 0.472234\n",
            "Epoch: 805 \tTraining Loss: 0.472166\n",
            "Epoch: 806 \tTraining Loss: 0.471700\n",
            "Epoch: 807 \tTraining Loss: 0.471858\n",
            "Epoch: 808 \tTraining Loss: 0.471598\n",
            "Epoch: 809 \tTraining Loss: 0.471218\n",
            "Epoch: 810 \tTraining Loss: 0.471313\n",
            "Epoch: 811 \tTraining Loss: 0.470913\n",
            "Epoch: 812 \tTraining Loss: 0.470775\n",
            "Epoch: 813 \tTraining Loss: 0.470758\n",
            "Epoch: 814 \tTraining Loss: 0.470454\n",
            "Epoch: 815 \tTraining Loss: 0.470186\n",
            "Epoch: 816 \tTraining Loss: 0.470198\n",
            "Epoch: 817 \tTraining Loss: 0.470013\n",
            "Epoch: 818 \tTraining Loss: 0.469620\n",
            "Epoch: 819 \tTraining Loss: 0.469645\n",
            "Epoch: 820 \tTraining Loss: 0.469535\n",
            "Epoch: 821 \tTraining Loss: 0.469123\n",
            "Epoch: 822 \tTraining Loss: 0.469074\n",
            "Epoch: 823 \tTraining Loss: 0.469026\n",
            "Epoch: 824 \tTraining Loss: 0.468634\n",
            "Epoch: 825 \tTraining Loss: 0.468482\n",
            "Epoch: 826 \tTraining Loss: 0.468472\n",
            "Epoch: 827 \tTraining Loss: 0.468197\n",
            "Epoch: 828 \tTraining Loss: 0.467940\n",
            "Epoch: 829 \tTraining Loss: 0.467825\n",
            "Epoch: 830 \tTraining Loss: 0.467716\n",
            "Epoch: 831 \tTraining Loss: 0.467517\n",
            "Epoch: 832 \tTraining Loss: 0.467256\n",
            "Epoch: 833 \tTraining Loss: 0.467120\n",
            "Epoch: 834 \tTraining Loss: 0.467017\n",
            "Epoch: 835 \tTraining Loss: 0.466833\n",
            "Epoch: 836 \tTraining Loss: 0.466616\n",
            "Epoch: 837 \tTraining Loss: 0.466417\n",
            "Epoch: 838 \tTraining Loss: 0.466277\n",
            "Epoch: 839 \tTraining Loss: 0.466151\n",
            "Epoch: 840 \tTraining Loss: 0.465984\n",
            "Epoch: 841 \tTraining Loss: 0.465784\n",
            "Epoch: 842 \tTraining Loss: 0.465584\n",
            "Epoch: 843 \tTraining Loss: 0.465417\n",
            "Epoch: 844 \tTraining Loss: 0.465274\n",
            "Epoch: 845 \tTraining Loss: 0.465137\n",
            "Epoch: 846 \tTraining Loss: 0.464978\n",
            "Epoch: 847 \tTraining Loss: 0.464791\n",
            "Epoch: 848 \tTraining Loss: 0.464596\n",
            "Epoch: 849 \tTraining Loss: 0.464406\n",
            "Epoch: 850 \tTraining Loss: 0.464240\n",
            "Epoch: 851 \tTraining Loss: 0.464093\n",
            "Epoch: 852 \tTraining Loss: 0.463948\n",
            "Epoch: 853 \tTraining Loss: 0.463804\n",
            "Epoch: 854 \tTraining Loss: 0.463648\n",
            "Epoch: 855 \tTraining Loss: 0.463463\n",
            "Epoch: 856 \tTraining Loss: 0.463269\n",
            "Epoch: 857 \tTraining Loss: 0.463079\n",
            "Epoch: 858 \tTraining Loss: 0.462900\n",
            "Epoch: 859 \tTraining Loss: 0.462729\n",
            "Epoch: 860 \tTraining Loss: 0.462565\n",
            "Epoch: 861 \tTraining Loss: 0.462408\n",
            "Epoch: 862 \tTraining Loss: 0.462258\n",
            "Epoch: 863 \tTraining Loss: 0.462123\n",
            "Epoch: 864 \tTraining Loss: 0.462009\n",
            "Epoch: 865 \tTraining Loss: 0.461949\n",
            "Epoch: 866 \tTraining Loss: 0.461968\n",
            "Epoch: 867 \tTraining Loss: 0.462207\n",
            "Epoch: 868 \tTraining Loss: 0.462757\n",
            "Epoch: 869 \tTraining Loss: 0.464085\n",
            "Epoch: 870 \tTraining Loss: 0.466166\n",
            "Epoch: 871 \tTraining Loss: 0.468753\n",
            "Epoch: 872 \tTraining Loss: 0.466957\n",
            "Epoch: 873 \tTraining Loss: 0.462482\n",
            "Epoch: 874 \tTraining Loss: 0.460476\n",
            "Epoch: 875 \tTraining Loss: 0.462869\n",
            "Epoch: 876 \tTraining Loss: 0.463799\n",
            "Epoch: 877 \tTraining Loss: 0.460911\n",
            "Epoch: 878 \tTraining Loss: 0.460094\n",
            "Epoch: 879 \tTraining Loss: 0.461982\n",
            "Epoch: 880 \tTraining Loss: 0.461464\n",
            "Epoch: 881 \tTraining Loss: 0.459507\n",
            "Epoch: 882 \tTraining Loss: 0.460065\n",
            "Epoch: 883 \tTraining Loss: 0.460786\n",
            "Epoch: 884 \tTraining Loss: 0.459430\n",
            "Epoch: 885 \tTraining Loss: 0.458847\n",
            "Epoch: 886 \tTraining Loss: 0.459729\n",
            "Epoch: 887 \tTraining Loss: 0.459277\n",
            "Epoch: 888 \tTraining Loss: 0.458335\n",
            "Epoch: 889 \tTraining Loss: 0.458652\n",
            "Epoch: 890 \tTraining Loss: 0.458773\n",
            "Epoch: 891 \tTraining Loss: 0.458048\n",
            "Epoch: 892 \tTraining Loss: 0.457851\n",
            "Epoch: 893 \tTraining Loss: 0.458109\n",
            "Epoch: 894 \tTraining Loss: 0.457899\n",
            "Epoch: 895 \tTraining Loss: 0.457493\n",
            "Epoch: 896 \tTraining Loss: 0.457503\n",
            "Epoch: 897 \tTraining Loss: 0.457430\n",
            "Epoch: 898 \tTraining Loss: 0.456889\n",
            "Epoch: 899 \tTraining Loss: 0.456644\n",
            "Epoch: 900 \tTraining Loss: 0.456732\n",
            "Epoch: 901 \tTraining Loss: 0.456497\n",
            "Epoch: 902 \tTraining Loss: 0.456102\n",
            "Epoch: 903 \tTraining Loss: 0.456016\n",
            "Epoch: 904 \tTraining Loss: 0.455994\n",
            "Epoch: 905 \tTraining Loss: 0.455766\n",
            "Epoch: 906 \tTraining Loss: 0.455476\n",
            "Epoch: 907 \tTraining Loss: 0.455385\n",
            "Epoch: 908 \tTraining Loss: 0.455336\n",
            "Epoch: 909 \tTraining Loss: 0.455095\n",
            "Epoch: 910 \tTraining Loss: 0.454871\n",
            "Epoch: 911 \tTraining Loss: 0.454786\n",
            "Epoch: 912 \tTraining Loss: 0.454694\n",
            "Epoch: 913 \tTraining Loss: 0.454503\n",
            "Epoch: 914 \tTraining Loss: 0.454308\n",
            "Epoch: 915 \tTraining Loss: 0.454228\n",
            "Epoch: 916 \tTraining Loss: 0.454134\n",
            "Epoch: 917 \tTraining Loss: 0.454018\n",
            "Epoch: 918 \tTraining Loss: 0.453813\n",
            "Epoch: 919 \tTraining Loss: 0.453708\n",
            "Epoch: 920 \tTraining Loss: 0.453512\n",
            "Epoch: 921 \tTraining Loss: 0.453330\n",
            "Epoch: 922 \tTraining Loss: 0.453106\n",
            "Epoch: 923 \tTraining Loss: 0.452919\n",
            "Epoch: 924 \tTraining Loss: 0.452781\n",
            "Epoch: 925 \tTraining Loss: 0.452646\n",
            "Epoch: 926 \tTraining Loss: 0.452493\n",
            "Epoch: 927 \tTraining Loss: 0.452342\n",
            "Epoch: 928 \tTraining Loss: 0.452209\n",
            "Epoch: 929 \tTraining Loss: 0.452080\n",
            "Epoch: 930 \tTraining Loss: 0.451960\n",
            "Epoch: 931 \tTraining Loss: 0.451803\n",
            "Epoch: 932 \tTraining Loss: 0.451656\n",
            "Epoch: 933 \tTraining Loss: 0.451482\n",
            "Epoch: 934 \tTraining Loss: 0.451338\n",
            "Epoch: 935 \tTraining Loss: 0.451166\n",
            "Epoch: 936 \tTraining Loss: 0.451004\n",
            "Epoch: 937 \tTraining Loss: 0.450826\n",
            "Epoch: 938 \tTraining Loss: 0.450656\n",
            "Epoch: 939 \tTraining Loss: 0.450499\n",
            "Epoch: 940 \tTraining Loss: 0.450351\n",
            "Epoch: 941 \tTraining Loss: 0.450208\n",
            "Epoch: 942 \tTraining Loss: 0.450066\n",
            "Epoch: 943 \tTraining Loss: 0.449924\n",
            "Epoch: 944 \tTraining Loss: 0.449775\n",
            "Epoch: 945 \tTraining Loss: 0.449630\n",
            "Epoch: 946 \tTraining Loss: 0.449482\n",
            "Epoch: 947 \tTraining Loss: 0.449341\n",
            "Epoch: 948 \tTraining Loss: 0.449195\n",
            "Epoch: 949 \tTraining Loss: 0.449058\n",
            "Epoch: 950 \tTraining Loss: 0.448908\n",
            "Epoch: 951 \tTraining Loss: 0.448767\n",
            "Epoch: 952 \tTraining Loss: 0.448615\n",
            "Epoch: 953 \tTraining Loss: 0.448471\n",
            "Epoch: 954 \tTraining Loss: 0.448313\n",
            "Epoch: 955 \tTraining Loss: 0.448166\n",
            "Epoch: 956 \tTraining Loss: 0.448007\n",
            "Epoch: 957 \tTraining Loss: 0.447857\n",
            "Epoch: 958 \tTraining Loss: 0.447697\n",
            "Epoch: 959 \tTraining Loss: 0.447545\n",
            "Epoch: 960 \tTraining Loss: 0.447385\n",
            "Epoch: 961 \tTraining Loss: 0.447231\n",
            "Epoch: 962 \tTraining Loss: 0.447074\n",
            "Epoch: 963 \tTraining Loss: 0.446920\n",
            "Epoch: 964 \tTraining Loss: 0.446766\n",
            "Epoch: 965 \tTraining Loss: 0.446614\n",
            "Epoch: 966 \tTraining Loss: 0.446462\n",
            "Epoch: 967 \tTraining Loss: 0.446310\n",
            "Epoch: 968 \tTraining Loss: 0.446155\n",
            "Epoch: 969 \tTraining Loss: 0.446002\n",
            "Epoch: 970 \tTraining Loss: 0.445852\n",
            "Epoch: 971 \tTraining Loss: 0.445710\n",
            "Epoch: 972 \tTraining Loss: 0.445574\n",
            "Epoch: 973 \tTraining Loss: 0.445452\n",
            "Epoch: 974 \tTraining Loss: 0.445362\n",
            "Epoch: 975 \tTraining Loss: 0.445345\n",
            "Epoch: 976 \tTraining Loss: 0.445476\n",
            "Epoch: 977 \tTraining Loss: 0.445921\n",
            "Epoch: 978 \tTraining Loss: 0.446948\n",
            "Epoch: 979 \tTraining Loss: 0.448991\n",
            "Epoch: 980 \tTraining Loss: 0.452386\n",
            "Epoch: 981 \tTraining Loss: 0.454960\n",
            "Epoch: 982 \tTraining Loss: 0.454842\n",
            "Epoch: 983 \tTraining Loss: 0.446799\n",
            "Epoch: 984 \tTraining Loss: 0.444931\n",
            "Epoch: 985 \tTraining Loss: 0.449667\n",
            "Epoch: 986 \tTraining Loss: 0.448456\n",
            "Epoch: 987 \tTraining Loss: 0.444254\n",
            "Epoch: 988 \tTraining Loss: 0.447916\n",
            "Epoch: 989 \tTraining Loss: 0.447576\n",
            "Epoch: 990 \tTraining Loss: 0.447250\n",
            "Epoch: 991 \tTraining Loss: 0.449722\n",
            "Epoch: 992 \tTraining Loss: 0.447008\n",
            "Epoch: 993 \tTraining Loss: 0.447544\n",
            "Epoch: 994 \tTraining Loss: 0.445194\n",
            "Epoch: 995 \tTraining Loss: 0.449470\n",
            "Epoch: 996 \tTraining Loss: 0.445218\n",
            "Epoch: 997 \tTraining Loss: 0.450800\n",
            "Epoch: 998 \tTraining Loss: 0.444470\n",
            "Epoch: 999 \tTraining Loss: 0.446936\n",
            "Epoch: 1000 \tTraining Loss: 0.443658\n",
            "Epoch: 1001 \tTraining Loss: 0.448245\n",
            "Epoch: 1002 \tTraining Loss: 0.443944\n",
            "Epoch: 1003 \tTraining Loss: 0.446755\n",
            "Epoch: 1004 \tTraining Loss: 0.443744\n",
            "Epoch: 1005 \tTraining Loss: 0.444760\n",
            "Epoch: 1006 \tTraining Loss: 0.443257\n",
            "Epoch: 1007 \tTraining Loss: 0.443044\n",
            "Epoch: 1008 \tTraining Loss: 0.443846\n",
            "Epoch: 1009 \tTraining Loss: 0.441940\n",
            "Epoch: 1010 \tTraining Loss: 0.443466\n",
            "Epoch: 1011 \tTraining Loss: 0.442398\n",
            "Epoch: 1012 \tTraining Loss: 0.444393\n",
            "Epoch: 1013 \tTraining Loss: 0.441824\n",
            "Epoch: 1014 \tTraining Loss: 0.443459\n",
            "Epoch: 1015 \tTraining Loss: 0.441419\n",
            "Epoch: 1016 \tTraining Loss: 0.441682\n",
            "Epoch: 1017 \tTraining Loss: 0.441943\n",
            "Epoch: 1018 \tTraining Loss: 0.441225\n",
            "Epoch: 1019 \tTraining Loss: 0.442072\n",
            "Epoch: 1020 \tTraining Loss: 0.441034\n",
            "Epoch: 1021 \tTraining Loss: 0.442136\n",
            "Epoch: 1022 \tTraining Loss: 0.440608\n",
            "Epoch: 1023 \tTraining Loss: 0.440578\n",
            "Epoch: 1024 \tTraining Loss: 0.442141\n",
            "Epoch: 1025 \tTraining Loss: 0.440474\n",
            "Epoch: 1026 \tTraining Loss: 0.442922\n",
            "Epoch: 1027 \tTraining Loss: 0.439480\n",
            "Epoch: 1028 \tTraining Loss: 0.444954\n",
            "Epoch: 1029 \tTraining Loss: 0.439592\n",
            "Epoch: 1030 \tTraining Loss: 0.440339\n",
            "Epoch: 1031 \tTraining Loss: 0.440705\n",
            "Epoch: 1032 \tTraining Loss: 0.439045\n",
            "Epoch: 1033 \tTraining Loss: 0.441454\n",
            "Epoch: 1034 \tTraining Loss: 0.438649\n",
            "Epoch: 1035 \tTraining Loss: 0.439491\n",
            "Epoch: 1036 \tTraining Loss: 0.439408\n",
            "Epoch: 1037 \tTraining Loss: 0.438153\n",
            "Epoch: 1038 \tTraining Loss: 0.438760\n",
            "Epoch: 1039 \tTraining Loss: 0.438443\n",
            "Epoch: 1040 \tTraining Loss: 0.437643\n",
            "Epoch: 1041 \tTraining Loss: 0.438375\n",
            "Epoch: 1042 \tTraining Loss: 0.437280\n",
            "Epoch: 1043 \tTraining Loss: 0.438208\n",
            "Epoch: 1044 \tTraining Loss: 0.436782\n",
            "Epoch: 1045 \tTraining Loss: 0.437208\n",
            "Epoch: 1046 \tTraining Loss: 0.436934\n",
            "Epoch: 1047 \tTraining Loss: 0.436462\n",
            "Epoch: 1048 \tTraining Loss: 0.436455\n",
            "Epoch: 1049 \tTraining Loss: 0.436286\n",
            "Epoch: 1050 \tTraining Loss: 0.436192\n",
            "Epoch: 1051 \tTraining Loss: 0.435889\n",
            "Epoch: 1052 \tTraining Loss: 0.435808\n",
            "Epoch: 1053 \tTraining Loss: 0.435607\n",
            "Epoch: 1054 \tTraining Loss: 0.435515\n",
            "Epoch: 1055 \tTraining Loss: 0.435336\n",
            "Epoch: 1056 \tTraining Loss: 0.435119\n",
            "Epoch: 1057 \tTraining Loss: 0.435063\n",
            "Epoch: 1058 \tTraining Loss: 0.434821\n",
            "Epoch: 1059 \tTraining Loss: 0.434763\n",
            "Epoch: 1060 \tTraining Loss: 0.434542\n",
            "Epoch: 1061 \tTraining Loss: 0.434487\n",
            "Epoch: 1062 \tTraining Loss: 0.434259\n",
            "Epoch: 1063 \tTraining Loss: 0.434207\n",
            "Epoch: 1064 \tTraining Loss: 0.433995\n",
            "Epoch: 1065 \tTraining Loss: 0.433918\n",
            "Epoch: 1066 \tTraining Loss: 0.433740\n",
            "Epoch: 1067 \tTraining Loss: 0.433649\n",
            "Epoch: 1068 \tTraining Loss: 0.433482\n",
            "Epoch: 1069 \tTraining Loss: 0.433385\n",
            "Epoch: 1070 \tTraining Loss: 0.433223\n",
            "Epoch: 1071 \tTraining Loss: 0.433105\n",
            "Epoch: 1072 \tTraining Loss: 0.432962\n",
            "Epoch: 1073 \tTraining Loss: 0.432615\n",
            "Epoch: 1074 \tTraining Loss: 0.432172\n",
            "Epoch: 1075 \tTraining Loss: 0.432042\n",
            "Epoch: 1076 \tTraining Loss: 0.431932\n",
            "Epoch: 1077 \tTraining Loss: 0.431808\n",
            "Epoch: 1078 \tTraining Loss: 0.431691\n",
            "Epoch: 1079 \tTraining Loss: 0.431550\n",
            "Epoch: 1080 \tTraining Loss: 0.431446\n",
            "Epoch: 1081 \tTraining Loss: 0.431311\n",
            "Epoch: 1082 \tTraining Loss: 0.431207\n",
            "Epoch: 1083 \tTraining Loss: 0.431105\n",
            "Epoch: 1084 \tTraining Loss: 0.431011\n",
            "Epoch: 1085 \tTraining Loss: 0.430939\n",
            "Epoch: 1086 \tTraining Loss: 0.430873\n",
            "Epoch: 1087 \tTraining Loss: 0.430830\n",
            "Epoch: 1088 \tTraining Loss: 0.430791\n",
            "Epoch: 1089 \tTraining Loss: 0.430764\n",
            "Epoch: 1090 \tTraining Loss: 0.430754\n",
            "Epoch: 1091 \tTraining Loss: 0.430715\n",
            "Epoch: 1092 \tTraining Loss: 0.430649\n",
            "Epoch: 1093 \tTraining Loss: 0.430511\n",
            "Epoch: 1094 \tTraining Loss: 0.430299\n",
            "Epoch: 1095 \tTraining Loss: 0.429997\n",
            "Epoch: 1096 \tTraining Loss: 0.429676\n",
            "Epoch: 1097 \tTraining Loss: 0.429382\n",
            "Epoch: 1098 \tTraining Loss: 0.429176\n",
            "Epoch: 1099 \tTraining Loss: 0.429062\n",
            "Epoch: 1100 \tTraining Loss: 0.429016\n",
            "Epoch: 1101 \tTraining Loss: 0.428996\n",
            "Epoch: 1102 \tTraining Loss: 0.428965\n",
            "Epoch: 1103 \tTraining Loss: 0.428903\n",
            "Epoch: 1104 \tTraining Loss: 0.428797\n",
            "Epoch: 1105 \tTraining Loss: 0.428655\n",
            "Epoch: 1106 \tTraining Loss: 0.428474\n",
            "Epoch: 1107 \tTraining Loss: 0.428277\n",
            "Epoch: 1108 \tTraining Loss: 0.428076\n",
            "Epoch: 1109 \tTraining Loss: 0.427892\n",
            "Epoch: 1110 \tTraining Loss: 0.427730\n",
            "Epoch: 1111 \tTraining Loss: 0.427592\n",
            "Epoch: 1112 \tTraining Loss: 0.427474\n",
            "Epoch: 1113 \tTraining Loss: 0.427370\n",
            "Epoch: 1114 \tTraining Loss: 0.427276\n",
            "Epoch: 1115 \tTraining Loss: 0.427192\n",
            "Epoch: 1116 \tTraining Loss: 0.427122\n",
            "Epoch: 1117 \tTraining Loss: 0.427070\n",
            "Epoch: 1118 \tTraining Loss: 0.427051\n",
            "Epoch: 1119 \tTraining Loss: 0.427080\n",
            "Epoch: 1120 \tTraining Loss: 0.427201\n",
            "Epoch: 1121 \tTraining Loss: 0.427455\n",
            "Epoch: 1122 \tTraining Loss: 0.427945\n",
            "Epoch: 1123 \tTraining Loss: 0.428728\n",
            "Epoch: 1124 \tTraining Loss: 0.429833\n",
            "Epoch: 1125 \tTraining Loss: 0.431009\n",
            "Epoch: 1126 \tTraining Loss: 0.431288\n",
            "Epoch: 1127 \tTraining Loss: 0.429923\n",
            "Epoch: 1128 \tTraining Loss: 0.427081\n",
            "Epoch: 1129 \tTraining Loss: 0.425578\n",
            "Epoch: 1130 \tTraining Loss: 0.426781\n",
            "Epoch: 1131 \tTraining Loss: 0.428064\n",
            "Epoch: 1132 \tTraining Loss: 0.427264\n",
            "Epoch: 1133 \tTraining Loss: 0.425491\n",
            "Epoch: 1134 \tTraining Loss: 0.425144\n",
            "Epoch: 1135 \tTraining Loss: 0.426137\n",
            "Epoch: 1136 \tTraining Loss: 0.426324\n",
            "Epoch: 1137 \tTraining Loss: 0.425209\n",
            "Epoch: 1138 \tTraining Loss: 0.424531\n",
            "Epoch: 1139 \tTraining Loss: 0.425015\n",
            "Epoch: 1140 \tTraining Loss: 0.425325\n",
            "Epoch: 1141 \tTraining Loss: 0.424731\n",
            "Epoch: 1142 \tTraining Loss: 0.424079\n",
            "Epoch: 1143 \tTraining Loss: 0.424201\n",
            "Epoch: 1144 \tTraining Loss: 0.424513\n",
            "Epoch: 1145 \tTraining Loss: 0.424248\n",
            "Epoch: 1146 \tTraining Loss: 0.423704\n",
            "Epoch: 1147 \tTraining Loss: 0.423547\n",
            "Epoch: 1148 \tTraining Loss: 0.423731\n",
            "Epoch: 1149 \tTraining Loss: 0.423698\n",
            "Epoch: 1150 \tTraining Loss: 0.423332\n",
            "Epoch: 1151 \tTraining Loss: 0.423047\n",
            "Epoch: 1152 \tTraining Loss: 0.423057\n",
            "Epoch: 1153 \tTraining Loss: 0.423105\n",
            "Epoch: 1154 \tTraining Loss: 0.422939\n",
            "Epoch: 1155 \tTraining Loss: 0.422653\n",
            "Epoch: 1156 \tTraining Loss: 0.422488\n",
            "Epoch: 1157 \tTraining Loss: 0.422476\n",
            "Epoch: 1158 \tTraining Loss: 0.422444\n",
            "Epoch: 1159 \tTraining Loss: 0.422284\n",
            "Epoch: 1160 \tTraining Loss: 0.422067\n",
            "Epoch: 1161 \tTraining Loss: 0.421924\n",
            "Epoch: 1162 \tTraining Loss: 0.421869\n",
            "Epoch: 1163 \tTraining Loss: 0.421812\n",
            "Epoch: 1164 \tTraining Loss: 0.421686\n",
            "Epoch: 1165 \tTraining Loss: 0.421514\n",
            "Epoch: 1166 \tTraining Loss: 0.421362\n",
            "Epoch: 1167 \tTraining Loss: 0.421261\n",
            "Epoch: 1168 \tTraining Loss: 0.421188\n",
            "Epoch: 1169 \tTraining Loss: 0.421097\n",
            "Epoch: 1170 \tTraining Loss: 0.420970\n",
            "Epoch: 1171 \tTraining Loss: 0.420825\n",
            "Epoch: 1172 \tTraining Loss: 0.420689\n",
            "Epoch: 1173 \tTraining Loss: 0.420578\n",
            "Epoch: 1174 \tTraining Loss: 0.420485\n",
            "Epoch: 1175 \tTraining Loss: 0.420390\n",
            "Epoch: 1176 \tTraining Loss: 0.420282\n",
            "Epoch: 1177 \tTraining Loss: 0.420159\n",
            "Epoch: 1178 \tTraining Loss: 0.420031\n",
            "Epoch: 1179 \tTraining Loss: 0.419905\n",
            "Epoch: 1180 \tTraining Loss: 0.419788\n",
            "Epoch: 1181 \tTraining Loss: 0.419679\n",
            "Epoch: 1182 \tTraining Loss: 0.419575\n",
            "Epoch: 1183 \tTraining Loss: 0.419472\n",
            "Epoch: 1184 \tTraining Loss: 0.419366\n",
            "Epoch: 1185 \tTraining Loss: 0.419258\n",
            "Epoch: 1186 \tTraining Loss: 0.419147\n",
            "Epoch: 1187 \tTraining Loss: 0.419033\n",
            "Epoch: 1188 \tTraining Loss: 0.418919\n",
            "Epoch: 1189 \tTraining Loss: 0.418804\n",
            "Epoch: 1190 \tTraining Loss: 0.418690\n",
            "Epoch: 1191 \tTraining Loss: 0.418576\n",
            "Epoch: 1192 \tTraining Loss: 0.418464\n",
            "Epoch: 1193 \tTraining Loss: 0.418354\n",
            "Epoch: 1194 \tTraining Loss: 0.418246\n",
            "Epoch: 1195 \tTraining Loss: 0.418143\n",
            "Epoch: 1196 \tTraining Loss: 0.418048\n",
            "Epoch: 1197 \tTraining Loss: 0.417965\n",
            "Epoch: 1198 \tTraining Loss: 0.417902\n",
            "Epoch: 1199 \tTraining Loss: 0.417877\n",
            "Epoch: 1200 \tTraining Loss: 0.417912\n",
            "Epoch: 1201 \tTraining Loss: 0.418063\n",
            "Epoch: 1202 \tTraining Loss: 0.418393\n",
            "Epoch: 1203 \tTraining Loss: 0.419071\n",
            "Epoch: 1204 \tTraining Loss: 0.420244\n",
            "Epoch: 1205 \tTraining Loss: 0.422068\n",
            "Epoch: 1206 \tTraining Loss: 0.424681\n",
            "Epoch: 1207 \tTraining Loss: 0.426418\n",
            "Epoch: 1208 \tTraining Loss: 0.425799\n",
            "Epoch: 1209 \tTraining Loss: 0.420544\n",
            "Epoch: 1210 \tTraining Loss: 0.416754\n",
            "Epoch: 1211 \tTraining Loss: 0.419452\n",
            "Epoch: 1212 \tTraining Loss: 0.421217\n",
            "Epoch: 1213 \tTraining Loss: 0.418375\n",
            "Epoch: 1214 \tTraining Loss: 0.416383\n",
            "Epoch: 1215 \tTraining Loss: 0.418383\n",
            "Epoch: 1216 \tTraining Loss: 0.418830\n",
            "Epoch: 1217 \tTraining Loss: 0.416631\n",
            "Epoch: 1218 \tTraining Loss: 0.416389\n",
            "Epoch: 1219 \tTraining Loss: 0.417685\n",
            "Epoch: 1220 \tTraining Loss: 0.417032\n",
            "Epoch: 1221 \tTraining Loss: 0.415719\n",
            "Epoch: 1222 \tTraining Loss: 0.416503\n",
            "Epoch: 1223 \tTraining Loss: 0.416674\n",
            "Epoch: 1224 \tTraining Loss: 0.415735\n",
            "Epoch: 1225 \tTraining Loss: 0.415405\n",
            "Epoch: 1226 \tTraining Loss: 0.416417\n",
            "Epoch: 1227 \tTraining Loss: 0.416108\n",
            "Epoch: 1228 \tTraining Loss: 0.415382\n",
            "Epoch: 1229 \tTraining Loss: 0.415467\n",
            "Epoch: 1230 \tTraining Loss: 0.415202\n",
            "Epoch: 1231 \tTraining Loss: 0.414535\n",
            "Epoch: 1232 \tTraining Loss: 0.414591\n",
            "Epoch: 1233 \tTraining Loss: 0.414678\n",
            "Epoch: 1234 \tTraining Loss: 0.414855\n",
            "Epoch: 1235 \tTraining Loss: 0.414066\n",
            "Epoch: 1236 \tTraining Loss: 0.414734\n",
            "Epoch: 1237 \tTraining Loss: 0.414132\n",
            "Epoch: 1238 \tTraining Loss: 0.414728\n",
            "Epoch: 1239 \tTraining Loss: 0.413830\n",
            "Epoch: 1240 \tTraining Loss: 0.414697\n",
            "Epoch: 1241 \tTraining Loss: 0.414869\n",
            "Epoch: 1242 \tTraining Loss: 0.414562\n",
            "Epoch: 1243 \tTraining Loss: 0.414090\n",
            "Epoch: 1244 \tTraining Loss: 0.413547\n",
            "Epoch: 1245 \tTraining Loss: 0.413745\n",
            "Epoch: 1246 \tTraining Loss: 0.412979\n",
            "Epoch: 1247 \tTraining Loss: 0.413021\n",
            "Epoch: 1248 \tTraining Loss: 0.413117\n",
            "Epoch: 1249 \tTraining Loss: 0.412737\n",
            "Epoch: 1250 \tTraining Loss: 0.413086\n",
            "Epoch: 1251 \tTraining Loss: 0.413029\n",
            "Epoch: 1252 \tTraining Loss: 0.412418\n",
            "Epoch: 1253 \tTraining Loss: 0.412492\n",
            "Epoch: 1254 \tTraining Loss: 0.412343\n",
            "Epoch: 1255 \tTraining Loss: 0.412064\n",
            "Epoch: 1256 \tTraining Loss: 0.412313\n",
            "Epoch: 1257 \tTraining Loss: 0.412251\n",
            "Epoch: 1258 \tTraining Loss: 0.411817\n",
            "Epoch: 1259 \tTraining Loss: 0.411880\n",
            "Epoch: 1260 \tTraining Loss: 0.411958\n",
            "Epoch: 1261 \tTraining Loss: 0.411449\n",
            "Epoch: 1262 \tTraining Loss: 0.411868\n",
            "Epoch: 1263 \tTraining Loss: 0.411357\n",
            "Epoch: 1264 \tTraining Loss: 0.411447\n",
            "Epoch: 1265 \tTraining Loss: 0.411502\n",
            "Epoch: 1266 \tTraining Loss: 0.410938\n",
            "Epoch: 1267 \tTraining Loss: 0.411457\n",
            "Epoch: 1268 \tTraining Loss: 0.410752\n",
            "Epoch: 1269 \tTraining Loss: 0.411154\n",
            "Epoch: 1270 \tTraining Loss: 0.410985\n",
            "Epoch: 1271 \tTraining Loss: 0.410400\n",
            "Epoch: 1272 \tTraining Loss: 0.410902\n",
            "Epoch: 1273 \tTraining Loss: 0.410230\n",
            "Epoch: 1274 \tTraining Loss: 0.410555\n",
            "Epoch: 1275 \tTraining Loss: 0.410503\n",
            "Epoch: 1276 \tTraining Loss: 0.409852\n",
            "Epoch: 1277 \tTraining Loss: 0.410507\n",
            "Epoch: 1278 \tTraining Loss: 0.409806\n",
            "Epoch: 1279 \tTraining Loss: 0.409869\n",
            "Epoch: 1280 \tTraining Loss: 0.410135\n",
            "Epoch: 1281 \tTraining Loss: 0.409405\n",
            "Epoch: 1282 \tTraining Loss: 0.409570\n",
            "Epoch: 1283 \tTraining Loss: 0.409547\n",
            "Epoch: 1284 \tTraining Loss: 0.409404\n",
            "Epoch: 1285 \tTraining Loss: 0.409039\n",
            "Epoch: 1286 \tTraining Loss: 0.409036\n",
            "Epoch: 1287 \tTraining Loss: 0.409341\n",
            "Epoch: 1288 \tTraining Loss: 0.408755\n",
            "Epoch: 1289 \tTraining Loss: 0.408644\n",
            "Epoch: 1290 \tTraining Loss: 0.408556\n",
            "Epoch: 1291 \tTraining Loss: 0.408383\n",
            "Epoch: 1292 \tTraining Loss: 0.408338\n",
            "Epoch: 1293 \tTraining Loss: 0.408235\n",
            "Epoch: 1294 \tTraining Loss: 0.408053\n",
            "Epoch: 1295 \tTraining Loss: 0.407871\n",
            "Epoch: 1296 \tTraining Loss: 0.407939\n",
            "Epoch: 1297 \tTraining Loss: 0.407675\n",
            "Epoch: 1298 \tTraining Loss: 0.407607\n",
            "Epoch: 1299 \tTraining Loss: 0.407543\n",
            "Epoch: 1300 \tTraining Loss: 0.407364\n",
            "Epoch: 1301 \tTraining Loss: 0.407231\n",
            "Epoch: 1302 \tTraining Loss: 0.407181\n",
            "Epoch: 1303 \tTraining Loss: 0.407023\n",
            "Epoch: 1304 \tTraining Loss: 0.406916\n",
            "Epoch: 1305 \tTraining Loss: 0.406821\n",
            "Epoch: 1306 \tTraining Loss: 0.406723\n",
            "Epoch: 1307 \tTraining Loss: 0.406595\n",
            "Epoch: 1308 \tTraining Loss: 0.406508\n",
            "Epoch: 1309 \tTraining Loss: 0.406403\n",
            "Epoch: 1310 \tTraining Loss: 0.406311\n",
            "Epoch: 1311 \tTraining Loss: 0.406207\n",
            "Epoch: 1312 \tTraining Loss: 0.406126\n",
            "Epoch: 1313 \tTraining Loss: 0.406047\n",
            "Epoch: 1314 \tTraining Loss: 0.406027\n",
            "Epoch: 1315 \tTraining Loss: 0.406070\n",
            "Epoch: 1316 \tTraining Loss: 0.406265\n",
            "Epoch: 1317 \tTraining Loss: 0.406801\n",
            "Epoch: 1318 \tTraining Loss: 0.407932\n",
            "Epoch: 1319 \tTraining Loss: 0.410316\n",
            "Epoch: 1320 \tTraining Loss: 0.415633\n",
            "Epoch: 1321 \tTraining Loss: 0.423767\n",
            "Epoch: 1322 \tTraining Loss: 0.441627\n",
            "Epoch: 1323 \tTraining Loss: 0.437909\n",
            "Epoch: 1324 \tTraining Loss: 0.414137\n",
            "Epoch: 1325 \tTraining Loss: 0.413703\n",
            "Epoch: 1326 \tTraining Loss: 0.427944\n",
            "Epoch: 1327 \tTraining Loss: 0.412384\n",
            "Epoch: 1328 \tTraining Loss: 0.415191\n",
            "Epoch: 1329 \tTraining Loss: 0.417453\n",
            "Epoch: 1330 \tTraining Loss: 0.408172\n",
            "Epoch: 1331 \tTraining Loss: 0.416761\n",
            "Epoch: 1332 \tTraining Loss: 0.411974\n",
            "Epoch: 1333 \tTraining Loss: 0.412815\n",
            "Epoch: 1334 \tTraining Loss: 0.416216\n",
            "Epoch: 1335 \tTraining Loss: 0.409177\n",
            "Epoch: 1336 \tTraining Loss: 0.414541\n",
            "Epoch: 1337 \tTraining Loss: 0.408494\n",
            "Epoch: 1338 \tTraining Loss: 0.412204\n",
            "Epoch: 1339 \tTraining Loss: 0.407445\n",
            "Epoch: 1340 \tTraining Loss: 0.411100\n",
            "Epoch: 1341 \tTraining Loss: 0.407131\n",
            "Epoch: 1342 \tTraining Loss: 0.408529\n",
            "Epoch: 1343 \tTraining Loss: 0.406827\n",
            "Epoch: 1344 \tTraining Loss: 0.406056\n",
            "Epoch: 1345 \tTraining Loss: 0.407409\n",
            "Epoch: 1346 \tTraining Loss: 0.404876\n",
            "Epoch: 1347 \tTraining Loss: 0.406672\n",
            "Epoch: 1348 \tTraining Loss: 0.405037\n",
            "Epoch: 1349 \tTraining Loss: 0.405263\n",
            "Epoch: 1350 \tTraining Loss: 0.405331\n",
            "Epoch: 1351 \tTraining Loss: 0.404712\n",
            "Epoch: 1352 \tTraining Loss: 0.404241\n",
            "Epoch: 1353 \tTraining Loss: 0.404812\n",
            "Epoch: 1354 \tTraining Loss: 0.403914\n",
            "Epoch: 1355 \tTraining Loss: 0.404181\n",
            "Epoch: 1356 \tTraining Loss: 0.403255\n",
            "Epoch: 1357 \tTraining Loss: 0.404012\n",
            "Epoch: 1358 \tTraining Loss: 0.403846\n",
            "Epoch: 1359 \tTraining Loss: 0.404419\n",
            "Epoch: 1360 \tTraining Loss: 0.403408\n",
            "Epoch: 1361 \tTraining Loss: 0.403754\n",
            "Epoch: 1362 \tTraining Loss: 0.402742\n",
            "Epoch: 1363 \tTraining Loss: 0.402837\n",
            "Epoch: 1364 \tTraining Loss: 0.402266\n",
            "Epoch: 1365 \tTraining Loss: 0.402989\n",
            "Epoch: 1366 \tTraining Loss: 0.402201\n",
            "Epoch: 1367 \tTraining Loss: 0.402102\n",
            "Epoch: 1368 \tTraining Loss: 0.402051\n",
            "Epoch: 1369 \tTraining Loss: 0.401653\n",
            "Epoch: 1370 \tTraining Loss: 0.401763\n",
            "Epoch: 1371 \tTraining Loss: 0.401406\n",
            "Epoch: 1372 \tTraining Loss: 0.401436\n",
            "Epoch: 1373 \tTraining Loss: 0.401326\n",
            "Epoch: 1374 \tTraining Loss: 0.401052\n",
            "Epoch: 1375 \tTraining Loss: 0.401158\n",
            "Epoch: 1376 \tTraining Loss: 0.400886\n",
            "Epoch: 1377 \tTraining Loss: 0.400837\n",
            "Epoch: 1378 \tTraining Loss: 0.400737\n",
            "Epoch: 1379 \tTraining Loss: 0.400532\n",
            "Epoch: 1380 \tTraining Loss: 0.400547\n",
            "Epoch: 1381 \tTraining Loss: 0.400325\n",
            "Epoch: 1382 \tTraining Loss: 0.400307\n",
            "Epoch: 1383 \tTraining Loss: 0.400177\n",
            "Epoch: 1384 \tTraining Loss: 0.400068\n",
            "Epoch: 1385 \tTraining Loss: 0.400013\n",
            "Epoch: 1386 \tTraining Loss: 0.399840\n",
            "Epoch: 1387 \tTraining Loss: 0.399814\n",
            "Epoch: 1388 \tTraining Loss: 0.399662\n",
            "Epoch: 1389 \tTraining Loss: 0.399598\n",
            "Epoch: 1390 \tTraining Loss: 0.399496\n",
            "Epoch: 1391 \tTraining Loss: 0.399389\n",
            "Epoch: 1392 \tTraining Loss: 0.399324\n",
            "Epoch: 1393 \tTraining Loss: 0.399198\n",
            "Epoch: 1394 \tTraining Loss: 0.399128\n",
            "Epoch: 1395 \tTraining Loss: 0.399018\n",
            "Epoch: 1396 \tTraining Loss: 0.398939\n",
            "Epoch: 1397 \tTraining Loss: 0.398840\n",
            "Epoch: 1398 \tTraining Loss: 0.398750\n",
            "Epoch: 1399 \tTraining Loss: 0.398663\n",
            "Epoch: 1400 \tTraining Loss: 0.398564\n",
            "Epoch: 1401 \tTraining Loss: 0.398484\n",
            "Epoch: 1402 \tTraining Loss: 0.398383\n",
            "Epoch: 1403 \tTraining Loss: 0.398302\n",
            "Epoch: 1404 \tTraining Loss: 0.398206\n",
            "Epoch: 1405 \tTraining Loss: 0.398117\n",
            "Epoch: 1406 \tTraining Loss: 0.398031\n",
            "Epoch: 1407 \tTraining Loss: 0.397937\n",
            "Epoch: 1408 \tTraining Loss: 0.397853\n",
            "Epoch: 1409 \tTraining Loss: 0.397759\n",
            "Epoch: 1410 \tTraining Loss: 0.397673\n",
            "Epoch: 1411 \tTraining Loss: 0.397585\n",
            "Epoch: 1412 \tTraining Loss: 0.397493\n",
            "Epoch: 1413 \tTraining Loss: 0.397409\n",
            "Epoch: 1414 \tTraining Loss: 0.397317\n",
            "Epoch: 1415 \tTraining Loss: 0.397231\n",
            "Epoch: 1416 \tTraining Loss: 0.397142\n",
            "Epoch: 1417 \tTraining Loss: 0.397052\n",
            "Epoch: 1418 \tTraining Loss: 0.396967\n",
            "Epoch: 1419 \tTraining Loss: 0.396877\n",
            "Epoch: 1420 \tTraining Loss: 0.396791\n",
            "Epoch: 1421 \tTraining Loss: 0.396702\n",
            "Epoch: 1422 \tTraining Loss: 0.396614\n",
            "Epoch: 1423 \tTraining Loss: 0.396528\n",
            "Epoch: 1424 \tTraining Loss: 0.396439\n",
            "Epoch: 1425 \tTraining Loss: 0.396352\n",
            "Epoch: 1426 \tTraining Loss: 0.396264\n",
            "Epoch: 1427 \tTraining Loss: 0.396177\n",
            "Epoch: 1428 \tTraining Loss: 0.396089\n",
            "Epoch: 1429 \tTraining Loss: 0.396002\n",
            "Epoch: 1430 \tTraining Loss: 0.395915\n",
            "Epoch: 1431 \tTraining Loss: 0.395827\n",
            "Epoch: 1432 \tTraining Loss: 0.395740\n",
            "Epoch: 1433 \tTraining Loss: 0.395653\n",
            "Epoch: 1434 \tTraining Loss: 0.395566\n",
            "Epoch: 1435 \tTraining Loss: 0.395479\n",
            "Epoch: 1436 \tTraining Loss: 0.395391\n",
            "Epoch: 1437 \tTraining Loss: 0.395305\n",
            "Epoch: 1438 \tTraining Loss: 0.395218\n",
            "Epoch: 1439 \tTraining Loss: 0.395131\n",
            "Epoch: 1440 \tTraining Loss: 0.395044\n",
            "Epoch: 1441 \tTraining Loss: 0.394957\n",
            "Epoch: 1442 \tTraining Loss: 0.394870\n",
            "Epoch: 1443 \tTraining Loss: 0.394784\n",
            "Epoch: 1444 \tTraining Loss: 0.394697\n",
            "Epoch: 1445 \tTraining Loss: 0.394610\n",
            "Epoch: 1446 \tTraining Loss: 0.394524\n",
            "Epoch: 1447 \tTraining Loss: 0.394437\n",
            "Epoch: 1448 \tTraining Loss: 0.394351\n",
            "Epoch: 1449 \tTraining Loss: 0.394264\n",
            "Epoch: 1450 \tTraining Loss: 0.394178\n",
            "Epoch: 1451 \tTraining Loss: 0.394092\n",
            "Epoch: 1452 \tTraining Loss: 0.394005\n",
            "Epoch: 1453 \tTraining Loss: 0.393919\n",
            "Epoch: 1454 \tTraining Loss: 0.393833\n",
            "Epoch: 1455 \tTraining Loss: 0.393747\n",
            "Epoch: 1456 \tTraining Loss: 0.393661\n",
            "Epoch: 1457 \tTraining Loss: 0.393575\n",
            "Epoch: 1458 \tTraining Loss: 0.393489\n",
            "Epoch: 1459 \tTraining Loss: 0.393404\n",
            "Epoch: 1460 \tTraining Loss: 0.393318\n",
            "Epoch: 1461 \tTraining Loss: 0.393232\n",
            "Epoch: 1462 \tTraining Loss: 0.393147\n",
            "Epoch: 1463 \tTraining Loss: 0.393061\n",
            "Epoch: 1464 \tTraining Loss: 0.392976\n",
            "Epoch: 1465 \tTraining Loss: 0.392891\n",
            "Epoch: 1466 \tTraining Loss: 0.392805\n",
            "Epoch: 1467 \tTraining Loss: 0.392720\n",
            "Epoch: 1468 \tTraining Loss: 0.392635\n",
            "Epoch: 1469 \tTraining Loss: 0.392550\n",
            "Epoch: 1470 \tTraining Loss: 0.392465\n",
            "Epoch: 1471 \tTraining Loss: 0.392380\n",
            "Epoch: 1472 \tTraining Loss: 0.392295\n",
            "Epoch: 1473 \tTraining Loss: 0.392211\n",
            "Epoch: 1474 \tTraining Loss: 0.392126\n",
            "Epoch: 1475 \tTraining Loss: 0.392041\n",
            "Epoch: 1476 \tTraining Loss: 0.391957\n",
            "Epoch: 1477 \tTraining Loss: 0.391872\n",
            "Epoch: 1478 \tTraining Loss: 0.391788\n",
            "Epoch: 1479 \tTraining Loss: 0.391704\n",
            "Epoch: 1480 \tTraining Loss: 0.391619\n",
            "Epoch: 1481 \tTraining Loss: 0.391535\n",
            "Epoch: 1482 \tTraining Loss: 0.391451\n",
            "Epoch: 1483 \tTraining Loss: 0.391367\n",
            "Epoch: 1484 \tTraining Loss: 0.391283\n",
            "Epoch: 1485 \tTraining Loss: 0.391199\n",
            "Epoch: 1486 \tTraining Loss: 0.391115\n",
            "Epoch: 1487 \tTraining Loss: 0.391031\n",
            "Epoch: 1488 \tTraining Loss: 0.390947\n",
            "Epoch: 1489 \tTraining Loss: 0.390864\n",
            "Epoch: 1490 \tTraining Loss: 0.390780\n",
            "Epoch: 1491 \tTraining Loss: 0.390697\n",
            "Epoch: 1492 \tTraining Loss: 0.390613\n",
            "Epoch: 1493 \tTraining Loss: 0.390530\n",
            "Epoch: 1494 \tTraining Loss: 0.390446\n",
            "Epoch: 1495 \tTraining Loss: 0.390363\n",
            "Epoch: 1496 \tTraining Loss: 0.390280\n",
            "Epoch: 1497 \tTraining Loss: 0.390196\n",
            "Epoch: 1498 \tTraining Loss: 0.390113\n",
            "Epoch: 1499 \tTraining Loss: 0.390030\n",
            "Epoch: 1500 \tTraining Loss: 0.389947\n",
            "Epoch: 1501 \tTraining Loss: 0.389864\n",
            "Epoch: 1502 \tTraining Loss: 0.389781\n",
            "Epoch: 1503 \tTraining Loss: 0.389698\n",
            "Epoch: 1504 \tTraining Loss: 0.389616\n",
            "Epoch: 1505 \tTraining Loss: 0.389533\n",
            "Epoch: 1506 \tTraining Loss: 0.389450\n",
            "Epoch: 1507 \tTraining Loss: 0.389368\n",
            "Epoch: 1508 \tTraining Loss: 0.389285\n",
            "Epoch: 1509 \tTraining Loss: 0.389202\n",
            "Epoch: 1510 \tTraining Loss: 0.389120\n",
            "Epoch: 1511 \tTraining Loss: 0.389038\n",
            "Epoch: 1512 \tTraining Loss: 0.388955\n",
            "Epoch: 1513 \tTraining Loss: 0.388873\n",
            "Epoch: 1514 \tTraining Loss: 0.388791\n",
            "Epoch: 1515 \tTraining Loss: 0.388709\n",
            "Epoch: 1516 \tTraining Loss: 0.388627\n",
            "Epoch: 1517 \tTraining Loss: 0.388545\n",
            "Epoch: 1518 \tTraining Loss: 0.388463\n",
            "Epoch: 1519 \tTraining Loss: 0.388381\n",
            "Epoch: 1520 \tTraining Loss: 0.388299\n",
            "Epoch: 1521 \tTraining Loss: 0.388217\n",
            "Epoch: 1522 \tTraining Loss: 0.388136\n",
            "Epoch: 1523 \tTraining Loss: 0.388054\n",
            "Epoch: 1524 \tTraining Loss: 0.387972\n",
            "Epoch: 1525 \tTraining Loss: 0.387891\n",
            "Epoch: 1526 \tTraining Loss: 0.387809\n",
            "Epoch: 1527 \tTraining Loss: 0.387728\n",
            "Epoch: 1528 \tTraining Loss: 0.387646\n",
            "Epoch: 1529 \tTraining Loss: 0.387565\n",
            "Epoch: 1530 \tTraining Loss: 0.387484\n",
            "Epoch: 1531 \tTraining Loss: 0.387402\n",
            "Epoch: 1532 \tTraining Loss: 0.387321\n",
            "Epoch: 1533 \tTraining Loss: 0.387240\n",
            "Epoch: 1534 \tTraining Loss: 0.387159\n",
            "Epoch: 1535 \tTraining Loss: 0.387077\n",
            "Epoch: 1536 \tTraining Loss: 0.386996\n",
            "Epoch: 1537 \tTraining Loss: 0.386915\n",
            "Epoch: 1538 \tTraining Loss: 0.386834\n",
            "Epoch: 1539 \tTraining Loss: 0.386753\n",
            "Epoch: 1540 \tTraining Loss: 0.386672\n",
            "Epoch: 1541 \tTraining Loss: 0.386591\n",
            "Epoch: 1542 \tTraining Loss: 0.386510\n",
            "Epoch: 1543 \tTraining Loss: 0.386429\n",
            "Epoch: 1544 \tTraining Loss: 0.386348\n",
            "Epoch: 1545 \tTraining Loss: 0.386267\n",
            "Epoch: 1546 \tTraining Loss: 0.386187\n",
            "Epoch: 1547 \tTraining Loss: 0.386106\n",
            "Epoch: 1548 \tTraining Loss: 0.386025\n",
            "Epoch: 1549 \tTraining Loss: 0.385944\n",
            "Epoch: 1550 \tTraining Loss: 0.385863\n",
            "Epoch: 1551 \tTraining Loss: 0.385782\n",
            "Epoch: 1552 \tTraining Loss: 0.385702\n",
            "Epoch: 1553 \tTraining Loss: 0.385621\n",
            "Epoch: 1554 \tTraining Loss: 0.385540\n",
            "Epoch: 1555 \tTraining Loss: 0.385459\n",
            "Epoch: 1556 \tTraining Loss: 0.385379\n",
            "Epoch: 1557 \tTraining Loss: 0.385298\n",
            "Epoch: 1558 \tTraining Loss: 0.385219\n",
            "Epoch: 1559 \tTraining Loss: 0.385141\n",
            "Epoch: 1560 \tTraining Loss: 0.385066\n",
            "Epoch: 1561 \tTraining Loss: 0.385001\n",
            "Epoch: 1562 \tTraining Loss: 0.384958\n",
            "Epoch: 1563 \tTraining Loss: 0.384977\n",
            "Epoch: 1564 \tTraining Loss: 0.385141\n",
            "Epoch: 1565 \tTraining Loss: 0.385713\n",
            "Epoch: 1566 \tTraining Loss: 0.387057\n",
            "Epoch: 1567 \tTraining Loss: 0.390445\n",
            "Epoch: 1568 \tTraining Loss: 0.393987\n",
            "Epoch: 1569 \tTraining Loss: 0.400860\n",
            "Epoch: 1570 \tTraining Loss: 0.397755\n",
            "Epoch: 1571 \tTraining Loss: 0.392223\n",
            "Epoch: 1572 \tTraining Loss: 0.385376\n",
            "Epoch: 1573 \tTraining Loss: 0.389555\n",
            "Epoch: 1574 \tTraining Loss: 0.393763\n",
            "Epoch: 1575 \tTraining Loss: 0.386325\n",
            "Epoch: 1576 \tTraining Loss: 0.387910\n",
            "Epoch: 1577 \tTraining Loss: 0.392413\n",
            "Epoch: 1578 \tTraining Loss: 0.386879\n",
            "Epoch: 1579 \tTraining Loss: 0.386294\n",
            "Epoch: 1580 \tTraining Loss: 0.389789\n",
            "Epoch: 1581 \tTraining Loss: 0.386049\n",
            "Epoch: 1582 \tTraining Loss: 0.385407\n",
            "Epoch: 1583 \tTraining Loss: 0.387482\n",
            "Epoch: 1584 \tTraining Loss: 0.384688\n",
            "Epoch: 1585 \tTraining Loss: 0.385458\n",
            "Epoch: 1586 \tTraining Loss: 0.386257\n",
            "Epoch: 1587 \tTraining Loss: 0.383903\n",
            "Epoch: 1588 \tTraining Loss: 0.384850\n",
            "Epoch: 1589 \tTraining Loss: 0.385208\n",
            "Epoch: 1590 \tTraining Loss: 0.383580\n",
            "Epoch: 1591 \tTraining Loss: 0.384225\n",
            "Epoch: 1592 \tTraining Loss: 0.384473\n",
            "Epoch: 1593 \tTraining Loss: 0.383173\n",
            "Epoch: 1594 \tTraining Loss: 0.383835\n",
            "Epoch: 1595 \tTraining Loss: 0.383910\n",
            "Epoch: 1596 \tTraining Loss: 0.382821\n",
            "Epoch: 1597 \tTraining Loss: 0.383420\n",
            "Epoch: 1598 \tTraining Loss: 0.383292\n",
            "Epoch: 1599 \tTraining Loss: 0.382554\n",
            "Epoch: 1600 \tTraining Loss: 0.382916\n",
            "Epoch: 1601 \tTraining Loss: 0.382889\n",
            "Epoch: 1602 \tTraining Loss: 0.382286\n",
            "Epoch: 1603 \tTraining Loss: 0.382571\n",
            "Epoch: 1604 \tTraining Loss: 0.382565\n",
            "Epoch: 1605 \tTraining Loss: 0.382023\n",
            "Epoch: 1606 \tTraining Loss: 0.382167\n",
            "Epoch: 1607 \tTraining Loss: 0.382309\n",
            "Epoch: 1608 \tTraining Loss: 0.381741\n",
            "Epoch: 1609 \tTraining Loss: 0.381895\n",
            "Epoch: 1610 \tTraining Loss: 0.381992\n",
            "Epoch: 1611 \tTraining Loss: 0.381495\n",
            "Epoch: 1612 \tTraining Loss: 0.381546\n",
            "Epoch: 1613 \tTraining Loss: 0.381724\n",
            "Epoch: 1614 \tTraining Loss: 0.381249\n",
            "Epoch: 1615 \tTraining Loss: 0.381304\n",
            "Epoch: 1616 \tTraining Loss: 0.381401\n",
            "Epoch: 1617 \tTraining Loss: 0.381033\n",
            "Epoch: 1618 \tTraining Loss: 0.380980\n",
            "Epoch: 1619 \tTraining Loss: 0.381081\n",
            "Epoch: 1620 \tTraining Loss: 0.380810\n",
            "Epoch: 1621 \tTraining Loss: 0.380776\n",
            "Epoch: 1622 \tTraining Loss: 0.380805\n",
            "Epoch: 1623 \tTraining Loss: 0.380594\n",
            "Epoch: 1624 \tTraining Loss: 0.380507\n",
            "Epoch: 1625 \tTraining Loss: 0.380570\n",
            "Epoch: 1626 \tTraining Loss: 0.380358\n",
            "Epoch: 1627 \tTraining Loss: 0.380374\n",
            "Epoch: 1628 \tTraining Loss: 0.380257\n",
            "Epoch: 1629 \tTraining Loss: 0.380103\n",
            "Epoch: 1630 \tTraining Loss: 0.380147\n",
            "Epoch: 1631 \tTraining Loss: 0.379976\n",
            "Epoch: 1632 \tTraining Loss: 0.379864\n",
            "Epoch: 1633 \tTraining Loss: 0.379869\n",
            "Epoch: 1634 \tTraining Loss: 0.379749\n",
            "Epoch: 1635 \tTraining Loss: 0.379614\n",
            "Epoch: 1636 \tTraining Loss: 0.379588\n",
            "Epoch: 1637 \tTraining Loss: 0.379499\n",
            "Epoch: 1638 \tTraining Loss: 0.379378\n",
            "Epoch: 1639 \tTraining Loss: 0.379368\n",
            "Epoch: 1640 \tTraining Loss: 0.379250\n",
            "Epoch: 1641 \tTraining Loss: 0.379170\n",
            "Epoch: 1642 \tTraining Loss: 0.379127\n",
            "Epoch: 1643 \tTraining Loss: 0.378996\n",
            "Epoch: 1644 \tTraining Loss: 0.378923\n",
            "Epoch: 1645 \tTraining Loss: 0.378886\n",
            "Epoch: 1646 \tTraining Loss: 0.378790\n",
            "Epoch: 1647 \tTraining Loss: 0.378694\n",
            "Epoch: 1648 \tTraining Loss: 0.378678\n",
            "Epoch: 1649 \tTraining Loss: 0.378604\n",
            "Epoch: 1650 \tTraining Loss: 0.378535\n",
            "Epoch: 1651 \tTraining Loss: 0.378402\n",
            "Epoch: 1652 \tTraining Loss: 0.378323\n",
            "Epoch: 1653 \tTraining Loss: 0.378283\n",
            "Epoch: 1654 \tTraining Loss: 0.378170\n",
            "Epoch: 1655 \tTraining Loss: 0.378141\n",
            "Epoch: 1656 \tTraining Loss: 0.378078\n",
            "Epoch: 1657 \tTraining Loss: 0.378008\n",
            "Epoch: 1658 \tTraining Loss: 0.377869\n",
            "Epoch: 1659 \tTraining Loss: 0.377876\n",
            "Epoch: 1660 \tTraining Loss: 0.377787\n",
            "Epoch: 1661 \tTraining Loss: 0.377767\n",
            "Epoch: 1662 \tTraining Loss: 0.377637\n",
            "Epoch: 1663 \tTraining Loss: 0.377510\n",
            "Epoch: 1664 \tTraining Loss: 0.377511\n",
            "Epoch: 1665 \tTraining Loss: 0.377413\n",
            "Epoch: 1666 \tTraining Loss: 0.377461\n",
            "Epoch: 1667 \tTraining Loss: 0.377239\n",
            "Epoch: 1668 \tTraining Loss: 0.377459\n",
            "Epoch: 1669 \tTraining Loss: 0.377097\n",
            "Epoch: 1670 \tTraining Loss: 0.377073\n",
            "Epoch: 1671 \tTraining Loss: 0.377012\n",
            "Epoch: 1672 \tTraining Loss: 0.376889\n",
            "Epoch: 1673 \tTraining Loss: 0.376854\n",
            "Epoch: 1674 \tTraining Loss: 0.376722\n",
            "Epoch: 1675 \tTraining Loss: 0.376660\n",
            "Epoch: 1676 \tTraining Loss: 0.376595\n",
            "Epoch: 1677 \tTraining Loss: 0.376512\n",
            "Epoch: 1678 \tTraining Loss: 0.376420\n",
            "Epoch: 1679 \tTraining Loss: 0.376373\n",
            "Epoch: 1680 \tTraining Loss: 0.376272\n",
            "Epoch: 1681 \tTraining Loss: 0.376203\n",
            "Epoch: 1682 \tTraining Loss: 0.376105\n",
            "Epoch: 1683 \tTraining Loss: 0.376064\n",
            "Epoch: 1684 \tTraining Loss: 0.375981\n",
            "Epoch: 1685 \tTraining Loss: 0.375925\n",
            "Epoch: 1686 \tTraining Loss: 0.375800\n",
            "Epoch: 1687 \tTraining Loss: 0.375787\n",
            "Epoch: 1688 \tTraining Loss: 0.375717\n",
            "Epoch: 1689 \tTraining Loss: 0.375676\n",
            "Epoch: 1690 \tTraining Loss: 0.375531\n",
            "Epoch: 1691 \tTraining Loss: 0.375644\n",
            "Epoch: 1692 \tTraining Loss: 0.375397\n",
            "Epoch: 1693 \tTraining Loss: 0.375375\n",
            "Epoch: 1694 \tTraining Loss: 0.375238\n",
            "Epoch: 1695 \tTraining Loss: 0.375321\n",
            "Epoch: 1696 \tTraining Loss: 0.375111\n",
            "Epoch: 1697 \tTraining Loss: 0.375090\n",
            "Epoch: 1698 \tTraining Loss: 0.374965\n",
            "Epoch: 1699 \tTraining Loss: 0.374929\n",
            "Epoch: 1700 \tTraining Loss: 0.374789\n",
            "Epoch: 1701 \tTraining Loss: 0.374736\n",
            "Epoch: 1702 \tTraining Loss: 0.374643\n",
            "Epoch: 1703 \tTraining Loss: 0.374568\n",
            "Epoch: 1704 \tTraining Loss: 0.374497\n",
            "Epoch: 1705 \tTraining Loss: 0.374421\n",
            "Epoch: 1706 \tTraining Loss: 0.374347\n",
            "Epoch: 1707 \tTraining Loss: 0.374272\n",
            "Epoch: 1708 \tTraining Loss: 0.374204\n",
            "Epoch: 1709 \tTraining Loss: 0.374124\n",
            "Epoch: 1710 \tTraining Loss: 0.374053\n",
            "Epoch: 1711 \tTraining Loss: 0.373980\n",
            "Epoch: 1712 \tTraining Loss: 0.373907\n",
            "Epoch: 1713 \tTraining Loss: 0.373835\n",
            "Epoch: 1714 \tTraining Loss: 0.373760\n",
            "Epoch: 1715 \tTraining Loss: 0.373689\n",
            "Epoch: 1716 \tTraining Loss: 0.373617\n",
            "Epoch: 1717 \tTraining Loss: 0.373546\n",
            "Epoch: 1718 \tTraining Loss: 0.373473\n",
            "Epoch: 1719 \tTraining Loss: 0.373401\n",
            "Epoch: 1720 \tTraining Loss: 0.373331\n",
            "Epoch: 1721 \tTraining Loss: 0.373263\n",
            "Epoch: 1722 \tTraining Loss: 0.373199\n",
            "Epoch: 1723 \tTraining Loss: 0.373140\n",
            "Epoch: 1724 \tTraining Loss: 0.373095\n",
            "Epoch: 1725 \tTraining Loss: 0.373080\n",
            "Epoch: 1726 \tTraining Loss: 0.373116\n",
            "Epoch: 1727 \tTraining Loss: 0.373273\n",
            "Epoch: 1728 \tTraining Loss: 0.373627\n",
            "Epoch: 1729 \tTraining Loss: 0.374405\n",
            "Epoch: 1730 \tTraining Loss: 0.376166\n",
            "Epoch: 1731 \tTraining Loss: 0.378571\n",
            "Epoch: 1732 \tTraining Loss: 0.384035\n",
            "Epoch: 1733 \tTraining Loss: 0.382768\n",
            "Epoch: 1734 \tTraining Loss: 0.381491\n",
            "Epoch: 1735 \tTraining Loss: 0.373971\n",
            "Epoch: 1736 \tTraining Loss: 0.374702\n",
            "Epoch: 1737 \tTraining Loss: 0.380094\n",
            "Epoch: 1738 \tTraining Loss: 0.375713\n",
            "Epoch: 1739 \tTraining Loss: 0.372475\n",
            "Epoch: 1740 \tTraining Loss: 0.374903\n",
            "Epoch: 1741 \tTraining Loss: 0.376073\n",
            "Epoch: 1742 \tTraining Loss: 0.374007\n",
            "Epoch: 1743 \tTraining Loss: 0.372089\n",
            "Epoch: 1744 \tTraining Loss: 0.374012\n",
            "Epoch: 1745 \tTraining Loss: 0.375128\n",
            "Epoch: 1746 \tTraining Loss: 0.372651\n",
            "Epoch: 1747 \tTraining Loss: 0.372180\n",
            "Epoch: 1748 \tTraining Loss: 0.373801\n",
            "Epoch: 1749 \tTraining Loss: 0.372764\n",
            "Epoch: 1750 \tTraining Loss: 0.371730\n",
            "Epoch: 1751 \tTraining Loss: 0.372510\n",
            "Epoch: 1752 \tTraining Loss: 0.372754\n",
            "Epoch: 1753 \tTraining Loss: 0.371556\n",
            "Epoch: 1754 \tTraining Loss: 0.371454\n",
            "Epoch: 1755 \tTraining Loss: 0.372025\n",
            "Epoch: 1756 \tTraining Loss: 0.371881\n",
            "Epoch: 1757 \tTraining Loss: 0.371106\n",
            "Epoch: 1758 \tTraining Loss: 0.371419\n",
            "Epoch: 1759 \tTraining Loss: 0.371626\n",
            "Epoch: 1760 \tTraining Loss: 0.371053\n",
            "Epoch: 1761 \tTraining Loss: 0.371018\n",
            "Epoch: 1762 \tTraining Loss: 0.371144\n",
            "Epoch: 1763 \tTraining Loss: 0.371533\n",
            "Epoch: 1764 \tTraining Loss: 0.370654\n",
            "Epoch: 1765 \tTraining Loss: 0.370867\n",
            "Epoch: 1766 \tTraining Loss: 0.370767\n",
            "Epoch: 1767 \tTraining Loss: 0.371376\n",
            "Epoch: 1768 \tTraining Loss: 0.370327\n",
            "Epoch: 1769 \tTraining Loss: 0.371263\n",
            "Epoch: 1770 \tTraining Loss: 0.370664\n",
            "Epoch: 1771 \tTraining Loss: 0.371001\n",
            "Epoch: 1772 \tTraining Loss: 0.370919\n",
            "Epoch: 1773 \tTraining Loss: 0.370039\n",
            "Epoch: 1774 \tTraining Loss: 0.370260\n",
            "Epoch: 1775 \tTraining Loss: 0.370020\n",
            "Epoch: 1776 \tTraining Loss: 0.369832\n",
            "Epoch: 1777 \tTraining Loss: 0.369898\n",
            "Epoch: 1778 \tTraining Loss: 0.369743\n",
            "Epoch: 1779 \tTraining Loss: 0.369904\n",
            "Epoch: 1780 \tTraining Loss: 0.369573\n",
            "Epoch: 1781 \tTraining Loss: 0.369722\n",
            "Epoch: 1782 \tTraining Loss: 0.369415\n",
            "Epoch: 1783 \tTraining Loss: 0.369391\n",
            "Epoch: 1784 \tTraining Loss: 0.369432\n",
            "Epoch: 1785 \tTraining Loss: 0.369163\n",
            "Epoch: 1786 \tTraining Loss: 0.369315\n",
            "Epoch: 1787 \tTraining Loss: 0.369023\n",
            "Epoch: 1788 \tTraining Loss: 0.369317\n",
            "Epoch: 1789 \tTraining Loss: 0.368868\n",
            "Epoch: 1790 \tTraining Loss: 0.368833\n",
            "Epoch: 1791 \tTraining Loss: 0.368829\n",
            "Epoch: 1792 \tTraining Loss: 0.368688\n",
            "Epoch: 1793 \tTraining Loss: 0.368844\n",
            "Epoch: 1794 \tTraining Loss: 0.368530\n",
            "Epoch: 1795 \tTraining Loss: 0.368470\n",
            "Epoch: 1796 \tTraining Loss: 0.368399\n",
            "Epoch: 1797 \tTraining Loss: 0.368301\n",
            "Epoch: 1798 \tTraining Loss: 0.368193\n",
            "Epoch: 1799 \tTraining Loss: 0.368209\n",
            "Epoch: 1800 \tTraining Loss: 0.368102\n",
            "Epoch: 1801 \tTraining Loss: 0.368118\n",
            "Epoch: 1802 \tTraining Loss: 0.368037\n",
            "Epoch: 1803 \tTraining Loss: 0.367929\n",
            "Epoch: 1804 \tTraining Loss: 0.368035\n",
            "Epoch: 1805 \tTraining Loss: 0.367870\n",
            "Epoch: 1806 \tTraining Loss: 0.367796\n",
            "Epoch: 1807 \tTraining Loss: 0.367646\n",
            "Epoch: 1808 \tTraining Loss: 0.367514\n",
            "Epoch: 1809 \tTraining Loss: 0.367705\n",
            "Epoch: 1810 \tTraining Loss: 0.367465\n",
            "Epoch: 1811 \tTraining Loss: 0.367457\n",
            "Epoch: 1812 \tTraining Loss: 0.367395\n",
            "Epoch: 1813 \tTraining Loss: 0.367272\n",
            "Epoch: 1814 \tTraining Loss: 0.367251\n",
            "Epoch: 1815 \tTraining Loss: 0.367113\n",
            "Epoch: 1816 \tTraining Loss: 0.367092\n",
            "Epoch: 1817 \tTraining Loss: 0.366962\n",
            "Epoch: 1818 \tTraining Loss: 0.366914\n",
            "Epoch: 1819 \tTraining Loss: 0.366820\n",
            "Epoch: 1820 \tTraining Loss: 0.366765\n",
            "Epoch: 1821 \tTraining Loss: 0.366648\n",
            "Epoch: 1822 \tTraining Loss: 0.366668\n",
            "Epoch: 1823 \tTraining Loss: 0.366538\n",
            "Epoch: 1824 \tTraining Loss: 0.366512\n",
            "Epoch: 1825 \tTraining Loss: 0.366388\n",
            "Epoch: 1826 \tTraining Loss: 0.366483\n",
            "Epoch: 1827 \tTraining Loss: 0.366317\n",
            "Epoch: 1828 \tTraining Loss: 0.366352\n",
            "Epoch: 1829 \tTraining Loss: 0.366226\n",
            "Epoch: 1830 \tTraining Loss: 0.366399\n",
            "Epoch: 1831 \tTraining Loss: 0.366177\n",
            "Epoch: 1832 \tTraining Loss: 0.366153\n",
            "Epoch: 1833 \tTraining Loss: 0.365954\n",
            "Epoch: 1834 \tTraining Loss: 0.365959\n",
            "Epoch: 1835 \tTraining Loss: 0.365758\n",
            "Epoch: 1836 \tTraining Loss: 0.365736\n",
            "Epoch: 1837 \tTraining Loss: 0.365637\n",
            "Epoch: 1838 \tTraining Loss: 0.365589\n",
            "Epoch: 1839 \tTraining Loss: 0.365569\n",
            "Epoch: 1840 \tTraining Loss: 0.365527\n",
            "Epoch: 1841 \tTraining Loss: 0.365557\n",
            "Epoch: 1842 \tTraining Loss: 0.365606\n",
            "Epoch: 1843 \tTraining Loss: 0.365687\n",
            "Epoch: 1844 \tTraining Loss: 0.365939\n",
            "Epoch: 1845 \tTraining Loss: 0.366265\n",
            "Epoch: 1846 \tTraining Loss: 0.366691\n",
            "Epoch: 1847 \tTraining Loss: 0.367411\n",
            "Epoch: 1848 \tTraining Loss: 0.367911\n",
            "Epoch: 1849 \tTraining Loss: 0.368924\n",
            "Epoch: 1850 \tTraining Loss: 0.368479\n",
            "Epoch: 1851 \tTraining Loss: 0.367950\n",
            "Epoch: 1852 \tTraining Loss: 0.365940\n",
            "Epoch: 1853 \tTraining Loss: 0.364696\n",
            "Epoch: 1854 \tTraining Loss: 0.364785\n",
            "Epoch: 1855 \tTraining Loss: 0.365638\n",
            "Epoch: 1856 \tTraining Loss: 0.366368\n",
            "Epoch: 1857 \tTraining Loss: 0.365999\n",
            "Epoch: 1858 \tTraining Loss: 0.365181\n",
            "Epoch: 1859 \tTraining Loss: 0.364355\n",
            "Epoch: 1860 \tTraining Loss: 0.364212\n",
            "Epoch: 1861 \tTraining Loss: 0.364669\n",
            "Epoch: 1862 \tTraining Loss: 0.364913\n",
            "Epoch: 1863 \tTraining Loss: 0.364749\n",
            "Epoch: 1864 \tTraining Loss: 0.364179\n",
            "Epoch: 1865 \tTraining Loss: 0.363766\n",
            "Epoch: 1866 \tTraining Loss: 0.363773\n",
            "Epoch: 1867 \tTraining Loss: 0.364002\n",
            "Epoch: 1868 \tTraining Loss: 0.364147\n",
            "Epoch: 1869 \tTraining Loss: 0.363972\n",
            "Epoch: 1870 \tTraining Loss: 0.363654\n",
            "Epoch: 1871 \tTraining Loss: 0.363417\n",
            "Epoch: 1872 \tTraining Loss: 0.363413\n",
            "Epoch: 1873 \tTraining Loss: 0.363531\n",
            "Epoch: 1874 \tTraining Loss: 0.363668\n",
            "Epoch: 1875 \tTraining Loss: 0.363520\n",
            "Epoch: 1876 \tTraining Loss: 0.363305\n",
            "Epoch: 1877 \tTraining Loss: 0.363092\n",
            "Epoch: 1878 \tTraining Loss: 0.362940\n",
            "Epoch: 1879 \tTraining Loss: 0.362890\n",
            "Epoch: 1880 \tTraining Loss: 0.362897\n",
            "Epoch: 1881 \tTraining Loss: 0.362892\n",
            "Epoch: 1882 \tTraining Loss: 0.362853\n",
            "Epoch: 1883 \tTraining Loss: 0.362751\n",
            "Epoch: 1884 \tTraining Loss: 0.362648\n",
            "Epoch: 1885 \tTraining Loss: 0.362540\n",
            "Epoch: 1886 \tTraining Loss: 0.362488\n",
            "Epoch: 1887 \tTraining Loss: 0.362444\n",
            "Epoch: 1888 \tTraining Loss: 0.362476\n",
            "Epoch: 1889 \tTraining Loss: 0.362441\n",
            "Epoch: 1890 \tTraining Loss: 0.362373\n",
            "Epoch: 1891 \tTraining Loss: 0.362291\n",
            "Epoch: 1892 \tTraining Loss: 0.362225\n",
            "Epoch: 1893 \tTraining Loss: 0.362120\n",
            "Epoch: 1894 \tTraining Loss: 0.362038\n",
            "Epoch: 1895 \tTraining Loss: 0.361944\n",
            "Epoch: 1896 \tTraining Loss: 0.361848\n",
            "Epoch: 1897 \tTraining Loss: 0.361767\n",
            "Epoch: 1898 \tTraining Loss: 0.361693\n",
            "Epoch: 1899 \tTraining Loss: 0.361635\n",
            "Epoch: 1900 \tTraining Loss: 0.361592\n",
            "Epoch: 1901 \tTraining Loss: 0.361550\n",
            "Epoch: 1902 \tTraining Loss: 0.361519\n",
            "Epoch: 1903 \tTraining Loss: 0.361467\n",
            "Epoch: 1904 \tTraining Loss: 0.361423\n",
            "Epoch: 1905 \tTraining Loss: 0.361367\n",
            "Epoch: 1906 \tTraining Loss: 0.361333\n",
            "Epoch: 1907 \tTraining Loss: 0.361270\n",
            "Epoch: 1908 \tTraining Loss: 0.361225\n",
            "Epoch: 1909 \tTraining Loss: 0.361192\n",
            "Epoch: 1910 \tTraining Loss: 0.361242\n",
            "Epoch: 1911 \tTraining Loss: 0.361392\n",
            "Epoch: 1912 \tTraining Loss: 0.362316\n",
            "Epoch: 1913 \tTraining Loss: 0.363039\n",
            "Epoch: 1914 \tTraining Loss: 0.366932\n",
            "Epoch: 1915 \tTraining Loss: 0.365751\n",
            "Epoch: 1916 \tTraining Loss: 0.372593\n",
            "Epoch: 1917 \tTraining Loss: 0.386952\n",
            "Epoch: 1918 \tTraining Loss: 0.369754\n",
            "Epoch: 1919 \tTraining Loss: 0.375532\n",
            "Epoch: 1920 \tTraining Loss: 0.368236\n",
            "Epoch: 1921 \tTraining Loss: 0.369477\n",
            "Epoch: 1922 \tTraining Loss: 0.369082\n",
            "Epoch: 1923 \tTraining Loss: 0.370404\n",
            "Epoch: 1924 \tTraining Loss: 0.373712\n",
            "Epoch: 1925 \tTraining Loss: 0.369292\n",
            "Epoch: 1926 \tTraining Loss: 0.372642\n",
            "Epoch: 1927 \tTraining Loss: 0.373732\n",
            "Epoch: 1928 \tTraining Loss: 0.365862\n",
            "Epoch: 1929 \tTraining Loss: 0.364012\n",
            "Epoch: 1930 \tTraining Loss: 0.367461\n",
            "Epoch: 1931 \tTraining Loss: 0.365880\n",
            "Epoch: 1932 \tTraining Loss: 0.367425\n",
            "Epoch: 1933 \tTraining Loss: 0.364683\n",
            "Epoch: 1934 \tTraining Loss: 0.363979\n",
            "Epoch: 1935 \tTraining Loss: 0.363552\n",
            "Epoch: 1936 \tTraining Loss: 0.362082\n",
            "Epoch: 1937 \tTraining Loss: 0.363854\n",
            "Epoch: 1938 \tTraining Loss: 0.363001\n",
            "Epoch: 1939 \tTraining Loss: 0.365262\n",
            "Epoch: 1940 \tTraining Loss: 0.360823\n",
            "Epoch: 1941 \tTraining Loss: 0.361425\n",
            "Epoch: 1942 \tTraining Loss: 0.362184\n",
            "Epoch: 1943 \tTraining Loss: 0.362420\n",
            "Epoch: 1944 \tTraining Loss: 0.361744\n",
            "Epoch: 1945 \tTraining Loss: 0.361545\n",
            "Epoch: 1946 \tTraining Loss: 0.362072\n",
            "Epoch: 1947 \tTraining Loss: 0.361594\n",
            "Epoch: 1948 \tTraining Loss: 0.362192\n",
            "Epoch: 1949 \tTraining Loss: 0.360509\n",
            "Epoch: 1950 \tTraining Loss: 0.364298\n",
            "Epoch: 1951 \tTraining Loss: 0.360225\n",
            "Epoch: 1952 \tTraining Loss: 0.361837\n",
            "Epoch: 1953 \tTraining Loss: 0.359895\n",
            "Epoch: 1954 \tTraining Loss: 0.361511\n",
            "Epoch: 1955 \tTraining Loss: 0.360554\n",
            "Epoch: 1956 \tTraining Loss: 0.361082\n",
            "Epoch: 1957 \tTraining Loss: 0.362407\n",
            "Epoch: 1958 \tTraining Loss: 0.360140\n",
            "Epoch: 1959 \tTraining Loss: 0.363678\n",
            "Epoch: 1960 \tTraining Loss: 0.360994\n",
            "Epoch: 1961 \tTraining Loss: 0.370960\n",
            "Epoch: 1962 \tTraining Loss: 0.360958\n",
            "Epoch: 1963 \tTraining Loss: 0.380273\n",
            "Epoch: 1964 \tTraining Loss: 0.362675\n",
            "Epoch: 1965 \tTraining Loss: 0.366202\n",
            "Epoch: 1966 \tTraining Loss: 0.368165\n",
            "Epoch: 1967 \tTraining Loss: 0.363559\n",
            "Epoch: 1968 \tTraining Loss: 0.367868\n",
            "Epoch: 1969 \tTraining Loss: 0.364805\n",
            "Epoch: 1970 \tTraining Loss: 0.362925\n",
            "Epoch: 1971 \tTraining Loss: 0.370284\n",
            "Epoch: 1972 \tTraining Loss: 0.361944\n",
            "Epoch: 1973 \tTraining Loss: 0.363310\n",
            "Epoch: 1974 \tTraining Loss: 0.362864\n",
            "Epoch: 1975 \tTraining Loss: 0.365831\n",
            "Epoch: 1976 \tTraining Loss: 0.362706\n",
            "Epoch: 1977 \tTraining Loss: 0.366164\n",
            "Epoch: 1978 \tTraining Loss: 0.361008\n",
            "Epoch: 1979 \tTraining Loss: 0.367633\n",
            "Epoch: 1980 \tTraining Loss: 0.362020\n",
            "Epoch: 1981 \tTraining Loss: 0.360674\n",
            "Epoch: 1982 \tTraining Loss: 0.366812\n",
            "Epoch: 1983 \tTraining Loss: 0.362193\n",
            "Epoch: 1984 \tTraining Loss: 0.362121\n",
            "Epoch: 1985 \tTraining Loss: 0.361726\n",
            "Epoch: 1986 \tTraining Loss: 0.361091\n",
            "Epoch: 1987 \tTraining Loss: 0.361185\n",
            "Epoch: 1988 \tTraining Loss: 0.360750\n",
            "Epoch: 1989 \tTraining Loss: 0.359292\n",
            "Epoch: 1990 \tTraining Loss: 0.358220\n",
            "Epoch: 1991 \tTraining Loss: 0.359658\n",
            "Epoch: 1992 \tTraining Loss: 0.358783\n",
            "Epoch: 1993 \tTraining Loss: 0.358324\n",
            "Epoch: 1994 \tTraining Loss: 0.358412\n",
            "Epoch: 1995 \tTraining Loss: 0.358393\n",
            "Epoch: 1996 \tTraining Loss: 0.358594\n",
            "Epoch: 1997 \tTraining Loss: 0.356955\n",
            "Epoch: 1998 \tTraining Loss: 0.358333\n",
            "Epoch: 1999 \tTraining Loss: 0.357094\n",
            "Epoch: 2000 \tTraining Loss: 0.357903\n",
            "Epoch: 2001 \tTraining Loss: 0.356797\n",
            "Epoch: 2002 \tTraining Loss: 0.356996\n",
            "Epoch: 2003 \tTraining Loss: 0.356669\n",
            "Epoch: 2004 \tTraining Loss: 0.356739\n",
            "Epoch: 2005 \tTraining Loss: 0.356533\n",
            "Epoch: 2006 \tTraining Loss: 0.356136\n",
            "Epoch: 2007 \tTraining Loss: 0.356430\n",
            "Epoch: 2008 \tTraining Loss: 0.356251\n",
            "Epoch: 2009 \tTraining Loss: 0.356626\n",
            "Epoch: 2010 \tTraining Loss: 0.355871\n",
            "Epoch: 2011 \tTraining Loss: 0.355920\n",
            "Epoch: 2012 \tTraining Loss: 0.355703\n",
            "Epoch: 2013 \tTraining Loss: 0.355617\n",
            "Epoch: 2014 \tTraining Loss: 0.355428\n",
            "Epoch: 2015 \tTraining Loss: 0.355483\n",
            "Epoch: 2016 \tTraining Loss: 0.355315\n",
            "Epoch: 2017 \tTraining Loss: 0.355374\n",
            "Epoch: 2018 \tTraining Loss: 0.355154\n",
            "Epoch: 2019 \tTraining Loss: 0.355113\n",
            "Epoch: 2020 \tTraining Loss: 0.354974\n",
            "Epoch: 2021 \tTraining Loss: 0.354946\n",
            "Epoch: 2022 \tTraining Loss: 0.354889\n",
            "Epoch: 2023 \tTraining Loss: 0.354793\n",
            "Epoch: 2024 \tTraining Loss: 0.354718\n",
            "Epoch: 2025 \tTraining Loss: 0.354616\n",
            "Epoch: 2026 \tTraining Loss: 0.354621\n",
            "Epoch: 2027 \tTraining Loss: 0.354495\n",
            "Epoch: 2028 \tTraining Loss: 0.354429\n",
            "Epoch: 2029 \tTraining Loss: 0.354381\n",
            "Epoch: 2030 \tTraining Loss: 0.354325\n",
            "Epoch: 2031 \tTraining Loss: 0.354257\n",
            "Epoch: 2032 \tTraining Loss: 0.354193\n",
            "Epoch: 2033 \tTraining Loss: 0.354166\n",
            "Epoch: 2034 \tTraining Loss: 0.354086\n",
            "Epoch: 2035 \tTraining Loss: 0.354029\n",
            "Epoch: 2036 \tTraining Loss: 0.353953\n",
            "Epoch: 2037 \tTraining Loss: 0.353907\n",
            "Epoch: 2038 \tTraining Loss: 0.353865\n",
            "Epoch: 2039 \tTraining Loss: 0.353807\n",
            "Epoch: 2040 \tTraining Loss: 0.353739\n",
            "Epoch: 2041 \tTraining Loss: 0.353672\n",
            "Epoch: 2042 \tTraining Loss: 0.353624\n",
            "Epoch: 2043 \tTraining Loss: 0.353564\n",
            "Epoch: 2044 \tTraining Loss: 0.353518\n",
            "Epoch: 2045 \tTraining Loss: 0.353458\n",
            "Epoch: 2046 \tTraining Loss: 0.353400\n",
            "Epoch: 2047 \tTraining Loss: 0.353342\n",
            "Epoch: 2048 \tTraining Loss: 0.353289\n",
            "Epoch: 2049 \tTraining Loss: 0.353240\n",
            "Epoch: 2050 \tTraining Loss: 0.353187\n",
            "Epoch: 2051 \tTraining Loss: 0.353130\n",
            "Epoch: 2052 \tTraining Loss: 0.353070\n",
            "Epoch: 2053 \tTraining Loss: 0.353019\n",
            "Epoch: 2054 \tTraining Loss: 0.352965\n",
            "Epoch: 2055 \tTraining Loss: 0.352912\n",
            "Epoch: 2056 \tTraining Loss: 0.352857\n",
            "Epoch: 2057 \tTraining Loss: 0.352802\n",
            "Epoch: 2058 \tTraining Loss: 0.352747\n",
            "Epoch: 2059 \tTraining Loss: 0.352691\n",
            "Epoch: 2060 \tTraining Loss: 0.352637\n",
            "Epoch: 2061 \tTraining Loss: 0.352581\n",
            "Epoch: 2062 \tTraining Loss: 0.352527\n",
            "Epoch: 2063 \tTraining Loss: 0.352468\n",
            "Epoch: 2064 \tTraining Loss: 0.352396\n",
            "Epoch: 2065 \tTraining Loss: 0.352333\n",
            "Epoch: 2066 \tTraining Loss: 0.352272\n",
            "Epoch: 2067 \tTraining Loss: 0.352198\n",
            "Epoch: 2068 \tTraining Loss: 0.352138\n",
            "Epoch: 2069 \tTraining Loss: 0.352074\n",
            "Epoch: 2070 \tTraining Loss: 0.352017\n",
            "Epoch: 2071 \tTraining Loss: 0.351962\n",
            "Epoch: 2072 \tTraining Loss: 0.351909\n",
            "Epoch: 2073 \tTraining Loss: 0.351858\n",
            "Epoch: 2074 \tTraining Loss: 0.351809\n",
            "Epoch: 2075 \tTraining Loss: 0.351762\n",
            "Epoch: 2076 \tTraining Loss: 0.351713\n",
            "Epoch: 2077 \tTraining Loss: 0.351657\n",
            "Epoch: 2078 \tTraining Loss: 0.351611\n",
            "Epoch: 2079 \tTraining Loss: 0.351551\n",
            "Epoch: 2080 \tTraining Loss: 0.351503\n",
            "Epoch: 2081 \tTraining Loss: 0.351444\n",
            "Epoch: 2082 \tTraining Loss: 0.351396\n",
            "Epoch: 2083 \tTraining Loss: 0.351337\n",
            "Epoch: 2084 \tTraining Loss: 0.351290\n",
            "Epoch: 2085 \tTraining Loss: 0.351232\n",
            "Epoch: 2086 \tTraining Loss: 0.351186\n",
            "Epoch: 2087 \tTraining Loss: 0.351129\n",
            "Epoch: 2088 \tTraining Loss: 0.351081\n",
            "Epoch: 2089 \tTraining Loss: 0.351028\n",
            "Epoch: 2090 \tTraining Loss: 0.350983\n",
            "Epoch: 2091 \tTraining Loss: 0.350928\n",
            "Epoch: 2092 \tTraining Loss: 0.350884\n",
            "Epoch: 2093 \tTraining Loss: 0.350825\n",
            "Epoch: 2094 \tTraining Loss: 0.350779\n",
            "Epoch: 2095 \tTraining Loss: 0.350721\n",
            "Epoch: 2096 \tTraining Loss: 0.350674\n",
            "Epoch: 2097 \tTraining Loss: 0.350620\n",
            "Epoch: 2098 \tTraining Loss: 0.350577\n",
            "Epoch: 2099 \tTraining Loss: 0.350522\n",
            "Epoch: 2100 \tTraining Loss: 0.350482\n",
            "Epoch: 2101 \tTraining Loss: 0.350433\n",
            "Epoch: 2102 \tTraining Loss: 0.350404\n",
            "Epoch: 2103 \tTraining Loss: 0.350372\n",
            "Epoch: 2104 \tTraining Loss: 0.350376\n",
            "Epoch: 2105 \tTraining Loss: 0.350402\n",
            "Epoch: 2106 \tTraining Loss: 0.350533\n",
            "Epoch: 2107 \tTraining Loss: 0.350770\n",
            "Epoch: 2108 \tTraining Loss: 0.351400\n",
            "Epoch: 2109 \tTraining Loss: 0.352367\n",
            "Epoch: 2110 \tTraining Loss: 0.354998\n",
            "Epoch: 2111 \tTraining Loss: 0.357521\n",
            "Epoch: 2112 \tTraining Loss: 0.365466\n",
            "Epoch: 2113 \tTraining Loss: 0.361656\n",
            "Epoch: 2114 \tTraining Loss: 0.360548\n",
            "Epoch: 2115 \tTraining Loss: 0.351272\n",
            "Epoch: 2116 \tTraining Loss: 0.354062\n",
            "Epoch: 2117 \tTraining Loss: 0.362707\n",
            "Epoch: 2118 \tTraining Loss: 0.354072\n",
            "Epoch: 2119 \tTraining Loss: 0.350999\n",
            "Epoch: 2120 \tTraining Loss: 0.356385\n",
            "Epoch: 2121 \tTraining Loss: 0.353872\n",
            "Epoch: 2122 \tTraining Loss: 0.350373\n",
            "Epoch: 2123 \tTraining Loss: 0.351762\n",
            "Epoch: 2124 \tTraining Loss: 0.353034\n",
            "Epoch: 2125 \tTraining Loss: 0.350934\n",
            "Epoch: 2126 \tTraining Loss: 0.349958\n",
            "Epoch: 2127 \tTraining Loss: 0.352367\n",
            "Epoch: 2128 \tTraining Loss: 0.351971\n",
            "Epoch: 2129 \tTraining Loss: 0.350099\n",
            "Epoch: 2130 \tTraining Loss: 0.352072\n",
            "Epoch: 2131 \tTraining Loss: 0.353091\n",
            "Epoch: 2132 \tTraining Loss: 0.350940\n",
            "Epoch: 2133 \tTraining Loss: 0.351176\n",
            "Epoch: 2134 \tTraining Loss: 0.351556\n",
            "Epoch: 2135 \tTraining Loss: 0.350501\n",
            "Epoch: 2136 \tTraining Loss: 0.349824\n",
            "Epoch: 2137 \tTraining Loss: 0.350897\n",
            "Epoch: 2138 \tTraining Loss: 0.349862\n",
            "Epoch: 2139 \tTraining Loss: 0.349247\n",
            "Epoch: 2140 \tTraining Loss: 0.349950\n",
            "Epoch: 2141 \tTraining Loss: 0.349385\n",
            "Epoch: 2142 \tTraining Loss: 0.348925\n",
            "Epoch: 2143 \tTraining Loss: 0.349570\n",
            "Epoch: 2144 \tTraining Loss: 0.349137\n",
            "Epoch: 2145 \tTraining Loss: 0.349562\n",
            "Epoch: 2146 \tTraining Loss: 0.348798\n",
            "Epoch: 2147 \tTraining Loss: 0.348991\n",
            "Epoch: 2148 \tTraining Loss: 0.348797\n",
            "Epoch: 2149 \tTraining Loss: 0.348371\n",
            "Epoch: 2150 \tTraining Loss: 0.349477\n",
            "Epoch: 2151 \tTraining Loss: 0.348479\n",
            "Epoch: 2152 \tTraining Loss: 0.348486\n",
            "Epoch: 2153 \tTraining Loss: 0.348304\n",
            "Epoch: 2154 \tTraining Loss: 0.348530\n",
            "Epoch: 2155 \tTraining Loss: 0.348141\n",
            "Epoch: 2156 \tTraining Loss: 0.348101\n",
            "Epoch: 2157 \tTraining Loss: 0.348183\n",
            "Epoch: 2158 \tTraining Loss: 0.347896\n",
            "Epoch: 2159 \tTraining Loss: 0.347942\n",
            "Epoch: 2160 \tTraining Loss: 0.347894\n",
            "Epoch: 2161 \tTraining Loss: 0.348129\n",
            "Epoch: 2162 \tTraining Loss: 0.347681\n",
            "Epoch: 2163 \tTraining Loss: 0.348424\n",
            "Epoch: 2164 \tTraining Loss: 0.347685\n",
            "Epoch: 2165 \tTraining Loss: 0.347995\n",
            "Epoch: 2166 \tTraining Loss: 0.347474\n",
            "Epoch: 2167 \tTraining Loss: 0.348251\n",
            "Epoch: 2168 \tTraining Loss: 0.347523\n",
            "Epoch: 2169 \tTraining Loss: 0.347861\n",
            "Epoch: 2170 \tTraining Loss: 0.347343\n",
            "Epoch: 2171 \tTraining Loss: 0.347690\n",
            "Epoch: 2172 \tTraining Loss: 0.347448\n",
            "Epoch: 2173 \tTraining Loss: 0.348135\n",
            "Epoch: 2174 \tTraining Loss: 0.347130\n",
            "Epoch: 2175 \tTraining Loss: 0.348309\n",
            "Epoch: 2176 \tTraining Loss: 0.348218\n",
            "Epoch: 2177 \tTraining Loss: 0.347014\n",
            "Epoch: 2178 \tTraining Loss: 0.347256\n",
            "Epoch: 2179 \tTraining Loss: 0.347075\n",
            "Epoch: 2180 \tTraining Loss: 0.346897\n",
            "Epoch: 2181 \tTraining Loss: 0.347280\n",
            "Epoch: 2182 \tTraining Loss: 0.346862\n",
            "Epoch: 2183 \tTraining Loss: 0.347490\n",
            "Epoch: 2184 \tTraining Loss: 0.346675\n",
            "Epoch: 2185 \tTraining Loss: 0.347301\n",
            "Epoch: 2186 \tTraining Loss: 0.346578\n",
            "Epoch: 2187 \tTraining Loss: 0.346813\n",
            "Epoch: 2188 \tTraining Loss: 0.346523\n",
            "Epoch: 2189 \tTraining Loss: 0.346572\n",
            "Epoch: 2190 \tTraining Loss: 0.346378\n",
            "Epoch: 2191 \tTraining Loss: 0.346299\n",
            "Epoch: 2192 \tTraining Loss: 0.346319\n",
            "Epoch: 2193 \tTraining Loss: 0.346167\n",
            "Epoch: 2194 \tTraining Loss: 0.346193\n",
            "Epoch: 2195 \tTraining Loss: 0.346058\n",
            "Epoch: 2196 \tTraining Loss: 0.346020\n",
            "Epoch: 2197 \tTraining Loss: 0.345941\n",
            "Epoch: 2198 \tTraining Loss: 0.345962\n",
            "Epoch: 2199 \tTraining Loss: 0.345898\n",
            "Epoch: 2200 \tTraining Loss: 0.345840\n",
            "Epoch: 2201 \tTraining Loss: 0.345757\n",
            "Epoch: 2202 \tTraining Loss: 0.345680\n",
            "Epoch: 2203 \tTraining Loss: 0.345703\n",
            "Epoch: 2204 \tTraining Loss: 0.345569\n",
            "Epoch: 2205 \tTraining Loss: 0.345650\n",
            "Epoch: 2206 \tTraining Loss: 0.345550\n",
            "Epoch: 2207 \tTraining Loss: 0.345513\n",
            "Epoch: 2208 \tTraining Loss: 0.345418\n",
            "Epoch: 2209 \tTraining Loss: 0.345308\n",
            "Epoch: 2210 \tTraining Loss: 0.345310\n",
            "Epoch: 2211 \tTraining Loss: 0.345216\n",
            "Epoch: 2212 \tTraining Loss: 0.345170\n",
            "Epoch: 2213 \tTraining Loss: 0.345132\n",
            "Epoch: 2214 \tTraining Loss: 0.345075\n",
            "Epoch: 2215 \tTraining Loss: 0.345018\n",
            "Epoch: 2216 \tTraining Loss: 0.344970\n",
            "Epoch: 2217 \tTraining Loss: 0.344921\n",
            "Epoch: 2218 \tTraining Loss: 0.344886\n",
            "Epoch: 2219 \tTraining Loss: 0.344833\n",
            "Epoch: 2220 \tTraining Loss: 0.344782\n",
            "Epoch: 2221 \tTraining Loss: 0.344747\n",
            "Epoch: 2222 \tTraining Loss: 0.344709\n",
            "Epoch: 2223 \tTraining Loss: 0.344675\n",
            "Epoch: 2224 \tTraining Loss: 0.344639\n",
            "Epoch: 2225 \tTraining Loss: 0.344612\n",
            "Epoch: 2226 \tTraining Loss: 0.344582\n",
            "Epoch: 2227 \tTraining Loss: 0.344567\n",
            "Epoch: 2228 \tTraining Loss: 0.344526\n",
            "Epoch: 2229 \tTraining Loss: 0.344481\n",
            "Epoch: 2230 \tTraining Loss: 0.344408\n",
            "Epoch: 2231 \tTraining Loss: 0.344340\n",
            "Epoch: 2232 \tTraining Loss: 0.344263\n",
            "Epoch: 2233 \tTraining Loss: 0.344191\n",
            "Epoch: 2234 \tTraining Loss: 0.344119\n",
            "Epoch: 2235 \tTraining Loss: 0.344058\n",
            "Epoch: 2236 \tTraining Loss: 0.344005\n",
            "Epoch: 2237 \tTraining Loss: 0.343954\n",
            "Epoch: 2238 \tTraining Loss: 0.343909\n",
            "Epoch: 2239 \tTraining Loss: 0.343864\n",
            "Epoch: 2240 \tTraining Loss: 0.343819\n",
            "Epoch: 2241 \tTraining Loss: 0.343777\n",
            "Epoch: 2242 \tTraining Loss: 0.343736\n",
            "Epoch: 2243 \tTraining Loss: 0.343696\n",
            "Epoch: 2244 \tTraining Loss: 0.343659\n",
            "Epoch: 2245 \tTraining Loss: 0.343627\n",
            "Epoch: 2246 \tTraining Loss: 0.343598\n",
            "Epoch: 2247 \tTraining Loss: 0.343572\n",
            "Epoch: 2248 \tTraining Loss: 0.343540\n",
            "Epoch: 2249 \tTraining Loss: 0.343505\n",
            "Epoch: 2250 \tTraining Loss: 0.343475\n",
            "Epoch: 2251 \tTraining Loss: 0.343428\n",
            "Epoch: 2252 \tTraining Loss: 0.343381\n",
            "Epoch: 2253 \tTraining Loss: 0.343318\n",
            "Epoch: 2254 \tTraining Loss: 0.343247\n",
            "Epoch: 2255 \tTraining Loss: 0.343176\n",
            "Epoch: 2256 \tTraining Loss: 0.343102\n",
            "Epoch: 2257 \tTraining Loss: 0.343036\n",
            "Epoch: 2258 \tTraining Loss: 0.342976\n",
            "Epoch: 2259 \tTraining Loss: 0.342923\n",
            "Epoch: 2260 \tTraining Loss: 0.342873\n",
            "Epoch: 2261 \tTraining Loss: 0.342827\n",
            "Epoch: 2262 \tTraining Loss: 0.342781\n",
            "Epoch: 2263 \tTraining Loss: 0.342737\n",
            "Epoch: 2264 \tTraining Loss: 0.342692\n",
            "Epoch: 2265 \tTraining Loss: 0.342648\n",
            "Epoch: 2266 \tTraining Loss: 0.342599\n",
            "Epoch: 2267 \tTraining Loss: 0.342553\n",
            "Epoch: 2268 \tTraining Loss: 0.342504\n",
            "Epoch: 2269 \tTraining Loss: 0.342458\n",
            "Epoch: 2270 \tTraining Loss: 0.342410\n",
            "Epoch: 2271 \tTraining Loss: 0.342367\n",
            "Epoch: 2272 \tTraining Loss: 0.342320\n",
            "Epoch: 2273 \tTraining Loss: 0.342280\n",
            "Epoch: 2274 \tTraining Loss: 0.342236\n",
            "Epoch: 2275 \tTraining Loss: 0.342203\n",
            "Epoch: 2276 \tTraining Loss: 0.342168\n",
            "Epoch: 2277 \tTraining Loss: 0.342152\n",
            "Epoch: 2278 \tTraining Loss: 0.342147\n",
            "Epoch: 2279 \tTraining Loss: 0.342169\n",
            "Epoch: 2280 \tTraining Loss: 0.342211\n",
            "Epoch: 2281 \tTraining Loss: 0.342235\n",
            "Epoch: 2282 \tTraining Loss: 0.342243\n",
            "Epoch: 2283 \tTraining Loss: 0.342142\n",
            "Epoch: 2284 \tTraining Loss: 0.342022\n",
            "Epoch: 2285 \tTraining Loss: 0.341909\n",
            "Epoch: 2286 \tTraining Loss: 0.341817\n",
            "Epoch: 2287 \tTraining Loss: 0.341791\n",
            "Epoch: 2288 \tTraining Loss: 0.341791\n",
            "Epoch: 2289 \tTraining Loss: 0.341843\n",
            "Epoch: 2290 \tTraining Loss: 0.341883\n",
            "Epoch: 2291 \tTraining Loss: 0.341983\n",
            "Epoch: 2292 \tTraining Loss: 0.342043\n",
            "Epoch: 2293 \tTraining Loss: 0.342239\n",
            "Epoch: 2294 \tTraining Loss: 0.342365\n",
            "Epoch: 2295 \tTraining Loss: 0.342784\n",
            "Epoch: 2296 \tTraining Loss: 0.342997\n",
            "Epoch: 2297 \tTraining Loss: 0.343715\n",
            "Epoch: 2298 \tTraining Loss: 0.343797\n",
            "Epoch: 2299 \tTraining Loss: 0.344422\n",
            "Epoch: 2300 \tTraining Loss: 0.343774\n",
            "Epoch: 2301 \tTraining Loss: 0.343229\n",
            "Epoch: 2302 \tTraining Loss: 0.341908\n",
            "Epoch: 2303 \tTraining Loss: 0.341083\n",
            "Epoch: 2304 \tTraining Loss: 0.341036\n",
            "Epoch: 2305 \tTraining Loss: 0.341502\n",
            "Epoch: 2306 \tTraining Loss: 0.342020\n",
            "Epoch: 2307 \tTraining Loss: 0.341950\n",
            "Epoch: 2308 \tTraining Loss: 0.341601\n",
            "Epoch: 2309 \tTraining Loss: 0.341018\n",
            "Epoch: 2310 \tTraining Loss: 0.340690\n",
            "Epoch: 2311 \tTraining Loss: 0.340758\n",
            "Epoch: 2312 \tTraining Loss: 0.340971\n",
            "Epoch: 2313 \tTraining Loss: 0.341123\n",
            "Epoch: 2314 \tTraining Loss: 0.340981\n",
            "Epoch: 2315 \tTraining Loss: 0.340785\n",
            "Epoch: 2316 \tTraining Loss: 0.340506\n",
            "Epoch: 2317 \tTraining Loss: 0.340537\n",
            "Epoch: 2318 \tTraining Loss: 0.340636\n",
            "Epoch: 2319 \tTraining Loss: 0.340714\n",
            "Epoch: 2320 \tTraining Loss: 0.340496\n",
            "Epoch: 2321 \tTraining Loss: 0.341197\n",
            "Epoch: 2322 \tTraining Loss: 0.340755\n",
            "Epoch: 2323 \tTraining Loss: 0.340602\n",
            "Epoch: 2324 \tTraining Loss: 0.340346\n",
            "Epoch: 2325 \tTraining Loss: 0.340088\n",
            "Epoch: 2326 \tTraining Loss: 0.340132\n",
            "Epoch: 2327 \tTraining Loss: 0.340174\n",
            "Epoch: 2328 \tTraining Loss: 0.340103\n",
            "Epoch: 2329 \tTraining Loss: 0.340090\n",
            "Epoch: 2330 \tTraining Loss: 0.339939\n",
            "Epoch: 2331 \tTraining Loss: 0.339820\n",
            "Epoch: 2332 \tTraining Loss: 0.339679\n",
            "Epoch: 2333 \tTraining Loss: 0.339638\n",
            "Epoch: 2334 \tTraining Loss: 0.339643\n",
            "Epoch: 2335 \tTraining Loss: 0.339673\n",
            "Epoch: 2336 \tTraining Loss: 0.339583\n",
            "Epoch: 2337 \tTraining Loss: 0.339558\n",
            "Epoch: 2338 \tTraining Loss: 0.339550\n",
            "Epoch: 2339 \tTraining Loss: 0.339454\n",
            "Epoch: 2340 \tTraining Loss: 0.339439\n",
            "Epoch: 2341 \tTraining Loss: 0.339394\n",
            "Epoch: 2342 \tTraining Loss: 0.339378\n",
            "Epoch: 2343 \tTraining Loss: 0.339213\n",
            "Epoch: 2344 \tTraining Loss: 0.339436\n",
            "Epoch: 2345 \tTraining Loss: 0.339470\n",
            "Epoch: 2346 \tTraining Loss: 0.339370\n",
            "Epoch: 2347 \tTraining Loss: 0.339090\n",
            "Epoch: 2348 \tTraining Loss: 0.339092\n",
            "Epoch: 2349 \tTraining Loss: 0.339186\n",
            "Epoch: 2350 \tTraining Loss: 0.339313\n",
            "Epoch: 2351 \tTraining Loss: 0.339284\n",
            "Epoch: 2352 \tTraining Loss: 0.338981\n",
            "Epoch: 2353 \tTraining Loss: 0.339046\n",
            "Epoch: 2354 \tTraining Loss: 0.338974\n",
            "Epoch: 2355 \tTraining Loss: 0.338783\n",
            "Epoch: 2356 \tTraining Loss: 0.338803\n",
            "Epoch: 2357 \tTraining Loss: 0.338665\n",
            "Epoch: 2358 \tTraining Loss: 0.338678\n",
            "Epoch: 2359 \tTraining Loss: 0.338616\n",
            "Epoch: 2360 \tTraining Loss: 0.338624\n",
            "Epoch: 2361 \tTraining Loss: 0.338535\n",
            "Epoch: 2362 \tTraining Loss: 0.338610\n",
            "Epoch: 2363 \tTraining Loss: 0.338630\n",
            "Epoch: 2364 \tTraining Loss: 0.338732\n",
            "Epoch: 2365 \tTraining Loss: 0.338628\n",
            "Epoch: 2366 \tTraining Loss: 0.338897\n",
            "Epoch: 2367 \tTraining Loss: 0.339045\n",
            "Epoch: 2368 \tTraining Loss: 0.339510\n",
            "Epoch: 2369 \tTraining Loss: 0.339965\n",
            "Epoch: 2370 \tTraining Loss: 0.341692\n",
            "Epoch: 2371 \tTraining Loss: 0.342893\n",
            "Epoch: 2372 \tTraining Loss: 0.344144\n",
            "Epoch: 2373 \tTraining Loss: 0.348987\n",
            "Epoch: 2374 \tTraining Loss: 0.346357\n",
            "Epoch: 2375 \tTraining Loss: 0.347115\n",
            "Epoch: 2376 \tTraining Loss: 0.340036\n",
            "Epoch: 2377 \tTraining Loss: 0.339334\n",
            "Epoch: 2378 \tTraining Loss: 0.343318\n",
            "Epoch: 2379 \tTraining Loss: 0.341778\n",
            "Epoch: 2380 \tTraining Loss: 0.338821\n",
            "Epoch: 2381 \tTraining Loss: 0.338383\n",
            "Epoch: 2382 \tTraining Loss: 0.340043\n",
            "Epoch: 2383 \tTraining Loss: 0.341358\n",
            "Epoch: 2384 \tTraining Loss: 0.338864\n",
            "Epoch: 2385 \tTraining Loss: 0.338191\n",
            "Epoch: 2386 \tTraining Loss: 0.339410\n",
            "Epoch: 2387 \tTraining Loss: 0.339545\n",
            "Epoch: 2388 \tTraining Loss: 0.338072\n",
            "Epoch: 2389 \tTraining Loss: 0.337837\n",
            "Epoch: 2390 \tTraining Loss: 0.338468\n",
            "Epoch: 2391 \tTraining Loss: 0.338443\n",
            "Epoch: 2392 \tTraining Loss: 0.337579\n",
            "Epoch: 2393 \tTraining Loss: 0.337756\n",
            "Epoch: 2394 \tTraining Loss: 0.337992\n",
            "Epoch: 2395 \tTraining Loss: 0.338137\n",
            "Epoch: 2396 \tTraining Loss: 0.337602\n",
            "Epoch: 2397 \tTraining Loss: 0.337503\n",
            "Epoch: 2398 \tTraining Loss: 0.337494\n",
            "Epoch: 2399 \tTraining Loss: 0.337252\n",
            "Epoch: 2400 \tTraining Loss: 0.336905\n",
            "Epoch: 2401 \tTraining Loss: 0.337303\n",
            "Epoch: 2402 \tTraining Loss: 0.337103\n",
            "Epoch: 2403 \tTraining Loss: 0.337578\n",
            "Epoch: 2404 \tTraining Loss: 0.336765\n",
            "Epoch: 2405 \tTraining Loss: 0.337829\n",
            "Epoch: 2406 \tTraining Loss: 0.336813\n",
            "Epoch: 2407 \tTraining Loss: 0.337633\n",
            "Epoch: 2408 \tTraining Loss: 0.336462\n",
            "Epoch: 2409 \tTraining Loss: 0.336655\n",
            "Epoch: 2410 \tTraining Loss: 0.336716\n",
            "Epoch: 2411 \tTraining Loss: 0.336512\n",
            "Epoch: 2412 \tTraining Loss: 0.336734\n",
            "Epoch: 2413 \tTraining Loss: 0.336406\n",
            "Epoch: 2414 \tTraining Loss: 0.337326\n",
            "Epoch: 2415 \tTraining Loss: 0.336286\n",
            "Epoch: 2416 \tTraining Loss: 0.336919\n",
            "Epoch: 2417 \tTraining Loss: 0.336093\n",
            "Epoch: 2418 \tTraining Loss: 0.336538\n",
            "Epoch: 2419 \tTraining Loss: 0.336051\n",
            "Epoch: 2420 \tTraining Loss: 0.337070\n",
            "Epoch: 2421 \tTraining Loss: 0.336516\n",
            "Epoch: 2422 \tTraining Loss: 0.336992\n",
            "Epoch: 2423 \tTraining Loss: 0.337385\n",
            "Epoch: 2424 \tTraining Loss: 0.335882\n",
            "Epoch: 2425 \tTraining Loss: 0.337477\n",
            "Epoch: 2426 \tTraining Loss: 0.337557\n",
            "Epoch: 2427 \tTraining Loss: 0.337557\n",
            "Epoch: 2428 \tTraining Loss: 0.337435\n",
            "Epoch: 2429 \tTraining Loss: 0.337253\n",
            "Epoch: 2430 \tTraining Loss: 0.336869\n",
            "Epoch: 2431 \tTraining Loss: 0.335636\n",
            "Epoch: 2432 \tTraining Loss: 0.336118\n",
            "Epoch: 2433 \tTraining Loss: 0.335861\n",
            "Epoch: 2434 \tTraining Loss: 0.335540\n",
            "Epoch: 2435 \tTraining Loss: 0.336313\n",
            "Epoch: 2436 \tTraining Loss: 0.335722\n",
            "Epoch: 2437 \tTraining Loss: 0.336088\n",
            "Epoch: 2438 \tTraining Loss: 0.335641\n",
            "Epoch: 2439 \tTraining Loss: 0.335259\n",
            "Epoch: 2440 \tTraining Loss: 0.336191\n",
            "Epoch: 2441 \tTraining Loss: 0.335342\n",
            "Epoch: 2442 \tTraining Loss: 0.335841\n",
            "Epoch: 2443 \tTraining Loss: 0.335398\n",
            "Epoch: 2444 \tTraining Loss: 0.334993\n",
            "Epoch: 2445 \tTraining Loss: 0.335898\n",
            "Epoch: 2446 \tTraining Loss: 0.334971\n",
            "Epoch: 2447 \tTraining Loss: 0.335219\n",
            "Epoch: 2448 \tTraining Loss: 0.335478\n",
            "Epoch: 2449 \tTraining Loss: 0.334764\n",
            "Epoch: 2450 \tTraining Loss: 0.335985\n",
            "Epoch: 2451 \tTraining Loss: 0.335860\n",
            "Epoch: 2452 \tTraining Loss: 0.335342\n",
            "Epoch: 2453 \tTraining Loss: 0.335507\n",
            "Epoch: 2454 \tTraining Loss: 0.335380\n",
            "Epoch: 2455 \tTraining Loss: 0.334758\n",
            "Epoch: 2456 \tTraining Loss: 0.335822\n",
            "Epoch: 2457 \tTraining Loss: 0.335614\n",
            "Epoch: 2458 \tTraining Loss: 0.334772\n",
            "Epoch: 2459 \tTraining Loss: 0.334895\n",
            "Epoch: 2460 \tTraining Loss: 0.334834\n",
            "Epoch: 2461 \tTraining Loss: 0.334513\n",
            "Epoch: 2462 \tTraining Loss: 0.334674\n",
            "Epoch: 2463 \tTraining Loss: 0.334448\n",
            "Epoch: 2464 \tTraining Loss: 0.334555\n",
            "Epoch: 2465 \tTraining Loss: 0.334516\n",
            "Epoch: 2466 \tTraining Loss: 0.334403\n",
            "Epoch: 2467 \tTraining Loss: 0.334270\n",
            "Epoch: 2468 \tTraining Loss: 0.334253\n",
            "Epoch: 2469 \tTraining Loss: 0.334190\n",
            "Epoch: 2470 \tTraining Loss: 0.334112\n",
            "Epoch: 2471 \tTraining Loss: 0.333923\n",
            "Epoch: 2472 \tTraining Loss: 0.333689\n",
            "Epoch: 2473 \tTraining Loss: 0.333716\n",
            "Epoch: 2474 \tTraining Loss: 0.333674\n",
            "Epoch: 2475 \tTraining Loss: 0.333706\n",
            "Epoch: 2476 \tTraining Loss: 0.333604\n",
            "Epoch: 2477 \tTraining Loss: 0.333669\n",
            "Epoch: 2478 \tTraining Loss: 0.333484\n",
            "Epoch: 2479 \tTraining Loss: 0.333430\n",
            "Epoch: 2480 \tTraining Loss: 0.333278\n",
            "Epoch: 2481 \tTraining Loss: 0.333278\n",
            "Epoch: 2482 \tTraining Loss: 0.333209\n",
            "Epoch: 2483 \tTraining Loss: 0.333203\n",
            "Epoch: 2484 \tTraining Loss: 0.333210\n",
            "Epoch: 2485 \tTraining Loss: 0.333073\n",
            "Epoch: 2486 \tTraining Loss: 0.333024\n",
            "Epoch: 2487 \tTraining Loss: 0.332961\n",
            "Epoch: 2488 \tTraining Loss: 0.332892\n",
            "Epoch: 2489 \tTraining Loss: 0.332880\n",
            "Epoch: 2490 \tTraining Loss: 0.332822\n",
            "Epoch: 2491 \tTraining Loss: 0.332790\n",
            "Epoch: 2492 \tTraining Loss: 0.332750\n",
            "Epoch: 2493 \tTraining Loss: 0.332686\n",
            "Epoch: 2494 \tTraining Loss: 0.332649\n",
            "Epoch: 2495 \tTraining Loss: 0.332579\n",
            "Epoch: 2496 \tTraining Loss: 0.332526\n",
            "Epoch: 2497 \tTraining Loss: 0.332504\n",
            "Epoch: 2498 \tTraining Loss: 0.332453\n",
            "Epoch: 2499 \tTraining Loss: 0.332412\n",
            "Epoch: 2500 \tTraining Loss: 0.332375\n",
            "Epoch: 2501 \tTraining Loss: 0.332313\n",
            "Epoch: 2502 \tTraining Loss: 0.332267\n",
            "Epoch: 2503 \tTraining Loss: 0.332223\n",
            "Epoch: 2504 \tTraining Loss: 0.332168\n",
            "Epoch: 2505 \tTraining Loss: 0.332124\n",
            "Epoch: 2506 \tTraining Loss: 0.332081\n",
            "Epoch: 2507 \tTraining Loss: 0.332034\n",
            "Epoch: 2508 \tTraining Loss: 0.331996\n",
            "Epoch: 2509 \tTraining Loss: 0.331953\n",
            "Epoch: 2510 \tTraining Loss: 0.331903\n",
            "Epoch: 2511 \tTraining Loss: 0.331858\n",
            "Epoch: 2512 \tTraining Loss: 0.331813\n",
            "Epoch: 2513 \tTraining Loss: 0.331764\n",
            "Epoch: 2514 \tTraining Loss: 0.331720\n",
            "Epoch: 2515 \tTraining Loss: 0.331677\n",
            "Epoch: 2516 \tTraining Loss: 0.331630\n",
            "Epoch: 2517 \tTraining Loss: 0.331586\n",
            "Epoch: 2518 \tTraining Loss: 0.331544\n",
            "Epoch: 2519 \tTraining Loss: 0.331500\n",
            "Epoch: 2520 \tTraining Loss: 0.331457\n",
            "Epoch: 2521 \tTraining Loss: 0.331417\n",
            "Epoch: 2522 \tTraining Loss: 0.331378\n",
            "Epoch: 2523 \tTraining Loss: 0.331341\n",
            "Epoch: 2524 \tTraining Loss: 0.331313\n",
            "Epoch: 2525 \tTraining Loss: 0.331299\n",
            "Epoch: 2526 \tTraining Loss: 0.331316\n",
            "Epoch: 2527 \tTraining Loss: 0.331381\n",
            "Epoch: 2528 \tTraining Loss: 0.331589\n",
            "Epoch: 2529 \tTraining Loss: 0.331970\n",
            "Epoch: 2530 \tTraining Loss: 0.333092\n",
            "Epoch: 2531 \tTraining Loss: 0.334853\n",
            "Epoch: 2532 \tTraining Loss: 0.341395\n",
            "Epoch: 2533 \tTraining Loss: 0.347908\n",
            "Epoch: 2534 \tTraining Loss: 0.404876\n",
            "Epoch: 2535 \tTraining Loss: 0.658273\n",
            "Epoch: 2536 \tTraining Loss: 0.611742\n",
            "Epoch: 2537 \tTraining Loss: 0.432524\n",
            "Epoch: 2538 \tTraining Loss: 0.545327\n",
            "Epoch: 2539 \tTraining Loss: 0.434782\n",
            "Epoch: 2540 \tTraining Loss: 0.506641\n",
            "Epoch: 2541 \tTraining Loss: 0.439048\n",
            "Epoch: 2542 \tTraining Loss: 0.533392\n",
            "Epoch: 2543 \tTraining Loss: 0.423766\n",
            "Epoch: 2544 \tTraining Loss: 0.441456\n",
            "Epoch: 2545 \tTraining Loss: 0.447787\n",
            "Epoch: 2546 \tTraining Loss: 0.422906\n",
            "Epoch: 2547 \tTraining Loss: 0.439228\n",
            "Epoch: 2548 \tTraining Loss: 0.384117\n",
            "Epoch: 2549 \tTraining Loss: 0.397121\n",
            "Epoch: 2550 \tTraining Loss: 0.386476\n",
            "Epoch: 2551 \tTraining Loss: 0.399356\n",
            "Epoch: 2552 \tTraining Loss: 0.370860\n",
            "Epoch: 2553 \tTraining Loss: 0.371550\n",
            "Epoch: 2554 \tTraining Loss: 0.375795\n",
            "Epoch: 2555 \tTraining Loss: 0.377520\n",
            "Epoch: 2556 \tTraining Loss: 0.368787\n",
            "Epoch: 2557 \tTraining Loss: 0.371831\n",
            "Epoch: 2558 \tTraining Loss: 0.364705\n",
            "Epoch: 2559 \tTraining Loss: 0.365258\n",
            "Epoch: 2560 \tTraining Loss: 0.359188\n",
            "Epoch: 2561 \tTraining Loss: 0.358305\n",
            "Epoch: 2562 \tTraining Loss: 0.354505\n",
            "Epoch: 2563 \tTraining Loss: 0.355352\n",
            "Epoch: 2564 \tTraining Loss: 0.353375\n",
            "Epoch: 2565 \tTraining Loss: 0.349045\n",
            "Epoch: 2566 \tTraining Loss: 0.349028\n",
            "Epoch: 2567 \tTraining Loss: 0.348790\n",
            "Epoch: 2568 \tTraining Loss: 0.343919\n",
            "Epoch: 2569 \tTraining Loss: 0.345498\n",
            "Epoch: 2570 \tTraining Loss: 0.342193\n",
            "Epoch: 2571 \tTraining Loss: 0.342229\n",
            "Epoch: 2572 \tTraining Loss: 0.339609\n",
            "Epoch: 2573 \tTraining Loss: 0.340773\n",
            "Epoch: 2574 \tTraining Loss: 0.339360\n",
            "Epoch: 2575 \tTraining Loss: 0.338453\n",
            "Epoch: 2576 \tTraining Loss: 0.338219\n",
            "Epoch: 2577 \tTraining Loss: 0.337981\n",
            "Epoch: 2578 \tTraining Loss: 0.337183\n",
            "Epoch: 2579 \tTraining Loss: 0.336286\n",
            "Epoch: 2580 \tTraining Loss: 0.335614\n",
            "Epoch: 2581 \tTraining Loss: 0.335892\n",
            "Epoch: 2582 \tTraining Loss: 0.335308\n",
            "Epoch: 2583 \tTraining Loss: 0.334951\n",
            "Epoch: 2584 \tTraining Loss: 0.334426\n",
            "Epoch: 2585 \tTraining Loss: 0.334364\n",
            "Epoch: 2586 \tTraining Loss: 0.334221\n",
            "Epoch: 2587 \tTraining Loss: 0.333730\n",
            "Epoch: 2588 \tTraining Loss: 0.333768\n",
            "Epoch: 2589 \tTraining Loss: 0.333324\n",
            "Epoch: 2590 \tTraining Loss: 0.333167\n",
            "Epoch: 2591 \tTraining Loss: 0.332860\n",
            "Epoch: 2592 \tTraining Loss: 0.332811\n",
            "Epoch: 2593 \tTraining Loss: 0.332734\n",
            "Epoch: 2594 \tTraining Loss: 0.332460\n",
            "Epoch: 2595 \tTraining Loss: 0.332446\n",
            "Epoch: 2596 \tTraining Loss: 0.332234\n",
            "Epoch: 2597 \tTraining Loss: 0.332195\n",
            "Epoch: 2598 \tTraining Loss: 0.331925\n",
            "Epoch: 2599 \tTraining Loss: 0.331871\n",
            "Epoch: 2600 \tTraining Loss: 0.331798\n",
            "Epoch: 2601 \tTraining Loss: 0.331718\n",
            "Epoch: 2602 \tTraining Loss: 0.331664\n",
            "Epoch: 2603 \tTraining Loss: 0.331531\n",
            "Epoch: 2604 \tTraining Loss: 0.331475\n",
            "Epoch: 2605 \tTraining Loss: 0.331322\n",
            "Epoch: 2606 \tTraining Loss: 0.331273\n",
            "Epoch: 2607 \tTraining Loss: 0.331194\n",
            "Epoch: 2608 \tTraining Loss: 0.331127\n",
            "Epoch: 2609 \tTraining Loss: 0.331060\n",
            "Epoch: 2610 \tTraining Loss: 0.330983\n",
            "Epoch: 2611 \tTraining Loss: 0.330927\n",
            "Epoch: 2612 \tTraining Loss: 0.330829\n",
            "Epoch: 2613 \tTraining Loss: 0.330790\n",
            "Epoch: 2614 \tTraining Loss: 0.330733\n",
            "Epoch: 2615 \tTraining Loss: 0.330685\n",
            "Epoch: 2616 \tTraining Loss: 0.330630\n",
            "Epoch: 2617 \tTraining Loss: 0.330580\n",
            "Epoch: 2618 \tTraining Loss: 0.330538\n",
            "Epoch: 2619 \tTraining Loss: 0.330473\n",
            "Epoch: 2620 \tTraining Loss: 0.330435\n",
            "Epoch: 2621 \tTraining Loss: 0.330380\n",
            "Epoch: 2622 \tTraining Loss: 0.330344\n",
            "Epoch: 2623 \tTraining Loss: 0.330294\n",
            "Epoch: 2624 \tTraining Loss: 0.330258\n",
            "Epoch: 2625 \tTraining Loss: 0.330216\n",
            "Epoch: 2626 \tTraining Loss: 0.330168\n",
            "Epoch: 2627 \tTraining Loss: 0.330127\n",
            "Epoch: 2628 \tTraining Loss: 0.330084\n",
            "Epoch: 2629 \tTraining Loss: 0.330051\n",
            "Epoch: 2630 \tTraining Loss: 0.330008\n",
            "Epoch: 2631 \tTraining Loss: 0.329972\n",
            "Epoch: 2632 \tTraining Loss: 0.329927\n",
            "Epoch: 2633 \tTraining Loss: 0.329892\n",
            "Epoch: 2634 \tTraining Loss: 0.329854\n",
            "Epoch: 2635 \tTraining Loss: 0.329818\n",
            "Epoch: 2636 \tTraining Loss: 0.329779\n",
            "Epoch: 2637 \tTraining Loss: 0.329742\n",
            "Epoch: 2638 \tTraining Loss: 0.329706\n",
            "Epoch: 2639 \tTraining Loss: 0.329669\n",
            "Epoch: 2640 \tTraining Loss: 0.329634\n",
            "Epoch: 2641 \tTraining Loss: 0.329598\n",
            "Epoch: 2642 \tTraining Loss: 0.329563\n",
            "Epoch: 2643 \tTraining Loss: 0.329527\n",
            "Epoch: 2644 \tTraining Loss: 0.329493\n",
            "Epoch: 2645 \tTraining Loss: 0.329459\n",
            "Epoch: 2646 \tTraining Loss: 0.329424\n",
            "Epoch: 2647 \tTraining Loss: 0.329391\n",
            "Epoch: 2648 \tTraining Loss: 0.329356\n",
            "Epoch: 2649 \tTraining Loss: 0.329323\n",
            "Epoch: 2650 \tTraining Loss: 0.329289\n",
            "Epoch: 2651 \tTraining Loss: 0.329256\n",
            "Epoch: 2652 \tTraining Loss: 0.329223\n",
            "Epoch: 2653 \tTraining Loss: 0.329190\n",
            "Epoch: 2654 \tTraining Loss: 0.329157\n",
            "Epoch: 2655 \tTraining Loss: 0.329125\n",
            "Epoch: 2656 \tTraining Loss: 0.329092\n",
            "Epoch: 2657 \tTraining Loss: 0.329060\n",
            "Epoch: 2658 \tTraining Loss: 0.329028\n",
            "Epoch: 2659 \tTraining Loss: 0.328995\n",
            "Epoch: 2660 \tTraining Loss: 0.328964\n",
            "Epoch: 2661 \tTraining Loss: 0.328932\n",
            "Epoch: 2662 \tTraining Loss: 0.328900\n",
            "Epoch: 2663 \tTraining Loss: 0.328868\n",
            "Epoch: 2664 \tTraining Loss: 0.328837\n",
            "Epoch: 2665 \tTraining Loss: 0.328805\n",
            "Epoch: 2666 \tTraining Loss: 0.328774\n",
            "Epoch: 2667 \tTraining Loss: 0.328742\n",
            "Epoch: 2668 \tTraining Loss: 0.328711\n",
            "Epoch: 2669 \tTraining Loss: 0.328680\n",
            "Epoch: 2670 \tTraining Loss: 0.328649\n",
            "Epoch: 2671 \tTraining Loss: 0.328618\n",
            "Epoch: 2672 \tTraining Loss: 0.328587\n",
            "Epoch: 2673 \tTraining Loss: 0.328556\n",
            "Epoch: 2674 \tTraining Loss: 0.328526\n",
            "Epoch: 2675 \tTraining Loss: 0.328495\n",
            "Epoch: 2676 \tTraining Loss: 0.328464\n",
            "Epoch: 2677 \tTraining Loss: 0.328434\n",
            "Epoch: 2678 \tTraining Loss: 0.328404\n",
            "Epoch: 2679 \tTraining Loss: 0.328373\n",
            "Epoch: 2680 \tTraining Loss: 0.328343\n",
            "Epoch: 2681 \tTraining Loss: 0.328313\n",
            "Epoch: 2682 \tTraining Loss: 0.328282\n",
            "Epoch: 2683 \tTraining Loss: 0.328252\n",
            "Epoch: 2684 \tTraining Loss: 0.328222\n",
            "Epoch: 2685 \tTraining Loss: 0.328192\n",
            "Epoch: 2686 \tTraining Loss: 0.328162\n",
            "Epoch: 2687 \tTraining Loss: 0.328132\n",
            "Epoch: 2688 \tTraining Loss: 0.328102\n",
            "Epoch: 2689 \tTraining Loss: 0.328073\n",
            "Epoch: 2690 \tTraining Loss: 0.328043\n",
            "Epoch: 2691 \tTraining Loss: 0.328013\n",
            "Epoch: 2692 \tTraining Loss: 0.327983\n",
            "Epoch: 2693 \tTraining Loss: 0.327954\n",
            "Epoch: 2694 \tTraining Loss: 0.327924\n",
            "Epoch: 2695 \tTraining Loss: 0.327895\n",
            "Epoch: 2696 \tTraining Loss: 0.327865\n",
            "Epoch: 2697 \tTraining Loss: 0.327836\n",
            "Epoch: 2698 \tTraining Loss: 0.327806\n",
            "Epoch: 2699 \tTraining Loss: 0.327777\n",
            "Epoch: 2700 \tTraining Loss: 0.327747\n",
            "Epoch: 2701 \tTraining Loss: 0.327718\n",
            "Epoch: 2702 \tTraining Loss: 0.327689\n",
            "Epoch: 2703 \tTraining Loss: 0.327660\n",
            "Epoch: 2704 \tTraining Loss: 0.327630\n",
            "Epoch: 2705 \tTraining Loss: 0.327601\n",
            "Epoch: 2706 \tTraining Loss: 0.327572\n",
            "Epoch: 2707 \tTraining Loss: 0.327543\n",
            "Epoch: 2708 \tTraining Loss: 0.327514\n",
            "Epoch: 2709 \tTraining Loss: 0.327485\n",
            "Epoch: 2710 \tTraining Loss: 0.327456\n",
            "Epoch: 2711 \tTraining Loss: 0.327427\n",
            "Epoch: 2712 \tTraining Loss: 0.327398\n",
            "Epoch: 2713 \tTraining Loss: 0.327369\n",
            "Epoch: 2714 \tTraining Loss: 0.327340\n",
            "Epoch: 2715 \tTraining Loss: 0.327311\n",
            "Epoch: 2716 \tTraining Loss: 0.327282\n",
            "Epoch: 2717 \tTraining Loss: 0.327253\n",
            "Epoch: 2718 \tTraining Loss: 0.327225\n",
            "Epoch: 2719 \tTraining Loss: 0.327196\n",
            "Epoch: 2720 \tTraining Loss: 0.327167\n",
            "Epoch: 2721 \tTraining Loss: 0.327138\n",
            "Epoch: 2722 \tTraining Loss: 0.327110\n",
            "Epoch: 2723 \tTraining Loss: 0.327081\n",
            "Epoch: 2724 \tTraining Loss: 0.327052\n",
            "Epoch: 2725 \tTraining Loss: 0.327024\n",
            "Epoch: 2726 \tTraining Loss: 0.326995\n",
            "Epoch: 2727 \tTraining Loss: 0.326966\n",
            "Epoch: 2728 \tTraining Loss: 0.326938\n",
            "Epoch: 2729 \tTraining Loss: 0.326909\n",
            "Epoch: 2730 \tTraining Loss: 0.326881\n",
            "Epoch: 2731 \tTraining Loss: 0.326852\n",
            "Epoch: 2732 \tTraining Loss: 0.326824\n",
            "Epoch: 2733 \tTraining Loss: 0.326795\n",
            "Epoch: 2734 \tTraining Loss: 0.326767\n",
            "Epoch: 2735 \tTraining Loss: 0.326738\n",
            "Epoch: 2736 \tTraining Loss: 0.326710\n",
            "Epoch: 2737 \tTraining Loss: 0.326681\n",
            "Epoch: 2738 \tTraining Loss: 0.326653\n",
            "Epoch: 2739 \tTraining Loss: 0.326625\n",
            "Epoch: 2740 \tTraining Loss: 0.326596\n",
            "Epoch: 2741 \tTraining Loss: 0.326568\n",
            "Epoch: 2742 \tTraining Loss: 0.326540\n",
            "Epoch: 2743 \tTraining Loss: 0.326511\n",
            "Epoch: 2744 \tTraining Loss: 0.326483\n",
            "Epoch: 2745 \tTraining Loss: 0.326455\n",
            "Epoch: 2746 \tTraining Loss: 0.326427\n",
            "Epoch: 2747 \tTraining Loss: 0.326398\n",
            "Epoch: 2748 \tTraining Loss: 0.326370\n",
            "Epoch: 2749 \tTraining Loss: 0.326342\n",
            "Epoch: 2750 \tTraining Loss: 0.326314\n",
            "Epoch: 2751 \tTraining Loss: 0.326286\n",
            "Epoch: 2752 \tTraining Loss: 0.326258\n",
            "Epoch: 2753 \tTraining Loss: 0.326230\n",
            "Epoch: 2754 \tTraining Loss: 0.326202\n",
            "Epoch: 2755 \tTraining Loss: 0.326175\n",
            "Epoch: 2756 \tTraining Loss: 0.326148\n",
            "Epoch: 2757 \tTraining Loss: 0.326121\n",
            "Epoch: 2758 \tTraining Loss: 0.326096\n",
            "Epoch: 2759 \tTraining Loss: 0.326069\n",
            "Epoch: 2760 \tTraining Loss: 0.326043\n",
            "Epoch: 2761 \tTraining Loss: 0.326013\n",
            "Epoch: 2762 \tTraining Loss: 0.325985\n",
            "Epoch: 2763 \tTraining Loss: 0.325954\n",
            "Epoch: 2764 \tTraining Loss: 0.325926\n",
            "Epoch: 2765 \tTraining Loss: 0.325896\n",
            "Epoch: 2766 \tTraining Loss: 0.325868\n",
            "Epoch: 2767 \tTraining Loss: 0.325840\n",
            "Epoch: 2768 \tTraining Loss: 0.325813\n",
            "Epoch: 2769 \tTraining Loss: 0.325786\n",
            "Epoch: 2770 \tTraining Loss: 0.325759\n",
            "Epoch: 2771 \tTraining Loss: 0.325732\n",
            "Epoch: 2772 \tTraining Loss: 0.325706\n",
            "Epoch: 2773 \tTraining Loss: 0.325677\n",
            "Epoch: 2774 \tTraining Loss: 0.325650\n",
            "Epoch: 2775 \tTraining Loss: 0.325620\n",
            "Epoch: 2776 \tTraining Loss: 0.325592\n",
            "Epoch: 2777 \tTraining Loss: 0.325563\n",
            "Epoch: 2778 \tTraining Loss: 0.325535\n",
            "Epoch: 2779 \tTraining Loss: 0.325507\n",
            "Epoch: 2780 \tTraining Loss: 0.325480\n",
            "Epoch: 2781 \tTraining Loss: 0.325452\n",
            "Epoch: 2782 \tTraining Loss: 0.325426\n",
            "Epoch: 2783 \tTraining Loss: 0.325398\n",
            "Epoch: 2784 \tTraining Loss: 0.325372\n",
            "Epoch: 2785 \tTraining Loss: 0.325343\n",
            "Epoch: 2786 \tTraining Loss: 0.325316\n",
            "Epoch: 2787 \tTraining Loss: 0.325286\n",
            "Epoch: 2788 \tTraining Loss: 0.325259\n",
            "Epoch: 2789 \tTraining Loss: 0.325230\n",
            "Epoch: 2790 \tTraining Loss: 0.325203\n",
            "Epoch: 2791 \tTraining Loss: 0.325175\n",
            "Epoch: 2792 \tTraining Loss: 0.325148\n",
            "Epoch: 2793 \tTraining Loss: 0.325120\n",
            "Epoch: 2794 \tTraining Loss: 0.325094\n",
            "Epoch: 2795 \tTraining Loss: 0.325065\n",
            "Epoch: 2796 \tTraining Loss: 0.325038\n",
            "Epoch: 2797 \tTraining Loss: 0.325009\n",
            "Epoch: 2798 \tTraining Loss: 0.324982\n",
            "Epoch: 2799 \tTraining Loss: 0.324952\n",
            "Epoch: 2800 \tTraining Loss: 0.324925\n",
            "Epoch: 2801 \tTraining Loss: 0.324897\n",
            "Epoch: 2802 \tTraining Loss: 0.324870\n",
            "Epoch: 2803 \tTraining Loss: 0.324841\n",
            "Epoch: 2804 \tTraining Loss: 0.324815\n",
            "Epoch: 2805 \tTraining Loss: 0.324786\n",
            "Epoch: 2806 \tTraining Loss: 0.324759\n",
            "Epoch: 2807 \tTraining Loss: 0.324729\n",
            "Epoch: 2808 \tTraining Loss: 0.324702\n",
            "Epoch: 2809 \tTraining Loss: 0.324672\n",
            "Epoch: 2810 \tTraining Loss: 0.324645\n",
            "Epoch: 2811 \tTraining Loss: 0.324617\n",
            "Epoch: 2812 \tTraining Loss: 0.324591\n",
            "Epoch: 2813 \tTraining Loss: 0.324562\n",
            "Epoch: 2814 \tTraining Loss: 0.324536\n",
            "Epoch: 2815 \tTraining Loss: 0.324506\n",
            "Epoch: 2816 \tTraining Loss: 0.324480\n",
            "Epoch: 2817 \tTraining Loss: 0.324449\n",
            "Epoch: 2818 \tTraining Loss: 0.324423\n",
            "Epoch: 2819 \tTraining Loss: 0.324394\n",
            "Epoch: 2820 \tTraining Loss: 0.324368\n",
            "Epoch: 2821 \tTraining Loss: 0.324339\n",
            "Epoch: 2822 \tTraining Loss: 0.324314\n",
            "Epoch: 2823 \tTraining Loss: 0.324285\n",
            "Epoch: 2824 \tTraining Loss: 0.324259\n",
            "Epoch: 2825 \tTraining Loss: 0.324228\n",
            "Epoch: 2826 \tTraining Loss: 0.324202\n",
            "Epoch: 2827 \tTraining Loss: 0.324173\n",
            "Epoch: 2828 \tTraining Loss: 0.324147\n",
            "Epoch: 2829 \tTraining Loss: 0.324118\n",
            "Epoch: 2830 \tTraining Loss: 0.324093\n",
            "Epoch: 2831 \tTraining Loss: 0.324063\n",
            "Epoch: 2832 \tTraining Loss: 0.324038\n",
            "Epoch: 2833 \tTraining Loss: 0.324008\n",
            "Epoch: 2834 \tTraining Loss: 0.323981\n",
            "Epoch: 2835 \tTraining Loss: 0.323952\n",
            "Epoch: 2836 \tTraining Loss: 0.323927\n",
            "Epoch: 2837 \tTraining Loss: 0.323898\n",
            "Epoch: 2838 \tTraining Loss: 0.323873\n",
            "Epoch: 2839 \tTraining Loss: 0.323843\n",
            "Epoch: 2840 \tTraining Loss: 0.323818\n",
            "Epoch: 2841 \tTraining Loss: 0.323788\n",
            "Epoch: 2842 \tTraining Loss: 0.323761\n",
            "Epoch: 2843 \tTraining Loss: 0.323732\n",
            "Epoch: 2844 \tTraining Loss: 0.323706\n",
            "Epoch: 2845 \tTraining Loss: 0.323678\n",
            "Epoch: 2846 \tTraining Loss: 0.323653\n",
            "Epoch: 2847 \tTraining Loss: 0.323623\n",
            "Epoch: 2848 \tTraining Loss: 0.323598\n",
            "Epoch: 2849 \tTraining Loss: 0.323568\n",
            "Epoch: 2850 \tTraining Loss: 0.323542\n",
            "Epoch: 2851 \tTraining Loss: 0.323512\n",
            "Epoch: 2852 \tTraining Loss: 0.323487\n",
            "Epoch: 2853 \tTraining Loss: 0.323458\n",
            "Epoch: 2854 \tTraining Loss: 0.323433\n",
            "Epoch: 2855 \tTraining Loss: 0.323404\n",
            "Epoch: 2856 \tTraining Loss: 0.323380\n",
            "Epoch: 2857 \tTraining Loss: 0.323349\n",
            "Epoch: 2858 \tTraining Loss: 0.323324\n",
            "Epoch: 2859 \tTraining Loss: 0.323294\n",
            "Epoch: 2860 \tTraining Loss: 0.323268\n",
            "Epoch: 2861 \tTraining Loss: 0.323238\n",
            "Epoch: 2862 \tTraining Loss: 0.323213\n",
            "Epoch: 2863 \tTraining Loss: 0.323184\n",
            "Epoch: 2864 \tTraining Loss: 0.323160\n",
            "Epoch: 2865 \tTraining Loss: 0.323130\n",
            "Epoch: 2866 \tTraining Loss: 0.323106\n",
            "Epoch: 2867 \tTraining Loss: 0.323076\n",
            "Epoch: 2868 \tTraining Loss: 0.323050\n",
            "Epoch: 2869 \tTraining Loss: 0.323020\n",
            "Epoch: 2870 \tTraining Loss: 0.322995\n",
            "Epoch: 2871 \tTraining Loss: 0.322966\n",
            "Epoch: 2872 \tTraining Loss: 0.322941\n",
            "Epoch: 2873 \tTraining Loss: 0.322911\n",
            "Epoch: 2874 \tTraining Loss: 0.322887\n",
            "Epoch: 2875 \tTraining Loss: 0.322857\n",
            "Epoch: 2876 \tTraining Loss: 0.322832\n",
            "Epoch: 2877 \tTraining Loss: 0.322803\n",
            "Epoch: 2878 \tTraining Loss: 0.322778\n",
            "Epoch: 2879 \tTraining Loss: 0.322748\n",
            "Epoch: 2880 \tTraining Loss: 0.322723\n",
            "Epoch: 2881 \tTraining Loss: 0.322693\n",
            "Epoch: 2882 \tTraining Loss: 0.322669\n",
            "Epoch: 2883 \tTraining Loss: 0.322639\n",
            "Epoch: 2884 \tTraining Loss: 0.322614\n",
            "Epoch: 2885 \tTraining Loss: 0.322584\n",
            "Epoch: 2886 \tTraining Loss: 0.322560\n",
            "Epoch: 2887 \tTraining Loss: 0.322531\n",
            "Epoch: 2888 \tTraining Loss: 0.322507\n",
            "Epoch: 2889 \tTraining Loss: 0.322476\n",
            "Epoch: 2890 \tTraining Loss: 0.322452\n",
            "Epoch: 2891 \tTraining Loss: 0.322422\n",
            "Epoch: 2892 \tTraining Loss: 0.322397\n",
            "Epoch: 2893 \tTraining Loss: 0.322367\n",
            "Epoch: 2894 \tTraining Loss: 0.322343\n",
            "Epoch: 2895 \tTraining Loss: 0.322313\n",
            "Epoch: 2896 \tTraining Loss: 0.322289\n",
            "Epoch: 2897 \tTraining Loss: 0.322259\n",
            "Epoch: 2898 \tTraining Loss: 0.322235\n",
            "Epoch: 2899 \tTraining Loss: 0.322205\n",
            "Epoch: 2900 \tTraining Loss: 0.322181\n",
            "Epoch: 2901 \tTraining Loss: 0.322150\n",
            "Epoch: 2902 \tTraining Loss: 0.322126\n",
            "Epoch: 2903 \tTraining Loss: 0.322096\n",
            "Epoch: 2904 \tTraining Loss: 0.322072\n",
            "Epoch: 2905 \tTraining Loss: 0.322042\n",
            "Epoch: 2906 \tTraining Loss: 0.322017\n",
            "Epoch: 2907 \tTraining Loss: 0.321988\n",
            "Epoch: 2908 \tTraining Loss: 0.321964\n",
            "Epoch: 2909 \tTraining Loss: 0.321934\n",
            "Epoch: 2910 \tTraining Loss: 0.321910\n",
            "Epoch: 2911 \tTraining Loss: 0.321880\n",
            "Epoch: 2912 \tTraining Loss: 0.321856\n",
            "Epoch: 2913 \tTraining Loss: 0.321826\n",
            "Epoch: 2914 \tTraining Loss: 0.321801\n",
            "Epoch: 2915 \tTraining Loss: 0.321771\n",
            "Epoch: 2916 \tTraining Loss: 0.321747\n",
            "Epoch: 2917 \tTraining Loss: 0.321717\n",
            "Epoch: 2918 \tTraining Loss: 0.321693\n",
            "Epoch: 2919 \tTraining Loss: 0.321663\n",
            "Epoch: 2920 \tTraining Loss: 0.321640\n",
            "Epoch: 2921 \tTraining Loss: 0.321609\n",
            "Epoch: 2922 \tTraining Loss: 0.321586\n",
            "Epoch: 2923 \tTraining Loss: 0.321556\n",
            "Epoch: 2924 \tTraining Loss: 0.321532\n",
            "Epoch: 2925 \tTraining Loss: 0.321501\n",
            "Epoch: 2926 \tTraining Loss: 0.321478\n",
            "Epoch: 2927 \tTraining Loss: 0.321447\n",
            "Epoch: 2928 \tTraining Loss: 0.321423\n",
            "Epoch: 2929 \tTraining Loss: 0.321393\n",
            "Epoch: 2930 \tTraining Loss: 0.321370\n",
            "Epoch: 2931 \tTraining Loss: 0.321340\n",
            "Epoch: 2932 \tTraining Loss: 0.321316\n",
            "Epoch: 2933 \tTraining Loss: 0.321286\n",
            "Epoch: 2934 \tTraining Loss: 0.321262\n",
            "Epoch: 2935 \tTraining Loss: 0.321232\n",
            "Epoch: 2936 \tTraining Loss: 0.321208\n",
            "Epoch: 2937 \tTraining Loss: 0.321178\n",
            "Epoch: 2938 \tTraining Loss: 0.321154\n",
            "Epoch: 2939 \tTraining Loss: 0.321124\n",
            "Epoch: 2940 \tTraining Loss: 0.321101\n",
            "Epoch: 2941 \tTraining Loss: 0.321070\n",
            "Epoch: 2942 \tTraining Loss: 0.321047\n",
            "Epoch: 2943 \tTraining Loss: 0.321017\n",
            "Epoch: 2944 \tTraining Loss: 0.320993\n",
            "Epoch: 2945 \tTraining Loss: 0.320963\n",
            "Epoch: 2946 \tTraining Loss: 0.320940\n",
            "Epoch: 2947 \tTraining Loss: 0.320909\n",
            "Epoch: 2948 \tTraining Loss: 0.320886\n",
            "Epoch: 2949 \tTraining Loss: 0.320855\n",
            "Epoch: 2950 \tTraining Loss: 0.320832\n",
            "Epoch: 2951 \tTraining Loss: 0.320802\n",
            "Epoch: 2952 \tTraining Loss: 0.320778\n",
            "Epoch: 2953 \tTraining Loss: 0.320748\n",
            "Epoch: 2954 \tTraining Loss: 0.320725\n",
            "Epoch: 2955 \tTraining Loss: 0.320694\n",
            "Epoch: 2956 \tTraining Loss: 0.320671\n",
            "Epoch: 2957 \tTraining Loss: 0.320641\n",
            "Epoch: 2958 \tTraining Loss: 0.320618\n",
            "Epoch: 2959 \tTraining Loss: 0.320587\n",
            "Epoch: 2960 \tTraining Loss: 0.320564\n",
            "Epoch: 2961 \tTraining Loss: 0.320534\n",
            "Epoch: 2962 \tTraining Loss: 0.320510\n",
            "Epoch: 2963 \tTraining Loss: 0.320480\n",
            "Epoch: 2964 \tTraining Loss: 0.320457\n",
            "Epoch: 2965 \tTraining Loss: 0.320426\n",
            "Epoch: 2966 \tTraining Loss: 0.320403\n",
            "Epoch: 2967 \tTraining Loss: 0.320373\n",
            "Epoch: 2968 \tTraining Loss: 0.320350\n",
            "Epoch: 2969 \tTraining Loss: 0.320320\n",
            "Epoch: 2970 \tTraining Loss: 0.320297\n",
            "Epoch: 2971 \tTraining Loss: 0.320266\n",
            "Epoch: 2972 \tTraining Loss: 0.320243\n",
            "Epoch: 2973 \tTraining Loss: 0.320212\n",
            "Epoch: 2974 \tTraining Loss: 0.320190\n",
            "Epoch: 2975 \tTraining Loss: 0.320159\n",
            "Epoch: 2976 \tTraining Loss: 0.320136\n",
            "Epoch: 2977 \tTraining Loss: 0.320105\n",
            "Epoch: 2978 \tTraining Loss: 0.320083\n",
            "Epoch: 2979 \tTraining Loss: 0.320052\n",
            "Epoch: 2980 \tTraining Loss: 0.320029\n",
            "Epoch: 2981 \tTraining Loss: 0.319999\n",
            "Epoch: 2982 \tTraining Loss: 0.319976\n",
            "Epoch: 2983 \tTraining Loss: 0.319945\n",
            "Epoch: 2984 \tTraining Loss: 0.319923\n",
            "Epoch: 2985 \tTraining Loss: 0.319892\n",
            "Epoch: 2986 \tTraining Loss: 0.319869\n",
            "Epoch: 2987 \tTraining Loss: 0.319839\n",
            "Epoch: 2988 \tTraining Loss: 0.319816\n",
            "Epoch: 2989 \tTraining Loss: 0.319785\n",
            "Epoch: 2990 \tTraining Loss: 0.319763\n",
            "Epoch: 2991 \tTraining Loss: 0.319732\n",
            "Epoch: 2992 \tTraining Loss: 0.319709\n",
            "Epoch: 2993 \tTraining Loss: 0.319679\n",
            "Epoch: 2994 \tTraining Loss: 0.319656\n",
            "Epoch: 2995 \tTraining Loss: 0.319625\n",
            "Epoch: 2996 \tTraining Loss: 0.319603\n",
            "Epoch: 2997 \tTraining Loss: 0.319572\n",
            "Epoch: 2998 \tTraining Loss: 0.319550\n",
            "Epoch: 2999 \tTraining Loss: 0.319519\n",
            "Epoch: 3000 \tTraining Loss: 0.319496\n",
            "Epoch: 3001 \tTraining Loss: 0.319466\n",
            "Epoch: 3002 \tTraining Loss: 0.319443\n",
            "Epoch: 3003 \tTraining Loss: 0.319412\n",
            "Epoch: 3004 \tTraining Loss: 0.319390\n",
            "Epoch: 3005 \tTraining Loss: 0.319359\n",
            "Epoch: 3006 \tTraining Loss: 0.319337\n",
            "Epoch: 3007 \tTraining Loss: 0.319306\n",
            "Epoch: 3008 \tTraining Loss: 0.319284\n",
            "Epoch: 3009 \tTraining Loss: 0.319253\n",
            "Epoch: 3010 \tTraining Loss: 0.319231\n",
            "Epoch: 3011 \tTraining Loss: 0.319200\n",
            "Epoch: 3012 \tTraining Loss: 0.319178\n",
            "Epoch: 3013 \tTraining Loss: 0.319147\n",
            "Epoch: 3014 \tTraining Loss: 0.319125\n",
            "Epoch: 3015 \tTraining Loss: 0.319094\n",
            "Epoch: 3016 \tTraining Loss: 0.319071\n",
            "Epoch: 3017 \tTraining Loss: 0.319041\n",
            "Epoch: 3018 \tTraining Loss: 0.319019\n",
            "Epoch: 3019 \tTraining Loss: 0.318988\n",
            "Epoch: 3020 \tTraining Loss: 0.318966\n",
            "Epoch: 3021 \tTraining Loss: 0.318935\n",
            "Epoch: 3022 \tTraining Loss: 0.318913\n",
            "Epoch: 3023 \tTraining Loss: 0.318881\n",
            "Epoch: 3024 \tTraining Loss: 0.318859\n",
            "Epoch: 3025 \tTraining Loss: 0.318828\n",
            "Epoch: 3026 \tTraining Loss: 0.318806\n",
            "Epoch: 3027 \tTraining Loss: 0.318775\n",
            "Epoch: 3028 \tTraining Loss: 0.318754\n",
            "Epoch: 3029 \tTraining Loss: 0.318723\n",
            "Epoch: 3030 \tTraining Loss: 0.318701\n",
            "Epoch: 3031 \tTraining Loss: 0.318670\n",
            "Epoch: 3032 \tTraining Loss: 0.318648\n",
            "Epoch: 3033 \tTraining Loss: 0.318617\n",
            "Epoch: 3034 \tTraining Loss: 0.318595\n",
            "Epoch: 3035 \tTraining Loss: 0.318564\n",
            "Epoch: 3036 \tTraining Loss: 0.318542\n",
            "Epoch: 3037 \tTraining Loss: 0.318511\n",
            "Epoch: 3038 \tTraining Loss: 0.318489\n",
            "Epoch: 3039 \tTraining Loss: 0.318458\n",
            "Epoch: 3040 \tTraining Loss: 0.318436\n",
            "Epoch: 3041 \tTraining Loss: 0.318405\n",
            "Epoch: 3042 \tTraining Loss: 0.318383\n",
            "Epoch: 3043 \tTraining Loss: 0.318352\n",
            "Epoch: 3044 \tTraining Loss: 0.318330\n",
            "Epoch: 3045 \tTraining Loss: 0.318299\n",
            "Epoch: 3046 \tTraining Loss: 0.318278\n",
            "Epoch: 3047 \tTraining Loss: 0.318247\n",
            "Epoch: 3048 \tTraining Loss: 0.318225\n",
            "Epoch: 3049 \tTraining Loss: 0.318194\n",
            "Epoch: 3050 \tTraining Loss: 0.318172\n",
            "Epoch: 3051 \tTraining Loss: 0.318141\n",
            "Epoch: 3052 \tTraining Loss: 0.318119\n",
            "Epoch: 3053 \tTraining Loss: 0.318088\n",
            "Epoch: 3054 \tTraining Loss: 0.318067\n",
            "Epoch: 3055 \tTraining Loss: 0.318035\n",
            "Epoch: 3056 \tTraining Loss: 0.318014\n",
            "Epoch: 3057 \tTraining Loss: 0.317983\n",
            "Epoch: 3058 \tTraining Loss: 0.317961\n",
            "Epoch: 3059 \tTraining Loss: 0.317930\n",
            "Epoch: 3060 \tTraining Loss: 0.317908\n",
            "Epoch: 3061 \tTraining Loss: 0.317877\n",
            "Epoch: 3062 \tTraining Loss: 0.317856\n",
            "Epoch: 3063 \tTraining Loss: 0.317825\n",
            "Epoch: 3064 \tTraining Loss: 0.317803\n",
            "Epoch: 3065 \tTraining Loss: 0.317772\n",
            "Epoch: 3066 \tTraining Loss: 0.317750\n",
            "Epoch: 3067 \tTraining Loss: 0.317719\n",
            "Epoch: 3068 \tTraining Loss: 0.317698\n",
            "Epoch: 3069 \tTraining Loss: 0.317667\n",
            "Epoch: 3070 \tTraining Loss: 0.317645\n",
            "Epoch: 3071 \tTraining Loss: 0.317614\n",
            "Epoch: 3072 \tTraining Loss: 0.317593\n",
            "Epoch: 3073 \tTraining Loss: 0.317561\n",
            "Epoch: 3074 \tTraining Loss: 0.317540\n",
            "Epoch: 3075 \tTraining Loss: 0.317509\n",
            "Epoch: 3076 \tTraining Loss: 0.317487\n",
            "Epoch: 3077 \tTraining Loss: 0.317456\n",
            "Epoch: 3078 \tTraining Loss: 0.317435\n",
            "Epoch: 3079 \tTraining Loss: 0.317404\n",
            "Epoch: 3080 \tTraining Loss: 0.317382\n",
            "Epoch: 3081 \tTraining Loss: 0.317351\n",
            "Epoch: 3082 \tTraining Loss: 0.317330\n",
            "Epoch: 3083 \tTraining Loss: 0.317298\n",
            "Epoch: 3084 \tTraining Loss: 0.317277\n",
            "Epoch: 3085 \tTraining Loss: 0.317246\n",
            "Epoch: 3086 \tTraining Loss: 0.317225\n",
            "Epoch: 3087 \tTraining Loss: 0.317193\n",
            "Epoch: 3088 \tTraining Loss: 0.317172\n",
            "Epoch: 3089 \tTraining Loss: 0.317141\n",
            "Epoch: 3090 \tTraining Loss: 0.317120\n",
            "Epoch: 3091 \tTraining Loss: 0.317088\n",
            "Epoch: 3092 \tTraining Loss: 0.317067\n",
            "Epoch: 3093 \tTraining Loss: 0.317036\n",
            "Epoch: 3094 \tTraining Loss: 0.317015\n",
            "Epoch: 3095 \tTraining Loss: 0.316984\n",
            "Epoch: 3096 \tTraining Loss: 0.316963\n",
            "Epoch: 3097 \tTraining Loss: 0.316931\n",
            "Epoch: 3098 \tTraining Loss: 0.316910\n",
            "Epoch: 3099 \tTraining Loss: 0.316879\n",
            "Epoch: 3100 \tTraining Loss: 0.316858\n",
            "Epoch: 3101 \tTraining Loss: 0.316826\n",
            "Epoch: 3102 \tTraining Loss: 0.316805\n",
            "Epoch: 3103 \tTraining Loss: 0.316774\n",
            "Epoch: 3104 \tTraining Loss: 0.316753\n",
            "Epoch: 3105 \tTraining Loss: 0.316722\n",
            "Epoch: 3106 \tTraining Loss: 0.316701\n",
            "Epoch: 3107 \tTraining Loss: 0.316669\n",
            "Epoch: 3108 \tTraining Loss: 0.316648\n",
            "Epoch: 3109 \tTraining Loss: 0.316617\n",
            "Epoch: 3110 \tTraining Loss: 0.316596\n",
            "Epoch: 3111 \tTraining Loss: 0.316564\n",
            "Epoch: 3112 \tTraining Loss: 0.316544\n",
            "Epoch: 3113 \tTraining Loss: 0.316512\n",
            "Epoch: 3114 \tTraining Loss: 0.316491\n",
            "Epoch: 3115 \tTraining Loss: 0.316460\n",
            "Epoch: 3116 \tTraining Loss: 0.316439\n",
            "Epoch: 3117 \tTraining Loss: 0.316408\n",
            "Epoch: 3118 \tTraining Loss: 0.316387\n",
            "Epoch: 3119 \tTraining Loss: 0.316355\n",
            "Epoch: 3120 \tTraining Loss: 0.316334\n",
            "Epoch: 3121 \tTraining Loss: 0.316303\n",
            "Epoch: 3122 \tTraining Loss: 0.316282\n",
            "Epoch: 3123 \tTraining Loss: 0.316251\n",
            "Epoch: 3124 \tTraining Loss: 0.316230\n",
            "Epoch: 3125 \tTraining Loss: 0.316198\n",
            "Epoch: 3126 \tTraining Loss: 0.316178\n",
            "Epoch: 3127 \tTraining Loss: 0.316146\n",
            "Epoch: 3128 \tTraining Loss: 0.316126\n",
            "Epoch: 3129 \tTraining Loss: 0.316094\n",
            "Epoch: 3130 \tTraining Loss: 0.316073\n",
            "Epoch: 3131 \tTraining Loss: 0.316042\n",
            "Epoch: 3132 \tTraining Loss: 0.316021\n",
            "Epoch: 3133 \tTraining Loss: 0.315990\n",
            "Epoch: 3134 \tTraining Loss: 0.315969\n",
            "Epoch: 3135 \tTraining Loss: 0.315937\n",
            "Epoch: 3136 \tTraining Loss: 0.315917\n",
            "Epoch: 3137 \tTraining Loss: 0.315885\n",
            "Epoch: 3138 \tTraining Loss: 0.315865\n",
            "Epoch: 3139 \tTraining Loss: 0.315833\n",
            "Epoch: 3140 \tTraining Loss: 0.315813\n",
            "Epoch: 3141 \tTraining Loss: 0.315781\n",
            "Epoch: 3142 \tTraining Loss: 0.315760\n",
            "Epoch: 3143 \tTraining Loss: 0.315729\n",
            "Epoch: 3144 \tTraining Loss: 0.315708\n",
            "Epoch: 3145 \tTraining Loss: 0.315677\n",
            "Epoch: 3146 \tTraining Loss: 0.315656\n",
            "Epoch: 3147 \tTraining Loss: 0.315625\n",
            "Epoch: 3148 \tTraining Loss: 0.315604\n",
            "Epoch: 3149 \tTraining Loss: 0.315573\n",
            "Epoch: 3150 \tTraining Loss: 0.315552\n",
            "Epoch: 3151 \tTraining Loss: 0.315520\n",
            "Epoch: 3152 \tTraining Loss: 0.315500\n",
            "Epoch: 3153 \tTraining Loss: 0.315468\n",
            "Epoch: 3154 \tTraining Loss: 0.315448\n",
            "Epoch: 3155 \tTraining Loss: 0.315416\n",
            "Epoch: 3156 \tTraining Loss: 0.315396\n",
            "Epoch: 3157 \tTraining Loss: 0.315364\n",
            "Epoch: 3158 \tTraining Loss: 0.315344\n",
            "Epoch: 3159 \tTraining Loss: 0.315312\n",
            "Epoch: 3160 \tTraining Loss: 0.315292\n",
            "Epoch: 3161 \tTraining Loss: 0.315260\n",
            "Epoch: 3162 \tTraining Loss: 0.315240\n",
            "Epoch: 3163 \tTraining Loss: 0.315208\n",
            "Epoch: 3164 \tTraining Loss: 0.315188\n",
            "Epoch: 3165 \tTraining Loss: 0.315156\n",
            "Epoch: 3166 \tTraining Loss: 0.315136\n",
            "Epoch: 3167 \tTraining Loss: 0.315104\n",
            "Epoch: 3168 \tTraining Loss: 0.315084\n",
            "Epoch: 3169 \tTraining Loss: 0.315052\n",
            "Epoch: 3170 \tTraining Loss: 0.315032\n",
            "Epoch: 3171 \tTraining Loss: 0.315000\n",
            "Epoch: 3172 \tTraining Loss: 0.314980\n",
            "Epoch: 3173 \tTraining Loss: 0.314948\n",
            "Epoch: 3174 \tTraining Loss: 0.314928\n",
            "Epoch: 3175 \tTraining Loss: 0.314896\n",
            "Epoch: 3176 \tTraining Loss: 0.314876\n",
            "Epoch: 3177 \tTraining Loss: 0.314844\n",
            "Epoch: 3178 \tTraining Loss: 0.314824\n",
            "Epoch: 3179 \tTraining Loss: 0.314793\n",
            "Epoch: 3180 \tTraining Loss: 0.314772\n",
            "Epoch: 3181 \tTraining Loss: 0.314741\n",
            "Epoch: 3182 \tTraining Loss: 0.314720\n",
            "Epoch: 3183 \tTraining Loss: 0.314689\n",
            "Epoch: 3184 \tTraining Loss: 0.314669\n",
            "Epoch: 3185 \tTraining Loss: 0.314637\n",
            "Epoch: 3186 \tTraining Loss: 0.314617\n",
            "Epoch: 3187 \tTraining Loss: 0.314585\n",
            "Epoch: 3188 \tTraining Loss: 0.314565\n",
            "Epoch: 3189 \tTraining Loss: 0.314533\n",
            "Epoch: 3190 \tTraining Loss: 0.314513\n",
            "Epoch: 3191 \tTraining Loss: 0.314481\n",
            "Epoch: 3192 \tTraining Loss: 0.314461\n",
            "Epoch: 3193 \tTraining Loss: 0.314429\n",
            "Epoch: 3194 \tTraining Loss: 0.314409\n",
            "Epoch: 3195 \tTraining Loss: 0.314377\n",
            "Epoch: 3196 \tTraining Loss: 0.314357\n",
            "Epoch: 3197 \tTraining Loss: 0.314326\n",
            "Epoch: 3198 \tTraining Loss: 0.314306\n",
            "Epoch: 3199 \tTraining Loss: 0.314274\n",
            "Epoch: 3200 \tTraining Loss: 0.314254\n",
            "Epoch: 3201 \tTraining Loss: 0.314222\n",
            "Epoch: 3202 \tTraining Loss: 0.314202\n",
            "Epoch: 3203 \tTraining Loss: 0.314170\n",
            "Epoch: 3204 \tTraining Loss: 0.314150\n",
            "Epoch: 3205 \tTraining Loss: 0.314118\n",
            "Epoch: 3206 \tTraining Loss: 0.314098\n",
            "Epoch: 3207 \tTraining Loss: 0.314067\n",
            "Epoch: 3208 \tTraining Loss: 0.314047\n",
            "Epoch: 3209 \tTraining Loss: 0.314015\n",
            "Epoch: 3210 \tTraining Loss: 0.313995\n",
            "Epoch: 3211 \tTraining Loss: 0.313963\n",
            "Epoch: 3212 \tTraining Loss: 0.313943\n",
            "Epoch: 3213 \tTraining Loss: 0.313911\n",
            "Epoch: 3214 \tTraining Loss: 0.313891\n",
            "Epoch: 3215 \tTraining Loss: 0.313860\n",
            "Epoch: 3216 \tTraining Loss: 0.313840\n",
            "Epoch: 3217 \tTraining Loss: 0.313808\n",
            "Epoch: 3218 \tTraining Loss: 0.313788\n",
            "Epoch: 3219 \tTraining Loss: 0.313756\n",
            "Epoch: 3220 \tTraining Loss: 0.313736\n",
            "Epoch: 3221 \tTraining Loss: 0.313704\n",
            "Epoch: 3222 \tTraining Loss: 0.313684\n",
            "Epoch: 3223 \tTraining Loss: 0.313653\n",
            "Epoch: 3224 \tTraining Loss: 0.313633\n",
            "Epoch: 3225 \tTraining Loss: 0.313601\n",
            "Epoch: 3226 \tTraining Loss: 0.313581\n",
            "Epoch: 3227 \tTraining Loss: 0.313549\n",
            "Epoch: 3228 \tTraining Loss: 0.313529\n",
            "Epoch: 3229 \tTraining Loss: 0.313498\n",
            "Epoch: 3230 \tTraining Loss: 0.313478\n",
            "Epoch: 3231 \tTraining Loss: 0.313446\n",
            "Epoch: 3232 \tTraining Loss: 0.313426\n",
            "Epoch: 3233 \tTraining Loss: 0.313394\n",
            "Epoch: 3234 \tTraining Loss: 0.313374\n",
            "Epoch: 3235 \tTraining Loss: 0.313343\n",
            "Epoch: 3236 \tTraining Loss: 0.313323\n",
            "Epoch: 3237 \tTraining Loss: 0.313291\n",
            "Epoch: 3238 \tTraining Loss: 0.313271\n",
            "Epoch: 3239 \tTraining Loss: 0.313239\n",
            "Epoch: 3240 \tTraining Loss: 0.313219\n",
            "Epoch: 3241 \tTraining Loss: 0.313188\n",
            "Epoch: 3242 \tTraining Loss: 0.313168\n",
            "Epoch: 3243 \tTraining Loss: 0.313136\n",
            "Epoch: 3244 \tTraining Loss: 0.313116\n",
            "Epoch: 3245 \tTraining Loss: 0.313084\n",
            "Epoch: 3246 \tTraining Loss: 0.313064\n",
            "Epoch: 3247 \tTraining Loss: 0.313033\n",
            "Epoch: 3248 \tTraining Loss: 0.313013\n",
            "Epoch: 3249 \tTraining Loss: 0.312981\n",
            "Epoch: 3250 \tTraining Loss: 0.312961\n",
            "Epoch: 3251 \tTraining Loss: 0.312929\n",
            "Epoch: 3252 \tTraining Loss: 0.312909\n",
            "Epoch: 3253 \tTraining Loss: 0.312877\n",
            "Epoch: 3254 \tTraining Loss: 0.312857\n",
            "Epoch: 3255 \tTraining Loss: 0.312826\n",
            "Epoch: 3256 \tTraining Loss: 0.312806\n",
            "Epoch: 3257 \tTraining Loss: 0.312774\n",
            "Epoch: 3258 \tTraining Loss: 0.312754\n",
            "Epoch: 3259 \tTraining Loss: 0.312723\n",
            "Epoch: 3260 \tTraining Loss: 0.312703\n",
            "Epoch: 3261 \tTraining Loss: 0.312671\n",
            "Epoch: 3262 \tTraining Loss: 0.312652\n",
            "Epoch: 3263 \tTraining Loss: 0.312620\n",
            "Epoch: 3264 \tTraining Loss: 0.312601\n",
            "Epoch: 3265 \tTraining Loss: 0.312569\n",
            "Epoch: 3266 \tTraining Loss: 0.312550\n",
            "Epoch: 3267 \tTraining Loss: 0.312519\n",
            "Epoch: 3268 \tTraining Loss: 0.312500\n",
            "Epoch: 3269 \tTraining Loss: 0.312468\n",
            "Epoch: 3270 \tTraining Loss: 0.312450\n",
            "Epoch: 3271 \tTraining Loss: 0.312418\n",
            "Epoch: 3272 \tTraining Loss: 0.312400\n",
            "Epoch: 3273 \tTraining Loss: 0.312369\n",
            "Epoch: 3274 \tTraining Loss: 0.312350\n",
            "Epoch: 3275 \tTraining Loss: 0.312319\n",
            "Epoch: 3276 \tTraining Loss: 0.312301\n",
            "Epoch: 3277 \tTraining Loss: 0.312271\n",
            "Epoch: 3278 \tTraining Loss: 0.312252\n",
            "Epoch: 3279 \tTraining Loss: 0.312222\n",
            "Epoch: 3280 \tTraining Loss: 0.312204\n",
            "Epoch: 3281 \tTraining Loss: 0.312174\n",
            "Epoch: 3282 \tTraining Loss: 0.312156\n",
            "Epoch: 3283 \tTraining Loss: 0.312126\n",
            "Epoch: 3284 \tTraining Loss: 0.312108\n",
            "Epoch: 3285 \tTraining Loss: 0.312078\n",
            "Epoch: 3286 \tTraining Loss: 0.312060\n",
            "Epoch: 3287 \tTraining Loss: 0.312029\n",
            "Epoch: 3288 \tTraining Loss: 0.312010\n",
            "Epoch: 3289 \tTraining Loss: 0.311979\n",
            "Epoch: 3290 \tTraining Loss: 0.311959\n",
            "Epoch: 3291 \tTraining Loss: 0.311927\n",
            "Epoch: 3292 \tTraining Loss: 0.311907\n",
            "Epoch: 3293 \tTraining Loss: 0.311874\n",
            "Epoch: 3294 \tTraining Loss: 0.311853\n",
            "Epoch: 3295 \tTraining Loss: 0.311820\n",
            "Epoch: 3296 \tTraining Loss: 0.311799\n",
            "Epoch: 3297 \tTraining Loss: 0.311766\n",
            "Epoch: 3298 \tTraining Loss: 0.311744\n",
            "Epoch: 3299 \tTraining Loss: 0.311712\n",
            "Epoch: 3300 \tTraining Loss: 0.311691\n",
            "Epoch: 3301 \tTraining Loss: 0.311659\n",
            "Epoch: 3302 \tTraining Loss: 0.311638\n",
            "Epoch: 3303 \tTraining Loss: 0.311607\n",
            "Epoch: 3304 \tTraining Loss: 0.311587\n",
            "Epoch: 3305 \tTraining Loss: 0.311556\n",
            "Epoch: 3306 \tTraining Loss: 0.311538\n",
            "Epoch: 3307 \tTraining Loss: 0.311508\n",
            "Epoch: 3308 \tTraining Loss: 0.311490\n",
            "Epoch: 3309 \tTraining Loss: 0.311461\n",
            "Epoch: 3310 \tTraining Loss: 0.311444\n",
            "Epoch: 3311 \tTraining Loss: 0.311416\n",
            "Epoch: 3312 \tTraining Loss: 0.311399\n",
            "Epoch: 3313 \tTraining Loss: 0.311371\n",
            "Epoch: 3314 \tTraining Loss: 0.311354\n",
            "Epoch: 3315 \tTraining Loss: 0.311326\n",
            "Epoch: 3316 \tTraining Loss: 0.311307\n",
            "Epoch: 3317 \tTraining Loss: 0.311277\n",
            "Epoch: 3318 \tTraining Loss: 0.311256\n",
            "Epoch: 3319 \tTraining Loss: 0.311224\n",
            "Epoch: 3320 \tTraining Loss: 0.311200\n",
            "Epoch: 3321 \tTraining Loss: 0.311166\n",
            "Epoch: 3322 \tTraining Loss: 0.311140\n",
            "Epoch: 3323 \tTraining Loss: 0.311104\n",
            "Epoch: 3324 \tTraining Loss: 0.311079\n",
            "Epoch: 3325 \tTraining Loss: 0.311043\n",
            "Epoch: 3326 \tTraining Loss: 0.311018\n",
            "Epoch: 3327 \tTraining Loss: 0.310983\n",
            "Epoch: 3328 \tTraining Loss: 0.310960\n",
            "Epoch: 3329 \tTraining Loss: 0.310926\n",
            "Epoch: 3330 \tTraining Loss: 0.310905\n",
            "Epoch: 3331 \tTraining Loss: 0.310872\n",
            "Epoch: 3332 \tTraining Loss: 0.310852\n",
            "Epoch: 3333 \tTraining Loss: 0.310821\n",
            "Epoch: 3334 \tTraining Loss: 0.310803\n",
            "Epoch: 3335 \tTraining Loss: 0.310773\n",
            "Epoch: 3336 \tTraining Loss: 0.310756\n",
            "Epoch: 3337 \tTraining Loss: 0.310729\n",
            "Epoch: 3338 \tTraining Loss: 0.310714\n",
            "Epoch: 3339 \tTraining Loss: 0.310690\n",
            "Epoch: 3340 \tTraining Loss: 0.310677\n",
            "Epoch: 3341 \tTraining Loss: 0.310656\n",
            "Epoch: 3342 \tTraining Loss: 0.310645\n",
            "Epoch: 3343 \tTraining Loss: 0.310627\n",
            "Epoch: 3344 \tTraining Loss: 0.310615\n",
            "Epoch: 3345 \tTraining Loss: 0.310595\n",
            "Epoch: 3346 \tTraining Loss: 0.310576\n",
            "Epoch: 3347 \tTraining Loss: 0.310547\n",
            "Epoch: 3348 \tTraining Loss: 0.310517\n",
            "Epoch: 3349 \tTraining Loss: 0.310477\n",
            "Epoch: 3350 \tTraining Loss: 0.310439\n",
            "Epoch: 3351 \tTraining Loss: 0.310393\n",
            "Epoch: 3352 \tTraining Loss: 0.310356\n",
            "Epoch: 3353 \tTraining Loss: 0.310312\n",
            "Epoch: 3354 \tTraining Loss: 0.310282\n",
            "Epoch: 3355 \tTraining Loss: 0.310244\n",
            "Epoch: 3356 \tTraining Loss: 0.310220\n",
            "Epoch: 3357 \tTraining Loss: 0.310186\n",
            "Epoch: 3358 \tTraining Loss: 0.310164\n",
            "Epoch: 3359 \tTraining Loss: 0.310133\n",
            "Epoch: 3360 \tTraining Loss: 0.310112\n",
            "Epoch: 3361 \tTraining Loss: 0.310081\n",
            "Epoch: 3362 \tTraining Loss: 0.310060\n",
            "Epoch: 3363 \tTraining Loss: 0.310029\n",
            "Epoch: 3364 \tTraining Loss: 0.310009\n",
            "Epoch: 3365 \tTraining Loss: 0.309977\n",
            "Epoch: 3366 \tTraining Loss: 0.309957\n",
            "Epoch: 3367 \tTraining Loss: 0.309926\n",
            "Epoch: 3368 \tTraining Loss: 0.309906\n",
            "Epoch: 3369 \tTraining Loss: 0.309876\n",
            "Epoch: 3370 \tTraining Loss: 0.309857\n",
            "Epoch: 3371 \tTraining Loss: 0.309828\n",
            "Epoch: 3372 \tTraining Loss: 0.309812\n",
            "Epoch: 3373 \tTraining Loss: 0.309787\n",
            "Epoch: 3374 \tTraining Loss: 0.309776\n",
            "Epoch: 3375 \tTraining Loss: 0.309760\n",
            "Epoch: 3376 \tTraining Loss: 0.309760\n",
            "Epoch: 3377 \tTraining Loss: 0.309761\n",
            "Epoch: 3378 \tTraining Loss: 0.309779\n",
            "Epoch: 3379 \tTraining Loss: 0.309806\n",
            "Epoch: 3380 \tTraining Loss: 0.309829\n",
            "Epoch: 3381 \tTraining Loss: 0.309858\n",
            "Epoch: 3382 \tTraining Loss: 0.309834\n",
            "Epoch: 3383 \tTraining Loss: 0.309797\n",
            "Epoch: 3384 \tTraining Loss: 0.309711\n",
            "Epoch: 3385 \tTraining Loss: 0.309612\n",
            "Epoch: 3386 \tTraining Loss: 0.309519\n",
            "Epoch: 3387 \tTraining Loss: 0.309437\n",
            "Epoch: 3388 \tTraining Loss: 0.309395\n",
            "Epoch: 3389 \tTraining Loss: 0.309363\n",
            "Epoch: 3390 \tTraining Loss: 0.309352\n",
            "Epoch: 3391 \tTraining Loss: 0.309340\n",
            "Epoch: 3392 \tTraining Loss: 0.309341\n",
            "Epoch: 3393 \tTraining Loss: 0.309316\n",
            "Epoch: 3394 \tTraining Loss: 0.309299\n",
            "Epoch: 3395 \tTraining Loss: 0.309258\n",
            "Epoch: 3396 \tTraining Loss: 0.309221\n",
            "Epoch: 3397 \tTraining Loss: 0.309173\n",
            "Epoch: 3398 \tTraining Loss: 0.309142\n",
            "Epoch: 3399 \tTraining Loss: 0.309104\n",
            "Epoch: 3400 \tTraining Loss: 0.309081\n",
            "Epoch: 3401 \tTraining Loss: 0.309055\n",
            "Epoch: 3402 \tTraining Loss: 0.309044\n",
            "Epoch: 3403 \tTraining Loss: 0.309025\n",
            "Epoch: 3404 \tTraining Loss: 0.309017\n",
            "Epoch: 3405 \tTraining Loss: 0.308998\n",
            "Epoch: 3406 \tTraining Loss: 0.308986\n",
            "Epoch: 3407 \tTraining Loss: 0.308957\n",
            "Epoch: 3408 \tTraining Loss: 0.308932\n",
            "Epoch: 3409 \tTraining Loss: 0.308896\n",
            "Epoch: 3410 \tTraining Loss: 0.308866\n",
            "Epoch: 3411 \tTraining Loss: 0.308825\n",
            "Epoch: 3412 \tTraining Loss: 0.308795\n",
            "Epoch: 3413 \tTraining Loss: 0.308756\n",
            "Epoch: 3414 \tTraining Loss: 0.308729\n",
            "Epoch: 3415 \tTraining Loss: 0.308693\n",
            "Epoch: 3416 \tTraining Loss: 0.308670\n",
            "Epoch: 3417 \tTraining Loss: 0.308636\n",
            "Epoch: 3418 \tTraining Loss: 0.308615\n",
            "Epoch: 3419 \tTraining Loss: 0.308584\n",
            "Epoch: 3420 \tTraining Loss: 0.308563\n",
            "Epoch: 3421 \tTraining Loss: 0.308532\n",
            "Epoch: 3422 \tTraining Loss: 0.308511\n",
            "Epoch: 3423 \tTraining Loss: 0.308479\n",
            "Epoch: 3424 \tTraining Loss: 0.308458\n",
            "Epoch: 3425 \tTraining Loss: 0.308428\n",
            "Epoch: 3426 \tTraining Loss: 0.308408\n",
            "Epoch: 3427 \tTraining Loss: 0.308377\n",
            "Epoch: 3428 \tTraining Loss: 0.308357\n",
            "Epoch: 3429 \tTraining Loss: 0.308327\n",
            "Epoch: 3430 \tTraining Loss: 0.308308\n",
            "Epoch: 3431 \tTraining Loss: 0.308281\n",
            "Epoch: 3432 \tTraining Loss: 0.308265\n",
            "Epoch: 3433 \tTraining Loss: 0.308241\n",
            "Epoch: 3434 \tTraining Loss: 0.308231\n",
            "Epoch: 3435 \tTraining Loss: 0.308216\n",
            "Epoch: 3436 \tTraining Loss: 0.308217\n",
            "Epoch: 3437 \tTraining Loss: 0.308219\n",
            "Epoch: 3438 \tTraining Loss: 0.308235\n",
            "Epoch: 3439 \tTraining Loss: 0.308260\n",
            "Epoch: 3440 \tTraining Loss: 0.308277\n",
            "Epoch: 3441 \tTraining Loss: 0.308300\n",
            "Epoch: 3442 \tTraining Loss: 0.308278\n",
            "Epoch: 3443 \tTraining Loss: 0.308247\n",
            "Epoch: 3444 \tTraining Loss: 0.308175\n",
            "Epoch: 3445 \tTraining Loss: 0.308093\n",
            "Epoch: 3446 \tTraining Loss: 0.308009\n",
            "Epoch: 3447 \tTraining Loss: 0.307923\n",
            "Epoch: 3448 \tTraining Loss: 0.307865\n",
            "Epoch: 3449 \tTraining Loss: 0.307818\n",
            "Epoch: 3450 \tTraining Loss: 0.307789\n",
            "Epoch: 3451 \tTraining Loss: 0.307762\n",
            "Epoch: 3452 \tTraining Loss: 0.307754\n",
            "Epoch: 3453 \tTraining Loss: 0.307735\n",
            "Epoch: 3454 \tTraining Loss: 0.307725\n",
            "Epoch: 3455 \tTraining Loss: 0.307700\n",
            "Epoch: 3456 \tTraining Loss: 0.307679\n",
            "Epoch: 3457 \tTraining Loss: 0.307641\n",
            "Epoch: 3458 \tTraining Loss: 0.307614\n",
            "Epoch: 3459 \tTraining Loss: 0.307574\n",
            "Epoch: 3460 \tTraining Loss: 0.307543\n",
            "Epoch: 3461 \tTraining Loss: 0.307502\n",
            "Epoch: 3462 \tTraining Loss: 0.307476\n",
            "Epoch: 3463 \tTraining Loss: 0.307444\n",
            "Epoch: 3464 \tTraining Loss: 0.307422\n",
            "Epoch: 3465 \tTraining Loss: 0.307394\n",
            "Epoch: 3466 \tTraining Loss: 0.307379\n",
            "Epoch: 3467 \tTraining Loss: 0.307354\n",
            "Epoch: 3468 \tTraining Loss: 0.307340\n",
            "Epoch: 3469 \tTraining Loss: 0.307316\n",
            "Epoch: 3470 \tTraining Loss: 0.307304\n",
            "Epoch: 3471 \tTraining Loss: 0.307284\n",
            "Epoch: 3472 \tTraining Loss: 0.307274\n",
            "Epoch: 3473 \tTraining Loss: 0.307255\n",
            "Epoch: 3474 \tTraining Loss: 0.307243\n",
            "Epoch: 3475 \tTraining Loss: 0.307223\n",
            "Epoch: 3476 \tTraining Loss: 0.307206\n",
            "Epoch: 3477 \tTraining Loss: 0.307182\n",
            "Epoch: 3478 \tTraining Loss: 0.307158\n",
            "Epoch: 3479 \tTraining Loss: 0.307126\n",
            "Epoch: 3480 \tTraining Loss: 0.307096\n",
            "Epoch: 3481 \tTraining Loss: 0.307059\n",
            "Epoch: 3482 \tTraining Loss: 0.307028\n",
            "Epoch: 3483 \tTraining Loss: 0.306991\n",
            "Epoch: 3484 \tTraining Loss: 0.306963\n",
            "Epoch: 3485 \tTraining Loss: 0.306929\n",
            "Epoch: 3486 \tTraining Loss: 0.306903\n",
            "Epoch: 3487 \tTraining Loss: 0.306870\n",
            "Epoch: 3488 \tTraining Loss: 0.306846\n",
            "Epoch: 3489 \tTraining Loss: 0.306814\n",
            "Epoch: 3490 \tTraining Loss: 0.306791\n",
            "Epoch: 3491 \tTraining Loss: 0.306760\n",
            "Epoch: 3492 \tTraining Loss: 0.306738\n",
            "Epoch: 3493 \tTraining Loss: 0.306710\n",
            "Epoch: 3494 \tTraining Loss: 0.306690\n",
            "Epoch: 3495 \tTraining Loss: 0.306665\n",
            "Epoch: 3496 \tTraining Loss: 0.306649\n",
            "Epoch: 3497 \tTraining Loss: 0.306628\n",
            "Epoch: 3498 \tTraining Loss: 0.306617\n",
            "Epoch: 3499 \tTraining Loss: 0.306602\n",
            "Epoch: 3500 \tTraining Loss: 0.306594\n",
            "Epoch: 3501 \tTraining Loss: 0.306581\n",
            "Epoch: 3502 \tTraining Loss: 0.306571\n",
            "Epoch: 3503 \tTraining Loss: 0.306556\n",
            "Epoch: 3504 \tTraining Loss: 0.306537\n",
            "Epoch: 3505 \tTraining Loss: 0.306515\n",
            "Epoch: 3506 \tTraining Loss: 0.306488\n",
            "Epoch: 3507 \tTraining Loss: 0.306462\n",
            "Epoch: 3508 \tTraining Loss: 0.306436\n",
            "Epoch: 3509 \tTraining Loss: 0.306418\n",
            "Epoch: 3510 \tTraining Loss: 0.306400\n",
            "Epoch: 3511 \tTraining Loss: 0.306401\n",
            "Epoch: 3512 \tTraining Loss: 0.306390\n",
            "Epoch: 3513 \tTraining Loss: 0.306412\n",
            "Epoch: 3514 \tTraining Loss: 0.306398\n",
            "Epoch: 3515 \tTraining Loss: 0.306433\n",
            "Epoch: 3516 \tTraining Loss: 0.306404\n",
            "Epoch: 3517 \tTraining Loss: 0.306437\n",
            "Epoch: 3518 \tTraining Loss: 0.306385\n",
            "Epoch: 3519 \tTraining Loss: 0.306398\n",
            "Epoch: 3520 \tTraining Loss: 0.306326\n",
            "Epoch: 3521 \tTraining Loss: 0.306308\n",
            "Epoch: 3522 \tTraining Loss: 0.306230\n",
            "Epoch: 3523 \tTraining Loss: 0.306186\n",
            "Epoch: 3524 \tTraining Loss: 0.306114\n",
            "Epoch: 3525 \tTraining Loss: 0.306057\n",
            "Epoch: 3526 \tTraining Loss: 0.305990\n",
            "Epoch: 3527 \tTraining Loss: 0.305922\n",
            "Epoch: 3528 \tTraining Loss: 0.305855\n",
            "Epoch: 3529 \tTraining Loss: 0.305787\n",
            "Epoch: 3530 \tTraining Loss: 0.305737\n",
            "Epoch: 3531 \tTraining Loss: 0.305691\n",
            "Epoch: 3532 \tTraining Loss: 0.305668\n",
            "Epoch: 3533 \tTraining Loss: 0.305644\n",
            "Epoch: 3534 \tTraining Loss: 0.305638\n",
            "Epoch: 3535 \tTraining Loss: 0.305622\n",
            "Epoch: 3536 \tTraining Loss: 0.305617\n",
            "Epoch: 3537 \tTraining Loss: 0.305593\n",
            "Epoch: 3538 \tTraining Loss: 0.305577\n",
            "Epoch: 3539 \tTraining Loss: 0.305541\n",
            "Epoch: 3540 \tTraining Loss: 0.305513\n",
            "Epoch: 3541 \tTraining Loss: 0.305469\n",
            "Epoch: 3542 \tTraining Loss: 0.305436\n",
            "Epoch: 3543 \tTraining Loss: 0.305394\n",
            "Epoch: 3544 \tTraining Loss: 0.305366\n",
            "Epoch: 3545 \tTraining Loss: 0.305330\n",
            "Epoch: 3546 \tTraining Loss: 0.305309\n",
            "Epoch: 3547 \tTraining Loss: 0.305279\n",
            "Epoch: 3548 \tTraining Loss: 0.305262\n",
            "Epoch: 3549 \tTraining Loss: 0.305235\n",
            "Epoch: 3550 \tTraining Loss: 0.305219\n",
            "Epoch: 3551 \tTraining Loss: 0.305191\n",
            "Epoch: 3552 \tTraining Loss: 0.305176\n",
            "Epoch: 3553 \tTraining Loss: 0.305149\n",
            "Epoch: 3554 \tTraining Loss: 0.305135\n",
            "Epoch: 3555 \tTraining Loss: 0.305109\n",
            "Epoch: 3556 \tTraining Loss: 0.305099\n",
            "Epoch: 3557 \tTraining Loss: 0.305076\n",
            "Epoch: 3558 \tTraining Loss: 0.305075\n",
            "Epoch: 3559 \tTraining Loss: 0.305059\n",
            "Epoch: 3560 \tTraining Loss: 0.305078\n",
            "Epoch: 3561 \tTraining Loss: 0.305078\n",
            "Epoch: 3562 \tTraining Loss: 0.305144\n",
            "Epoch: 3563 \tTraining Loss: 0.305176\n",
            "Epoch: 3564 \tTraining Loss: 0.305363\n",
            "Epoch: 3565 \tTraining Loss: 0.305475\n",
            "Epoch: 3566 \tTraining Loss: 0.306018\n",
            "Epoch: 3567 \tTraining Loss: 0.306384\n",
            "Epoch: 3568 \tTraining Loss: 0.308354\n",
            "Epoch: 3569 \tTraining Loss: 0.310040\n",
            "Epoch: 3570 \tTraining Loss: 0.320543\n",
            "Epoch: 3571 \tTraining Loss: 0.323506\n",
            "Epoch: 3572 \tTraining Loss: 0.373304\n",
            "Epoch: 3573 \tTraining Loss: 0.371195\n",
            "Epoch: 3574 \tTraining Loss: 0.452420\n",
            "Epoch: 3575 \tTraining Loss: 0.494232\n",
            "Epoch: 3576 \tTraining Loss: 0.387082\n",
            "Epoch: 3577 \tTraining Loss: 0.450807\n",
            "Epoch: 3578 \tTraining Loss: 0.480304\n",
            "Epoch: 3579 \tTraining Loss: 0.452181\n",
            "Epoch: 3580 \tTraining Loss: 0.467469\n",
            "Epoch: 3581 \tTraining Loss: 0.383785\n",
            "Epoch: 3582 \tTraining Loss: 0.481275\n",
            "Epoch: 3583 \tTraining Loss: 0.398602\n",
            "Epoch: 3584 \tTraining Loss: 0.393863\n",
            "Epoch: 3585 \tTraining Loss: 0.413699\n",
            "Epoch: 3586 \tTraining Loss: 0.394586\n",
            "Epoch: 3587 \tTraining Loss: 0.367300\n",
            "Epoch: 3588 \tTraining Loss: 0.403714\n",
            "Epoch: 3589 \tTraining Loss: 0.370684\n",
            "Epoch: 3590 \tTraining Loss: 0.375157\n",
            "Epoch: 3591 \tTraining Loss: 0.355401\n",
            "Epoch: 3592 \tTraining Loss: 0.360176\n",
            "Epoch: 3593 \tTraining Loss: 0.346748\n",
            "Epoch: 3594 \tTraining Loss: 0.362784\n",
            "Epoch: 3595 \tTraining Loss: 0.344180\n",
            "Epoch: 3596 \tTraining Loss: 0.352288\n",
            "Epoch: 3597 \tTraining Loss: 0.340375\n",
            "Epoch: 3598 \tTraining Loss: 0.351143\n",
            "Epoch: 3599 \tTraining Loss: 0.328428\n",
            "Epoch: 3600 \tTraining Loss: 0.340279\n",
            "Epoch: 3601 \tTraining Loss: 0.333359\n",
            "Epoch: 3602 \tTraining Loss: 0.337691\n",
            "Epoch: 3603 \tTraining Loss: 0.323706\n",
            "Epoch: 3604 \tTraining Loss: 0.323638\n",
            "Epoch: 3605 \tTraining Loss: 0.324732\n",
            "Epoch: 3606 \tTraining Loss: 0.324741\n",
            "Epoch: 3607 \tTraining Loss: 0.319505\n",
            "Epoch: 3608 \tTraining Loss: 0.317652\n",
            "Epoch: 3609 \tTraining Loss: 0.318460\n",
            "Epoch: 3610 \tTraining Loss: 0.320275\n",
            "Epoch: 3611 \tTraining Loss: 0.316810\n",
            "Epoch: 3612 \tTraining Loss: 0.316794\n",
            "Epoch: 3613 \tTraining Loss: 0.313138\n",
            "Epoch: 3614 \tTraining Loss: 0.315189\n",
            "Epoch: 3615 \tTraining Loss: 0.314661\n",
            "Epoch: 3616 \tTraining Loss: 0.312315\n",
            "Epoch: 3617 \tTraining Loss: 0.312946\n",
            "Epoch: 3618 \tTraining Loss: 0.312251\n",
            "Epoch: 3619 \tTraining Loss: 0.312085\n",
            "Epoch: 3620 \tTraining Loss: 0.310992\n",
            "Epoch: 3621 \tTraining Loss: 0.311024\n",
            "Epoch: 3622 \tTraining Loss: 0.311473\n",
            "Epoch: 3623 \tTraining Loss: 0.309343\n",
            "Epoch: 3624 \tTraining Loss: 0.309835\n",
            "Epoch: 3625 \tTraining Loss: 0.310233\n",
            "Epoch: 3626 \tTraining Loss: 0.309080\n",
            "Epoch: 3627 \tTraining Loss: 0.308372\n",
            "Epoch: 3628 \tTraining Loss: 0.309294\n",
            "Epoch: 3629 \tTraining Loss: 0.308783\n",
            "Epoch: 3630 \tTraining Loss: 0.307936\n",
            "Epoch: 3631 \tTraining Loss: 0.308253\n",
            "Epoch: 3632 \tTraining Loss: 0.308070\n",
            "Epoch: 3633 \tTraining Loss: 0.307478\n",
            "Epoch: 3634 \tTraining Loss: 0.307798\n",
            "Epoch: 3635 \tTraining Loss: 0.307351\n",
            "Epoch: 3636 \tTraining Loss: 0.307156\n",
            "Epoch: 3637 \tTraining Loss: 0.307730\n",
            "Epoch: 3638 \tTraining Loss: 0.307236\n",
            "Epoch: 3639 \tTraining Loss: 0.306694\n",
            "Epoch: 3640 \tTraining Loss: 0.307521\n",
            "Epoch: 3641 \tTraining Loss: 0.306930\n",
            "Epoch: 3642 \tTraining Loss: 0.306426\n",
            "Epoch: 3643 \tTraining Loss: 0.307379\n",
            "Epoch: 3644 \tTraining Loss: 0.307050\n",
            "Epoch: 3645 \tTraining Loss: 0.306080\n",
            "Epoch: 3646 \tTraining Loss: 0.306489\n",
            "Epoch: 3647 \tTraining Loss: 0.306188\n",
            "Epoch: 3648 \tTraining Loss: 0.305926\n",
            "Epoch: 3649 \tTraining Loss: 0.305970\n",
            "Epoch: 3650 \tTraining Loss: 0.305740\n",
            "Epoch: 3651 \tTraining Loss: 0.305713\n",
            "Epoch: 3652 \tTraining Loss: 0.305776\n",
            "Epoch: 3653 \tTraining Loss: 0.305593\n",
            "Epoch: 3654 \tTraining Loss: 0.305438\n",
            "Epoch: 3655 \tTraining Loss: 0.305436\n",
            "Epoch: 3656 \tTraining Loss: 0.305484\n",
            "Epoch: 3657 \tTraining Loss: 0.305256\n",
            "Epoch: 3658 \tTraining Loss: 0.305233\n",
            "Epoch: 3659 \tTraining Loss: 0.305232\n",
            "Epoch: 3660 \tTraining Loss: 0.305163\n",
            "Epoch: 3661 \tTraining Loss: 0.305000\n",
            "Epoch: 3662 \tTraining Loss: 0.305016\n",
            "Epoch: 3663 \tTraining Loss: 0.304861\n",
            "Epoch: 3664 \tTraining Loss: 0.304839\n",
            "Epoch: 3665 \tTraining Loss: 0.304771\n",
            "Epoch: 3666 \tTraining Loss: 0.304631\n",
            "Epoch: 3667 \tTraining Loss: 0.304615\n",
            "Epoch: 3668 \tTraining Loss: 0.304530\n",
            "Epoch: 3669 \tTraining Loss: 0.304517\n",
            "Epoch: 3670 \tTraining Loss: 0.304442\n",
            "Epoch: 3671 \tTraining Loss: 0.304411\n",
            "Epoch: 3672 \tTraining Loss: 0.304348\n",
            "Epoch: 3673 \tTraining Loss: 0.304317\n",
            "Epoch: 3674 \tTraining Loss: 0.304294\n",
            "Epoch: 3675 \tTraining Loss: 0.304250\n",
            "Epoch: 3676 \tTraining Loss: 0.304231\n",
            "Epoch: 3677 \tTraining Loss: 0.304181\n",
            "Epoch: 3678 \tTraining Loss: 0.304167\n",
            "Epoch: 3679 \tTraining Loss: 0.304118\n",
            "Epoch: 3680 \tTraining Loss: 0.304089\n",
            "Epoch: 3681 \tTraining Loss: 0.304052\n",
            "Epoch: 3682 \tTraining Loss: 0.304013\n",
            "Epoch: 3683 \tTraining Loss: 0.303991\n",
            "Epoch: 3684 \tTraining Loss: 0.303951\n",
            "Epoch: 3685 \tTraining Loss: 0.303928\n",
            "Epoch: 3686 \tTraining Loss: 0.303893\n",
            "Epoch: 3687 \tTraining Loss: 0.303868\n",
            "Epoch: 3688 \tTraining Loss: 0.303842\n",
            "Epoch: 3689 \tTraining Loss: 0.303810\n",
            "Epoch: 3690 \tTraining Loss: 0.303788\n",
            "Epoch: 3691 \tTraining Loss: 0.303757\n",
            "Epoch: 3692 \tTraining Loss: 0.303734\n",
            "Epoch: 3693 \tTraining Loss: 0.303709\n",
            "Epoch: 3694 \tTraining Loss: 0.303680\n",
            "Epoch: 3695 \tTraining Loss: 0.303657\n",
            "Epoch: 3696 \tTraining Loss: 0.303628\n",
            "Epoch: 3697 \tTraining Loss: 0.303605\n",
            "Epoch: 3698 \tTraining Loss: 0.303580\n",
            "Epoch: 3699 \tTraining Loss: 0.303553\n",
            "Epoch: 3700 \tTraining Loss: 0.303530\n",
            "Epoch: 3701 \tTraining Loss: 0.303504\n",
            "Epoch: 3702 \tTraining Loss: 0.303480\n",
            "Epoch: 3703 \tTraining Loss: 0.303457\n",
            "Epoch: 3704 \tTraining Loss: 0.303431\n",
            "Epoch: 3705 \tTraining Loss: 0.303408\n",
            "Epoch: 3706 \tTraining Loss: 0.303385\n",
            "Epoch: 3707 \tTraining Loss: 0.303361\n",
            "Epoch: 3708 \tTraining Loss: 0.303338\n",
            "Epoch: 3709 \tTraining Loss: 0.303315\n",
            "Epoch: 3710 \tTraining Loss: 0.303292\n",
            "Epoch: 3711 \tTraining Loss: 0.303270\n",
            "Epoch: 3712 \tTraining Loss: 0.303247\n",
            "Epoch: 3713 \tTraining Loss: 0.303224\n",
            "Epoch: 3714 \tTraining Loss: 0.303202\n",
            "Epoch: 3715 \tTraining Loss: 0.303179\n",
            "Epoch: 3716 \tTraining Loss: 0.303157\n",
            "Epoch: 3717 \tTraining Loss: 0.303135\n",
            "Epoch: 3718 \tTraining Loss: 0.303113\n",
            "Epoch: 3719 \tTraining Loss: 0.303091\n",
            "Epoch: 3720 \tTraining Loss: 0.303069\n",
            "Epoch: 3721 \tTraining Loss: 0.303047\n",
            "Epoch: 3722 \tTraining Loss: 0.303026\n",
            "Epoch: 3723 \tTraining Loss: 0.303004\n",
            "Epoch: 3724 \tTraining Loss: 0.302983\n",
            "Epoch: 3725 \tTraining Loss: 0.302961\n",
            "Epoch: 3726 \tTraining Loss: 0.302940\n",
            "Epoch: 3727 \tTraining Loss: 0.302918\n",
            "Epoch: 3728 \tTraining Loss: 0.302897\n",
            "Epoch: 3729 \tTraining Loss: 0.302876\n",
            "Epoch: 3730 \tTraining Loss: 0.302855\n",
            "Epoch: 3731 \tTraining Loss: 0.302834\n",
            "Epoch: 3732 \tTraining Loss: 0.302813\n",
            "Epoch: 3733 \tTraining Loss: 0.302792\n",
            "Epoch: 3734 \tTraining Loss: 0.302771\n",
            "Epoch: 3735 \tTraining Loss: 0.302751\n",
            "Epoch: 3736 \tTraining Loss: 0.302730\n",
            "Epoch: 3737 \tTraining Loss: 0.302709\n",
            "Epoch: 3738 \tTraining Loss: 0.302688\n",
            "Epoch: 3739 \tTraining Loss: 0.302668\n",
            "Epoch: 3740 \tTraining Loss: 0.302647\n",
            "Epoch: 3741 \tTraining Loss: 0.302627\n",
            "Epoch: 3742 \tTraining Loss: 0.302606\n",
            "Epoch: 3743 \tTraining Loss: 0.302586\n",
            "Epoch: 3744 \tTraining Loss: 0.302566\n",
            "Epoch: 3745 \tTraining Loss: 0.302545\n",
            "Epoch: 3746 \tTraining Loss: 0.302525\n",
            "Epoch: 3747 \tTraining Loss: 0.302505\n",
            "Epoch: 3748 \tTraining Loss: 0.302484\n",
            "Epoch: 3749 \tTraining Loss: 0.302464\n",
            "Epoch: 3750 \tTraining Loss: 0.302444\n",
            "Epoch: 3751 \tTraining Loss: 0.302423\n",
            "Epoch: 3752 \tTraining Loss: 0.302403\n",
            "Epoch: 3753 \tTraining Loss: 0.302383\n",
            "Epoch: 3754 \tTraining Loss: 0.302363\n",
            "Epoch: 3755 \tTraining Loss: 0.302342\n",
            "Epoch: 3756 \tTraining Loss: 0.302322\n",
            "Epoch: 3757 \tTraining Loss: 0.302301\n",
            "Epoch: 3758 \tTraining Loss: 0.302281\n",
            "Epoch: 3759 \tTraining Loss: 0.302260\n",
            "Epoch: 3760 \tTraining Loss: 0.302240\n",
            "Epoch: 3761 \tTraining Loss: 0.302219\n",
            "Epoch: 3762 \tTraining Loss: 0.302198\n",
            "Epoch: 3763 \tTraining Loss: 0.302177\n",
            "Epoch: 3764 \tTraining Loss: 0.302156\n",
            "Epoch: 3765 \tTraining Loss: 0.302135\n",
            "Epoch: 3766 \tTraining Loss: 0.302113\n",
            "Epoch: 3767 \tTraining Loss: 0.302092\n",
            "Epoch: 3768 \tTraining Loss: 0.302070\n",
            "Epoch: 3769 \tTraining Loss: 0.302048\n",
            "Epoch: 3770 \tTraining Loss: 0.302026\n",
            "Epoch: 3771 \tTraining Loss: 0.302004\n",
            "Epoch: 3772 \tTraining Loss: 0.301983\n",
            "Epoch: 3773 \tTraining Loss: 0.301961\n",
            "Epoch: 3774 \tTraining Loss: 0.301940\n",
            "Epoch: 3775 \tTraining Loss: 0.301919\n",
            "Epoch: 3776 \tTraining Loss: 0.301898\n",
            "Epoch: 3777 \tTraining Loss: 0.301877\n",
            "Epoch: 3778 \tTraining Loss: 0.301856\n",
            "Epoch: 3779 \tTraining Loss: 0.301836\n",
            "Epoch: 3780 \tTraining Loss: 0.301816\n",
            "Epoch: 3781 \tTraining Loss: 0.301796\n",
            "Epoch: 3782 \tTraining Loss: 0.301776\n",
            "Epoch: 3783 \tTraining Loss: 0.301756\n",
            "Epoch: 3784 \tTraining Loss: 0.301737\n",
            "Epoch: 3785 \tTraining Loss: 0.301717\n",
            "Epoch: 3786 \tTraining Loss: 0.301698\n",
            "Epoch: 3787 \tTraining Loss: 0.301679\n",
            "Epoch: 3788 \tTraining Loss: 0.301660\n",
            "Epoch: 3789 \tTraining Loss: 0.301641\n",
            "Epoch: 3790 \tTraining Loss: 0.301622\n",
            "Epoch: 3791 \tTraining Loss: 0.301603\n",
            "Epoch: 3792 \tTraining Loss: 0.301584\n",
            "Epoch: 3793 \tTraining Loss: 0.301566\n",
            "Epoch: 3794 \tTraining Loss: 0.301547\n",
            "Epoch: 3795 \tTraining Loss: 0.301528\n",
            "Epoch: 3796 \tTraining Loss: 0.301509\n",
            "Epoch: 3797 \tTraining Loss: 0.301491\n",
            "Epoch: 3798 \tTraining Loss: 0.301472\n",
            "Epoch: 3799 \tTraining Loss: 0.301453\n",
            "Epoch: 3800 \tTraining Loss: 0.301435\n",
            "Epoch: 3801 \tTraining Loss: 0.301416\n",
            "Epoch: 3802 \tTraining Loss: 0.301397\n",
            "Epoch: 3803 \tTraining Loss: 0.301379\n",
            "Epoch: 3804 \tTraining Loss: 0.301360\n",
            "Epoch: 3805 \tTraining Loss: 0.301342\n",
            "Epoch: 3806 \tTraining Loss: 0.301323\n",
            "Epoch: 3807 \tTraining Loss: 0.301305\n",
            "Epoch: 3808 \tTraining Loss: 0.301286\n",
            "Epoch: 3809 \tTraining Loss: 0.301268\n",
            "Epoch: 3810 \tTraining Loss: 0.301249\n",
            "Epoch: 3811 \tTraining Loss: 0.301231\n",
            "Epoch: 3812 \tTraining Loss: 0.301212\n",
            "Epoch: 3813 \tTraining Loss: 0.301194\n",
            "Epoch: 3814 \tTraining Loss: 0.301175\n",
            "Epoch: 3815 \tTraining Loss: 0.301157\n",
            "Epoch: 3816 \tTraining Loss: 0.301138\n",
            "Epoch: 3817 \tTraining Loss: 0.301120\n",
            "Epoch: 3818 \tTraining Loss: 0.301102\n",
            "Epoch: 3819 \tTraining Loss: 0.301083\n",
            "Epoch: 3820 \tTraining Loss: 0.301065\n",
            "Epoch: 3821 \tTraining Loss: 0.301046\n",
            "Epoch: 3822 \tTraining Loss: 0.301028\n",
            "Epoch: 3823 \tTraining Loss: 0.301010\n",
            "Epoch: 3824 \tTraining Loss: 0.300992\n",
            "Epoch: 3825 \tTraining Loss: 0.300973\n",
            "Epoch: 3826 \tTraining Loss: 0.300955\n",
            "Epoch: 3827 \tTraining Loss: 0.300937\n",
            "Epoch: 3828 \tTraining Loss: 0.300918\n",
            "Epoch: 3829 \tTraining Loss: 0.300900\n",
            "Epoch: 3830 \tTraining Loss: 0.300882\n",
            "Epoch: 3831 \tTraining Loss: 0.300864\n",
            "Epoch: 3832 \tTraining Loss: 0.300845\n",
            "Epoch: 3833 \tTraining Loss: 0.300827\n",
            "Epoch: 3834 \tTraining Loss: 0.300809\n",
            "Epoch: 3835 \tTraining Loss: 0.300791\n",
            "Epoch: 3836 \tTraining Loss: 0.300773\n",
            "Epoch: 3837 \tTraining Loss: 0.300754\n",
            "Epoch: 3838 \tTraining Loss: 0.300736\n",
            "Epoch: 3839 \tTraining Loss: 0.300718\n",
            "Epoch: 3840 \tTraining Loss: 0.300700\n",
            "Epoch: 3841 \tTraining Loss: 0.300682\n",
            "Epoch: 3842 \tTraining Loss: 0.300664\n",
            "Epoch: 3843 \tTraining Loss: 0.300645\n",
            "Epoch: 3844 \tTraining Loss: 0.300627\n",
            "Epoch: 3845 \tTraining Loss: 0.300609\n",
            "Epoch: 3846 \tTraining Loss: 0.300591\n",
            "Epoch: 3847 \tTraining Loss: 0.300573\n",
            "Epoch: 3848 \tTraining Loss: 0.300555\n",
            "Epoch: 3849 \tTraining Loss: 0.300537\n",
            "Epoch: 3850 \tTraining Loss: 0.300519\n",
            "Epoch: 3851 \tTraining Loss: 0.300501\n",
            "Epoch: 3852 \tTraining Loss: 0.300483\n",
            "Epoch: 3853 \tTraining Loss: 0.300464\n",
            "Epoch: 3854 \tTraining Loss: 0.300446\n",
            "Epoch: 3855 \tTraining Loss: 0.300428\n",
            "Epoch: 3856 \tTraining Loss: 0.300410\n",
            "Epoch: 3857 \tTraining Loss: 0.300392\n",
            "Epoch: 3858 \tTraining Loss: 0.300374\n",
            "Epoch: 3859 \tTraining Loss: 0.300356\n",
            "Epoch: 3860 \tTraining Loss: 0.300338\n",
            "Epoch: 3861 \tTraining Loss: 0.300320\n",
            "Epoch: 3862 \tTraining Loss: 0.300302\n",
            "Epoch: 3863 \tTraining Loss: 0.300284\n",
            "Epoch: 3864 \tTraining Loss: 0.300266\n",
            "Epoch: 3865 \tTraining Loss: 0.300248\n",
            "Epoch: 3866 \tTraining Loss: 0.300230\n",
            "Epoch: 3867 \tTraining Loss: 0.300212\n",
            "Epoch: 3868 \tTraining Loss: 0.300194\n",
            "Epoch: 3869 \tTraining Loss: 0.300176\n",
            "Epoch: 3870 \tTraining Loss: 0.300158\n",
            "Epoch: 3871 \tTraining Loss: 0.300140\n",
            "Epoch: 3872 \tTraining Loss: 0.300122\n",
            "Epoch: 3873 \tTraining Loss: 0.300104\n",
            "Epoch: 3874 \tTraining Loss: 0.300086\n",
            "Epoch: 3875 \tTraining Loss: 0.300068\n",
            "Epoch: 3876 \tTraining Loss: 0.300050\n",
            "Epoch: 3877 \tTraining Loss: 0.300032\n",
            "Epoch: 3878 \tTraining Loss: 0.300015\n",
            "Epoch: 3879 \tTraining Loss: 0.299997\n",
            "Epoch: 3880 \tTraining Loss: 0.299979\n",
            "Epoch: 3881 \tTraining Loss: 0.299961\n",
            "Epoch: 3882 \tTraining Loss: 0.299943\n",
            "Epoch: 3883 \tTraining Loss: 0.299925\n",
            "Epoch: 3884 \tTraining Loss: 0.299907\n",
            "Epoch: 3885 \tTraining Loss: 0.299889\n",
            "Epoch: 3886 \tTraining Loss: 0.299871\n",
            "Epoch: 3887 \tTraining Loss: 0.299853\n",
            "Epoch: 3888 \tTraining Loss: 0.299835\n",
            "Epoch: 3889 \tTraining Loss: 0.299817\n",
            "Epoch: 3890 \tTraining Loss: 0.299800\n",
            "Epoch: 3891 \tTraining Loss: 0.299782\n",
            "Epoch: 3892 \tTraining Loss: 0.299764\n",
            "Epoch: 3893 \tTraining Loss: 0.299746\n",
            "Epoch: 3894 \tTraining Loss: 0.299728\n",
            "Epoch: 3895 \tTraining Loss: 0.299710\n",
            "Epoch: 3896 \tTraining Loss: 0.299692\n",
            "Epoch: 3897 \tTraining Loss: 0.299674\n",
            "Epoch: 3898 \tTraining Loss: 0.299657\n",
            "Epoch: 3899 \tTraining Loss: 0.299639\n",
            "Epoch: 3900 \tTraining Loss: 0.299621\n",
            "Epoch: 3901 \tTraining Loss: 0.299603\n",
            "Epoch: 3902 \tTraining Loss: 0.299585\n",
            "Epoch: 3903 \tTraining Loss: 0.299567\n",
            "Epoch: 3904 \tTraining Loss: 0.299549\n",
            "Epoch: 3905 \tTraining Loss: 0.299532\n",
            "Epoch: 3906 \tTraining Loss: 0.299514\n",
            "Epoch: 3907 \tTraining Loss: 0.299496\n",
            "Epoch: 3908 \tTraining Loss: 0.299478\n",
            "Epoch: 3909 \tTraining Loss: 0.299460\n",
            "Epoch: 3910 \tTraining Loss: 0.299442\n",
            "Epoch: 3911 \tTraining Loss: 0.299425\n",
            "Epoch: 3912 \tTraining Loss: 0.299407\n",
            "Epoch: 3913 \tTraining Loss: 0.299389\n",
            "Epoch: 3914 \tTraining Loss: 0.299371\n",
            "Epoch: 3915 \tTraining Loss: 0.299353\n",
            "Epoch: 3916 \tTraining Loss: 0.299335\n",
            "Epoch: 3917 \tTraining Loss: 0.299318\n",
            "Epoch: 3918 \tTraining Loss: 0.299300\n",
            "Epoch: 3919 \tTraining Loss: 0.299282\n",
            "Epoch: 3920 \tTraining Loss: 0.299264\n",
            "Epoch: 3921 \tTraining Loss: 0.299246\n",
            "Epoch: 3922 \tTraining Loss: 0.299229\n",
            "Epoch: 3923 \tTraining Loss: 0.299211\n",
            "Epoch: 3924 \tTraining Loss: 0.299193\n",
            "Epoch: 3925 \tTraining Loss: 0.299175\n",
            "Epoch: 3926 \tTraining Loss: 0.299157\n",
            "Epoch: 3927 \tTraining Loss: 0.299140\n",
            "Epoch: 3928 \tTraining Loss: 0.299122\n",
            "Epoch: 3929 \tTraining Loss: 0.299104\n",
            "Epoch: 3930 \tTraining Loss: 0.299086\n",
            "Epoch: 3931 \tTraining Loss: 0.299068\n",
            "Epoch: 3932 \tTraining Loss: 0.299051\n",
            "Epoch: 3933 \tTraining Loss: 0.299033\n",
            "Epoch: 3934 \tTraining Loss: 0.299015\n",
            "Epoch: 3935 \tTraining Loss: 0.298997\n",
            "Epoch: 3936 \tTraining Loss: 0.298980\n",
            "Epoch: 3937 \tTraining Loss: 0.298962\n",
            "Epoch: 3938 \tTraining Loss: 0.298944\n",
            "Epoch: 3939 \tTraining Loss: 0.298926\n",
            "Epoch: 3940 \tTraining Loss: 0.298908\n",
            "Epoch: 3941 \tTraining Loss: 0.298891\n",
            "Epoch: 3942 \tTraining Loss: 0.298873\n",
            "Epoch: 3943 \tTraining Loss: 0.298855\n",
            "Epoch: 3944 \tTraining Loss: 0.298837\n",
            "Epoch: 3945 \tTraining Loss: 0.298820\n",
            "Epoch: 3946 \tTraining Loss: 0.298802\n",
            "Epoch: 3947 \tTraining Loss: 0.298784\n",
            "Epoch: 3948 \tTraining Loss: 0.298766\n",
            "Epoch: 3949 \tTraining Loss: 0.298749\n",
            "Epoch: 3950 \tTraining Loss: 0.298731\n",
            "Epoch: 3951 \tTraining Loss: 0.298713\n",
            "Epoch: 3952 \tTraining Loss: 0.298695\n",
            "Epoch: 3953 \tTraining Loss: 0.298678\n",
            "Epoch: 3954 \tTraining Loss: 0.298660\n",
            "Epoch: 3955 \tTraining Loss: 0.298642\n",
            "Epoch: 3956 \tTraining Loss: 0.298624\n",
            "Epoch: 3957 \tTraining Loss: 0.298607\n",
            "Epoch: 3958 \tTraining Loss: 0.298589\n",
            "Epoch: 3959 \tTraining Loss: 0.298571\n",
            "Epoch: 3960 \tTraining Loss: 0.298553\n",
            "Epoch: 3961 \tTraining Loss: 0.298536\n",
            "Epoch: 3962 \tTraining Loss: 0.298518\n",
            "Epoch: 3963 \tTraining Loss: 0.298500\n",
            "Epoch: 3964 \tTraining Loss: 0.298482\n",
            "Epoch: 3965 \tTraining Loss: 0.298465\n",
            "Epoch: 3966 \tTraining Loss: 0.298447\n",
            "Epoch: 3967 \tTraining Loss: 0.298429\n",
            "Epoch: 3968 \tTraining Loss: 0.298411\n",
            "Epoch: 3969 \tTraining Loss: 0.298394\n",
            "Epoch: 3970 \tTraining Loss: 0.298376\n",
            "Epoch: 3971 \tTraining Loss: 0.298358\n",
            "Epoch: 3972 \tTraining Loss: 0.298341\n",
            "Epoch: 3973 \tTraining Loss: 0.298323\n",
            "Epoch: 3974 \tTraining Loss: 0.298305\n",
            "Epoch: 3975 \tTraining Loss: 0.298287\n",
            "Epoch: 3976 \tTraining Loss: 0.298270\n",
            "Epoch: 3977 \tTraining Loss: 0.298252\n",
            "Epoch: 3978 \tTraining Loss: 0.298234\n",
            "Epoch: 3979 \tTraining Loss: 0.298217\n",
            "Epoch: 3980 \tTraining Loss: 0.298199\n",
            "Epoch: 3981 \tTraining Loss: 0.298181\n",
            "Epoch: 3982 \tTraining Loss: 0.298163\n",
            "Epoch: 3983 \tTraining Loss: 0.298146\n",
            "Epoch: 3984 \tTraining Loss: 0.298128\n",
            "Epoch: 3985 \tTraining Loss: 0.298110\n",
            "Epoch: 3986 \tTraining Loss: 0.298092\n",
            "Epoch: 3987 \tTraining Loss: 0.298075\n",
            "Epoch: 3988 \tTraining Loss: 0.298057\n",
            "Epoch: 3989 \tTraining Loss: 0.298039\n",
            "Epoch: 3990 \tTraining Loss: 0.298022\n",
            "Epoch: 3991 \tTraining Loss: 0.298004\n",
            "Epoch: 3992 \tTraining Loss: 0.297986\n",
            "Epoch: 3993 \tTraining Loss: 0.297969\n",
            "Epoch: 3994 \tTraining Loss: 0.297951\n",
            "Epoch: 3995 \tTraining Loss: 0.297933\n",
            "Epoch: 3996 \tTraining Loss: 0.297915\n",
            "Epoch: 3997 \tTraining Loss: 0.297898\n",
            "Epoch: 3998 \tTraining Loss: 0.297880\n",
            "Epoch: 3999 \tTraining Loss: 0.297862\n",
            "Epoch: 4000 \tTraining Loss: 0.297844\n",
            "Epoch: 4001 \tTraining Loss: 0.297827\n",
            "Epoch: 4002 \tTraining Loss: 0.297809\n",
            "Epoch: 4003 \tTraining Loss: 0.297791\n",
            "Epoch: 4004 \tTraining Loss: 0.297774\n",
            "Epoch: 4005 \tTraining Loss: 0.297756\n",
            "Epoch: 4006 \tTraining Loss: 0.297738\n",
            "Epoch: 4007 \tTraining Loss: 0.297721\n",
            "Epoch: 4008 \tTraining Loss: 0.297703\n",
            "Epoch: 4009 \tTraining Loss: 0.297685\n",
            "Epoch: 4010 \tTraining Loss: 0.297668\n",
            "Epoch: 4011 \tTraining Loss: 0.297650\n",
            "Epoch: 4012 \tTraining Loss: 0.297633\n",
            "Epoch: 4013 \tTraining Loss: 0.297616\n",
            "Epoch: 4014 \tTraining Loss: 0.297600\n",
            "Epoch: 4015 \tTraining Loss: 0.297585\n",
            "Epoch: 4016 \tTraining Loss: 0.297570\n",
            "Epoch: 4017 \tTraining Loss: 0.297555\n",
            "Epoch: 4018 \tTraining Loss: 0.297540\n",
            "Epoch: 4019 \tTraining Loss: 0.297520\n",
            "Epoch: 4020 \tTraining Loss: 0.297501\n",
            "Epoch: 4021 \tTraining Loss: 0.297480\n",
            "Epoch: 4022 \tTraining Loss: 0.297461\n",
            "Epoch: 4023 \tTraining Loss: 0.297442\n",
            "Epoch: 4024 \tTraining Loss: 0.297425\n",
            "Epoch: 4025 \tTraining Loss: 0.297407\n",
            "Epoch: 4026 \tTraining Loss: 0.297391\n",
            "Epoch: 4027 \tTraining Loss: 0.297375\n",
            "Epoch: 4028 \tTraining Loss: 0.297359\n",
            "Epoch: 4029 \tTraining Loss: 0.297343\n",
            "Epoch: 4030 \tTraining Loss: 0.297327\n",
            "Epoch: 4031 \tTraining Loss: 0.297307\n",
            "Epoch: 4032 \tTraining Loss: 0.297289\n",
            "Epoch: 4033 \tTraining Loss: 0.297269\n",
            "Epoch: 4034 \tTraining Loss: 0.297251\n",
            "Epoch: 4035 \tTraining Loss: 0.297232\n",
            "Epoch: 4036 \tTraining Loss: 0.297215\n",
            "Epoch: 4037 \tTraining Loss: 0.297198\n",
            "Epoch: 4038 \tTraining Loss: 0.297182\n",
            "Epoch: 4039 \tTraining Loss: 0.297165\n",
            "Epoch: 4040 \tTraining Loss: 0.297149\n",
            "Epoch: 4041 \tTraining Loss: 0.297130\n",
            "Epoch: 4042 \tTraining Loss: 0.297113\n",
            "Epoch: 4043 \tTraining Loss: 0.297093\n",
            "Epoch: 4044 \tTraining Loss: 0.297076\n",
            "Epoch: 4045 \tTraining Loss: 0.297057\n",
            "Epoch: 4046 \tTraining Loss: 0.297040\n",
            "Epoch: 4047 \tTraining Loss: 0.297022\n",
            "Epoch: 4048 \tTraining Loss: 0.297006\n",
            "Epoch: 4049 \tTraining Loss: 0.296988\n",
            "Epoch: 4050 \tTraining Loss: 0.296972\n",
            "Epoch: 4051 \tTraining Loss: 0.296953\n",
            "Epoch: 4052 \tTraining Loss: 0.296936\n",
            "Epoch: 4053 \tTraining Loss: 0.296917\n",
            "Epoch: 4054 \tTraining Loss: 0.296900\n",
            "Epoch: 4055 \tTraining Loss: 0.296881\n",
            "Epoch: 4056 \tTraining Loss: 0.296865\n",
            "Epoch: 4057 \tTraining Loss: 0.296846\n",
            "Epoch: 4058 \tTraining Loss: 0.296830\n",
            "Epoch: 4059 \tTraining Loss: 0.296812\n",
            "Epoch: 4060 \tTraining Loss: 0.296796\n",
            "Epoch: 4061 \tTraining Loss: 0.296776\n",
            "Epoch: 4062 \tTraining Loss: 0.296760\n",
            "Epoch: 4063 \tTraining Loss: 0.296740\n",
            "Epoch: 4064 \tTraining Loss: 0.296724\n",
            "Epoch: 4065 \tTraining Loss: 0.296705\n",
            "Epoch: 4066 \tTraining Loss: 0.296689\n",
            "Epoch: 4067 \tTraining Loss: 0.296670\n",
            "Epoch: 4068 \tTraining Loss: 0.296654\n",
            "Epoch: 4069 \tTraining Loss: 0.296635\n",
            "Epoch: 4070 \tTraining Loss: 0.296619\n",
            "Epoch: 4071 \tTraining Loss: 0.296599\n",
            "Epoch: 4072 \tTraining Loss: 0.296583\n",
            "Epoch: 4073 \tTraining Loss: 0.296564\n",
            "Epoch: 4074 \tTraining Loss: 0.296548\n",
            "Epoch: 4075 \tTraining Loss: 0.296529\n",
            "Epoch: 4076 \tTraining Loss: 0.296513\n",
            "Epoch: 4077 \tTraining Loss: 0.296494\n",
            "Epoch: 4078 \tTraining Loss: 0.296478\n",
            "Epoch: 4079 \tTraining Loss: 0.296458\n",
            "Epoch: 4080 \tTraining Loss: 0.296442\n",
            "Epoch: 4081 \tTraining Loss: 0.296423\n",
            "Epoch: 4082 \tTraining Loss: 0.296406\n",
            "Epoch: 4083 \tTraining Loss: 0.296387\n",
            "Epoch: 4084 \tTraining Loss: 0.296371\n",
            "Epoch: 4085 \tTraining Loss: 0.296352\n",
            "Epoch: 4086 \tTraining Loss: 0.296336\n",
            "Epoch: 4087 \tTraining Loss: 0.296317\n",
            "Epoch: 4088 \tTraining Loss: 0.296301\n",
            "Epoch: 4089 \tTraining Loss: 0.296281\n",
            "Epoch: 4090 \tTraining Loss: 0.296265\n",
            "Epoch: 4091 \tTraining Loss: 0.296246\n",
            "Epoch: 4092 \tTraining Loss: 0.296230\n",
            "Epoch: 4093 \tTraining Loss: 0.296211\n",
            "Epoch: 4094 \tTraining Loss: 0.296195\n",
            "Epoch: 4095 \tTraining Loss: 0.296175\n",
            "Epoch: 4096 \tTraining Loss: 0.296160\n",
            "Epoch: 4097 \tTraining Loss: 0.296140\n",
            "Epoch: 4098 \tTraining Loss: 0.296124\n",
            "Epoch: 4099 \tTraining Loss: 0.296104\n",
            "Epoch: 4100 \tTraining Loss: 0.296089\n",
            "Epoch: 4101 \tTraining Loss: 0.296069\n",
            "Epoch: 4102 \tTraining Loss: 0.296054\n",
            "Epoch: 4103 \tTraining Loss: 0.296034\n",
            "Epoch: 4104 \tTraining Loss: 0.296018\n",
            "Epoch: 4105 \tTraining Loss: 0.295998\n",
            "Epoch: 4106 \tTraining Loss: 0.295983\n",
            "Epoch: 4107 \tTraining Loss: 0.295963\n",
            "Epoch: 4108 \tTraining Loss: 0.295948\n",
            "Epoch: 4109 \tTraining Loss: 0.295928\n",
            "Epoch: 4110 \tTraining Loss: 0.295912\n",
            "Epoch: 4111 \tTraining Loss: 0.295892\n",
            "Epoch: 4112 \tTraining Loss: 0.295877\n",
            "Epoch: 4113 \tTraining Loss: 0.295857\n",
            "Epoch: 4114 \tTraining Loss: 0.295841\n",
            "Epoch: 4115 \tTraining Loss: 0.295821\n",
            "Epoch: 4116 \tTraining Loss: 0.295806\n",
            "Epoch: 4117 \tTraining Loss: 0.295786\n",
            "Epoch: 4118 \tTraining Loss: 0.295771\n",
            "Epoch: 4119 \tTraining Loss: 0.295750\n",
            "Epoch: 4120 \tTraining Loss: 0.295735\n",
            "Epoch: 4121 \tTraining Loss: 0.295715\n",
            "Epoch: 4122 \tTraining Loss: 0.295700\n",
            "Epoch: 4123 \tTraining Loss: 0.295679\n",
            "Epoch: 4124 \tTraining Loss: 0.295664\n",
            "Epoch: 4125 \tTraining Loss: 0.295644\n",
            "Epoch: 4126 \tTraining Loss: 0.295629\n",
            "Epoch: 4127 \tTraining Loss: 0.295609\n",
            "Epoch: 4128 \tTraining Loss: 0.295594\n",
            "Epoch: 4129 \tTraining Loss: 0.295573\n",
            "Epoch: 4130 \tTraining Loss: 0.295558\n",
            "Epoch: 4131 \tTraining Loss: 0.295538\n",
            "Epoch: 4132 \tTraining Loss: 0.295523\n",
            "Epoch: 4133 \tTraining Loss: 0.295502\n",
            "Epoch: 4134 \tTraining Loss: 0.295487\n",
            "Epoch: 4135 \tTraining Loss: 0.295467\n",
            "Epoch: 4136 \tTraining Loss: 0.295452\n",
            "Epoch: 4137 \tTraining Loss: 0.295431\n",
            "Epoch: 4138 \tTraining Loss: 0.295416\n",
            "Epoch: 4139 \tTraining Loss: 0.295396\n",
            "Epoch: 4140 \tTraining Loss: 0.295381\n",
            "Epoch: 4141 \tTraining Loss: 0.295360\n",
            "Epoch: 4142 \tTraining Loss: 0.295346\n",
            "Epoch: 4143 \tTraining Loss: 0.295325\n",
            "Epoch: 4144 \tTraining Loss: 0.295310\n",
            "Epoch: 4145 \tTraining Loss: 0.295289\n",
            "Epoch: 4146 \tTraining Loss: 0.295275\n",
            "Epoch: 4147 \tTraining Loss: 0.295254\n",
            "Epoch: 4148 \tTraining Loss: 0.295239\n",
            "Epoch: 4149 \tTraining Loss: 0.295218\n",
            "Epoch: 4150 \tTraining Loss: 0.295204\n",
            "Epoch: 4151 \tTraining Loss: 0.295183\n",
            "Epoch: 4152 \tTraining Loss: 0.295168\n",
            "Epoch: 4153 \tTraining Loss: 0.295147\n",
            "Epoch: 4154 \tTraining Loss: 0.295133\n",
            "Epoch: 4155 \tTraining Loss: 0.295112\n",
            "Epoch: 4156 \tTraining Loss: 0.295097\n",
            "Epoch: 4157 \tTraining Loss: 0.295076\n",
            "Epoch: 4158 \tTraining Loss: 0.295062\n",
            "Epoch: 4159 \tTraining Loss: 0.295040\n",
            "Epoch: 4160 \tTraining Loss: 0.295026\n",
            "Epoch: 4161 \tTraining Loss: 0.295005\n",
            "Epoch: 4162 \tTraining Loss: 0.294991\n",
            "Epoch: 4163 \tTraining Loss: 0.294969\n",
            "Epoch: 4164 \tTraining Loss: 0.294955\n",
            "Epoch: 4165 \tTraining Loss: 0.294934\n",
            "Epoch: 4166 \tTraining Loss: 0.294920\n",
            "Epoch: 4167 \tTraining Loss: 0.294898\n",
            "Epoch: 4168 \tTraining Loss: 0.294884\n",
            "Epoch: 4169 \tTraining Loss: 0.294863\n",
            "Epoch: 4170 \tTraining Loss: 0.294849\n",
            "Epoch: 4171 \tTraining Loss: 0.294827\n",
            "Epoch: 4172 \tTraining Loss: 0.294813\n",
            "Epoch: 4173 \tTraining Loss: 0.294791\n",
            "Epoch: 4174 \tTraining Loss: 0.294778\n",
            "Epoch: 4175 \tTraining Loss: 0.294756\n",
            "Epoch: 4176 \tTraining Loss: 0.294742\n",
            "Epoch: 4177 \tTraining Loss: 0.294720\n",
            "Epoch: 4178 \tTraining Loss: 0.294706\n",
            "Epoch: 4179 \tTraining Loss: 0.294684\n",
            "Epoch: 4180 \tTraining Loss: 0.294671\n",
            "Epoch: 4181 \tTraining Loss: 0.294649\n",
            "Epoch: 4182 \tTraining Loss: 0.294635\n",
            "Epoch: 4183 \tTraining Loss: 0.294613\n",
            "Epoch: 4184 \tTraining Loss: 0.294600\n",
            "Epoch: 4185 \tTraining Loss: 0.294578\n",
            "Epoch: 4186 \tTraining Loss: 0.294564\n",
            "Epoch: 4187 \tTraining Loss: 0.294542\n",
            "Epoch: 4188 \tTraining Loss: 0.294529\n",
            "Epoch: 4189 \tTraining Loss: 0.294506\n",
            "Epoch: 4190 \tTraining Loss: 0.294493\n",
            "Epoch: 4191 \tTraining Loss: 0.294471\n",
            "Epoch: 4192 \tTraining Loss: 0.294457\n",
            "Epoch: 4193 \tTraining Loss: 0.294435\n",
            "Epoch: 4194 \tTraining Loss: 0.294422\n",
            "Epoch: 4195 \tTraining Loss: 0.294399\n",
            "Epoch: 4196 \tTraining Loss: 0.294386\n",
            "Epoch: 4197 \tTraining Loss: 0.294364\n",
            "Epoch: 4198 \tTraining Loss: 0.294351\n",
            "Epoch: 4199 \tTraining Loss: 0.294328\n",
            "Epoch: 4200 \tTraining Loss: 0.294315\n",
            "Epoch: 4201 \tTraining Loss: 0.294292\n",
            "Epoch: 4202 \tTraining Loss: 0.294279\n",
            "Epoch: 4203 \tTraining Loss: 0.294257\n",
            "Epoch: 4204 \tTraining Loss: 0.294244\n",
            "Epoch: 4205 \tTraining Loss: 0.294221\n",
            "Epoch: 4206 \tTraining Loss: 0.294208\n",
            "Epoch: 4207 \tTraining Loss: 0.294185\n",
            "Epoch: 4208 \tTraining Loss: 0.294172\n",
            "Epoch: 4209 \tTraining Loss: 0.294149\n",
            "Epoch: 4210 \tTraining Loss: 0.294137\n",
            "Epoch: 4211 \tTraining Loss: 0.294114\n",
            "Epoch: 4212 \tTraining Loss: 0.294101\n",
            "Epoch: 4213 \tTraining Loss: 0.294078\n",
            "Epoch: 4214 \tTraining Loss: 0.294066\n",
            "Epoch: 4215 \tTraining Loss: 0.294042\n",
            "Epoch: 4216 \tTraining Loss: 0.294030\n",
            "Epoch: 4217 \tTraining Loss: 0.294007\n",
            "Epoch: 4218 \tTraining Loss: 0.293994\n",
            "Epoch: 4219 \tTraining Loss: 0.293971\n",
            "Epoch: 4220 \tTraining Loss: 0.293958\n",
            "Epoch: 4221 \tTraining Loss: 0.293935\n",
            "Epoch: 4222 \tTraining Loss: 0.293923\n",
            "Epoch: 4223 \tTraining Loss: 0.293899\n",
            "Epoch: 4224 \tTraining Loss: 0.293887\n",
            "Epoch: 4225 \tTraining Loss: 0.293864\n",
            "Epoch: 4226 \tTraining Loss: 0.293851\n",
            "Epoch: 4227 \tTraining Loss: 0.293828\n",
            "Epoch: 4228 \tTraining Loss: 0.293816\n",
            "Epoch: 4229 \tTraining Loss: 0.293792\n",
            "Epoch: 4230 \tTraining Loss: 0.293780\n",
            "Epoch: 4231 \tTraining Loss: 0.293756\n",
            "Epoch: 4232 \tTraining Loss: 0.293745\n",
            "Epoch: 4233 \tTraining Loss: 0.293721\n",
            "Epoch: 4234 \tTraining Loss: 0.293709\n",
            "Epoch: 4235 \tTraining Loss: 0.293685\n",
            "Epoch: 4236 \tTraining Loss: 0.293673\n",
            "Epoch: 4237 \tTraining Loss: 0.293649\n",
            "Epoch: 4238 \tTraining Loss: 0.293638\n",
            "Epoch: 4239 \tTraining Loss: 0.293614\n",
            "Epoch: 4240 \tTraining Loss: 0.293602\n",
            "Epoch: 4241 \tTraining Loss: 0.293578\n",
            "Epoch: 4242 \tTraining Loss: 0.293567\n",
            "Epoch: 4243 \tTraining Loss: 0.293543\n",
            "Epoch: 4244 \tTraining Loss: 0.293532\n",
            "Epoch: 4245 \tTraining Loss: 0.293507\n",
            "Epoch: 4246 \tTraining Loss: 0.293496\n",
            "Epoch: 4247 \tTraining Loss: 0.293472\n",
            "Epoch: 4248 \tTraining Loss: 0.293461\n",
            "Epoch: 4249 \tTraining Loss: 0.293437\n",
            "Epoch: 4250 \tTraining Loss: 0.293426\n",
            "Epoch: 4251 \tTraining Loss: 0.293402\n",
            "Epoch: 4252 \tTraining Loss: 0.293392\n",
            "Epoch: 4253 \tTraining Loss: 0.293367\n",
            "Epoch: 4254 \tTraining Loss: 0.293356\n",
            "Epoch: 4255 \tTraining Loss: 0.293332\n",
            "Epoch: 4256 \tTraining Loss: 0.293323\n",
            "Epoch: 4257 \tTraining Loss: 0.293298\n",
            "Epoch: 4258 \tTraining Loss: 0.293289\n",
            "Epoch: 4259 \tTraining Loss: 0.293265\n",
            "Epoch: 4260 \tTraining Loss: 0.293257\n",
            "Epoch: 4261 \tTraining Loss: 0.293231\n",
            "Epoch: 4262 \tTraining Loss: 0.293223\n",
            "Epoch: 4263 \tTraining Loss: 0.293199\n",
            "Epoch: 4264 \tTraining Loss: 0.293194\n",
            "Epoch: 4265 \tTraining Loss: 0.293167\n",
            "Epoch: 4266 \tTraining Loss: 0.293160\n",
            "Epoch: 4267 \tTraining Loss: 0.293137\n",
            "Epoch: 4268 \tTraining Loss: 0.293134\n",
            "Epoch: 4269 \tTraining Loss: 0.293105\n",
            "Epoch: 4270 \tTraining Loss: 0.293097\n",
            "Epoch: 4271 \tTraining Loss: 0.293075\n",
            "Epoch: 4272 \tTraining Loss: 0.293078\n",
            "Epoch: 4273 \tTraining Loss: 0.293037\n",
            "Epoch: 4274 \tTraining Loss: 0.293022\n",
            "Epoch: 4275 \tTraining Loss: 0.293003\n",
            "Epoch: 4276 \tTraining Loss: 0.293008\n",
            "Epoch: 4277 \tTraining Loss: 0.292971\n",
            "Epoch: 4278 \tTraining Loss: 0.292960\n",
            "Epoch: 4279 \tTraining Loss: 0.292933\n",
            "Epoch: 4280 \tTraining Loss: 0.292932\n",
            "Epoch: 4281 \tTraining Loss: 0.292897\n",
            "Epoch: 4282 \tTraining Loss: 0.292889\n",
            "Epoch: 4283 \tTraining Loss: 0.292866\n",
            "Epoch: 4284 \tTraining Loss: 0.292871\n",
            "Epoch: 4285 \tTraining Loss: 0.292825\n",
            "Epoch: 4286 \tTraining Loss: 0.292807\n",
            "Epoch: 4287 \tTraining Loss: 0.292782\n",
            "Epoch: 4288 \tTraining Loss: 0.292780\n",
            "Epoch: 4289 \tTraining Loss: 0.292759\n",
            "Epoch: 4290 \tTraining Loss: 0.292771\n",
            "Epoch: 4291 \tTraining Loss: 0.292679\n",
            "Epoch: 4292 \tTraining Loss: 0.292837\n",
            "Epoch: 4293 \tTraining Loss: 0.292897\n",
            "Epoch: 4294 \tTraining Loss: 0.293388\n",
            "Epoch: 4295 \tTraining Loss: 0.292741\n",
            "Epoch: 4296 \tTraining Loss: 0.293156\n",
            "Epoch: 4297 \tTraining Loss: 0.292857\n",
            "Epoch: 4298 \tTraining Loss: 0.293062\n",
            "Epoch: 4299 \tTraining Loss: 0.292589\n",
            "Epoch: 4300 \tTraining Loss: 0.294084\n",
            "Epoch: 4301 \tTraining Loss: 0.293136\n",
            "Epoch: 4302 \tTraining Loss: 0.293269\n",
            "Epoch: 4303 \tTraining Loss: 0.292813\n",
            "Epoch: 4304 \tTraining Loss: 0.292761\n",
            "Epoch: 4305 \tTraining Loss: 0.292634\n",
            "Epoch: 4306 \tTraining Loss: 0.292522\n",
            "Epoch: 4307 \tTraining Loss: 0.292969\n",
            "Epoch: 4308 \tTraining Loss: 0.292529\n",
            "Epoch: 4309 \tTraining Loss: 0.292785\n",
            "Epoch: 4310 \tTraining Loss: 0.292778\n",
            "Epoch: 4311 \tTraining Loss: 0.293010\n",
            "Epoch: 4312 \tTraining Loss: 0.292793\n",
            "Epoch: 4313 \tTraining Loss: 0.292466\n",
            "Epoch: 4314 \tTraining Loss: 0.292452\n",
            "Epoch: 4315 \tTraining Loss: 0.292419\n",
            "Epoch: 4316 \tTraining Loss: 0.293080\n",
            "Epoch: 4317 \tTraining Loss: 0.293000\n",
            "Epoch: 4318 \tTraining Loss: 0.292862\n",
            "Epoch: 4319 \tTraining Loss: 0.292665\n",
            "Epoch: 4320 \tTraining Loss: 0.292285\n",
            "Epoch: 4321 \tTraining Loss: 0.292926\n",
            "Epoch: 4322 \tTraining Loss: 0.292779\n",
            "Epoch: 4323 \tTraining Loss: 0.293838\n",
            "Epoch: 4324 \tTraining Loss: 0.292856\n",
            "Epoch: 4325 \tTraining Loss: 0.292518\n",
            "Epoch: 4326 \tTraining Loss: 0.292938\n",
            "Epoch: 4327 \tTraining Loss: 0.293033\n",
            "Epoch: 4328 \tTraining Loss: 0.293603\n",
            "Epoch: 4329 \tTraining Loss: 0.292493\n",
            "Epoch: 4330 \tTraining Loss: 0.292272\n",
            "Epoch: 4331 \tTraining Loss: 0.292551\n",
            "Epoch: 4332 \tTraining Loss: 0.292601\n",
            "Epoch: 4333 \tTraining Loss: 0.292258\n",
            "Epoch: 4334 \tTraining Loss: 0.292072\n",
            "Epoch: 4335 \tTraining Loss: 0.292082\n",
            "Epoch: 4336 \tTraining Loss: 0.292155\n",
            "Epoch: 4337 \tTraining Loss: 0.292098\n",
            "Epoch: 4338 \tTraining Loss: 0.291983\n",
            "Epoch: 4339 \tTraining Loss: 0.291926\n",
            "Epoch: 4340 \tTraining Loss: 0.291960\n",
            "Epoch: 4341 \tTraining Loss: 0.291980\n",
            "Epoch: 4342 \tTraining Loss: 0.291876\n",
            "Epoch: 4343 \tTraining Loss: 0.291800\n",
            "Epoch: 4344 \tTraining Loss: 0.291829\n",
            "Epoch: 4345 \tTraining Loss: 0.291809\n",
            "Epoch: 4346 \tTraining Loss: 0.291793\n",
            "Epoch: 4347 \tTraining Loss: 0.291775\n",
            "Epoch: 4348 \tTraining Loss: 0.291701\n",
            "Epoch: 4349 \tTraining Loss: 0.291667\n",
            "Epoch: 4350 \tTraining Loss: 0.291701\n",
            "Epoch: 4351 \tTraining Loss: 0.291693\n",
            "Epoch: 4352 \tTraining Loss: 0.291669\n",
            "Epoch: 4353 \tTraining Loss: 0.291615\n",
            "Epoch: 4354 \tTraining Loss: 0.291565\n",
            "Epoch: 4355 \tTraining Loss: 0.291573\n",
            "Epoch: 4356 \tTraining Loss: 0.291575\n",
            "Epoch: 4357 \tTraining Loss: 0.291567\n",
            "Epoch: 4358 \tTraining Loss: 0.291525\n",
            "Epoch: 4359 \tTraining Loss: 0.291473\n",
            "Epoch: 4360 \tTraining Loss: 0.291445\n",
            "Epoch: 4361 \tTraining Loss: 0.291454\n",
            "Epoch: 4362 \tTraining Loss: 0.291451\n",
            "Epoch: 4363 \tTraining Loss: 0.291433\n",
            "Epoch: 4364 \tTraining Loss: 0.291399\n",
            "Epoch: 4365 \tTraining Loss: 0.291350\n",
            "Epoch: 4366 \tTraining Loss: 0.291331\n",
            "Epoch: 4367 \tTraining Loss: 0.291326\n",
            "Epoch: 4368 \tTraining Loss: 0.291319\n",
            "Epoch: 4369 \tTraining Loss: 0.291310\n",
            "Epoch: 4370 \tTraining Loss: 0.291275\n",
            "Epoch: 4371 \tTraining Loss: 0.291237\n",
            "Epoch: 4372 \tTraining Loss: 0.291213\n",
            "Epoch: 4373 \tTraining Loss: 0.291197\n",
            "Epoch: 4374 \tTraining Loss: 0.291191\n",
            "Epoch: 4375 \tTraining Loss: 0.291179\n",
            "Epoch: 4376 \tTraining Loss: 0.291156\n",
            "Epoch: 4377 \tTraining Loss: 0.291127\n",
            "Epoch: 4378 \tTraining Loss: 0.291099\n",
            "Epoch: 4379 \tTraining Loss: 0.291076\n",
            "Epoch: 4380 \tTraining Loss: 0.291060\n",
            "Epoch: 4381 \tTraining Loss: 0.291045\n",
            "Epoch: 4382 \tTraining Loss: 0.291029\n",
            "Epoch: 4383 \tTraining Loss: 0.291012\n",
            "Epoch: 4384 \tTraining Loss: 0.290990\n",
            "Epoch: 4385 \tTraining Loss: 0.290968\n",
            "Epoch: 4386 \tTraining Loss: 0.290945\n",
            "Epoch: 4387 \tTraining Loss: 0.290923\n",
            "Epoch: 4388 \tTraining Loss: 0.290903\n",
            "Epoch: 4389 \tTraining Loss: 0.290885\n",
            "Epoch: 4390 \tTraining Loss: 0.290868\n",
            "Epoch: 4391 \tTraining Loss: 0.290850\n",
            "Epoch: 4392 \tTraining Loss: 0.290832\n",
            "Epoch: 4393 \tTraining Loss: 0.290813\n",
            "Epoch: 4394 \tTraining Loss: 0.290794\n",
            "Epoch: 4395 \tTraining Loss: 0.290774\n",
            "Epoch: 4396 \tTraining Loss: 0.290754\n",
            "Epoch: 4397 \tTraining Loss: 0.290733\n",
            "Epoch: 4398 \tTraining Loss: 0.290713\n",
            "Epoch: 4399 \tTraining Loss: 0.290693\n",
            "Epoch: 4400 \tTraining Loss: 0.290673\n",
            "Epoch: 4401 \tTraining Loss: 0.290654\n",
            "Epoch: 4402 \tTraining Loss: 0.290635\n",
            "Epoch: 4403 \tTraining Loss: 0.290616\n",
            "Epoch: 4404 \tTraining Loss: 0.290597\n",
            "Epoch: 4405 \tTraining Loss: 0.290578\n",
            "Epoch: 4406 \tTraining Loss: 0.290559\n",
            "Epoch: 4407 \tTraining Loss: 0.290540\n",
            "Epoch: 4408 \tTraining Loss: 0.290521\n",
            "Epoch: 4409 \tTraining Loss: 0.290503\n",
            "Epoch: 4410 \tTraining Loss: 0.290485\n",
            "Epoch: 4411 \tTraining Loss: 0.290467\n",
            "Epoch: 4412 \tTraining Loss: 0.290450\n",
            "Epoch: 4413 \tTraining Loss: 0.290435\n",
            "Epoch: 4414 \tTraining Loss: 0.290420\n",
            "Epoch: 4415 \tTraining Loss: 0.290410\n",
            "Epoch: 4416 \tTraining Loss: 0.290402\n",
            "Epoch: 4417 \tTraining Loss: 0.290406\n",
            "Epoch: 4418 \tTraining Loss: 0.290412\n",
            "Epoch: 4419 \tTraining Loss: 0.290447\n",
            "Epoch: 4420 \tTraining Loss: 0.290462\n",
            "Epoch: 4421 \tTraining Loss: 0.290532\n",
            "Epoch: 4422 \tTraining Loss: 0.290507\n",
            "Epoch: 4423 \tTraining Loss: 0.290524\n",
            "Epoch: 4424 \tTraining Loss: 0.290406\n",
            "Epoch: 4425 \tTraining Loss: 0.290321\n",
            "Epoch: 4426 \tTraining Loss: 0.290225\n",
            "Epoch: 4427 \tTraining Loss: 0.290169\n",
            "Epoch: 4428 \tTraining Loss: 0.290137\n",
            "Epoch: 4429 \tTraining Loss: 0.290124\n",
            "Epoch: 4430 \tTraining Loss: 0.290126\n",
            "Epoch: 4431 \tTraining Loss: 0.290137\n",
            "Epoch: 4432 \tTraining Loss: 0.290173\n",
            "Epoch: 4433 \tTraining Loss: 0.290199\n",
            "Epoch: 4434 \tTraining Loss: 0.290267\n",
            "Epoch: 4435 \tTraining Loss: 0.290236\n",
            "Epoch: 4436 \tTraining Loss: 0.290225\n",
            "Epoch: 4437 \tTraining Loss: 0.290109\n",
            "Epoch: 4438 \tTraining Loss: 0.290026\n",
            "Epoch: 4439 \tTraining Loss: 0.289954\n",
            "Epoch: 4440 \tTraining Loss: 0.289912\n",
            "Epoch: 4441 \tTraining Loss: 0.289889\n",
            "Epoch: 4442 \tTraining Loss: 0.289879\n",
            "Epoch: 4443 \tTraining Loss: 0.289881\n",
            "Epoch: 4444 \tTraining Loss: 0.289891\n",
            "Epoch: 4445 \tTraining Loss: 0.289924\n",
            "Epoch: 4446 \tTraining Loss: 0.289944\n",
            "Epoch: 4447 \tTraining Loss: 0.289999\n",
            "Epoch: 4448 \tTraining Loss: 0.289970\n",
            "Epoch: 4449 \tTraining Loss: 0.289965\n",
            "Epoch: 4450 \tTraining Loss: 0.289868\n",
            "Epoch: 4451 \tTraining Loss: 0.289799\n",
            "Epoch: 4452 \tTraining Loss: 0.289725\n",
            "Epoch: 4453 \tTraining Loss: 0.289678\n",
            "Epoch: 4454 \tTraining Loss: 0.289644\n",
            "Epoch: 4455 \tTraining Loss: 0.289620\n",
            "Epoch: 4456 \tTraining Loss: 0.289603\n",
            "Epoch: 4457 \tTraining Loss: 0.289591\n",
            "Epoch: 4458 \tTraining Loss: 0.289585\n",
            "Epoch: 4459 \tTraining Loss: 0.289584\n",
            "Epoch: 4460 \tTraining Loss: 0.289597\n",
            "Epoch: 4461 \tTraining Loss: 0.289609\n",
            "Epoch: 4462 \tTraining Loss: 0.289651\n",
            "Epoch: 4463 \tTraining Loss: 0.289659\n",
            "Epoch: 4464 \tTraining Loss: 0.289711\n",
            "Epoch: 4465 \tTraining Loss: 0.289670\n",
            "Epoch: 4466 \tTraining Loss: 0.289667\n",
            "Epoch: 4467 \tTraining Loss: 0.289567\n",
            "Epoch: 4468 \tTraining Loss: 0.289500\n",
            "Epoch: 4469 \tTraining Loss: 0.289415\n",
            "Epoch: 4470 \tTraining Loss: 0.289361\n",
            "Epoch: 4471 \tTraining Loss: 0.289322\n",
            "Epoch: 4472 \tTraining Loss: 0.289295\n",
            "Epoch: 4473 \tTraining Loss: 0.289276\n",
            "Epoch: 4474 \tTraining Loss: 0.289262\n",
            "Epoch: 4475 \tTraining Loss: 0.289253\n",
            "Epoch: 4476 \tTraining Loss: 0.289248\n",
            "Epoch: 4477 \tTraining Loss: 0.289255\n",
            "Epoch: 4478 \tTraining Loss: 0.289263\n",
            "Epoch: 4479 \tTraining Loss: 0.289299\n",
            "Epoch: 4480 \tTraining Loss: 0.289324\n",
            "Epoch: 4481 \tTraining Loss: 0.289380\n",
            "Epoch: 4482 \tTraining Loss: 0.289369\n",
            "Epoch: 4483 \tTraining Loss: 0.289404\n",
            "Epoch: 4484 \tTraining Loss: 0.289316\n",
            "Epoch: 4485 \tTraining Loss: 0.289256\n",
            "Epoch: 4486 \tTraining Loss: 0.289150\n",
            "Epoch: 4487 \tTraining Loss: 0.289066\n",
            "Epoch: 4488 \tTraining Loss: 0.289007\n",
            "Epoch: 4489 \tTraining Loss: 0.288971\n",
            "Epoch: 4490 \tTraining Loss: 0.288953\n",
            "Epoch: 4491 \tTraining Loss: 0.288944\n",
            "Epoch: 4492 \tTraining Loss: 0.288955\n",
            "Epoch: 4493 \tTraining Loss: 0.288940\n",
            "Epoch: 4494 \tTraining Loss: 0.288951\n",
            "Epoch: 4495 \tTraining Loss: 0.288957\n",
            "Epoch: 4496 \tTraining Loss: 0.288988\n",
            "Epoch: 4497 \tTraining Loss: 0.288987\n",
            "Epoch: 4498 \tTraining Loss: 0.289021\n",
            "Epoch: 4499 \tTraining Loss: 0.289001\n",
            "Epoch: 4500 \tTraining Loss: 0.289007\n",
            "Epoch: 4501 \tTraining Loss: 0.288992\n",
            "Epoch: 4502 \tTraining Loss: 0.288912\n",
            "Epoch: 4503 \tTraining Loss: 0.289033\n",
            "Epoch: 4504 \tTraining Loss: 0.288854\n",
            "Epoch: 4505 \tTraining Loss: 0.288808\n",
            "Epoch: 4506 \tTraining Loss: 0.288744\n",
            "Epoch: 4507 \tTraining Loss: 0.288746\n",
            "Epoch: 4508 \tTraining Loss: 0.288703\n",
            "Epoch: 4509 \tTraining Loss: 0.288663\n",
            "Epoch: 4510 \tTraining Loss: 0.288676\n",
            "Epoch: 4511 \tTraining Loss: 0.288670\n",
            "Epoch: 4512 \tTraining Loss: 0.288575\n",
            "Epoch: 4513 \tTraining Loss: 0.288704\n",
            "Epoch: 4514 \tTraining Loss: 0.288986\n",
            "Epoch: 4515 \tTraining Loss: 0.288915\n",
            "Epoch: 4516 \tTraining Loss: 0.288818\n",
            "Epoch: 4517 \tTraining Loss: 0.288991\n",
            "Epoch: 4518 \tTraining Loss: 0.289144\n",
            "Epoch: 4519 \tTraining Loss: 0.288924\n",
            "Epoch: 4520 \tTraining Loss: 0.289148\n",
            "Epoch: 4521 \tTraining Loss: 0.288657\n",
            "Epoch: 4522 \tTraining Loss: 0.288499\n",
            "Epoch: 4523 \tTraining Loss: 0.289075\n",
            "Epoch: 4524 \tTraining Loss: 0.288487\n",
            "Epoch: 4525 \tTraining Loss: 0.289822\n",
            "Epoch: 4526 \tTraining Loss: 0.288380\n",
            "Epoch: 4527 \tTraining Loss: 0.288627\n",
            "Epoch: 4528 \tTraining Loss: 0.288440\n",
            "Epoch: 4529 \tTraining Loss: 0.288514\n",
            "Epoch: 4530 \tTraining Loss: 0.288952\n",
            "Epoch: 4531 \tTraining Loss: 0.288937\n",
            "Epoch: 4532 \tTraining Loss: 0.289337\n",
            "Epoch: 4533 \tTraining Loss: 0.288719\n",
            "Epoch: 4534 \tTraining Loss: 0.288955\n",
            "Epoch: 4535 \tTraining Loss: 0.288248\n",
            "Epoch: 4536 \tTraining Loss: 0.288793\n",
            "Epoch: 4537 \tTraining Loss: 0.288388\n",
            "Epoch: 4538 \tTraining Loss: 0.288649\n",
            "Epoch: 4539 \tTraining Loss: 0.288470\n",
            "Epoch: 4540 \tTraining Loss: 0.288278\n",
            "Epoch: 4541 \tTraining Loss: 0.288157\n",
            "Epoch: 4542 \tTraining Loss: 0.287972\n",
            "Epoch: 4543 \tTraining Loss: 0.287939\n",
            "Epoch: 4544 \tTraining Loss: 0.287871\n",
            "Epoch: 4545 \tTraining Loss: 0.287858\n",
            "Epoch: 4546 \tTraining Loss: 0.287834\n",
            "Epoch: 4547 \tTraining Loss: 0.287836\n",
            "Epoch: 4548 \tTraining Loss: 0.287820\n",
            "Epoch: 4549 \tTraining Loss: 0.287814\n",
            "Epoch: 4550 \tTraining Loss: 0.287841\n",
            "Epoch: 4551 \tTraining Loss: 0.287841\n",
            "Epoch: 4552 \tTraining Loss: 0.287855\n",
            "Epoch: 4553 \tTraining Loss: 0.287865\n",
            "Epoch: 4554 \tTraining Loss: 0.287832\n",
            "Epoch: 4555 \tTraining Loss: 0.287722\n",
            "Epoch: 4556 \tTraining Loss: 0.287728\n",
            "Epoch: 4557 \tTraining Loss: 0.287576\n",
            "Epoch: 4558 \tTraining Loss: 0.287565\n",
            "Epoch: 4559 \tTraining Loss: 0.287553\n",
            "Epoch: 4560 \tTraining Loss: 0.287547\n",
            "Epoch: 4561 \tTraining Loss: 0.287560\n",
            "Epoch: 4562 \tTraining Loss: 0.287621\n",
            "Epoch: 4563 \tTraining Loss: 0.287692\n",
            "Epoch: 4564 \tTraining Loss: 0.287736\n",
            "Epoch: 4565 \tTraining Loss: 0.287751\n",
            "Epoch: 4566 \tTraining Loss: 0.287723\n",
            "Epoch: 4567 \tTraining Loss: 0.287669\n",
            "Epoch: 4568 \tTraining Loss: 0.287610\n",
            "Epoch: 4569 \tTraining Loss: 0.287392\n",
            "Epoch: 4570 \tTraining Loss: 0.287550\n",
            "Epoch: 4571 \tTraining Loss: 0.287330\n",
            "Epoch: 4572 \tTraining Loss: 0.287445\n",
            "Epoch: 4573 \tTraining Loss: 0.287442\n",
            "Epoch: 4574 \tTraining Loss: 0.287482\n",
            "Epoch: 4575 \tTraining Loss: 0.287569\n",
            "Epoch: 4576 \tTraining Loss: 0.287629\n",
            "Epoch: 4577 \tTraining Loss: 0.287698\n",
            "Epoch: 4578 \tTraining Loss: 0.287415\n",
            "Epoch: 4579 \tTraining Loss: 0.287548\n",
            "Epoch: 4580 \tTraining Loss: 0.287148\n",
            "Epoch: 4581 \tTraining Loss: 0.287195\n",
            "Epoch: 4582 \tTraining Loss: 0.287204\n",
            "Epoch: 4583 \tTraining Loss: 0.287198\n",
            "Epoch: 4584 \tTraining Loss: 0.287498\n",
            "Epoch: 4585 \tTraining Loss: 0.287513\n",
            "Epoch: 4586 \tTraining Loss: 0.287737\n",
            "Epoch: 4587 \tTraining Loss: 0.287604\n",
            "Epoch: 4588 \tTraining Loss: 0.287583\n",
            "Epoch: 4589 \tTraining Loss: 0.287050\n",
            "Epoch: 4590 \tTraining Loss: 0.287034\n",
            "Epoch: 4591 \tTraining Loss: 0.287103\n",
            "Epoch: 4592 \tTraining Loss: 0.287190\n",
            "Epoch: 4593 \tTraining Loss: 0.287593\n",
            "Epoch: 4594 \tTraining Loss: 0.287555\n",
            "Epoch: 4595 \tTraining Loss: 0.287547\n",
            "Epoch: 4596 \tTraining Loss: 0.287338\n",
            "Epoch: 4597 \tTraining Loss: 0.287133\n",
            "Epoch: 4598 \tTraining Loss: 0.286788\n",
            "Epoch: 4599 \tTraining Loss: 0.286937\n",
            "Epoch: 4600 \tTraining Loss: 0.287075\n",
            "Epoch: 4601 \tTraining Loss: 0.287412\n",
            "Epoch: 4602 \tTraining Loss: 0.287102\n",
            "Epoch: 4603 \tTraining Loss: 0.287062\n",
            "Epoch: 4604 \tTraining Loss: 0.286758\n",
            "Epoch: 4605 \tTraining Loss: 0.286828\n",
            "Epoch: 4606 \tTraining Loss: 0.286637\n",
            "Epoch: 4607 \tTraining Loss: 0.286886\n",
            "Epoch: 4608 \tTraining Loss: 0.286811\n",
            "Epoch: 4609 \tTraining Loss: 0.286923\n",
            "Epoch: 4610 \tTraining Loss: 0.286733\n",
            "Epoch: 4611 \tTraining Loss: 0.286759\n",
            "Epoch: 4612 \tTraining Loss: 0.286679\n",
            "Epoch: 4613 \tTraining Loss: 0.286482\n",
            "Epoch: 4614 \tTraining Loss: 0.286819\n",
            "Epoch: 4615 \tTraining Loss: 0.286494\n",
            "Epoch: 4616 \tTraining Loss: 0.286611\n",
            "Epoch: 4617 \tTraining Loss: 0.286563\n",
            "Epoch: 4618 \tTraining Loss: 0.286499\n",
            "Epoch: 4619 \tTraining Loss: 0.286555\n",
            "Epoch: 4620 \tTraining Loss: 0.286567\n",
            "Epoch: 4621 \tTraining Loss: 0.286652\n",
            "Epoch: 4622 \tTraining Loss: 0.286356\n",
            "Epoch: 4623 \tTraining Loss: 0.286849\n",
            "Epoch: 4624 \tTraining Loss: 0.286466\n",
            "Epoch: 4625 \tTraining Loss: 0.287027\n",
            "Epoch: 4626 \tTraining Loss: 0.286362\n",
            "Epoch: 4627 \tTraining Loss: 0.286539\n",
            "Epoch: 4628 \tTraining Loss: 0.286253\n",
            "Epoch: 4629 \tTraining Loss: 0.286314\n",
            "Epoch: 4630 \tTraining Loss: 0.286325\n",
            "Epoch: 4631 \tTraining Loss: 0.286233\n",
            "Epoch: 4632 \tTraining Loss: 0.286228\n",
            "Epoch: 4633 \tTraining Loss: 0.286192\n",
            "Epoch: 4634 \tTraining Loss: 0.286170\n",
            "Epoch: 4635 \tTraining Loss: 0.286170\n",
            "Epoch: 4636 \tTraining Loss: 0.286218\n",
            "Epoch: 4637 \tTraining Loss: 0.286091\n",
            "Epoch: 4638 \tTraining Loss: 0.286050\n",
            "Epoch: 4639 \tTraining Loss: 0.286208\n",
            "Epoch: 4640 \tTraining Loss: 0.286049\n",
            "Epoch: 4641 \tTraining Loss: 0.286245\n",
            "Epoch: 4642 \tTraining Loss: 0.285939\n",
            "Epoch: 4643 \tTraining Loss: 0.286488\n",
            "Epoch: 4644 \tTraining Loss: 0.285965\n",
            "Epoch: 4645 \tTraining Loss: 0.286221\n",
            "Epoch: 4646 \tTraining Loss: 0.286213\n",
            "Epoch: 4647 \tTraining Loss: 0.286487\n",
            "Epoch: 4648 \tTraining Loss: 0.286467\n",
            "Epoch: 4649 \tTraining Loss: 0.286653\n",
            "Epoch: 4650 \tTraining Loss: 0.286422\n",
            "Epoch: 4651 \tTraining Loss: 0.286254\n",
            "Epoch: 4652 \tTraining Loss: 0.286000\n",
            "Epoch: 4653 \tTraining Loss: 0.285846\n",
            "Epoch: 4654 \tTraining Loss: 0.285708\n",
            "Epoch: 4655 \tTraining Loss: 0.285686\n",
            "Epoch: 4656 \tTraining Loss: 0.285691\n",
            "Epoch: 4657 \tTraining Loss: 0.285703\n",
            "Epoch: 4658 \tTraining Loss: 0.285735\n",
            "Epoch: 4659 \tTraining Loss: 0.285797\n",
            "Epoch: 4660 \tTraining Loss: 0.285856\n",
            "Epoch: 4661 \tTraining Loss: 0.285849\n",
            "Epoch: 4662 \tTraining Loss: 0.285889\n",
            "Epoch: 4663 \tTraining Loss: 0.285822\n",
            "Epoch: 4664 \tTraining Loss: 0.285849\n",
            "Epoch: 4665 \tTraining Loss: 0.285762\n",
            "Epoch: 4666 \tTraining Loss: 0.285742\n",
            "Epoch: 4667 \tTraining Loss: 0.285591\n",
            "Epoch: 4668 \tTraining Loss: 0.285853\n",
            "Epoch: 4669 \tTraining Loss: 0.285519\n",
            "Epoch: 4670 \tTraining Loss: 0.285616\n",
            "Epoch: 4671 \tTraining Loss: 0.285520\n",
            "Epoch: 4672 \tTraining Loss: 0.285537\n",
            "Epoch: 4673 \tTraining Loss: 0.285441\n",
            "Epoch: 4674 \tTraining Loss: 0.285440\n",
            "Epoch: 4675 \tTraining Loss: 0.285351\n",
            "Epoch: 4676 \tTraining Loss: 0.285467\n",
            "Epoch: 4677 \tTraining Loss: 0.285434\n",
            "Epoch: 4678 \tTraining Loss: 0.285494\n",
            "Epoch: 4679 \tTraining Loss: 0.285395\n",
            "Epoch: 4680 \tTraining Loss: 0.285765\n",
            "Epoch: 4681 \tTraining Loss: 0.285502\n",
            "Epoch: 4682 \tTraining Loss: 0.285750\n",
            "Epoch: 4683 \tTraining Loss: 0.285666\n",
            "Epoch: 4684 \tTraining Loss: 0.285795\n",
            "Epoch: 4685 \tTraining Loss: 0.285465\n",
            "Epoch: 4686 \tTraining Loss: 0.285330\n",
            "Epoch: 4687 \tTraining Loss: 0.285122\n",
            "Epoch: 4688 \tTraining Loss: 0.285191\n",
            "Epoch: 4689 \tTraining Loss: 0.285111\n",
            "Epoch: 4690 \tTraining Loss: 0.285181\n",
            "Epoch: 4691 \tTraining Loss: 0.285176\n",
            "Epoch: 4692 \tTraining Loss: 0.285221\n",
            "Epoch: 4693 \tTraining Loss: 0.285305\n",
            "Epoch: 4694 \tTraining Loss: 0.285339\n",
            "Epoch: 4695 \tTraining Loss: 0.285335\n",
            "Epoch: 4696 \tTraining Loss: 0.285197\n",
            "Epoch: 4697 \tTraining Loss: 0.285072\n",
            "Epoch: 4698 \tTraining Loss: 0.284965\n",
            "Epoch: 4699 \tTraining Loss: 0.284824\n",
            "Epoch: 4700 \tTraining Loss: 0.284889\n",
            "Epoch: 4701 \tTraining Loss: 0.284818\n",
            "Epoch: 4702 \tTraining Loss: 0.284859\n",
            "Epoch: 4703 \tTraining Loss: 0.284829\n",
            "Epoch: 4704 \tTraining Loss: 0.284879\n",
            "Epoch: 4705 \tTraining Loss: 0.284942\n",
            "Epoch: 4706 \tTraining Loss: 0.284975\n",
            "Epoch: 4707 \tTraining Loss: 0.284925\n",
            "Epoch: 4708 \tTraining Loss: 0.284885\n",
            "Epoch: 4709 \tTraining Loss: 0.284758\n",
            "Epoch: 4710 \tTraining Loss: 0.284710\n",
            "Epoch: 4711 \tTraining Loss: 0.284738\n",
            "Epoch: 4712 \tTraining Loss: 0.284720\n",
            "Epoch: 4713 \tTraining Loss: 0.284821\n",
            "Epoch: 4714 \tTraining Loss: 0.284846\n",
            "Epoch: 4715 \tTraining Loss: 0.284903\n",
            "Epoch: 4716 \tTraining Loss: 0.285156\n",
            "Epoch: 4717 \tTraining Loss: 0.285238\n",
            "Epoch: 4718 \tTraining Loss: 0.285696\n",
            "Epoch: 4719 \tTraining Loss: 0.285691\n",
            "Epoch: 4720 \tTraining Loss: 0.286660\n",
            "Epoch: 4721 \tTraining Loss: 0.286667\n",
            "Epoch: 4722 \tTraining Loss: 0.288950\n",
            "Epoch: 4723 \tTraining Loss: 0.289896\n",
            "Epoch: 4724 \tTraining Loss: 0.301911\n",
            "Epoch: 4725 \tTraining Loss: 0.307935\n",
            "Epoch: 4726 \tTraining Loss: 0.414329\n",
            "Epoch: 4727 \tTraining Loss: 0.485620\n",
            "Epoch: 4728 \tTraining Loss: 0.610034\n",
            "Epoch: 4729 \tTraining Loss: 0.459227\n",
            "Epoch: 4730 \tTraining Loss: 0.672086\n",
            "Epoch: 4731 \tTraining Loss: 0.503406\n",
            "Epoch: 4732 \tTraining Loss: 0.492776\n",
            "Epoch: 4733 \tTraining Loss: 0.489247\n",
            "Epoch: 4734 \tTraining Loss: 0.460128\n",
            "Epoch: 4735 \tTraining Loss: 0.423465\n",
            "Epoch: 4736 \tTraining Loss: 0.456549\n",
            "Epoch: 4737 \tTraining Loss: 0.402793\n",
            "Epoch: 4738 \tTraining Loss: 0.407112\n",
            "Epoch: 4739 \tTraining Loss: 0.379287\n",
            "Epoch: 4740 \tTraining Loss: 0.377554\n",
            "Epoch: 4741 \tTraining Loss: 0.362393\n",
            "Epoch: 4742 \tTraining Loss: 0.360394\n",
            "Epoch: 4743 \tTraining Loss: 0.335779\n",
            "Epoch: 4744 \tTraining Loss: 0.344766\n",
            "Epoch: 4745 \tTraining Loss: 0.353663\n",
            "Epoch: 4746 \tTraining Loss: 0.328692\n",
            "Epoch: 4747 \tTraining Loss: 0.327045\n",
            "Epoch: 4748 \tTraining Loss: 0.339910\n",
            "Epoch: 4749 \tTraining Loss: 0.320751\n",
            "Epoch: 4750 \tTraining Loss: 0.322512\n",
            "Epoch: 4751 \tTraining Loss: 0.319193\n",
            "Epoch: 4752 \tTraining Loss: 0.309917\n",
            "Epoch: 4753 \tTraining Loss: 0.309807\n",
            "Epoch: 4754 \tTraining Loss: 0.304352\n",
            "Epoch: 4755 \tTraining Loss: 0.305560\n",
            "Epoch: 4756 \tTraining Loss: 0.307584\n",
            "Epoch: 4757 \tTraining Loss: 0.305641\n",
            "Epoch: 4758 \tTraining Loss: 0.302481\n",
            "Epoch: 4759 \tTraining Loss: 0.301828\n",
            "Epoch: 4760 \tTraining Loss: 0.298163\n",
            "Epoch: 4761 \tTraining Loss: 0.299807\n",
            "Epoch: 4762 \tTraining Loss: 0.297040\n",
            "Epoch: 4763 \tTraining Loss: 0.296308\n",
            "Epoch: 4764 \tTraining Loss: 0.295481\n",
            "Epoch: 4765 \tTraining Loss: 0.293707\n",
            "Epoch: 4766 \tTraining Loss: 0.294451\n",
            "Epoch: 4767 \tTraining Loss: 0.293908\n",
            "Epoch: 4768 \tTraining Loss: 0.292955\n",
            "Epoch: 4769 \tTraining Loss: 0.291247\n",
            "Epoch: 4770 \tTraining Loss: 0.290963\n",
            "Epoch: 4771 \tTraining Loss: 0.290256\n",
            "Epoch: 4772 \tTraining Loss: 0.290689\n",
            "Epoch: 4773 \tTraining Loss: 0.289941\n",
            "Epoch: 4774 \tTraining Loss: 0.289260\n",
            "Epoch: 4775 \tTraining Loss: 0.288981\n",
            "Epoch: 4776 \tTraining Loss: 0.288268\n",
            "Epoch: 4777 \tTraining Loss: 0.288222\n",
            "Epoch: 4778 \tTraining Loss: 0.288142\n",
            "Epoch: 4779 \tTraining Loss: 0.288083\n",
            "Epoch: 4780 \tTraining Loss: 0.287857\n",
            "Epoch: 4781 \tTraining Loss: 0.287268\n",
            "Epoch: 4782 \tTraining Loss: 0.287116\n",
            "Epoch: 4783 \tTraining Loss: 0.286930\n",
            "Epoch: 4784 \tTraining Loss: 0.286702\n",
            "Epoch: 4785 \tTraining Loss: 0.286491\n",
            "Epoch: 4786 \tTraining Loss: 0.286419\n",
            "Epoch: 4787 \tTraining Loss: 0.286218\n",
            "Epoch: 4788 \tTraining Loss: 0.286243\n",
            "Epoch: 4789 \tTraining Loss: 0.286030\n",
            "Epoch: 4790 \tTraining Loss: 0.285926\n",
            "Epoch: 4791 \tTraining Loss: 0.285857\n",
            "Epoch: 4792 \tTraining Loss: 0.285858\n",
            "Epoch: 4793 \tTraining Loss: 0.285865\n",
            "Epoch: 4794 \tTraining Loss: 0.285766\n",
            "Epoch: 4795 \tTraining Loss: 0.285596\n",
            "Epoch: 4796 \tTraining Loss: 0.285719\n",
            "Epoch: 4797 \tTraining Loss: 0.285430\n",
            "Epoch: 4798 \tTraining Loss: 0.285459\n",
            "Epoch: 4799 \tTraining Loss: 0.285402\n",
            "Epoch: 4800 \tTraining Loss: 0.285234\n",
            "Epoch: 4801 \tTraining Loss: 0.285365\n",
            "Epoch: 4802 \tTraining Loss: 0.285091\n",
            "Epoch: 4803 \tTraining Loss: 0.285124\n",
            "Epoch: 4804 \tTraining Loss: 0.285051\n",
            "Epoch: 4805 \tTraining Loss: 0.284970\n",
            "Epoch: 4806 \tTraining Loss: 0.284893\n",
            "Epoch: 4807 \tTraining Loss: 0.284888\n",
            "Epoch: 4808 \tTraining Loss: 0.284841\n",
            "Epoch: 4809 \tTraining Loss: 0.284817\n",
            "Epoch: 4810 \tTraining Loss: 0.284760\n",
            "Epoch: 4811 \tTraining Loss: 0.284804\n",
            "Epoch: 4812 \tTraining Loss: 0.284748\n",
            "Epoch: 4813 \tTraining Loss: 0.284714\n",
            "Epoch: 4814 \tTraining Loss: 0.284628\n",
            "Epoch: 4815 \tTraining Loss: 0.284580\n",
            "Epoch: 4816 \tTraining Loss: 0.284564\n",
            "Epoch: 4817 \tTraining Loss: 0.284551\n",
            "Epoch: 4818 \tTraining Loss: 0.284541\n",
            "Epoch: 4819 \tTraining Loss: 0.284483\n",
            "Epoch: 4820 \tTraining Loss: 0.284537\n",
            "Epoch: 4821 \tTraining Loss: 0.284551\n",
            "Epoch: 4822 \tTraining Loss: 0.284563\n",
            "Epoch: 4823 \tTraining Loss: 0.284379\n",
            "Epoch: 4824 \tTraining Loss: 0.284668\n",
            "Epoch: 4825 \tTraining Loss: 0.284333\n",
            "Epoch: 4826 \tTraining Loss: 0.284429\n",
            "Epoch: 4827 \tTraining Loss: 0.284345\n",
            "Epoch: 4828 \tTraining Loss: 0.284379\n",
            "Epoch: 4829 \tTraining Loss: 0.284295\n",
            "Epoch: 4830 \tTraining Loss: 0.284295\n",
            "Epoch: 4831 \tTraining Loss: 0.284195\n",
            "Epoch: 4832 \tTraining Loss: 0.284277\n",
            "Epoch: 4833 \tTraining Loss: 0.284322\n",
            "Epoch: 4834 \tTraining Loss: 0.284323\n",
            "Epoch: 4835 \tTraining Loss: 0.284126\n",
            "Epoch: 4836 \tTraining Loss: 0.284388\n",
            "Epoch: 4837 \tTraining Loss: 0.284074\n",
            "Epoch: 4838 \tTraining Loss: 0.284160\n",
            "Epoch: 4839 \tTraining Loss: 0.284072\n",
            "Epoch: 4840 \tTraining Loss: 0.284206\n",
            "Epoch: 4841 \tTraining Loss: 0.283998\n",
            "Epoch: 4842 \tTraining Loss: 0.284042\n",
            "Epoch: 4843 \tTraining Loss: 0.283962\n",
            "Epoch: 4844 \tTraining Loss: 0.284115\n",
            "Epoch: 4845 \tTraining Loss: 0.283920\n",
            "Epoch: 4846 \tTraining Loss: 0.283972\n",
            "Epoch: 4847 \tTraining Loss: 0.283880\n",
            "Epoch: 4848 \tTraining Loss: 0.283998\n",
            "Epoch: 4849 \tTraining Loss: 0.283916\n",
            "Epoch: 4850 \tTraining Loss: 0.283899\n",
            "Epoch: 4851 \tTraining Loss: 0.283907\n",
            "Epoch: 4852 \tTraining Loss: 0.283822\n",
            "Epoch: 4853 \tTraining Loss: 0.283786\n",
            "Epoch: 4854 \tTraining Loss: 0.283880\n",
            "Epoch: 4855 \tTraining Loss: 0.283812\n",
            "Epoch: 4856 \tTraining Loss: 0.283803\n",
            "Epoch: 4857 \tTraining Loss: 0.283741\n",
            "Epoch: 4858 \tTraining Loss: 0.283726\n",
            "Epoch: 4859 \tTraining Loss: 0.283681\n",
            "Epoch: 4860 \tTraining Loss: 0.283792\n",
            "Epoch: 4861 \tTraining Loss: 0.283743\n",
            "Epoch: 4862 \tTraining Loss: 0.283768\n",
            "Epoch: 4863 \tTraining Loss: 0.283609\n",
            "Epoch: 4864 \tTraining Loss: 0.283907\n",
            "Epoch: 4865 \tTraining Loss: 0.283570\n",
            "Epoch: 4866 \tTraining Loss: 0.283685\n",
            "Epoch: 4867 \tTraining Loss: 0.283595\n",
            "Epoch: 4868 \tTraining Loss: 0.283713\n",
            "Epoch: 4869 \tTraining Loss: 0.283505\n",
            "Epoch: 4870 \tTraining Loss: 0.283556\n",
            "Epoch: 4871 \tTraining Loss: 0.283471\n",
            "Epoch: 4872 \tTraining Loss: 0.283630\n",
            "Epoch: 4873 \tTraining Loss: 0.283451\n",
            "Epoch: 4874 \tTraining Loss: 0.283455\n",
            "Epoch: 4875 \tTraining Loss: 0.283494\n",
            "Epoch: 4876 \tTraining Loss: 0.283500\n",
            "Epoch: 4877 \tTraining Loss: 0.283452\n",
            "Epoch: 4878 \tTraining Loss: 0.283512\n",
            "Epoch: 4879 \tTraining Loss: 0.283343\n",
            "Epoch: 4880 \tTraining Loss: 0.283423\n",
            "Epoch: 4881 \tTraining Loss: 0.283320\n",
            "Epoch: 4882 \tTraining Loss: 0.283530\n",
            "Epoch: 4883 \tTraining Loss: 0.283289\n",
            "Epoch: 4884 \tTraining Loss: 0.283512\n",
            "Epoch: 4885 \tTraining Loss: 0.283362\n",
            "Epoch: 4886 \tTraining Loss: 0.283452\n",
            "Epoch: 4887 \tTraining Loss: 0.283277\n",
            "Epoch: 4888 \tTraining Loss: 0.283677\n",
            "Epoch: 4889 \tTraining Loss: 0.283373\n",
            "Epoch: 4890 \tTraining Loss: 0.283393\n",
            "Epoch: 4891 \tTraining Loss: 0.283190\n",
            "Epoch: 4892 \tTraining Loss: 0.283222\n",
            "Epoch: 4893 \tTraining Loss: 0.283177\n",
            "Epoch: 4894 \tTraining Loss: 0.283294\n",
            "Epoch: 4895 \tTraining Loss: 0.283279\n",
            "Epoch: 4896 \tTraining Loss: 0.283410\n",
            "Epoch: 4897 \tTraining Loss: 0.283130\n",
            "Epoch: 4898 \tTraining Loss: 0.283806\n",
            "Epoch: 4899 \tTraining Loss: 0.283197\n",
            "Epoch: 4900 \tTraining Loss: 0.283851\n",
            "Epoch: 4901 \tTraining Loss: 0.283999\n",
            "Epoch: 4902 \tTraining Loss: 0.283266\n",
            "Epoch: 4903 \tTraining Loss: 0.284061\n",
            "Epoch: 4904 \tTraining Loss: 0.283772\n",
            "Epoch: 4905 \tTraining Loss: 0.283219\n",
            "Epoch: 4906 \tTraining Loss: 0.283793\n",
            "Epoch: 4907 \tTraining Loss: 0.283395\n",
            "Epoch: 4908 \tTraining Loss: 0.283335\n",
            "Epoch: 4909 \tTraining Loss: 0.283697\n",
            "Epoch: 4910 \tTraining Loss: 0.283448\n",
            "Epoch: 4911 \tTraining Loss: 0.284883\n",
            "Epoch: 4912 \tTraining Loss: 0.284543\n",
            "Epoch: 4913 \tTraining Loss: 0.283110\n",
            "Epoch: 4914 \tTraining Loss: 0.284413\n",
            "Epoch: 4915 \tTraining Loss: 0.284277\n",
            "Epoch: 4916 \tTraining Loss: 0.283017\n",
            "Epoch: 4917 \tTraining Loss: 0.283682\n",
            "Epoch: 4918 \tTraining Loss: 0.283601\n",
            "Epoch: 4919 \tTraining Loss: 0.283122\n",
            "Epoch: 4920 \tTraining Loss: 0.283757\n",
            "Epoch: 4921 \tTraining Loss: 0.283844\n",
            "Epoch: 4922 \tTraining Loss: 0.282933\n",
            "Epoch: 4923 \tTraining Loss: 0.283752\n",
            "Epoch: 4924 \tTraining Loss: 0.284277\n",
            "Epoch: 4925 \tTraining Loss: 0.283374\n",
            "Epoch: 4926 \tTraining Loss: 0.283229\n",
            "Epoch: 4927 \tTraining Loss: 0.283896\n",
            "Epoch: 4928 \tTraining Loss: 0.283631\n",
            "Epoch: 4929 \tTraining Loss: 0.282994\n",
            "Epoch: 4930 \tTraining Loss: 0.283933\n",
            "Epoch: 4931 \tTraining Loss: 0.283853\n",
            "Epoch: 4932 \tTraining Loss: 0.283269\n",
            "Epoch: 4933 \tTraining Loss: 0.283475\n",
            "Epoch: 4934 \tTraining Loss: 0.283670\n",
            "Epoch: 4935 \tTraining Loss: 0.283430\n",
            "Epoch: 4936 \tTraining Loss: 0.282900\n",
            "Epoch: 4937 \tTraining Loss: 0.283557\n",
            "Epoch: 4938 \tTraining Loss: 0.283519\n",
            "Epoch: 4939 \tTraining Loss: 0.282993\n",
            "Epoch: 4940 \tTraining Loss: 0.282850\n",
            "Epoch: 4941 \tTraining Loss: 0.283646\n",
            "Epoch: 4942 \tTraining Loss: 0.282936\n",
            "Epoch: 4943 \tTraining Loss: 0.283210\n",
            "Epoch: 4944 \tTraining Loss: 0.283225\n",
            "Epoch: 4945 \tTraining Loss: 0.282736\n",
            "Epoch: 4946 \tTraining Loss: 0.282964\n",
            "Epoch: 4947 \tTraining Loss: 0.283006\n",
            "Epoch: 4948 \tTraining Loss: 0.282585\n",
            "Epoch: 4949 \tTraining Loss: 0.282744\n",
            "Epoch: 4950 \tTraining Loss: 0.282797\n",
            "Epoch: 4951 \tTraining Loss: 0.282709\n",
            "Epoch: 4952 \tTraining Loss: 0.282542\n",
            "Epoch: 4953 \tTraining Loss: 0.282682\n",
            "Epoch: 4954 \tTraining Loss: 0.282434\n",
            "Epoch: 4955 \tTraining Loss: 0.282501\n",
            "Epoch: 4956 \tTraining Loss: 0.282513\n",
            "Epoch: 4957 \tTraining Loss: 0.282407\n",
            "Epoch: 4958 \tTraining Loss: 0.282353\n",
            "Epoch: 4959 \tTraining Loss: 0.282468\n",
            "Epoch: 4960 \tTraining Loss: 0.282363\n",
            "Epoch: 4961 \tTraining Loss: 0.282435\n",
            "Epoch: 4962 \tTraining Loss: 0.282345\n",
            "Epoch: 4963 \tTraining Loss: 0.282293\n",
            "Epoch: 4964 \tTraining Loss: 0.282261\n",
            "Epoch: 4965 \tTraining Loss: 0.282280\n",
            "Epoch: 4966 \tTraining Loss: 0.282286\n",
            "Epoch: 4967 \tTraining Loss: 0.282204\n",
            "Epoch: 4968 \tTraining Loss: 0.282322\n",
            "Epoch: 4969 \tTraining Loss: 0.282186\n",
            "Epoch: 4970 \tTraining Loss: 0.282301\n",
            "Epoch: 4971 \tTraining Loss: 0.282328\n",
            "Epoch: 4972 \tTraining Loss: 0.282124\n",
            "Epoch: 4973 \tTraining Loss: 0.282374\n",
            "Epoch: 4974 \tTraining Loss: 0.282186\n",
            "Epoch: 4975 \tTraining Loss: 0.282299\n",
            "Epoch: 4976 \tTraining Loss: 0.282426\n",
            "Epoch: 4977 \tTraining Loss: 0.282077\n",
            "Epoch: 4978 \tTraining Loss: 0.282517\n",
            "Epoch: 4979 \tTraining Loss: 0.282038\n",
            "Epoch: 4980 \tTraining Loss: 0.282177\n",
            "Epoch: 4981 \tTraining Loss: 0.282183\n",
            "Epoch: 4982 \tTraining Loss: 0.282035\n",
            "Epoch: 4983 \tTraining Loss: 0.282171\n",
            "Epoch: 4984 \tTraining Loss: 0.281989\n",
            "Epoch: 4985 \tTraining Loss: 0.282034\n",
            "Epoch: 4986 \tTraining Loss: 0.282075\n",
            "Epoch: 4987 \tTraining Loss: 0.281989\n",
            "Epoch: 4988 \tTraining Loss: 0.281949\n",
            "Epoch: 4989 \tTraining Loss: 0.281905\n",
            "Epoch: 4990 \tTraining Loss: 0.281909\n",
            "Epoch: 4991 \tTraining Loss: 0.281889\n",
            "Epoch: 4992 \tTraining Loss: 0.281860\n",
            "Epoch: 4993 \tTraining Loss: 0.281838\n",
            "Epoch: 4994 \tTraining Loss: 0.281838\n",
            "Epoch: 4995 \tTraining Loss: 0.281809\n",
            "Epoch: 4996 \tTraining Loss: 0.281829\n",
            "Epoch: 4997 \tTraining Loss: 0.281822\n",
            "Epoch: 4998 \tTraining Loss: 0.281809\n",
            "Epoch: 4999 \tTraining Loss: 0.281777\n",
            "Epoch: 5000 \tTraining Loss: 0.281741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Test"
      ],
      "metadata": {
        "id": "cvR46PfAyo3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test.float())\n",
        "    predicted_labels = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "predicted_labels = np.where(predicted_labels == 0 ,-1,1)\n",
        "y_test = np.where(y_test == 0 ,-1,1)\n",
        "\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3a3528-90ef-4b32-c04a-2b32f7f28377",
        "id": "jfgd2J_eyo3R"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1\n",
            "  1  1  1  1  1 -1 -1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# a = np.array(*predicted_labels)\n",
        "test_accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f'Test Accuracy: {100 * test_accuracy:.2f}%')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
        "import seaborn as sns\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "34c2505a-30ac-4c2a-f649-998ffc55478b",
        "id": "Kk0UnwvMyo3R"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 51.12%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with use شاخص کل"
      ],
      "metadata": {
        "id": "beOLOze0yKFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Add indicators to Features"
      ],
      "metadata": {
        "id": "2057QXhuFx8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = op.join('/content/drive/My Drive/','DL','DL_HW05','فولاد-ت.csv')  # Path to the Data folder\n",
        "df = pd.read_csv(dir)\n",
        "aroon = df.ta.aroon(inplace=True)\n",
        "aroon = aroon['AROONU_14'].values[indices_Foolad].astype(np.float64)\n",
        "macd = df.ta.macd(inplace=True)\n",
        "macd = macd['MACD_12_26_9'].values[indices_Foolad].astype(np.float64)\n",
        "rsi = df.ta.rsi(inplace=True)\n",
        "rsi = rsi[indices_Foolad].astype(np.float64)\n",
        "\n",
        "adjClose = df['adjClose'].values[indices_Foolad].astype(np.float64)\n",
        "\n",
        "df = All\n",
        "aroon_all = df.ta.aroon(inplace=True)\n",
        "aroon_all = aroon_all['AROONU_14'].values[indices_All].astype(np.float64)\n",
        "macd_all = df.ta.macd(inplace=True)\n",
        "macd_all = macd_all['MACD_12_26_9'].values[indices_All].astype(np.float64)\n",
        "rsi_all = df.ta.rsi(inplace=True)\n",
        "rsi_all = rsi_all[indices_All].astype(np.float64)\n",
        "\n",
        "close_all = df['close'].values[indices_All].astype(np.float64)\n",
        "\n",
        "features = np.vstack((adjClose[1:],aroon[1:],macd[1:],rsi[1:],close_all[1:],aroon_all[1:],macd_all[1:],rsi_all[1:]))\n",
        "\n",
        "labels = np.zeros(2006)\n",
        "labels = np.where(merged_Foolad[1:] >= merged_Foolad[:-1], 1, 0)"
      ],
      "metadata": {
        "id": "-_e2Ub2IGFVA"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AxduNbi0Hoy",
        "outputId": "dfe4d3c1-5ffe-414c-ef08-fb5d7bdbe895"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 2006)"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Split data and adjust window size"
      ],
      "metadata": {
        "id": "yBWpRFTOER7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 5\n",
        "X, y = create_windows(np.hstack((features.T, labels.reshape(-1, 1))), window_size)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_size = int(len(X) * 0.80)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ],
      "metadata": {
        "id": "d5tMeP_LER7S"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define RNN Model"
      ],
      "metadata": {
        "id": "gOcpS-qcER7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "YH6PX4dLER7S"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 8  # 8 features\n",
        "hidden_size = 50\n",
        "output_size = 2  # Two classes\n",
        "model = RNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)"
      ],
      "metadata": {
        "id": "yQ_lOi-1ER7S"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Training"
      ],
      "metadata": {
        "id": "mxyFnZb3ER7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    outputs = model(X_train.float())\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        loss.item()\n",
        "        ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a99734-9963-4960-87ab-e1ab7d75847d",
        "id": "K9ZnIi0cER7S"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.721856\n",
            "Epoch: 2 \tTraining Loss: 0.714767\n",
            "Epoch: 3 \tTraining Loss: 0.707852\n",
            "Epoch: 4 \tTraining Loss: 0.701611\n",
            "Epoch: 5 \tTraining Loss: 0.695966\n",
            "Epoch: 6 \tTraining Loss: 0.690766\n",
            "Epoch: 7 \tTraining Loss: 0.685799\n",
            "Epoch: 8 \tTraining Loss: 0.681520\n",
            "Epoch: 9 \tTraining Loss: 0.678068\n",
            "Epoch: 10 \tTraining Loss: 0.675152\n",
            "Epoch: 11 \tTraining Loss: 0.672652\n",
            "Epoch: 12 \tTraining Loss: 0.670265\n",
            "Epoch: 13 \tTraining Loss: 0.668003\n",
            "Epoch: 14 \tTraining Loss: 0.665860\n",
            "Epoch: 15 \tTraining Loss: 0.664279\n",
            "Epoch: 16 \tTraining Loss: 0.663153\n",
            "Epoch: 17 \tTraining Loss: 0.662080\n",
            "Epoch: 18 \tTraining Loss: 0.660950\n",
            "Epoch: 19 \tTraining Loss: 0.659731\n",
            "Epoch: 20 \tTraining Loss: 0.658411\n",
            "Epoch: 21 \tTraining Loss: 0.657062\n",
            "Epoch: 22 \tTraining Loss: 0.655836\n",
            "Epoch: 23 \tTraining Loss: 0.654801\n",
            "Epoch: 24 \tTraining Loss: 0.653856\n",
            "Epoch: 25 \tTraining Loss: 0.652999\n",
            "Epoch: 26 \tTraining Loss: 0.652111\n",
            "Epoch: 27 \tTraining Loss: 0.651201\n",
            "Epoch: 28 \tTraining Loss: 0.650387\n",
            "Epoch: 29 \tTraining Loss: 0.649759\n",
            "Epoch: 30 \tTraining Loss: 0.649261\n",
            "Epoch: 31 \tTraining Loss: 0.648784\n",
            "Epoch: 32 \tTraining Loss: 0.648307\n",
            "Epoch: 33 \tTraining Loss: 0.647797\n",
            "Epoch: 34 \tTraining Loss: 0.647244\n",
            "Epoch: 35 \tTraining Loss: 0.646713\n",
            "Epoch: 36 \tTraining Loss: 0.646241\n",
            "Epoch: 37 \tTraining Loss: 0.645825\n",
            "Epoch: 38 \tTraining Loss: 0.645426\n",
            "Epoch: 39 \tTraining Loss: 0.645045\n",
            "Epoch: 40 \tTraining Loss: 0.644707\n",
            "Epoch: 41 \tTraining Loss: 0.644415\n",
            "Epoch: 42 \tTraining Loss: 0.644178\n",
            "Epoch: 43 \tTraining Loss: 0.643926\n",
            "Epoch: 44 \tTraining Loss: 0.643699\n",
            "Epoch: 45 \tTraining Loss: 0.643510\n",
            "Epoch: 46 \tTraining Loss: 0.643331\n",
            "Epoch: 47 \tTraining Loss: 0.643167\n",
            "Epoch: 48 \tTraining Loss: 0.643015\n",
            "Epoch: 49 \tTraining Loss: 0.642856\n",
            "Epoch: 50 \tTraining Loss: 0.642673\n",
            "Epoch: 51 \tTraining Loss: 0.642451\n",
            "Epoch: 52 \tTraining Loss: 0.642186\n",
            "Epoch: 53 \tTraining Loss: 0.641945\n",
            "Epoch: 54 \tTraining Loss: 0.641767\n",
            "Epoch: 55 \tTraining Loss: 0.641572\n",
            "Epoch: 56 \tTraining Loss: 0.641333\n",
            "Epoch: 57 \tTraining Loss: 0.641064\n",
            "Epoch: 58 \tTraining Loss: 0.640797\n",
            "Epoch: 59 \tTraining Loss: 0.640555\n",
            "Epoch: 60 \tTraining Loss: 0.640361\n",
            "Epoch: 61 \tTraining Loss: 0.640194\n",
            "Epoch: 62 \tTraining Loss: 0.640016\n",
            "Epoch: 63 \tTraining Loss: 0.639827\n",
            "Epoch: 64 \tTraining Loss: 0.639650\n",
            "Epoch: 65 \tTraining Loss: 0.639482\n",
            "Epoch: 66 \tTraining Loss: 0.639306\n",
            "Epoch: 67 \tTraining Loss: 0.639191\n",
            "Epoch: 68 \tTraining Loss: 0.639111\n",
            "Epoch: 69 \tTraining Loss: 0.638990\n",
            "Epoch: 70 \tTraining Loss: 0.638864\n",
            "Epoch: 71 \tTraining Loss: 0.638763\n",
            "Epoch: 72 \tTraining Loss: 0.638668\n",
            "Epoch: 73 \tTraining Loss: 0.638574\n",
            "Epoch: 74 \tTraining Loss: 0.638478\n",
            "Epoch: 75 \tTraining Loss: 0.638388\n",
            "Epoch: 76 \tTraining Loss: 0.638306\n",
            "Epoch: 77 \tTraining Loss: 0.638228\n",
            "Epoch: 78 \tTraining Loss: 0.638150\n",
            "Epoch: 79 \tTraining Loss: 0.638062\n",
            "Epoch: 80 \tTraining Loss: 0.637970\n",
            "Epoch: 81 \tTraining Loss: 0.637887\n",
            "Epoch: 82 \tTraining Loss: 0.637809\n",
            "Epoch: 83 \tTraining Loss: 0.637733\n",
            "Epoch: 84 \tTraining Loss: 0.637654\n",
            "Epoch: 85 \tTraining Loss: 0.637558\n",
            "Epoch: 86 \tTraining Loss: 0.637486\n",
            "Epoch: 87 \tTraining Loss: 0.637411\n",
            "Epoch: 88 \tTraining Loss: 0.637311\n",
            "Epoch: 89 \tTraining Loss: 0.637219\n",
            "Epoch: 90 \tTraining Loss: 0.637149\n",
            "Epoch: 91 \tTraining Loss: 0.637066\n",
            "Epoch: 92 \tTraining Loss: 0.636971\n",
            "Epoch: 93 \tTraining Loss: 0.636894\n",
            "Epoch: 94 \tTraining Loss: 0.636819\n",
            "Epoch: 95 \tTraining Loss: 0.636745\n",
            "Epoch: 96 \tTraining Loss: 0.636670\n",
            "Epoch: 97 \tTraining Loss: 0.636597\n",
            "Epoch: 98 \tTraining Loss: 0.636527\n",
            "Epoch: 99 \tTraining Loss: 0.636451\n",
            "Epoch: 100 \tTraining Loss: 0.636376\n",
            "Epoch: 101 \tTraining Loss: 0.636306\n",
            "Epoch: 102 \tTraining Loss: 0.636234\n",
            "Epoch: 103 \tTraining Loss: 0.636162\n",
            "Epoch: 104 \tTraining Loss: 0.636093\n",
            "Epoch: 105 \tTraining Loss: 0.636021\n",
            "Epoch: 106 \tTraining Loss: 0.635948\n",
            "Epoch: 107 \tTraining Loss: 0.635875\n",
            "Epoch: 108 \tTraining Loss: 0.635799\n",
            "Epoch: 109 \tTraining Loss: 0.635722\n",
            "Epoch: 110 \tTraining Loss: 0.635645\n",
            "Epoch: 111 \tTraining Loss: 0.635561\n",
            "Epoch: 112 \tTraining Loss: 0.635471\n",
            "Epoch: 113 \tTraining Loss: 0.635376\n",
            "Epoch: 114 \tTraining Loss: 0.635282\n",
            "Epoch: 115 \tTraining Loss: 0.635183\n",
            "Epoch: 116 \tTraining Loss: 0.635091\n",
            "Epoch: 117 \tTraining Loss: 0.635001\n",
            "Epoch: 118 \tTraining Loss: 0.634916\n",
            "Epoch: 119 \tTraining Loss: 0.634841\n",
            "Epoch: 120 \tTraining Loss: 0.634767\n",
            "Epoch: 121 \tTraining Loss: 0.634696\n",
            "Epoch: 122 \tTraining Loss: 0.634625\n",
            "Epoch: 123 \tTraining Loss: 0.634552\n",
            "Epoch: 124 \tTraining Loss: 0.634481\n",
            "Epoch: 125 \tTraining Loss: 0.634409\n",
            "Epoch: 126 \tTraining Loss: 0.634335\n",
            "Epoch: 127 \tTraining Loss: 0.634261\n",
            "Epoch: 128 \tTraining Loss: 0.634183\n",
            "Epoch: 129 \tTraining Loss: 0.634102\n",
            "Epoch: 130 \tTraining Loss: 0.634016\n",
            "Epoch: 131 \tTraining Loss: 0.633922\n",
            "Epoch: 132 \tTraining Loss: 0.633821\n",
            "Epoch: 133 \tTraining Loss: 0.633712\n",
            "Epoch: 134 \tTraining Loss: 0.633600\n",
            "Epoch: 135 \tTraining Loss: 0.633494\n",
            "Epoch: 136 \tTraining Loss: 0.633397\n",
            "Epoch: 137 \tTraining Loss: 0.633307\n",
            "Epoch: 138 \tTraining Loss: 0.633219\n",
            "Epoch: 139 \tTraining Loss: 0.633132\n",
            "Epoch: 140 \tTraining Loss: 0.633046\n",
            "Epoch: 141 \tTraining Loss: 0.632973\n",
            "Epoch: 142 \tTraining Loss: 0.632895\n",
            "Epoch: 143 \tTraining Loss: 0.632809\n",
            "Epoch: 144 \tTraining Loss: 0.632718\n",
            "Epoch: 145 \tTraining Loss: 0.632627\n",
            "Epoch: 146 \tTraining Loss: 0.632536\n",
            "Epoch: 147 \tTraining Loss: 0.632436\n",
            "Epoch: 148 \tTraining Loss: 0.632323\n",
            "Epoch: 149 \tTraining Loss: 0.632195\n",
            "Epoch: 150 \tTraining Loss: 0.632076\n",
            "Epoch: 151 \tTraining Loss: 0.631949\n",
            "Epoch: 152 \tTraining Loss: 0.631948\n",
            "Epoch: 153 \tTraining Loss: 0.631708\n",
            "Epoch: 154 \tTraining Loss: 0.631533\n",
            "Epoch: 155 \tTraining Loss: 0.631486\n",
            "Epoch: 156 \tTraining Loss: 0.631255\n",
            "Epoch: 157 \tTraining Loss: 0.631186\n",
            "Epoch: 158 \tTraining Loss: 0.631102\n",
            "Epoch: 159 \tTraining Loss: 0.631013\n",
            "Epoch: 160 \tTraining Loss: 0.630925\n",
            "Epoch: 161 \tTraining Loss: 0.630828\n",
            "Epoch: 162 \tTraining Loss: 0.630721\n",
            "Epoch: 163 \tTraining Loss: 0.630617\n",
            "Epoch: 164 \tTraining Loss: 0.630518\n",
            "Epoch: 165 \tTraining Loss: 0.630417\n",
            "Epoch: 166 \tTraining Loss: 0.630311\n",
            "Epoch: 167 \tTraining Loss: 0.630200\n",
            "Epoch: 168 \tTraining Loss: 0.630087\n",
            "Epoch: 169 \tTraining Loss: 0.629976\n",
            "Epoch: 170 \tTraining Loss: 0.629868\n",
            "Epoch: 171 \tTraining Loss: 0.629756\n",
            "Epoch: 172 \tTraining Loss: 0.629641\n",
            "Epoch: 173 \tTraining Loss: 0.629528\n",
            "Epoch: 174 \tTraining Loss: 0.629404\n",
            "Epoch: 175 \tTraining Loss: 0.629283\n",
            "Epoch: 176 \tTraining Loss: 0.629166\n",
            "Epoch: 177 \tTraining Loss: 0.629044\n",
            "Epoch: 178 \tTraining Loss: 0.628932\n",
            "Epoch: 179 \tTraining Loss: 0.628815\n",
            "Epoch: 180 \tTraining Loss: 0.628703\n",
            "Epoch: 181 \tTraining Loss: 0.628592\n",
            "Epoch: 182 \tTraining Loss: 0.628472\n",
            "Epoch: 183 \tTraining Loss: 0.628353\n",
            "Epoch: 184 \tTraining Loss: 0.628225\n",
            "Epoch: 185 \tTraining Loss: 0.628095\n",
            "Epoch: 186 \tTraining Loss: 0.627957\n",
            "Epoch: 187 \tTraining Loss: 0.627812\n",
            "Epoch: 188 \tTraining Loss: 0.627661\n",
            "Epoch: 189 \tTraining Loss: 0.627500\n",
            "Epoch: 190 \tTraining Loss: 0.627339\n",
            "Epoch: 191 \tTraining Loss: 0.627183\n",
            "Epoch: 192 \tTraining Loss: 0.627023\n",
            "Epoch: 193 \tTraining Loss: 0.626845\n",
            "Epoch: 194 \tTraining Loss: 0.626658\n",
            "Epoch: 195 \tTraining Loss: 0.626472\n",
            "Epoch: 196 \tTraining Loss: 0.626280\n",
            "Epoch: 197 \tTraining Loss: 0.626078\n",
            "Epoch: 198 \tTraining Loss: 0.625869\n",
            "Epoch: 199 \tTraining Loss: 0.625659\n",
            "Epoch: 200 \tTraining Loss: 0.625458\n",
            "Epoch: 201 \tTraining Loss: 0.625254\n",
            "Epoch: 202 \tTraining Loss: 0.625035\n",
            "Epoch: 203 \tTraining Loss: 0.624812\n",
            "Epoch: 204 \tTraining Loss: 0.624620\n",
            "Epoch: 205 \tTraining Loss: 0.624353\n",
            "Epoch: 206 \tTraining Loss: 0.624016\n",
            "Epoch: 207 \tTraining Loss: 0.623776\n",
            "Epoch: 208 \tTraining Loss: 0.623397\n",
            "Epoch: 209 \tTraining Loss: 0.623097\n",
            "Epoch: 210 \tTraining Loss: 0.622754\n",
            "Epoch: 211 \tTraining Loss: 0.622484\n",
            "Epoch: 212 \tTraining Loss: 0.622221\n",
            "Epoch: 213 \tTraining Loss: 0.621917\n",
            "Epoch: 214 \tTraining Loss: 0.621695\n",
            "Epoch: 215 \tTraining Loss: 0.621400\n",
            "Epoch: 216 \tTraining Loss: 0.621123\n",
            "Epoch: 217 \tTraining Loss: 0.620846\n",
            "Epoch: 218 \tTraining Loss: 0.620525\n",
            "Epoch: 219 \tTraining Loss: 0.620263\n",
            "Epoch: 220 \tTraining Loss: 0.619976\n",
            "Epoch: 221 \tTraining Loss: 0.619711\n",
            "Epoch: 222 \tTraining Loss: 0.619459\n",
            "Epoch: 223 \tTraining Loss: 0.619185\n",
            "Epoch: 224 \tTraining Loss: 0.618932\n",
            "Epoch: 225 \tTraining Loss: 0.618686\n",
            "Epoch: 226 \tTraining Loss: 0.618403\n",
            "Epoch: 227 \tTraining Loss: 0.618167\n",
            "Epoch: 228 \tTraining Loss: 0.617913\n",
            "Epoch: 229 \tTraining Loss: 0.617653\n",
            "Epoch: 230 \tTraining Loss: 0.617407\n",
            "Epoch: 231 \tTraining Loss: 0.617154\n",
            "Epoch: 232 \tTraining Loss: 0.616902\n",
            "Epoch: 233 \tTraining Loss: 0.616650\n",
            "Epoch: 234 \tTraining Loss: 0.616373\n",
            "Epoch: 235 \tTraining Loss: 0.616118\n",
            "Epoch: 236 \tTraining Loss: 0.615845\n",
            "Epoch: 237 \tTraining Loss: 0.615609\n",
            "Epoch: 238 \tTraining Loss: 0.615366\n",
            "Epoch: 239 \tTraining Loss: 0.615117\n",
            "Epoch: 240 \tTraining Loss: 0.614887\n",
            "Epoch: 241 \tTraining Loss: 0.614603\n",
            "Epoch: 242 \tTraining Loss: 0.614335\n",
            "Epoch: 243 \tTraining Loss: 0.614074\n",
            "Epoch: 244 \tTraining Loss: 0.613834\n",
            "Epoch: 245 \tTraining Loss: 0.613603\n",
            "Epoch: 246 \tTraining Loss: 0.613342\n",
            "Epoch: 247 \tTraining Loss: 0.613056\n",
            "Epoch: 248 \tTraining Loss: 0.612786\n",
            "Epoch: 249 \tTraining Loss: 0.612546\n",
            "Epoch: 250 \tTraining Loss: 0.612342\n",
            "Epoch: 251 \tTraining Loss: 0.612074\n",
            "Epoch: 252 \tTraining Loss: 0.611812\n",
            "Epoch: 253 \tTraining Loss: 0.611563\n",
            "Epoch: 254 \tTraining Loss: 0.611318\n",
            "Epoch: 255 \tTraining Loss: 0.611053\n",
            "Epoch: 256 \tTraining Loss: 0.610732\n",
            "Epoch: 257 \tTraining Loss: 0.610494\n",
            "Epoch: 258 \tTraining Loss: 0.610218\n",
            "Epoch: 259 \tTraining Loss: 0.609944\n",
            "Epoch: 260 \tTraining Loss: 0.609688\n",
            "Epoch: 261 \tTraining Loss: 0.609444\n",
            "Epoch: 262 \tTraining Loss: 0.609183\n",
            "Epoch: 263 \tTraining Loss: 0.608944\n",
            "Epoch: 264 \tTraining Loss: 0.608718\n",
            "Epoch: 265 \tTraining Loss: 0.608489\n",
            "Epoch: 266 \tTraining Loss: 0.608253\n",
            "Epoch: 267 \tTraining Loss: 0.608011\n",
            "Epoch: 268 \tTraining Loss: 0.607754\n",
            "Epoch: 269 \tTraining Loss: 0.607502\n",
            "Epoch: 270 \tTraining Loss: 0.607267\n",
            "Epoch: 271 \tTraining Loss: 0.607042\n",
            "Epoch: 272 \tTraining Loss: 0.606828\n",
            "Epoch: 273 \tTraining Loss: 0.606603\n",
            "Epoch: 274 \tTraining Loss: 0.606368\n",
            "Epoch: 275 \tTraining Loss: 0.606120\n",
            "Epoch: 276 \tTraining Loss: 0.605869\n",
            "Epoch: 277 \tTraining Loss: 0.605628\n",
            "Epoch: 278 \tTraining Loss: 0.605392\n",
            "Epoch: 279 \tTraining Loss: 0.605169\n",
            "Epoch: 280 \tTraining Loss: 0.604946\n",
            "Epoch: 281 \tTraining Loss: 0.604730\n",
            "Epoch: 282 \tTraining Loss: 0.604508\n",
            "Epoch: 283 \tTraining Loss: 0.604269\n",
            "Epoch: 284 \tTraining Loss: 0.604017\n",
            "Epoch: 285 \tTraining Loss: 0.603770\n",
            "Epoch: 286 \tTraining Loss: 0.603531\n",
            "Epoch: 287 \tTraining Loss: 0.603303\n",
            "Epoch: 288 \tTraining Loss: 0.603083\n",
            "Epoch: 289 \tTraining Loss: 0.602866\n",
            "Epoch: 290 \tTraining Loss: 0.602651\n",
            "Epoch: 291 \tTraining Loss: 0.602423\n",
            "Epoch: 292 \tTraining Loss: 0.602187\n",
            "Epoch: 293 \tTraining Loss: 0.601940\n",
            "Epoch: 294 \tTraining Loss: 0.601698\n",
            "Epoch: 295 \tTraining Loss: 0.601460\n",
            "Epoch: 296 \tTraining Loss: 0.601228\n",
            "Epoch: 297 \tTraining Loss: 0.601003\n",
            "Epoch: 298 \tTraining Loss: 0.600780\n",
            "Epoch: 299 \tTraining Loss: 0.600561\n",
            "Epoch: 300 \tTraining Loss: 0.600342\n",
            "Epoch: 301 \tTraining Loss: 0.600123\n",
            "Epoch: 302 \tTraining Loss: 0.599887\n",
            "Epoch: 303 \tTraining Loss: 0.599644\n",
            "Epoch: 304 \tTraining Loss: 0.599385\n",
            "Epoch: 305 \tTraining Loss: 0.599125\n",
            "Epoch: 306 \tTraining Loss: 0.598871\n",
            "Epoch: 307 \tTraining Loss: 0.598627\n",
            "Epoch: 308 \tTraining Loss: 0.598391\n",
            "Epoch: 309 \tTraining Loss: 0.598161\n",
            "Epoch: 310 \tTraining Loss: 0.597937\n",
            "Epoch: 311 \tTraining Loss: 0.597708\n",
            "Epoch: 312 \tTraining Loss: 0.597476\n",
            "Epoch: 313 \tTraining Loss: 0.597202\n",
            "Epoch: 314 \tTraining Loss: 0.596911\n",
            "Epoch: 315 \tTraining Loss: 0.596619\n",
            "Epoch: 316 \tTraining Loss: 0.596355\n",
            "Epoch: 317 \tTraining Loss: 0.596118\n",
            "Epoch: 318 \tTraining Loss: 0.595886\n",
            "Epoch: 319 \tTraining Loss: 0.595648\n",
            "Epoch: 320 \tTraining Loss: 0.595381\n",
            "Epoch: 321 \tTraining Loss: 0.595099\n",
            "Epoch: 322 \tTraining Loss: 0.594815\n",
            "Epoch: 323 \tTraining Loss: 0.594545\n",
            "Epoch: 324 \tTraining Loss: 0.594281\n",
            "Epoch: 325 \tTraining Loss: 0.594022\n",
            "Epoch: 326 \tTraining Loss: 0.593787\n",
            "Epoch: 327 \tTraining Loss: 0.593544\n",
            "Epoch: 328 \tTraining Loss: 0.593284\n",
            "Epoch: 329 \tTraining Loss: 0.593015\n",
            "Epoch: 330 \tTraining Loss: 0.592757\n",
            "Epoch: 331 \tTraining Loss: 0.592488\n",
            "Epoch: 332 \tTraining Loss: 0.592208\n",
            "Epoch: 333 \tTraining Loss: 0.591923\n",
            "Epoch: 334 \tTraining Loss: 0.591648\n",
            "Epoch: 335 \tTraining Loss: 0.591385\n",
            "Epoch: 336 \tTraining Loss: 0.591133\n",
            "Epoch: 337 \tTraining Loss: 0.590895\n",
            "Epoch: 338 \tTraining Loss: 0.590671\n",
            "Epoch: 339 \tTraining Loss: 0.590486\n",
            "Epoch: 340 \tTraining Loss: 0.590268\n",
            "Epoch: 341 \tTraining Loss: 0.589989\n",
            "Epoch: 342 \tTraining Loss: 0.589612\n",
            "Epoch: 343 \tTraining Loss: 0.589267\n",
            "Epoch: 344 \tTraining Loss: 0.589030\n",
            "Epoch: 345 \tTraining Loss: 0.588856\n",
            "Epoch: 346 \tTraining Loss: 0.588639\n",
            "Epoch: 347 \tTraining Loss: 0.588280\n",
            "Epoch: 348 \tTraining Loss: 0.587966\n",
            "Epoch: 349 \tTraining Loss: 0.587703\n",
            "Epoch: 350 \tTraining Loss: 0.587490\n",
            "Epoch: 351 \tTraining Loss: 0.587260\n",
            "Epoch: 352 \tTraining Loss: 0.586947\n",
            "Epoch: 353 \tTraining Loss: 0.586608\n",
            "Epoch: 354 \tTraining Loss: 0.586384\n",
            "Epoch: 355 \tTraining Loss: 0.586149\n",
            "Epoch: 356 \tTraining Loss: 0.585905\n",
            "Epoch: 357 \tTraining Loss: 0.585589\n",
            "Epoch: 358 \tTraining Loss: 0.585281\n",
            "Epoch: 359 \tTraining Loss: 0.585035\n",
            "Epoch: 360 \tTraining Loss: 0.584804\n",
            "Epoch: 361 \tTraining Loss: 0.584552\n",
            "Epoch: 362 \tTraining Loss: 0.584236\n",
            "Epoch: 363 \tTraining Loss: 0.583944\n",
            "Epoch: 364 \tTraining Loss: 0.583678\n",
            "Epoch: 365 \tTraining Loss: 0.583447\n",
            "Epoch: 366 \tTraining Loss: 0.583179\n",
            "Epoch: 367 \tTraining Loss: 0.582886\n",
            "Epoch: 368 \tTraining Loss: 0.582587\n",
            "Epoch: 369 \tTraining Loss: 0.582319\n",
            "Epoch: 370 \tTraining Loss: 0.582063\n",
            "Epoch: 371 \tTraining Loss: 0.581800\n",
            "Epoch: 372 \tTraining Loss: 0.581517\n",
            "Epoch: 373 \tTraining Loss: 0.581215\n",
            "Epoch: 374 \tTraining Loss: 0.580925\n",
            "Epoch: 375 \tTraining Loss: 0.580648\n",
            "Epoch: 376 \tTraining Loss: 0.580385\n",
            "Epoch: 377 \tTraining Loss: 0.580126\n",
            "Epoch: 378 \tTraining Loss: 0.579877\n",
            "Epoch: 379 \tTraining Loss: 0.579621\n",
            "Epoch: 380 \tTraining Loss: 0.579369\n",
            "Epoch: 381 \tTraining Loss: 0.579105\n",
            "Epoch: 382 \tTraining Loss: 0.578831\n",
            "Epoch: 383 \tTraining Loss: 0.578543\n",
            "Epoch: 384 \tTraining Loss: 0.578256\n",
            "Epoch: 385 \tTraining Loss: 0.577974\n",
            "Epoch: 386 \tTraining Loss: 0.577704\n",
            "Epoch: 387 \tTraining Loss: 0.577450\n",
            "Epoch: 388 \tTraining Loss: 0.577207\n",
            "Epoch: 389 \tTraining Loss: 0.576970\n",
            "Epoch: 390 \tTraining Loss: 0.576739\n",
            "Epoch: 391 \tTraining Loss: 0.576516\n",
            "Epoch: 392 \tTraining Loss: 0.576293\n",
            "Epoch: 393 \tTraining Loss: 0.576077\n",
            "Epoch: 394 \tTraining Loss: 0.575838\n",
            "Epoch: 395 \tTraining Loss: 0.575558\n",
            "Epoch: 396 \tTraining Loss: 0.575237\n",
            "Epoch: 397 \tTraining Loss: 0.574925\n",
            "Epoch: 398 \tTraining Loss: 0.574665\n",
            "Epoch: 399 \tTraining Loss: 0.574449\n",
            "Epoch: 400 \tTraining Loss: 0.574252\n",
            "Epoch: 401 \tTraining Loss: 0.574040\n",
            "Epoch: 402 \tTraining Loss: 0.573790\n",
            "Epoch: 403 \tTraining Loss: 0.573505\n",
            "Epoch: 404 \tTraining Loss: 0.573215\n",
            "Epoch: 405 \tTraining Loss: 0.572951\n",
            "Epoch: 406 \tTraining Loss: 0.572721\n",
            "Epoch: 407 \tTraining Loss: 0.572507\n",
            "Epoch: 408 \tTraining Loss: 0.572291\n",
            "Epoch: 409 \tTraining Loss: 0.572057\n",
            "Epoch: 410 \tTraining Loss: 0.571801\n",
            "Epoch: 411 \tTraining Loss: 0.571533\n",
            "Epoch: 412 \tTraining Loss: 0.571267\n",
            "Epoch: 413 \tTraining Loss: 0.571013\n",
            "Epoch: 414 \tTraining Loss: 0.570773\n",
            "Epoch: 415 \tTraining Loss: 0.570544\n",
            "Epoch: 416 \tTraining Loss: 0.570319\n",
            "Epoch: 417 \tTraining Loss: 0.570096\n",
            "Epoch: 418 \tTraining Loss: 0.569874\n",
            "Epoch: 419 \tTraining Loss: 0.569655\n",
            "Epoch: 420 \tTraining Loss: 0.569432\n",
            "Epoch: 421 \tTraining Loss: 0.569203\n",
            "Epoch: 422 \tTraining Loss: 0.568953\n",
            "Epoch: 423 \tTraining Loss: 0.568687\n",
            "Epoch: 424 \tTraining Loss: 0.568407\n",
            "Epoch: 425 \tTraining Loss: 0.568130\n",
            "Epoch: 426 \tTraining Loss: 0.567864\n",
            "Epoch: 427 \tTraining Loss: 0.567614\n",
            "Epoch: 428 \tTraining Loss: 0.567376\n",
            "Epoch: 429 \tTraining Loss: 0.567147\n",
            "Epoch: 430 \tTraining Loss: 0.566927\n",
            "Epoch: 431 \tTraining Loss: 0.566718\n",
            "Epoch: 432 \tTraining Loss: 0.566521\n",
            "Epoch: 433 \tTraining Loss: 0.566325\n",
            "Epoch: 434 \tTraining Loss: 0.566124\n",
            "Epoch: 435 \tTraining Loss: 0.565868\n",
            "Epoch: 436 \tTraining Loss: 0.565564\n",
            "Epoch: 437 \tTraining Loss: 0.565235\n",
            "Epoch: 438 \tTraining Loss: 0.564945\n",
            "Epoch: 439 \tTraining Loss: 0.564707\n",
            "Epoch: 440 \tTraining Loss: 0.564395\n",
            "Epoch: 441 \tTraining Loss: 0.564240\n",
            "Epoch: 442 \tTraining Loss: 0.563890\n",
            "Epoch: 443 \tTraining Loss: 0.563639\n",
            "Epoch: 444 \tTraining Loss: 0.563383\n",
            "Epoch: 445 \tTraining Loss: 0.563118\n",
            "Epoch: 446 \tTraining Loss: 0.562880\n",
            "Epoch: 447 \tTraining Loss: 0.562698\n",
            "Epoch: 448 \tTraining Loss: 0.562479\n",
            "Epoch: 449 \tTraining Loss: 0.562246\n",
            "Epoch: 450 \tTraining Loss: 0.562040\n",
            "Epoch: 451 \tTraining Loss: 0.561775\n",
            "Epoch: 452 \tTraining Loss: 0.561484\n",
            "Epoch: 453 \tTraining Loss: 0.561230\n",
            "Epoch: 454 \tTraining Loss: 0.561013\n",
            "Epoch: 455 \tTraining Loss: 0.560749\n",
            "Epoch: 456 \tTraining Loss: 0.560520\n",
            "Epoch: 457 \tTraining Loss: 0.560314\n",
            "Epoch: 458 \tTraining Loss: 0.560088\n",
            "Epoch: 459 \tTraining Loss: 0.559848\n",
            "Epoch: 460 \tTraining Loss: 0.559633\n",
            "Epoch: 461 \tTraining Loss: 0.559416\n",
            "Epoch: 462 \tTraining Loss: 0.559187\n",
            "Epoch: 463 \tTraining Loss: 0.558959\n",
            "Epoch: 464 \tTraining Loss: 0.558717\n",
            "Epoch: 465 \tTraining Loss: 0.558478\n",
            "Epoch: 466 \tTraining Loss: 0.558209\n",
            "Epoch: 467 \tTraining Loss: 0.557927\n",
            "Epoch: 468 \tTraining Loss: 0.557648\n",
            "Epoch: 469 \tTraining Loss: 0.557384\n",
            "Epoch: 470 \tTraining Loss: 0.557150\n",
            "Epoch: 471 \tTraining Loss: 0.556930\n",
            "Epoch: 472 \tTraining Loss: 0.556720\n",
            "Epoch: 473 \tTraining Loss: 0.556506\n",
            "Epoch: 474 \tTraining Loss: 0.556306\n",
            "Epoch: 475 \tTraining Loss: 0.556126\n",
            "Epoch: 476 \tTraining Loss: 0.555973\n",
            "Epoch: 477 \tTraining Loss: 0.555798\n",
            "Epoch: 478 \tTraining Loss: 0.555596\n",
            "Epoch: 479 \tTraining Loss: 0.555300\n",
            "Epoch: 480 \tTraining Loss: 0.554966\n",
            "Epoch: 481 \tTraining Loss: 0.554622\n",
            "Epoch: 482 \tTraining Loss: 0.554359\n",
            "Epoch: 483 \tTraining Loss: 0.554186\n",
            "Epoch: 484 \tTraining Loss: 0.554040\n",
            "Epoch: 485 \tTraining Loss: 0.553865\n",
            "Epoch: 486 \tTraining Loss: 0.553616\n",
            "Epoch: 487 \tTraining Loss: 0.553335\n",
            "Epoch: 488 \tTraining Loss: 0.553057\n",
            "Epoch: 489 \tTraining Loss: 0.552816\n",
            "Epoch: 490 \tTraining Loss: 0.552611\n",
            "Epoch: 491 \tTraining Loss: 0.552421\n",
            "Epoch: 492 \tTraining Loss: 0.552231\n",
            "Epoch: 493 \tTraining Loss: 0.552016\n",
            "Epoch: 494 \tTraining Loss: 0.551771\n",
            "Epoch: 495 \tTraining Loss: 0.551504\n",
            "Epoch: 496 \tTraining Loss: 0.551251\n",
            "Epoch: 497 \tTraining Loss: 0.551016\n",
            "Epoch: 498 \tTraining Loss: 0.550790\n",
            "Epoch: 499 \tTraining Loss: 0.550566\n",
            "Epoch: 500 \tTraining Loss: 0.550337\n",
            "Epoch: 501 \tTraining Loss: 0.550114\n",
            "Epoch: 502 \tTraining Loss: 0.549899\n",
            "Epoch: 503 \tTraining Loss: 0.549680\n",
            "Epoch: 504 \tTraining Loss: 0.549467\n",
            "Epoch: 505 \tTraining Loss: 0.549201\n",
            "Epoch: 506 \tTraining Loss: 0.548980\n",
            "Epoch: 507 \tTraining Loss: 0.548700\n",
            "Epoch: 508 \tTraining Loss: 0.548397\n",
            "Epoch: 509 \tTraining Loss: 0.548125\n",
            "Epoch: 510 \tTraining Loss: 0.547839\n",
            "Epoch: 511 \tTraining Loss: 0.547605\n",
            "Epoch: 512 \tTraining Loss: 0.547349\n",
            "Epoch: 513 \tTraining Loss: 0.547134\n",
            "Epoch: 514 \tTraining Loss: 0.546893\n",
            "Epoch: 515 \tTraining Loss: 0.546677\n",
            "Epoch: 516 \tTraining Loss: 0.546464\n",
            "Epoch: 517 \tTraining Loss: 0.546288\n",
            "Epoch: 518 \tTraining Loss: 0.546123\n",
            "Epoch: 519 \tTraining Loss: 0.545985\n",
            "Epoch: 520 \tTraining Loss: 0.545843\n",
            "Epoch: 521 \tTraining Loss: 0.545603\n",
            "Epoch: 522 \tTraining Loss: 0.545242\n",
            "Epoch: 523 \tTraining Loss: 0.544778\n",
            "Epoch: 524 \tTraining Loss: 0.544367\n",
            "Epoch: 525 \tTraining Loss: 0.544151\n",
            "Epoch: 526 \tTraining Loss: 0.544017\n",
            "Epoch: 527 \tTraining Loss: 0.543890\n",
            "Epoch: 528 \tTraining Loss: 0.543634\n",
            "Epoch: 529 \tTraining Loss: 0.543305\n",
            "Epoch: 530 \tTraining Loss: 0.542943\n",
            "Epoch: 531 \tTraining Loss: 0.542675\n",
            "Epoch: 532 \tTraining Loss: 0.542484\n",
            "Epoch: 533 \tTraining Loss: 0.542321\n",
            "Epoch: 534 \tTraining Loss: 0.542119\n",
            "Epoch: 535 \tTraining Loss: 0.541856\n",
            "Epoch: 536 \tTraining Loss: 0.541553\n",
            "Epoch: 537 \tTraining Loss: 0.541262\n",
            "Epoch: 538 \tTraining Loss: 0.541001\n",
            "Epoch: 539 \tTraining Loss: 0.540772\n",
            "Epoch: 540 \tTraining Loss: 0.540560\n",
            "Epoch: 541 \tTraining Loss: 0.540352\n",
            "Epoch: 542 \tTraining Loss: 0.540129\n",
            "Epoch: 543 \tTraining Loss: 0.539916\n",
            "Epoch: 544 \tTraining Loss: 0.539669\n",
            "Epoch: 545 \tTraining Loss: 0.539397\n",
            "Epoch: 546 \tTraining Loss: 0.539119\n",
            "Epoch: 547 \tTraining Loss: 0.538850\n",
            "Epoch: 548 \tTraining Loss: 0.538586\n",
            "Epoch: 549 \tTraining Loss: 0.538328\n",
            "Epoch: 550 \tTraining Loss: 0.538078\n",
            "Epoch: 551 \tTraining Loss: 0.537827\n",
            "Epoch: 552 \tTraining Loss: 0.537582\n",
            "Epoch: 553 \tTraining Loss: 0.537346\n",
            "Epoch: 554 \tTraining Loss: 0.537116\n",
            "Epoch: 555 \tTraining Loss: 0.536891\n",
            "Epoch: 556 \tTraining Loss: 0.536687\n",
            "Epoch: 557 \tTraining Loss: 0.536533\n",
            "Epoch: 558 \tTraining Loss: 0.536504\n",
            "Epoch: 559 \tTraining Loss: 0.536716\n",
            "Epoch: 560 \tTraining Loss: 0.537350\n",
            "Epoch: 561 \tTraining Loss: 0.537851\n",
            "Epoch: 562 \tTraining Loss: 0.537279\n",
            "Epoch: 563 \tTraining Loss: 0.535294\n",
            "Epoch: 564 \tTraining Loss: 0.535200\n",
            "Epoch: 565 \tTraining Loss: 0.536142\n",
            "Epoch: 566 \tTraining Loss: 0.535059\n",
            "Epoch: 567 \tTraining Loss: 0.534545\n",
            "Epoch: 568 \tTraining Loss: 0.534605\n",
            "Epoch: 569 \tTraining Loss: 0.534100\n",
            "Epoch: 570 \tTraining Loss: 0.533786\n",
            "Epoch: 571 \tTraining Loss: 0.534173\n",
            "Epoch: 572 \tTraining Loss: 0.533425\n",
            "Epoch: 573 \tTraining Loss: 0.533003\n",
            "Epoch: 574 \tTraining Loss: 0.532976\n",
            "Epoch: 575 \tTraining Loss: 0.532952\n",
            "Epoch: 576 \tTraining Loss: 0.532377\n",
            "Epoch: 577 \tTraining Loss: 0.532221\n",
            "Epoch: 578 \tTraining Loss: 0.532086\n",
            "Epoch: 579 \tTraining Loss: 0.531623\n",
            "Epoch: 580 \tTraining Loss: 0.531205\n",
            "Epoch: 581 \tTraining Loss: 0.531083\n",
            "Epoch: 582 \tTraining Loss: 0.530952\n",
            "Epoch: 583 \tTraining Loss: 0.530439\n",
            "Epoch: 584 \tTraining Loss: 0.530384\n",
            "Epoch: 585 \tTraining Loss: 0.530103\n",
            "Epoch: 586 \tTraining Loss: 0.529954\n",
            "Epoch: 587 \tTraining Loss: 0.529663\n",
            "Epoch: 588 \tTraining Loss: 0.529508\n",
            "Epoch: 589 \tTraining Loss: 0.529109\n",
            "Epoch: 590 \tTraining Loss: 0.528998\n",
            "Epoch: 591 \tTraining Loss: 0.528723\n",
            "Epoch: 592 \tTraining Loss: 0.528521\n",
            "Epoch: 593 \tTraining Loss: 0.528253\n",
            "Epoch: 594 \tTraining Loss: 0.528107\n",
            "Epoch: 595 \tTraining Loss: 0.527831\n",
            "Epoch: 596 \tTraining Loss: 0.527671\n",
            "Epoch: 597 \tTraining Loss: 0.527397\n",
            "Epoch: 598 \tTraining Loss: 0.527202\n",
            "Epoch: 599 \tTraining Loss: 0.526981\n",
            "Epoch: 600 \tTraining Loss: 0.526771\n",
            "Epoch: 601 \tTraining Loss: 0.526544\n",
            "Epoch: 602 \tTraining Loss: 0.526336\n",
            "Epoch: 603 \tTraining Loss: 0.526117\n",
            "Epoch: 604 \tTraining Loss: 0.525910\n",
            "Epoch: 605 \tTraining Loss: 0.525704\n",
            "Epoch: 606 \tTraining Loss: 0.525487\n",
            "Epoch: 607 \tTraining Loss: 0.525276\n",
            "Epoch: 608 \tTraining Loss: 0.525050\n",
            "Epoch: 609 \tTraining Loss: 0.524849\n",
            "Epoch: 610 \tTraining Loss: 0.524638\n",
            "Epoch: 611 \tTraining Loss: 0.524417\n",
            "Epoch: 612 \tTraining Loss: 0.524209\n",
            "Epoch: 613 \tTraining Loss: 0.524007\n",
            "Epoch: 614 \tTraining Loss: 0.523791\n",
            "Epoch: 615 \tTraining Loss: 0.523575\n",
            "Epoch: 616 \tTraining Loss: 0.523369\n",
            "Epoch: 617 \tTraining Loss: 0.523160\n",
            "Epoch: 618 \tTraining Loss: 0.522946\n",
            "Epoch: 619 \tTraining Loss: 0.522736\n",
            "Epoch: 620 \tTraining Loss: 0.522529\n",
            "Epoch: 621 \tTraining Loss: 0.522318\n",
            "Epoch: 622 \tTraining Loss: 0.522106\n",
            "Epoch: 623 \tTraining Loss: 0.521902\n",
            "Epoch: 624 \tTraining Loss: 0.521689\n",
            "Epoch: 625 \tTraining Loss: 0.521483\n",
            "Epoch: 626 \tTraining Loss: 0.521278\n",
            "Epoch: 627 \tTraining Loss: 0.521070\n",
            "Epoch: 628 \tTraining Loss: 0.520868\n",
            "Epoch: 629 \tTraining Loss: 0.520673\n",
            "Epoch: 630 \tTraining Loss: 0.520490\n",
            "Epoch: 631 \tTraining Loss: 0.520349\n",
            "Epoch: 632 \tTraining Loss: 0.520306\n",
            "Epoch: 633 \tTraining Loss: 0.520510\n",
            "Epoch: 634 \tTraining Loss: 0.521222\n",
            "Epoch: 635 \tTraining Loss: 0.522572\n",
            "Epoch: 636 \tTraining Loss: 0.523582\n",
            "Epoch: 637 \tTraining Loss: 0.521675\n",
            "Epoch: 638 \tTraining Loss: 0.518955\n",
            "Epoch: 639 \tTraining Loss: 0.520506\n",
            "Epoch: 640 \tTraining Loss: 0.521046\n",
            "Epoch: 641 \tTraining Loss: 0.518707\n",
            "Epoch: 642 \tTraining Loss: 0.519392\n",
            "Epoch: 643 \tTraining Loss: 0.519494\n",
            "Epoch: 644 \tTraining Loss: 0.518335\n",
            "Epoch: 645 \tTraining Loss: 0.518543\n",
            "Epoch: 646 \tTraining Loss: 0.518244\n",
            "Epoch: 647 \tTraining Loss: 0.517923\n",
            "Epoch: 648 \tTraining Loss: 0.517533\n",
            "Epoch: 649 \tTraining Loss: 0.517554\n",
            "Epoch: 650 \tTraining Loss: 0.517055\n",
            "Epoch: 651 \tTraining Loss: 0.516919\n",
            "Epoch: 652 \tTraining Loss: 0.516978\n",
            "Epoch: 653 \tTraining Loss: 0.516195\n",
            "Epoch: 654 \tTraining Loss: 0.516485\n",
            "Epoch: 655 \tTraining Loss: 0.515993\n",
            "Epoch: 656 \tTraining Loss: 0.515827\n",
            "Epoch: 657 \tTraining Loss: 0.515786\n",
            "Epoch: 658 \tTraining Loss: 0.515301\n",
            "Epoch: 659 \tTraining Loss: 0.515390\n",
            "Epoch: 660 \tTraining Loss: 0.515020\n",
            "Epoch: 661 \tTraining Loss: 0.514901\n",
            "Epoch: 662 \tTraining Loss: 0.514687\n",
            "Epoch: 663 \tTraining Loss: 0.514479\n",
            "Epoch: 664 \tTraining Loss: 0.514355\n",
            "Epoch: 665 \tTraining Loss: 0.514050\n",
            "Epoch: 666 \tTraining Loss: 0.514007\n",
            "Epoch: 667 \tTraining Loss: 0.513719\n",
            "Epoch: 668 \tTraining Loss: 0.513551\n",
            "Epoch: 669 \tTraining Loss: 0.513430\n",
            "Epoch: 670 \tTraining Loss: 0.513158\n",
            "Epoch: 671 \tTraining Loss: 0.513047\n",
            "Epoch: 672 \tTraining Loss: 0.512831\n",
            "Epoch: 673 \tTraining Loss: 0.512653\n",
            "Epoch: 674 \tTraining Loss: 0.512487\n",
            "Epoch: 675 \tTraining Loss: 0.512280\n",
            "Epoch: 676 \tTraining Loss: 0.512139\n",
            "Epoch: 677 \tTraining Loss: 0.511925\n",
            "Epoch: 678 \tTraining Loss: 0.511760\n",
            "Epoch: 679 \tTraining Loss: 0.511597\n",
            "Epoch: 680 \tTraining Loss: 0.511384\n",
            "Epoch: 681 \tTraining Loss: 0.511234\n",
            "Epoch: 682 \tTraining Loss: 0.511047\n",
            "Epoch: 683 \tTraining Loss: 0.510860\n",
            "Epoch: 684 \tTraining Loss: 0.510697\n",
            "Epoch: 685 \tTraining Loss: 0.510505\n",
            "Epoch: 686 \tTraining Loss: 0.510338\n",
            "Epoch: 687 \tTraining Loss: 0.510159\n",
            "Epoch: 688 \tTraining Loss: 0.509971\n",
            "Epoch: 689 \tTraining Loss: 0.509808\n",
            "Epoch: 690 \tTraining Loss: 0.509624\n",
            "Epoch: 691 \tTraining Loss: 0.509443\n",
            "Epoch: 692 \tTraining Loss: 0.509273\n",
            "Epoch: 693 \tTraining Loss: 0.509089\n",
            "Epoch: 694 \tTraining Loss: 0.508915\n",
            "Epoch: 695 \tTraining Loss: 0.508738\n",
            "Epoch: 696 \tTraining Loss: 0.508555\n",
            "Epoch: 697 \tTraining Loss: 0.508382\n",
            "Epoch: 698 \tTraining Loss: 0.508205\n",
            "Epoch: 699 \tTraining Loss: 0.508023\n",
            "Epoch: 700 \tTraining Loss: 0.507848\n",
            "Epoch: 701 \tTraining Loss: 0.507670\n",
            "Epoch: 702 \tTraining Loss: 0.507490\n",
            "Epoch: 703 \tTraining Loss: 0.507314\n",
            "Epoch: 704 \tTraining Loss: 0.507134\n",
            "Epoch: 705 \tTraining Loss: 0.506953\n",
            "Epoch: 706 \tTraining Loss: 0.506776\n",
            "Epoch: 707 \tTraining Loss: 0.506597\n",
            "Epoch: 708 \tTraining Loss: 0.506416\n",
            "Epoch: 709 \tTraining Loss: 0.506237\n",
            "Epoch: 710 \tTraining Loss: 0.506057\n",
            "Epoch: 711 \tTraining Loss: 0.505875\n",
            "Epoch: 712 \tTraining Loss: 0.505695\n",
            "Epoch: 713 \tTraining Loss: 0.505514\n",
            "Epoch: 714 \tTraining Loss: 0.505330\n",
            "Epoch: 715 \tTraining Loss: 0.505149\n",
            "Epoch: 716 \tTraining Loss: 0.504967\n",
            "Epoch: 717 \tTraining Loss: 0.504782\n",
            "Epoch: 718 \tTraining Loss: 0.504598\n",
            "Epoch: 719 \tTraining Loss: 0.504413\n",
            "Epoch: 720 \tTraining Loss: 0.504226\n",
            "Epoch: 721 \tTraining Loss: 0.504035\n",
            "Epoch: 722 \tTraining Loss: 0.503844\n",
            "Epoch: 723 \tTraining Loss: 0.503648\n",
            "Epoch: 724 \tTraining Loss: 0.503450\n",
            "Epoch: 725 \tTraining Loss: 0.503252\n",
            "Epoch: 726 \tTraining Loss: 0.503036\n",
            "Epoch: 727 \tTraining Loss: 0.502818\n",
            "Epoch: 728 \tTraining Loss: 0.502550\n",
            "Epoch: 729 \tTraining Loss: 0.502221\n",
            "Epoch: 730 \tTraining Loss: 0.502060\n",
            "Epoch: 731 \tTraining Loss: 0.501842\n",
            "Epoch: 732 \tTraining Loss: 0.501563\n",
            "Epoch: 733 \tTraining Loss: 0.501534\n",
            "Epoch: 734 \tTraining Loss: 0.501855\n",
            "Epoch: 735 \tTraining Loss: 0.502801\n",
            "Epoch: 736 \tTraining Loss: 0.505065\n",
            "Epoch: 737 \tTraining Loss: 0.507406\n",
            "Epoch: 738 \tTraining Loss: 0.505051\n",
            "Epoch: 739 \tTraining Loss: 0.500862\n",
            "Epoch: 740 \tTraining Loss: 0.501983\n",
            "Epoch: 741 \tTraining Loss: 0.503157\n",
            "Epoch: 742 \tTraining Loss: 0.501021\n",
            "Epoch: 743 \tTraining Loss: 0.500200\n",
            "Epoch: 744 \tTraining Loss: 0.501500\n",
            "Epoch: 745 \tTraining Loss: 0.500233\n",
            "Epoch: 746 \tTraining Loss: 0.499492\n",
            "Epoch: 747 \tTraining Loss: 0.500691\n",
            "Epoch: 748 \tTraining Loss: 0.499166\n",
            "Epoch: 749 \tTraining Loss: 0.498999\n",
            "Epoch: 750 \tTraining Loss: 0.499485\n",
            "Epoch: 751 \tTraining Loss: 0.498427\n",
            "Epoch: 752 \tTraining Loss: 0.498521\n",
            "Epoch: 753 \tTraining Loss: 0.498474\n",
            "Epoch: 754 \tTraining Loss: 0.498016\n",
            "Epoch: 755 \tTraining Loss: 0.497698\n",
            "Epoch: 756 \tTraining Loss: 0.497865\n",
            "Epoch: 757 \tTraining Loss: 0.497395\n",
            "Epoch: 758 \tTraining Loss: 0.497124\n",
            "Epoch: 759 \tTraining Loss: 0.497293\n",
            "Epoch: 760 \tTraining Loss: 0.496747\n",
            "Epoch: 761 \tTraining Loss: 0.496651\n",
            "Epoch: 762 \tTraining Loss: 0.496661\n",
            "Epoch: 763 \tTraining Loss: 0.496257\n",
            "Epoch: 764 \tTraining Loss: 0.496122\n",
            "Epoch: 765 \tTraining Loss: 0.496028\n",
            "Epoch: 766 \tTraining Loss: 0.495807\n",
            "Epoch: 767 \tTraining Loss: 0.495564\n",
            "Epoch: 768 \tTraining Loss: 0.495486\n",
            "Epoch: 769 \tTraining Loss: 0.495329\n",
            "Epoch: 770 \tTraining Loss: 0.495027\n",
            "Epoch: 771 \tTraining Loss: 0.494982\n",
            "Epoch: 772 \tTraining Loss: 0.494815\n",
            "Epoch: 773 \tTraining Loss: 0.494563\n",
            "Epoch: 774 \tTraining Loss: 0.494453\n",
            "Epoch: 775 \tTraining Loss: 0.494303\n",
            "Epoch: 776 \tTraining Loss: 0.494112\n",
            "Epoch: 777 \tTraining Loss: 0.493934\n",
            "Epoch: 778 \tTraining Loss: 0.493803\n",
            "Epoch: 779 \tTraining Loss: 0.493654\n",
            "Epoch: 780 \tTraining Loss: 0.493441\n",
            "Epoch: 781 \tTraining Loss: 0.493304\n",
            "Epoch: 782 \tTraining Loss: 0.493172\n",
            "Epoch: 783 \tTraining Loss: 0.492976\n",
            "Epoch: 784 \tTraining Loss: 0.492815\n",
            "Epoch: 785 \tTraining Loss: 0.492673\n",
            "Epoch: 786 \tTraining Loss: 0.492515\n",
            "Epoch: 787 \tTraining Loss: 0.492347\n",
            "Epoch: 788 \tTraining Loss: 0.492179\n",
            "Epoch: 789 \tTraining Loss: 0.492036\n",
            "Epoch: 790 \tTraining Loss: 0.491884\n",
            "Epoch: 791 \tTraining Loss: 0.491706\n",
            "Epoch: 792 \tTraining Loss: 0.491552\n",
            "Epoch: 793 \tTraining Loss: 0.491404\n",
            "Epoch: 794 \tTraining Loss: 0.491243\n",
            "Epoch: 795 \tTraining Loss: 0.491082\n",
            "Epoch: 796 \tTraining Loss: 0.490920\n",
            "Epoch: 797 \tTraining Loss: 0.490767\n",
            "Epoch: 798 \tTraining Loss: 0.490615\n",
            "Epoch: 799 \tTraining Loss: 0.490452\n",
            "Epoch: 800 \tTraining Loss: 0.490290\n",
            "Epoch: 801 \tTraining Loss: 0.490136\n",
            "Epoch: 802 \tTraining Loss: 0.489980\n",
            "Epoch: 803 \tTraining Loss: 0.489822\n",
            "Epoch: 804 \tTraining Loss: 0.489662\n",
            "Epoch: 805 \tTraining Loss: 0.489501\n",
            "Epoch: 806 \tTraining Loss: 0.489344\n",
            "Epoch: 807 \tTraining Loss: 0.489189\n",
            "Epoch: 808 \tTraining Loss: 0.489030\n",
            "Epoch: 809 \tTraining Loss: 0.488869\n",
            "Epoch: 810 \tTraining Loss: 0.488709\n",
            "Epoch: 811 \tTraining Loss: 0.488550\n",
            "Epoch: 812 \tTraining Loss: 0.488391\n",
            "Epoch: 813 \tTraining Loss: 0.488233\n",
            "Epoch: 814 \tTraining Loss: 0.488074\n",
            "Epoch: 815 \tTraining Loss: 0.487912\n",
            "Epoch: 816 \tTraining Loss: 0.487752\n",
            "Epoch: 817 \tTraining Loss: 0.487592\n",
            "Epoch: 818 \tTraining Loss: 0.487432\n",
            "Epoch: 819 \tTraining Loss: 0.487272\n",
            "Epoch: 820 \tTraining Loss: 0.487113\n",
            "Epoch: 821 \tTraining Loss: 0.486953\n",
            "Epoch: 822 \tTraining Loss: 0.486793\n",
            "Epoch: 823 \tTraining Loss: 0.486632\n",
            "Epoch: 824 \tTraining Loss: 0.486472\n",
            "Epoch: 825 \tTraining Loss: 0.486312\n",
            "Epoch: 826 \tTraining Loss: 0.486152\n",
            "Epoch: 827 \tTraining Loss: 0.485992\n",
            "Epoch: 828 \tTraining Loss: 0.485832\n",
            "Epoch: 829 \tTraining Loss: 0.485673\n",
            "Epoch: 830 \tTraining Loss: 0.485514\n",
            "Epoch: 831 \tTraining Loss: 0.485356\n",
            "Epoch: 832 \tTraining Loss: 0.485198\n",
            "Epoch: 833 \tTraining Loss: 0.485042\n",
            "Epoch: 834 \tTraining Loss: 0.484888\n",
            "Epoch: 835 \tTraining Loss: 0.484739\n",
            "Epoch: 836 \tTraining Loss: 0.484599\n",
            "Epoch: 837 \tTraining Loss: 0.484486\n",
            "Epoch: 838 \tTraining Loss: 0.484428\n",
            "Epoch: 839 \tTraining Loss: 0.484516\n",
            "Epoch: 840 \tTraining Loss: 0.484860\n",
            "Epoch: 841 \tTraining Loss: 0.485859\n",
            "Epoch: 842 \tTraining Loss: 0.487193\n",
            "Epoch: 843 \tTraining Loss: 0.488841\n",
            "Epoch: 844 \tTraining Loss: 0.486544\n",
            "Epoch: 845 \tTraining Loss: 0.483733\n",
            "Epoch: 846 \tTraining Loss: 0.483737\n",
            "Epoch: 847 \tTraining Loss: 0.485480\n",
            "Epoch: 848 \tTraining Loss: 0.485769\n",
            "Epoch: 849 \tTraining Loss: 0.483529\n",
            "Epoch: 850 \tTraining Loss: 0.482976\n",
            "Epoch: 851 \tTraining Loss: 0.484274\n",
            "Epoch: 852 \tTraining Loss: 0.483793\n",
            "Epoch: 853 \tTraining Loss: 0.482308\n",
            "Epoch: 854 \tTraining Loss: 0.482515\n",
            "Epoch: 855 \tTraining Loss: 0.483146\n",
            "Epoch: 856 \tTraining Loss: 0.482468\n",
            "Epoch: 857 \tTraining Loss: 0.481674\n",
            "Epoch: 858 \tTraining Loss: 0.482155\n",
            "Epoch: 859 \tTraining Loss: 0.482381\n",
            "Epoch: 860 \tTraining Loss: 0.481553\n",
            "Epoch: 861 \tTraining Loss: 0.481225\n",
            "Epoch: 862 \tTraining Loss: 0.481570\n",
            "Epoch: 863 \tTraining Loss: 0.481371\n",
            "Epoch: 864 \tTraining Loss: 0.480731\n",
            "Epoch: 865 \tTraining Loss: 0.480750\n",
            "Epoch: 866 \tTraining Loss: 0.480896\n",
            "Epoch: 867 \tTraining Loss: 0.480578\n",
            "Epoch: 868 \tTraining Loss: 0.480154\n",
            "Epoch: 869 \tTraining Loss: 0.480214\n",
            "Epoch: 870 \tTraining Loss: 0.480260\n",
            "Epoch: 871 \tTraining Loss: 0.479890\n",
            "Epoch: 872 \tTraining Loss: 0.479643\n",
            "Epoch: 873 \tTraining Loss: 0.479650\n",
            "Epoch: 874 \tTraining Loss: 0.479601\n",
            "Epoch: 875 \tTraining Loss: 0.479295\n",
            "Epoch: 876 \tTraining Loss: 0.479090\n",
            "Epoch: 877 \tTraining Loss: 0.479060\n",
            "Epoch: 878 \tTraining Loss: 0.478983\n",
            "Epoch: 879 \tTraining Loss: 0.478743\n",
            "Epoch: 880 \tTraining Loss: 0.478553\n",
            "Epoch: 881 \tTraining Loss: 0.478485\n",
            "Epoch: 882 \tTraining Loss: 0.478391\n",
            "Epoch: 883 \tTraining Loss: 0.478215\n",
            "Epoch: 884 \tTraining Loss: 0.478027\n",
            "Epoch: 885 \tTraining Loss: 0.477916\n",
            "Epoch: 886 \tTraining Loss: 0.477834\n",
            "Epoch: 887 \tTraining Loss: 0.477691\n",
            "Epoch: 888 \tTraining Loss: 0.477516\n",
            "Epoch: 889 \tTraining Loss: 0.477371\n",
            "Epoch: 890 \tTraining Loss: 0.477267\n",
            "Epoch: 891 \tTraining Loss: 0.477160\n",
            "Epoch: 892 \tTraining Loss: 0.477012\n",
            "Epoch: 893 \tTraining Loss: 0.476857\n",
            "Epoch: 894 \tTraining Loss: 0.476716\n",
            "Epoch: 895 \tTraining Loss: 0.476604\n",
            "Epoch: 896 \tTraining Loss: 0.476489\n",
            "Epoch: 897 \tTraining Loss: 0.476352\n",
            "Epoch: 898 \tTraining Loss: 0.476208\n",
            "Epoch: 899 \tTraining Loss: 0.476066\n",
            "Epoch: 900 \tTraining Loss: 0.475942\n",
            "Epoch: 901 \tTraining Loss: 0.475824\n",
            "Epoch: 902 \tTraining Loss: 0.475699\n",
            "Epoch: 903 \tTraining Loss: 0.475566\n",
            "Epoch: 904 \tTraining Loss: 0.475426\n",
            "Epoch: 905 \tTraining Loss: 0.475290\n",
            "Epoch: 906 \tTraining Loss: 0.475162\n",
            "Epoch: 907 \tTraining Loss: 0.475037\n",
            "Epoch: 908 \tTraining Loss: 0.474913\n",
            "Epoch: 909 \tTraining Loss: 0.474784\n",
            "Epoch: 910 \tTraining Loss: 0.474651\n",
            "Epoch: 911 \tTraining Loss: 0.474516\n",
            "Epoch: 912 \tTraining Loss: 0.474381\n",
            "Epoch: 913 \tTraining Loss: 0.474247\n",
            "Epoch: 914 \tTraining Loss: 0.474115\n",
            "Epoch: 915 \tTraining Loss: 0.473985\n",
            "Epoch: 916 \tTraining Loss: 0.473858\n",
            "Epoch: 917 \tTraining Loss: 0.473731\n",
            "Epoch: 918 \tTraining Loss: 0.473605\n",
            "Epoch: 919 \tTraining Loss: 0.473478\n",
            "Epoch: 920 \tTraining Loss: 0.473352\n",
            "Epoch: 921 \tTraining Loss: 0.473225\n",
            "Epoch: 922 \tTraining Loss: 0.473100\n",
            "Epoch: 923 \tTraining Loss: 0.472976\n",
            "Epoch: 924 \tTraining Loss: 0.472854\n",
            "Epoch: 925 \tTraining Loss: 0.472738\n",
            "Epoch: 926 \tTraining Loss: 0.472626\n",
            "Epoch: 927 \tTraining Loss: 0.472528\n",
            "Epoch: 928 \tTraining Loss: 0.472445\n",
            "Epoch: 929 \tTraining Loss: 0.472401\n",
            "Epoch: 930 \tTraining Loss: 0.472394\n",
            "Epoch: 931 \tTraining Loss: 0.472501\n",
            "Epoch: 932 \tTraining Loss: 0.472659\n",
            "Epoch: 933 \tTraining Loss: 0.473069\n",
            "Epoch: 934 \tTraining Loss: 0.473251\n",
            "Epoch: 935 \tTraining Loss: 0.473509\n",
            "Epoch: 936 \tTraining Loss: 0.472812\n",
            "Epoch: 937 \tTraining Loss: 0.471959\n",
            "Epoch: 938 \tTraining Loss: 0.471183\n",
            "Epoch: 939 \tTraining Loss: 0.471013\n",
            "Epoch: 940 \tTraining Loss: 0.471336\n",
            "Epoch: 941 \tTraining Loss: 0.471693\n",
            "Epoch: 942 \tTraining Loss: 0.471861\n",
            "Epoch: 943 \tTraining Loss: 0.471327\n",
            "Epoch: 944 \tTraining Loss: 0.470689\n",
            "Epoch: 945 \tTraining Loss: 0.470238\n",
            "Epoch: 946 \tTraining Loss: 0.470233\n",
            "Epoch: 947 \tTraining Loss: 0.470481\n",
            "Epoch: 948 \tTraining Loss: 0.470563\n",
            "Epoch: 949 \tTraining Loss: 0.470406\n",
            "Epoch: 950 \tTraining Loss: 0.469872\n",
            "Epoch: 951 \tTraining Loss: 0.469442\n",
            "Epoch: 952 \tTraining Loss: 0.469309\n",
            "Epoch: 953 \tTraining Loss: 0.469390\n",
            "Epoch: 954 \tTraining Loss: 0.469471\n",
            "Epoch: 955 \tTraining Loss: 0.469331\n",
            "Epoch: 956 \tTraining Loss: 0.469051\n",
            "Epoch: 957 \tTraining Loss: 0.468727\n",
            "Epoch: 958 \tTraining Loss: 0.468542\n",
            "Epoch: 959 \tTraining Loss: 0.468488\n",
            "Epoch: 960 \tTraining Loss: 0.468479\n",
            "Epoch: 961 \tTraining Loss: 0.468445\n",
            "Epoch: 962 \tTraining Loss: 0.468263\n",
            "Epoch: 963 \tTraining Loss: 0.468007\n",
            "Epoch: 964 \tTraining Loss: 0.467709\n",
            "Epoch: 965 \tTraining Loss: 0.467493\n",
            "Epoch: 966 \tTraining Loss: 0.467364\n",
            "Epoch: 967 \tTraining Loss: 0.467304\n",
            "Epoch: 968 \tTraining Loss: 0.467208\n",
            "Epoch: 969 \tTraining Loss: 0.467079\n",
            "Epoch: 970 \tTraining Loss: 0.466991\n",
            "Epoch: 971 \tTraining Loss: 0.466986\n",
            "Epoch: 972 \tTraining Loss: 0.467072\n",
            "Epoch: 973 \tTraining Loss: 0.467100\n",
            "Epoch: 974 \tTraining Loss: 0.467218\n",
            "Epoch: 975 \tTraining Loss: 0.467172\n",
            "Epoch: 976 \tTraining Loss: 0.467129\n",
            "Epoch: 977 \tTraining Loss: 0.466686\n",
            "Epoch: 978 \tTraining Loss: 0.466328\n",
            "Epoch: 979 \tTraining Loss: 0.465925\n",
            "Epoch: 980 \tTraining Loss: 0.465685\n",
            "Epoch: 981 \tTraining Loss: 0.465524\n",
            "Epoch: 982 \tTraining Loss: 0.465490\n",
            "Epoch: 983 \tTraining Loss: 0.465523\n",
            "Epoch: 984 \tTraining Loss: 0.465550\n",
            "Epoch: 985 \tTraining Loss: 0.465553\n",
            "Epoch: 986 \tTraining Loss: 0.465439\n",
            "Epoch: 987 \tTraining Loss: 0.465313\n",
            "Epoch: 988 \tTraining Loss: 0.465042\n",
            "Epoch: 989 \tTraining Loss: 0.464763\n",
            "Epoch: 990 \tTraining Loss: 0.464513\n",
            "Epoch: 991 \tTraining Loss: 0.464338\n",
            "Epoch: 992 \tTraining Loss: 0.464217\n",
            "Epoch: 993 \tTraining Loss: 0.464123\n",
            "Epoch: 994 \tTraining Loss: 0.464094\n",
            "Epoch: 995 \tTraining Loss: 0.464070\n",
            "Epoch: 996 \tTraining Loss: 0.464071\n",
            "Epoch: 997 \tTraining Loss: 0.464026\n",
            "Epoch: 998 \tTraining Loss: 0.464021\n",
            "Epoch: 999 \tTraining Loss: 0.463927\n",
            "Epoch: 1000 \tTraining Loss: 0.463891\n",
            "Epoch: 1001 \tTraining Loss: 0.463681\n",
            "Epoch: 1002 \tTraining Loss: 0.463527\n",
            "Epoch: 1003 \tTraining Loss: 0.463293\n",
            "Epoch: 1004 \tTraining Loss: 0.463047\n",
            "Epoch: 1005 \tTraining Loss: 0.462809\n",
            "Epoch: 1006 \tTraining Loss: 0.462645\n",
            "Epoch: 1007 \tTraining Loss: 0.462515\n",
            "Epoch: 1008 \tTraining Loss: 0.462402\n",
            "Epoch: 1009 \tTraining Loss: 0.462318\n",
            "Epoch: 1010 \tTraining Loss: 0.462270\n",
            "Epoch: 1011 \tTraining Loss: 0.462271\n",
            "Epoch: 1012 \tTraining Loss: 0.462274\n",
            "Epoch: 1013 \tTraining Loss: 0.462353\n",
            "Epoch: 1014 \tTraining Loss: 0.462389\n",
            "Epoch: 1015 \tTraining Loss: 0.462533\n",
            "Epoch: 1016 \tTraining Loss: 0.462464\n",
            "Epoch: 1017 \tTraining Loss: 0.462450\n",
            "Epoch: 1018 \tTraining Loss: 0.462104\n",
            "Epoch: 1019 \tTraining Loss: 0.461786\n",
            "Epoch: 1020 \tTraining Loss: 0.461317\n",
            "Epoch: 1021 \tTraining Loss: 0.461005\n",
            "Epoch: 1022 \tTraining Loss: 0.460853\n",
            "Epoch: 1023 \tTraining Loss: 0.460822\n",
            "Epoch: 1024 \tTraining Loss: 0.460874\n",
            "Epoch: 1025 \tTraining Loss: 0.460945\n",
            "Epoch: 1026 \tTraining Loss: 0.461082\n",
            "Epoch: 1027 \tTraining Loss: 0.461058\n",
            "Epoch: 1028 \tTraining Loss: 0.461026\n",
            "Epoch: 1029 \tTraining Loss: 0.460746\n",
            "Epoch: 1030 \tTraining Loss: 0.460491\n",
            "Epoch: 1031 \tTraining Loss: 0.460104\n",
            "Epoch: 1032 \tTraining Loss: 0.459823\n",
            "Epoch: 1033 \tTraining Loss: 0.459662\n",
            "Epoch: 1034 \tTraining Loss: 0.459598\n",
            "Epoch: 1035 \tTraining Loss: 0.459594\n",
            "Epoch: 1036 \tTraining Loss: 0.459611\n",
            "Epoch: 1037 \tTraining Loss: 0.459673\n",
            "Epoch: 1038 \tTraining Loss: 0.459647\n",
            "Epoch: 1039 \tTraining Loss: 0.459625\n",
            "Epoch: 1040 \tTraining Loss: 0.459457\n",
            "Epoch: 1041 \tTraining Loss: 0.459324\n",
            "Epoch: 1042 \tTraining Loss: 0.459045\n",
            "Epoch: 1043 \tTraining Loss: 0.458802\n",
            "Epoch: 1044 \tTraining Loss: 0.458565\n",
            "Epoch: 1045 \tTraining Loss: 0.458399\n",
            "Epoch: 1046 \tTraining Loss: 0.458277\n",
            "Epoch: 1047 \tTraining Loss: 0.458186\n",
            "Epoch: 1048 \tTraining Loss: 0.458132\n",
            "Epoch: 1049 \tTraining Loss: 0.458104\n",
            "Epoch: 1050 \tTraining Loss: 0.458100\n",
            "Epoch: 1051 \tTraining Loss: 0.458084\n",
            "Epoch: 1052 \tTraining Loss: 0.458136\n",
            "Epoch: 1053 \tTraining Loss: 0.458136\n",
            "Epoch: 1054 \tTraining Loss: 0.458248\n",
            "Epoch: 1055 \tTraining Loss: 0.458116\n",
            "Epoch: 1056 \tTraining Loss: 0.458053\n",
            "Epoch: 1057 \tTraining Loss: 0.457749\n",
            "Epoch: 1058 \tTraining Loss: 0.457468\n",
            "Epoch: 1059 \tTraining Loss: 0.457122\n",
            "Epoch: 1060 \tTraining Loss: 0.456866\n",
            "Epoch: 1061 \tTraining Loss: 0.456700\n",
            "Epoch: 1062 \tTraining Loss: 0.456603\n",
            "Epoch: 1063 \tTraining Loss: 0.456553\n",
            "Epoch: 1064 \tTraining Loss: 0.456543\n",
            "Epoch: 1065 \tTraining Loss: 0.456597\n",
            "Epoch: 1066 \tTraining Loss: 0.456634\n",
            "Epoch: 1067 \tTraining Loss: 0.456766\n",
            "Epoch: 1068 \tTraining Loss: 0.456735\n",
            "Epoch: 1069 \tTraining Loss: 0.456792\n",
            "Epoch: 1070 \tTraining Loss: 0.456574\n",
            "Epoch: 1071 \tTraining Loss: 0.456361\n",
            "Epoch: 1072 \tTraining Loss: 0.455968\n",
            "Epoch: 1073 \tTraining Loss: 0.455658\n",
            "Epoch: 1074 \tTraining Loss: 0.455396\n",
            "Epoch: 1075 \tTraining Loss: 0.455237\n",
            "Epoch: 1076 \tTraining Loss: 0.455168\n",
            "Epoch: 1077 \tTraining Loss: 0.455165\n",
            "Epoch: 1078 \tTraining Loss: 0.455223\n",
            "Epoch: 1079 \tTraining Loss: 0.455262\n",
            "Epoch: 1080 \tTraining Loss: 0.455384\n",
            "Epoch: 1081 \tTraining Loss: 0.455360\n",
            "Epoch: 1082 \tTraining Loss: 0.455397\n",
            "Epoch: 1083 \tTraining Loss: 0.455166\n",
            "Epoch: 1084 \tTraining Loss: 0.454949\n",
            "Epoch: 1085 \tTraining Loss: 0.454574\n",
            "Epoch: 1086 \tTraining Loss: 0.454278\n",
            "Epoch: 1087 \tTraining Loss: 0.454041\n",
            "Epoch: 1088 \tTraining Loss: 0.453904\n",
            "Epoch: 1089 \tTraining Loss: 0.453850\n",
            "Epoch: 1090 \tTraining Loss: 0.453848\n",
            "Epoch: 1091 \tTraining Loss: 0.453899\n",
            "Epoch: 1092 \tTraining Loss: 0.453929\n",
            "Epoch: 1093 \tTraining Loss: 0.454034\n",
            "Epoch: 1094 \tTraining Loss: 0.454002\n",
            "Epoch: 1095 \tTraining Loss: 0.454040\n",
            "Epoch: 1096 \tTraining Loss: 0.453820\n",
            "Epoch: 1097 \tTraining Loss: 0.453627\n",
            "Epoch: 1098 \tTraining Loss: 0.453265\n",
            "Epoch: 1099 \tTraining Loss: 0.452973\n",
            "Epoch: 1100 \tTraining Loss: 0.452731\n",
            "Epoch: 1101 \tTraining Loss: 0.452584\n",
            "Epoch: 1102 \tTraining Loss: 0.452515\n",
            "Epoch: 1103 \tTraining Loss: 0.452499\n",
            "Epoch: 1104 \tTraining Loss: 0.452529\n",
            "Epoch: 1105 \tTraining Loss: 0.452553\n",
            "Epoch: 1106 \tTraining Loss: 0.452648\n",
            "Epoch: 1107 \tTraining Loss: 0.452640\n",
            "Epoch: 1108 \tTraining Loss: 0.452724\n",
            "Epoch: 1109 \tTraining Loss: 0.452553\n",
            "Epoch: 1110 \tTraining Loss: 0.452430\n",
            "Epoch: 1111 \tTraining Loss: 0.452085\n",
            "Epoch: 1112 \tTraining Loss: 0.451789\n",
            "Epoch: 1113 \tTraining Loss: 0.451497\n",
            "Epoch: 1114 \tTraining Loss: 0.451297\n",
            "Epoch: 1115 \tTraining Loss: 0.451184\n",
            "Epoch: 1116 \tTraining Loss: 0.451136\n",
            "Epoch: 1117 \tTraining Loss: 0.451134\n",
            "Epoch: 1118 \tTraining Loss: 0.451150\n",
            "Epoch: 1119 \tTraining Loss: 0.451229\n",
            "Epoch: 1120 \tTraining Loss: 0.451255\n",
            "Epoch: 1121 \tTraining Loss: 0.451391\n",
            "Epoch: 1122 \tTraining Loss: 0.451299\n",
            "Epoch: 1123 \tTraining Loss: 0.451292\n",
            "Epoch: 1124 \tTraining Loss: 0.450993\n",
            "Epoch: 1125 \tTraining Loss: 0.450728\n",
            "Epoch: 1126 \tTraining Loss: 0.450361\n",
            "Epoch: 1127 \tTraining Loss: 0.450078\n",
            "Epoch: 1128 \tTraining Loss: 0.449887\n",
            "Epoch: 1129 \tTraining Loss: 0.449791\n",
            "Epoch: 1130 \tTraining Loss: 0.449759\n",
            "Epoch: 1131 \tTraining Loss: 0.449762\n",
            "Epoch: 1132 \tTraining Loss: 0.449822\n",
            "Epoch: 1133 \tTraining Loss: 0.449871\n",
            "Epoch: 1134 \tTraining Loss: 0.450032\n",
            "Epoch: 1135 \tTraining Loss: 0.450000\n",
            "Epoch: 1136 \tTraining Loss: 0.450076\n",
            "Epoch: 1137 \tTraining Loss: 0.449814\n",
            "Epoch: 1138 \tTraining Loss: 0.449591\n",
            "Epoch: 1139 \tTraining Loss: 0.449188\n",
            "Epoch: 1140 \tTraining Loss: 0.448857\n",
            "Epoch: 1141 \tTraining Loss: 0.448617\n",
            "Epoch: 1142 \tTraining Loss: 0.448501\n",
            "Epoch: 1143 \tTraining Loss: 0.448473\n",
            "Epoch: 1144 \tTraining Loss: 0.448488\n",
            "Epoch: 1145 \tTraining Loss: 0.448560\n",
            "Epoch: 1146 \tTraining Loss: 0.448618\n",
            "Epoch: 1147 \tTraining Loss: 0.448792\n",
            "Epoch: 1148 \tTraining Loss: 0.448745\n",
            "Epoch: 1149 \tTraining Loss: 0.448796\n",
            "Epoch: 1150 \tTraining Loss: 0.448492\n",
            "Epoch: 1151 \tTraining Loss: 0.448229\n",
            "Epoch: 1152 \tTraining Loss: 0.447836\n",
            "Epoch: 1153 \tTraining Loss: 0.447529\n",
            "Epoch: 1154 \tTraining Loss: 0.447340\n",
            "Epoch: 1155 \tTraining Loss: 0.447279\n",
            "Epoch: 1156 \tTraining Loss: 0.447298\n",
            "Epoch: 1157 \tTraining Loss: 0.447337\n",
            "Epoch: 1158 \tTraining Loss: 0.447431\n",
            "Epoch: 1159 \tTraining Loss: 0.447475\n",
            "Epoch: 1160 \tTraining Loss: 0.447635\n",
            "Epoch: 1161 \tTraining Loss: 0.447498\n",
            "Epoch: 1162 \tTraining Loss: 0.447432\n",
            "Epoch: 1163 \tTraining Loss: 0.447079\n",
            "Epoch: 1164 \tTraining Loss: 0.446784\n",
            "Epoch: 1165 \tTraining Loss: 0.446459\n",
            "Epoch: 1166 \tTraining Loss: 0.446230\n",
            "Epoch: 1167 \tTraining Loss: 0.446131\n",
            "Epoch: 1168 \tTraining Loss: 0.446133\n",
            "Epoch: 1169 \tTraining Loss: 0.446179\n",
            "Epoch: 1170 \tTraining Loss: 0.446201\n",
            "Epoch: 1171 \tTraining Loss: 0.446284\n",
            "Epoch: 1172 \tTraining Loss: 0.446291\n",
            "Epoch: 1173 \tTraining Loss: 0.446407\n",
            "Epoch: 1174 \tTraining Loss: 0.446193\n",
            "Epoch: 1175 \tTraining Loss: 0.446049\n",
            "Epoch: 1176 \tTraining Loss: 0.445736\n",
            "Epoch: 1177 \tTraining Loss: 0.445472\n",
            "Epoch: 1178 \tTraining Loss: 0.445197\n",
            "Epoch: 1179 \tTraining Loss: 0.445015\n",
            "Epoch: 1180 \tTraining Loss: 0.444951\n",
            "Epoch: 1181 \tTraining Loss: 0.444953\n",
            "Epoch: 1182 \tTraining Loss: 0.444972\n",
            "Epoch: 1183 \tTraining Loss: 0.444971\n",
            "Epoch: 1184 \tTraining Loss: 0.445050\n",
            "Epoch: 1185 \tTraining Loss: 0.445065\n",
            "Epoch: 1186 \tTraining Loss: 0.445187\n",
            "Epoch: 1187 \tTraining Loss: 0.445005\n",
            "Epoch: 1188 \tTraining Loss: 0.444917\n",
            "Epoch: 1189 \tTraining Loss: 0.444648\n",
            "Epoch: 1190 \tTraining Loss: 0.444394\n",
            "Epoch: 1191 \tTraining Loss: 0.444079\n",
            "Epoch: 1192 \tTraining Loss: 0.443862\n",
            "Epoch: 1193 \tTraining Loss: 0.443760\n",
            "Epoch: 1194 \tTraining Loss: 0.443714\n",
            "Epoch: 1195 \tTraining Loss: 0.443685\n",
            "Epoch: 1196 \tTraining Loss: 0.443671\n",
            "Epoch: 1197 \tTraining Loss: 0.443739\n",
            "Epoch: 1198 \tTraining Loss: 0.443776\n",
            "Epoch: 1199 \tTraining Loss: 0.443918\n",
            "Epoch: 1200 \tTraining Loss: 0.443838\n",
            "Epoch: 1201 \tTraining Loss: 0.443884\n",
            "Epoch: 1202 \tTraining Loss: 0.443691\n",
            "Epoch: 1203 \tTraining Loss: 0.443506\n",
            "Epoch: 1204 \tTraining Loss: 0.443140\n",
            "Epoch: 1205 \tTraining Loss: 0.442855\n",
            "Epoch: 1206 \tTraining Loss: 0.442641\n",
            "Epoch: 1207 \tTraining Loss: 0.442502\n",
            "Epoch: 1208 \tTraining Loss: 0.442404\n",
            "Epoch: 1209 \tTraining Loss: 0.442354\n",
            "Epoch: 1210 \tTraining Loss: 0.442370\n",
            "Epoch: 1211 \tTraining Loss: 0.442396\n",
            "Epoch: 1212 \tTraining Loss: 0.442484\n",
            "Epoch: 1213 \tTraining Loss: 0.442481\n",
            "Epoch: 1214 \tTraining Loss: 0.442595\n",
            "Epoch: 1215 \tTraining Loss: 0.442543\n",
            "Epoch: 1216 \tTraining Loss: 0.442544\n",
            "Epoch: 1217 \tTraining Loss: 0.442287\n",
            "Epoch: 1218 \tTraining Loss: 0.442088\n",
            "Epoch: 1219 \tTraining Loss: 0.441769\n",
            "Epoch: 1220 \tTraining Loss: 0.441538\n",
            "Epoch: 1221 \tTraining Loss: 0.441300\n",
            "Epoch: 1222 \tTraining Loss: 0.441139\n",
            "Epoch: 1223 \tTraining Loss: 0.441046\n",
            "Epoch: 1224 \tTraining Loss: 0.440991\n",
            "Epoch: 1225 \tTraining Loss: 0.440959\n",
            "Epoch: 1226 \tTraining Loss: 0.440937\n",
            "Epoch: 1227 \tTraining Loss: 0.440962\n",
            "Epoch: 1228 \tTraining Loss: 0.440978\n",
            "Epoch: 1229 \tTraining Loss: 0.441054\n",
            "Epoch: 1230 \tTraining Loss: 0.441054\n",
            "Epoch: 1231 \tTraining Loss: 0.441169\n",
            "Epoch: 1232 \tTraining Loss: 0.441102\n",
            "Epoch: 1233 \tTraining Loss: 0.441201\n",
            "Epoch: 1234 \tTraining Loss: 0.440962\n",
            "Epoch: 1235 \tTraining Loss: 0.440846\n",
            "Epoch: 1236 \tTraining Loss: 0.440490\n",
            "Epoch: 1237 \tTraining Loss: 0.440206\n",
            "Epoch: 1238 \tTraining Loss: 0.439898\n",
            "Epoch: 1239 \tTraining Loss: 0.439679\n",
            "Epoch: 1240 \tTraining Loss: 0.439551\n",
            "Epoch: 1241 \tTraining Loss: 0.439494\n",
            "Epoch: 1242 \tTraining Loss: 0.439481\n",
            "Epoch: 1243 \tTraining Loss: 0.439486\n",
            "Epoch: 1244 \tTraining Loss: 0.439536\n",
            "Epoch: 1245 \tTraining Loss: 0.439558\n",
            "Epoch: 1246 \tTraining Loss: 0.439663\n",
            "Epoch: 1247 \tTraining Loss: 0.439619\n",
            "Epoch: 1248 \tTraining Loss: 0.439685\n",
            "Epoch: 1249 \tTraining Loss: 0.439520\n",
            "Epoch: 1250 \tTraining Loss: 0.439423\n",
            "Epoch: 1251 \tTraining Loss: 0.439133\n",
            "Epoch: 1252 \tTraining Loss: 0.438893\n",
            "Epoch: 1253 \tTraining Loss: 0.438617\n",
            "Epoch: 1254 \tTraining Loss: 0.438417\n",
            "Epoch: 1255 \tTraining Loss: 0.438255\n",
            "Epoch: 1256 \tTraining Loss: 0.438140\n",
            "Epoch: 1257 \tTraining Loss: 0.438064\n",
            "Epoch: 1258 \tTraining Loss: 0.438017\n",
            "Epoch: 1259 \tTraining Loss: 0.437995\n",
            "Epoch: 1260 \tTraining Loss: 0.437980\n",
            "Epoch: 1261 \tTraining Loss: 0.438011\n",
            "Epoch: 1262 \tTraining Loss: 0.438028\n",
            "Epoch: 1263 \tTraining Loss: 0.438139\n",
            "Epoch: 1264 \tTraining Loss: 0.438179\n",
            "Epoch: 1265 \tTraining Loss: 0.438381\n",
            "Epoch: 1266 \tTraining Loss: 0.438377\n",
            "Epoch: 1267 \tTraining Loss: 0.438616\n",
            "Epoch: 1268 \tTraining Loss: 0.438393\n",
            "Epoch: 1269 \tTraining Loss: 0.438356\n",
            "Epoch: 1270 \tTraining Loss: 0.437805\n",
            "Epoch: 1271 \tTraining Loss: 0.437369\n",
            "Epoch: 1272 \tTraining Loss: 0.436928\n",
            "Epoch: 1273 \tTraining Loss: 0.436681\n",
            "Epoch: 1274 \tTraining Loss: 0.436624\n",
            "Epoch: 1275 \tTraining Loss: 0.436704\n",
            "Epoch: 1276 \tTraining Loss: 0.436859\n",
            "Epoch: 1277 \tTraining Loss: 0.436926\n",
            "Epoch: 1278 \tTraining Loss: 0.437004\n",
            "Epoch: 1279 \tTraining Loss: 0.436869\n",
            "Epoch: 1280 \tTraining Loss: 0.436778\n",
            "Epoch: 1281 \tTraining Loss: 0.436434\n",
            "Epoch: 1282 \tTraining Loss: 0.436151\n",
            "Epoch: 1283 \tTraining Loss: 0.435891\n",
            "Epoch: 1284 \tTraining Loss: 0.435737\n",
            "Epoch: 1285 \tTraining Loss: 0.435663\n",
            "Epoch: 1286 \tTraining Loss: 0.435644\n",
            "Epoch: 1287 \tTraining Loss: 0.435675\n",
            "Epoch: 1288 \tTraining Loss: 0.435700\n",
            "Epoch: 1289 \tTraining Loss: 0.435745\n",
            "Epoch: 1290 \tTraining Loss: 0.435716\n",
            "Epoch: 1291 \tTraining Loss: 0.435752\n",
            "Epoch: 1292 \tTraining Loss: 0.435656\n",
            "Epoch: 1293 \tTraining Loss: 0.435642\n",
            "Epoch: 1294 \tTraining Loss: 0.435418\n",
            "Epoch: 1295 \tTraining Loss: 0.435261\n",
            "Epoch: 1296 \tTraining Loss: 0.435021\n",
            "Epoch: 1297 \tTraining Loss: 0.434819\n",
            "Epoch: 1298 \tTraining Loss: 0.434618\n",
            "Epoch: 1299 \tTraining Loss: 0.434467\n",
            "Epoch: 1300 \tTraining Loss: 0.434371\n",
            "Epoch: 1301 \tTraining Loss: 0.434309\n",
            "Epoch: 1302 \tTraining Loss: 0.434260\n",
            "Epoch: 1303 \tTraining Loss: 0.434220\n",
            "Epoch: 1304 \tTraining Loss: 0.434214\n",
            "Epoch: 1305 \tTraining Loss: 0.434219\n",
            "Epoch: 1306 \tTraining Loss: 0.434295\n",
            "Epoch: 1307 \tTraining Loss: 0.434329\n",
            "Epoch: 1308 \tTraining Loss: 0.434516\n",
            "Epoch: 1309 \tTraining Loss: 0.434590\n",
            "Epoch: 1310 \tTraining Loss: 0.434883\n",
            "Epoch: 1311 \tTraining Loss: 0.434867\n",
            "Epoch: 1312 \tTraining Loss: 0.435090\n",
            "Epoch: 1313 \tTraining Loss: 0.434754\n",
            "Epoch: 1314 \tTraining Loss: 0.434618\n",
            "Epoch: 1315 \tTraining Loss: 0.433909\n",
            "Epoch: 1316 \tTraining Loss: 0.433392\n",
            "Epoch: 1317 \tTraining Loss: 0.433045\n",
            "Epoch: 1318 \tTraining Loss: 0.432961\n",
            "Epoch: 1319 \tTraining Loss: 0.433060\n",
            "Epoch: 1320 \tTraining Loss: 0.433204\n",
            "Epoch: 1321 \tTraining Loss: 0.433385\n",
            "Epoch: 1322 \tTraining Loss: 0.433316\n",
            "Epoch: 1323 \tTraining Loss: 0.433173\n",
            "Epoch: 1324 \tTraining Loss: 0.432858\n",
            "Epoch: 1325 \tTraining Loss: 0.432610\n",
            "Epoch: 1326 \tTraining Loss: 0.432351\n",
            "Epoch: 1327 \tTraining Loss: 0.432193\n",
            "Epoch: 1328 \tTraining Loss: 0.432145\n",
            "Epoch: 1329 \tTraining Loss: 0.432170\n",
            "Epoch: 1330 \tTraining Loss: 0.432228\n",
            "Epoch: 1331 \tTraining Loss: 0.432226\n",
            "Epoch: 1332 \tTraining Loss: 0.432258\n",
            "Epoch: 1333 \tTraining Loss: 0.432189\n",
            "Epoch: 1334 \tTraining Loss: 0.432135\n",
            "Epoch: 1335 \tTraining Loss: 0.431985\n",
            "Epoch: 1336 \tTraining Loss: 0.431871\n",
            "Epoch: 1337 \tTraining Loss: 0.431684\n",
            "Epoch: 1338 \tTraining Loss: 0.431536\n",
            "Epoch: 1339 \tTraining Loss: 0.431363\n",
            "Epoch: 1340 \tTraining Loss: 0.431225\n",
            "Epoch: 1341 \tTraining Loss: 0.431107\n",
            "Epoch: 1342 \tTraining Loss: 0.431009\n",
            "Epoch: 1343 \tTraining Loss: 0.430920\n",
            "Epoch: 1344 \tTraining Loss: 0.430842\n",
            "Epoch: 1345 \tTraining Loss: 0.430778\n",
            "Epoch: 1346 \tTraining Loss: 0.430722\n",
            "Epoch: 1347 \tTraining Loss: 0.430675\n",
            "Epoch: 1348 \tTraining Loss: 0.430636\n",
            "Epoch: 1349 \tTraining Loss: 0.430630\n",
            "Epoch: 1350 \tTraining Loss: 0.430649\n",
            "Epoch: 1351 \tTraining Loss: 0.430772\n",
            "Epoch: 1352 \tTraining Loss: 0.430927\n",
            "Epoch: 1353 \tTraining Loss: 0.431418\n",
            "Epoch: 1354 \tTraining Loss: 0.431897\n",
            "Epoch: 1355 \tTraining Loss: 0.433257\n",
            "Epoch: 1356 \tTraining Loss: 0.434112\n",
            "Epoch: 1357 \tTraining Loss: 0.435961\n",
            "Epoch: 1358 \tTraining Loss: 0.434052\n",
            "Epoch: 1359 \tTraining Loss: 0.431776\n",
            "Epoch: 1360 \tTraining Loss: 0.429922\n",
            "Epoch: 1361 \tTraining Loss: 0.430657\n",
            "Epoch: 1362 \tTraining Loss: 0.432438\n",
            "Epoch: 1363 \tTraining Loss: 0.432146\n",
            "Epoch: 1364 \tTraining Loss: 0.430620\n",
            "Epoch: 1365 \tTraining Loss: 0.429379\n",
            "Epoch: 1366 \tTraining Loss: 0.430341\n",
            "Epoch: 1367 \tTraining Loss: 0.431563\n",
            "Epoch: 1368 \tTraining Loss: 0.430780\n",
            "Epoch: 1369 \tTraining Loss: 0.429652\n",
            "Epoch: 1370 \tTraining Loss: 0.429235\n",
            "Epoch: 1371 \tTraining Loss: 0.430000\n",
            "Epoch: 1372 \tTraining Loss: 0.430720\n",
            "Epoch: 1373 \tTraining Loss: 0.429699\n",
            "Epoch: 1374 \tTraining Loss: 0.428827\n",
            "Epoch: 1375 \tTraining Loss: 0.428983\n",
            "Epoch: 1376 \tTraining Loss: 0.429384\n",
            "Epoch: 1377 \tTraining Loss: 0.429443\n",
            "Epoch: 1378 \tTraining Loss: 0.428682\n",
            "Epoch: 1379 \tTraining Loss: 0.428384\n",
            "Epoch: 1380 \tTraining Loss: 0.428592\n",
            "Epoch: 1381 \tTraining Loss: 0.428699\n",
            "Epoch: 1382 \tTraining Loss: 0.428510\n",
            "Epoch: 1383 \tTraining Loss: 0.428051\n",
            "Epoch: 1384 \tTraining Loss: 0.428016\n",
            "Epoch: 1385 \tTraining Loss: 0.428145\n",
            "Epoch: 1386 \tTraining Loss: 0.428142\n",
            "Epoch: 1387 \tTraining Loss: 0.427944\n",
            "Epoch: 1388 \tTraining Loss: 0.427625\n",
            "Epoch: 1389 \tTraining Loss: 0.427633\n",
            "Epoch: 1390 \tTraining Loss: 0.427645\n",
            "Epoch: 1391 \tTraining Loss: 0.427619\n",
            "Epoch: 1392 \tTraining Loss: 0.427513\n",
            "Epoch: 1393 \tTraining Loss: 0.427273\n",
            "Epoch: 1394 \tTraining Loss: 0.427186\n",
            "Epoch: 1395 \tTraining Loss: 0.427167\n",
            "Epoch: 1396 \tTraining Loss: 0.427152\n",
            "Epoch: 1397 \tTraining Loss: 0.427094\n",
            "Epoch: 1398 \tTraining Loss: 0.426935\n",
            "Epoch: 1399 \tTraining Loss: 0.426807\n",
            "Epoch: 1400 \tTraining Loss: 0.426745\n",
            "Epoch: 1401 \tTraining Loss: 0.426686\n",
            "Epoch: 1402 \tTraining Loss: 0.426654\n",
            "Epoch: 1403 \tTraining Loss: 0.426581\n",
            "Epoch: 1404 \tTraining Loss: 0.426466\n",
            "Epoch: 1405 \tTraining Loss: 0.426366\n",
            "Epoch: 1406 \tTraining Loss: 0.426275\n",
            "Epoch: 1407 \tTraining Loss: 0.426223\n",
            "Epoch: 1408 \tTraining Loss: 0.426171\n",
            "Epoch: 1409 \tTraining Loss: 0.426102\n",
            "Epoch: 1410 \tTraining Loss: 0.426021\n",
            "Epoch: 1411 \tTraining Loss: 0.425929\n",
            "Epoch: 1412 \tTraining Loss: 0.425837\n",
            "Epoch: 1413 \tTraining Loss: 0.425761\n",
            "Epoch: 1414 \tTraining Loss: 0.425698\n",
            "Epoch: 1415 \tTraining Loss: 0.425634\n",
            "Epoch: 1416 \tTraining Loss: 0.425570\n",
            "Epoch: 1417 \tTraining Loss: 0.425493\n",
            "Epoch: 1418 \tTraining Loss: 0.425416\n",
            "Epoch: 1419 \tTraining Loss: 0.425335\n",
            "Epoch: 1420 \tTraining Loss: 0.425254\n",
            "Epoch: 1421 \tTraining Loss: 0.425178\n",
            "Epoch: 1422 \tTraining Loss: 0.425107\n",
            "Epoch: 1423 \tTraining Loss: 0.425037\n",
            "Epoch: 1424 \tTraining Loss: 0.424967\n",
            "Epoch: 1425 \tTraining Loss: 0.424897\n",
            "Epoch: 1426 \tTraining Loss: 0.424824\n",
            "Epoch: 1427 \tTraining Loss: 0.424751\n",
            "Epoch: 1428 \tTraining Loss: 0.424675\n",
            "Epoch: 1429 \tTraining Loss: 0.424598\n",
            "Epoch: 1430 \tTraining Loss: 0.424522\n",
            "Epoch: 1431 \tTraining Loss: 0.424445\n",
            "Epoch: 1432 \tTraining Loss: 0.424370\n",
            "Epoch: 1433 \tTraining Loss: 0.424295\n",
            "Epoch: 1434 \tTraining Loss: 0.424221\n",
            "Epoch: 1435 \tTraining Loss: 0.424147\n",
            "Epoch: 1436 \tTraining Loss: 0.424073\n",
            "Epoch: 1437 \tTraining Loss: 0.423999\n",
            "Epoch: 1438 \tTraining Loss: 0.423925\n",
            "Epoch: 1439 \tTraining Loss: 0.423851\n",
            "Epoch: 1440 \tTraining Loss: 0.423776\n",
            "Epoch: 1441 \tTraining Loss: 0.423701\n",
            "Epoch: 1442 \tTraining Loss: 0.423625\n",
            "Epoch: 1443 \tTraining Loss: 0.423549\n",
            "Epoch: 1444 \tTraining Loss: 0.423472\n",
            "Epoch: 1445 \tTraining Loss: 0.423395\n",
            "Epoch: 1446 \tTraining Loss: 0.423320\n",
            "Epoch: 1447 \tTraining Loss: 0.423253\n",
            "Epoch: 1448 \tTraining Loss: 0.423205\n",
            "Epoch: 1449 \tTraining Loss: 0.423176\n",
            "Epoch: 1450 \tTraining Loss: 0.423191\n",
            "Epoch: 1451 \tTraining Loss: 0.423240\n",
            "Epoch: 1452 \tTraining Loss: 0.423461\n",
            "Epoch: 1453 \tTraining Loss: 0.423794\n",
            "Epoch: 1454 \tTraining Loss: 0.424856\n",
            "Epoch: 1455 \tTraining Loss: 0.426281\n",
            "Epoch: 1456 \tTraining Loss: 0.431100\n",
            "Epoch: 1457 \tTraining Loss: 0.437949\n",
            "Epoch: 1458 \tTraining Loss: 0.442368\n",
            "Epoch: 1459 \tTraining Loss: 0.430935\n",
            "Epoch: 1460 \tTraining Loss: 0.439498\n",
            "Epoch: 1461 \tTraining Loss: 0.437016\n",
            "Epoch: 1462 \tTraining Loss: 0.434648\n",
            "Epoch: 1463 \tTraining Loss: 0.439641\n",
            "Epoch: 1464 \tTraining Loss: 0.442733\n",
            "Epoch: 1465 \tTraining Loss: 0.434997\n",
            "Epoch: 1466 \tTraining Loss: 0.432264\n",
            "Epoch: 1467 \tTraining Loss: 0.438967\n",
            "Epoch: 1468 \tTraining Loss: 0.431089\n",
            "Epoch: 1469 \tTraining Loss: 0.434492\n",
            "Epoch: 1470 \tTraining Loss: 0.434722\n",
            "Epoch: 1471 \tTraining Loss: 0.426714\n",
            "Epoch: 1472 \tTraining Loss: 0.435148\n",
            "Epoch: 1473 \tTraining Loss: 0.426179\n",
            "Epoch: 1474 \tTraining Loss: 0.428516\n",
            "Epoch: 1475 \tTraining Loss: 0.427573\n",
            "Epoch: 1476 \tTraining Loss: 0.425066\n",
            "Epoch: 1477 \tTraining Loss: 0.426361\n",
            "Epoch: 1478 \tTraining Loss: 0.423424\n",
            "Epoch: 1479 \tTraining Loss: 0.426501\n",
            "Epoch: 1480 \tTraining Loss: 0.423381\n",
            "Epoch: 1481 \tTraining Loss: 0.424478\n",
            "Epoch: 1482 \tTraining Loss: 0.423089\n",
            "Epoch: 1483 \tTraining Loss: 0.423498\n",
            "Epoch: 1484 \tTraining Loss: 0.423455\n",
            "Epoch: 1485 \tTraining Loss: 0.422284\n",
            "Epoch: 1486 \tTraining Loss: 0.423337\n",
            "Epoch: 1487 \tTraining Loss: 0.421994\n",
            "Epoch: 1488 \tTraining Loss: 0.422183\n",
            "Epoch: 1489 \tTraining Loss: 0.421816\n",
            "Epoch: 1490 \tTraining Loss: 0.421849\n",
            "Epoch: 1491 \tTraining Loss: 0.421623\n",
            "Epoch: 1492 \tTraining Loss: 0.421259\n",
            "Epoch: 1493 \tTraining Loss: 0.421423\n",
            "Epoch: 1494 \tTraining Loss: 0.421147\n",
            "Epoch: 1495 \tTraining Loss: 0.421042\n",
            "Epoch: 1496 \tTraining Loss: 0.420999\n",
            "Epoch: 1497 \tTraining Loss: 0.420751\n",
            "Epoch: 1498 \tTraining Loss: 0.420856\n",
            "Epoch: 1499 \tTraining Loss: 0.420459\n",
            "Epoch: 1500 \tTraining Loss: 0.420710\n",
            "Epoch: 1501 \tTraining Loss: 0.420259\n",
            "Epoch: 1502 \tTraining Loss: 0.420449\n",
            "Epoch: 1503 \tTraining Loss: 0.420134\n",
            "Epoch: 1504 \tTraining Loss: 0.420227\n",
            "Epoch: 1505 \tTraining Loss: 0.420083\n",
            "Epoch: 1506 \tTraining Loss: 0.419939\n",
            "Epoch: 1507 \tTraining Loss: 0.420010\n",
            "Epoch: 1508 \tTraining Loss: 0.419809\n",
            "Epoch: 1509 \tTraining Loss: 0.419795\n",
            "Epoch: 1510 \tTraining Loss: 0.419689\n",
            "Epoch: 1511 \tTraining Loss: 0.419638\n",
            "Epoch: 1512 \tTraining Loss: 0.419567\n",
            "Epoch: 1513 \tTraining Loss: 0.419491\n",
            "Epoch: 1514 \tTraining Loss: 0.419409\n",
            "Epoch: 1515 \tTraining Loss: 0.419378\n",
            "Epoch: 1516 \tTraining Loss: 0.419280\n",
            "Epoch: 1517 \tTraining Loss: 0.419233\n",
            "Epoch: 1518 \tTraining Loss: 0.419160\n",
            "Epoch: 1519 \tTraining Loss: 0.419096\n",
            "Epoch: 1520 \tTraining Loss: 0.419047\n",
            "Epoch: 1521 \tTraining Loss: 0.418957\n",
            "Epoch: 1522 \tTraining Loss: 0.418916\n",
            "Epoch: 1523 \tTraining Loss: 0.418846\n",
            "Epoch: 1524 \tTraining Loss: 0.418770\n",
            "Epoch: 1525 \tTraining Loss: 0.418725\n",
            "Epoch: 1526 \tTraining Loss: 0.418646\n",
            "Epoch: 1527 \tTraining Loss: 0.418599\n",
            "Epoch: 1528 \tTraining Loss: 0.418526\n",
            "Epoch: 1529 \tTraining Loss: 0.418468\n",
            "Epoch: 1530 \tTraining Loss: 0.418410\n",
            "Epoch: 1531 \tTraining Loss: 0.418339\n",
            "Epoch: 1532 \tTraining Loss: 0.418286\n",
            "Epoch: 1533 \tTraining Loss: 0.418220\n",
            "Epoch: 1534 \tTraining Loss: 0.418160\n",
            "Epoch: 1535 \tTraining Loss: 0.418100\n",
            "Epoch: 1536 \tTraining Loss: 0.418039\n",
            "Epoch: 1537 \tTraining Loss: 0.417977\n",
            "Epoch: 1538 \tTraining Loss: 0.417919\n",
            "Epoch: 1539 \tTraining Loss: 0.417857\n",
            "Epoch: 1540 \tTraining Loss: 0.417796\n",
            "Epoch: 1541 \tTraining Loss: 0.417738\n",
            "Epoch: 1542 \tTraining Loss: 0.417675\n",
            "Epoch: 1543 \tTraining Loss: 0.417617\n",
            "Epoch: 1544 \tTraining Loss: 0.417556\n",
            "Epoch: 1545 \tTraining Loss: 0.417495\n",
            "Epoch: 1546 \tTraining Loss: 0.417437\n",
            "Epoch: 1547 \tTraining Loss: 0.417375\n",
            "Epoch: 1548 \tTraining Loss: 0.417316\n",
            "Epoch: 1549 \tTraining Loss: 0.417256\n",
            "Epoch: 1550 \tTraining Loss: 0.417195\n",
            "Epoch: 1551 \tTraining Loss: 0.417135\n",
            "Epoch: 1552 \tTraining Loss: 0.417075\n",
            "Epoch: 1553 \tTraining Loss: 0.417015\n",
            "Epoch: 1554 \tTraining Loss: 0.416955\n",
            "Epoch: 1555 \tTraining Loss: 0.416894\n",
            "Epoch: 1556 \tTraining Loss: 0.416834\n",
            "Epoch: 1557 \tTraining Loss: 0.416773\n",
            "Epoch: 1558 \tTraining Loss: 0.416713\n",
            "Epoch: 1559 \tTraining Loss: 0.416653\n",
            "Epoch: 1560 \tTraining Loss: 0.416592\n",
            "Epoch: 1561 \tTraining Loss: 0.416531\n",
            "Epoch: 1562 \tTraining Loss: 0.416471\n",
            "Epoch: 1563 \tTraining Loss: 0.416409\n",
            "Epoch: 1564 \tTraining Loss: 0.416348\n",
            "Epoch: 1565 \tTraining Loss: 0.416288\n",
            "Epoch: 1566 \tTraining Loss: 0.416226\n",
            "Epoch: 1567 \tTraining Loss: 0.416165\n",
            "Epoch: 1568 \tTraining Loss: 0.416104\n",
            "Epoch: 1569 \tTraining Loss: 0.416043\n",
            "Epoch: 1570 \tTraining Loss: 0.415982\n",
            "Epoch: 1571 \tTraining Loss: 0.415920\n",
            "Epoch: 1572 \tTraining Loss: 0.415859\n",
            "Epoch: 1573 \tTraining Loss: 0.415797\n",
            "Epoch: 1574 \tTraining Loss: 0.415736\n",
            "Epoch: 1575 \tTraining Loss: 0.415674\n",
            "Epoch: 1576 \tTraining Loss: 0.415612\n",
            "Epoch: 1577 \tTraining Loss: 0.415550\n",
            "Epoch: 1578 \tTraining Loss: 0.415488\n",
            "Epoch: 1579 \tTraining Loss: 0.415426\n",
            "Epoch: 1580 \tTraining Loss: 0.415364\n",
            "Epoch: 1581 \tTraining Loss: 0.415302\n",
            "Epoch: 1582 \tTraining Loss: 0.415239\n",
            "Epoch: 1583 \tTraining Loss: 0.415177\n",
            "Epoch: 1584 \tTraining Loss: 0.415115\n",
            "Epoch: 1585 \tTraining Loss: 0.415052\n",
            "Epoch: 1586 \tTraining Loss: 0.414990\n",
            "Epoch: 1587 \tTraining Loss: 0.414927\n",
            "Epoch: 1588 \tTraining Loss: 0.414864\n",
            "Epoch: 1589 \tTraining Loss: 0.414802\n",
            "Epoch: 1590 \tTraining Loss: 0.414739\n",
            "Epoch: 1591 \tTraining Loss: 0.414677\n",
            "Epoch: 1592 \tTraining Loss: 0.414614\n",
            "Epoch: 1593 \tTraining Loss: 0.414551\n",
            "Epoch: 1594 \tTraining Loss: 0.414489\n",
            "Epoch: 1595 \tTraining Loss: 0.414426\n",
            "Epoch: 1596 \tTraining Loss: 0.414364\n",
            "Epoch: 1597 \tTraining Loss: 0.414301\n",
            "Epoch: 1598 \tTraining Loss: 0.414239\n",
            "Epoch: 1599 \tTraining Loss: 0.414177\n",
            "Epoch: 1600 \tTraining Loss: 0.414114\n",
            "Epoch: 1601 \tTraining Loss: 0.414052\n",
            "Epoch: 1602 \tTraining Loss: 0.413990\n",
            "Epoch: 1603 \tTraining Loss: 0.413927\n",
            "Epoch: 1604 \tTraining Loss: 0.413865\n",
            "Epoch: 1605 \tTraining Loss: 0.413803\n",
            "Epoch: 1606 \tTraining Loss: 0.413741\n",
            "Epoch: 1607 \tTraining Loss: 0.413679\n",
            "Epoch: 1608 \tTraining Loss: 0.413617\n",
            "Epoch: 1609 \tTraining Loss: 0.413554\n",
            "Epoch: 1610 \tTraining Loss: 0.413492\n",
            "Epoch: 1611 \tTraining Loss: 0.413430\n",
            "Epoch: 1612 \tTraining Loss: 0.413368\n",
            "Epoch: 1613 \tTraining Loss: 0.413306\n",
            "Epoch: 1614 \tTraining Loss: 0.413244\n",
            "Epoch: 1615 \tTraining Loss: 0.413182\n",
            "Epoch: 1616 \tTraining Loss: 0.413120\n",
            "Epoch: 1617 \tTraining Loss: 0.413058\n",
            "Epoch: 1618 \tTraining Loss: 0.412996\n",
            "Epoch: 1619 \tTraining Loss: 0.412935\n",
            "Epoch: 1620 \tTraining Loss: 0.412873\n",
            "Epoch: 1621 \tTraining Loss: 0.412811\n",
            "Epoch: 1622 \tTraining Loss: 0.412750\n",
            "Epoch: 1623 \tTraining Loss: 0.412688\n",
            "Epoch: 1624 \tTraining Loss: 0.412627\n",
            "Epoch: 1625 \tTraining Loss: 0.412565\n",
            "Epoch: 1626 \tTraining Loss: 0.412503\n",
            "Epoch: 1627 \tTraining Loss: 0.412442\n",
            "Epoch: 1628 \tTraining Loss: 0.412380\n",
            "Epoch: 1629 \tTraining Loss: 0.412319\n",
            "Epoch: 1630 \tTraining Loss: 0.412257\n",
            "Epoch: 1631 \tTraining Loss: 0.412196\n",
            "Epoch: 1632 \tTraining Loss: 0.412135\n",
            "Epoch: 1633 \tTraining Loss: 0.412073\n",
            "Epoch: 1634 \tTraining Loss: 0.412012\n",
            "Epoch: 1635 \tTraining Loss: 0.411950\n",
            "Epoch: 1636 \tTraining Loss: 0.411889\n",
            "Epoch: 1637 \tTraining Loss: 0.411827\n",
            "Epoch: 1638 \tTraining Loss: 0.411766\n",
            "Epoch: 1639 \tTraining Loss: 0.411704\n",
            "Epoch: 1640 \tTraining Loss: 0.411643\n",
            "Epoch: 1641 \tTraining Loss: 0.411581\n",
            "Epoch: 1642 \tTraining Loss: 0.411520\n",
            "Epoch: 1643 \tTraining Loss: 0.411458\n",
            "Epoch: 1644 \tTraining Loss: 0.411397\n",
            "Epoch: 1645 \tTraining Loss: 0.411335\n",
            "Epoch: 1646 \tTraining Loss: 0.411274\n",
            "Epoch: 1647 \tTraining Loss: 0.411212\n",
            "Epoch: 1648 \tTraining Loss: 0.411151\n",
            "Epoch: 1649 \tTraining Loss: 0.411089\n",
            "Epoch: 1650 \tTraining Loss: 0.411028\n",
            "Epoch: 1651 \tTraining Loss: 0.410966\n",
            "Epoch: 1652 \tTraining Loss: 0.410905\n",
            "Epoch: 1653 \tTraining Loss: 0.410843\n",
            "Epoch: 1654 \tTraining Loss: 0.410782\n",
            "Epoch: 1655 \tTraining Loss: 0.410720\n",
            "Epoch: 1656 \tTraining Loss: 0.410658\n",
            "Epoch: 1657 \tTraining Loss: 0.410597\n",
            "Epoch: 1658 \tTraining Loss: 0.410535\n",
            "Epoch: 1659 \tTraining Loss: 0.410474\n",
            "Epoch: 1660 \tTraining Loss: 0.410412\n",
            "Epoch: 1661 \tTraining Loss: 0.410351\n",
            "Epoch: 1662 \tTraining Loss: 0.410289\n",
            "Epoch: 1663 \tTraining Loss: 0.410227\n",
            "Epoch: 1664 \tTraining Loss: 0.410166\n",
            "Epoch: 1665 \tTraining Loss: 0.410104\n",
            "Epoch: 1666 \tTraining Loss: 0.410042\n",
            "Epoch: 1667 \tTraining Loss: 0.409981\n",
            "Epoch: 1668 \tTraining Loss: 0.409919\n",
            "Epoch: 1669 \tTraining Loss: 0.409857\n",
            "Epoch: 1670 \tTraining Loss: 0.409796\n",
            "Epoch: 1671 \tTraining Loss: 0.409734\n",
            "Epoch: 1672 \tTraining Loss: 0.409672\n",
            "Epoch: 1673 \tTraining Loss: 0.409611\n",
            "Epoch: 1674 \tTraining Loss: 0.409549\n",
            "Epoch: 1675 \tTraining Loss: 0.409487\n",
            "Epoch: 1676 \tTraining Loss: 0.409425\n",
            "Epoch: 1677 \tTraining Loss: 0.409364\n",
            "Epoch: 1678 \tTraining Loss: 0.409302\n",
            "Epoch: 1679 \tTraining Loss: 0.409240\n",
            "Epoch: 1680 \tTraining Loss: 0.409178\n",
            "Epoch: 1681 \tTraining Loss: 0.409116\n",
            "Epoch: 1682 \tTraining Loss: 0.409055\n",
            "Epoch: 1683 \tTraining Loss: 0.408993\n",
            "Epoch: 1684 \tTraining Loss: 0.408931\n",
            "Epoch: 1685 \tTraining Loss: 0.408869\n",
            "Epoch: 1686 \tTraining Loss: 0.408807\n",
            "Epoch: 1687 \tTraining Loss: 0.408745\n",
            "Epoch: 1688 \tTraining Loss: 0.408683\n",
            "Epoch: 1689 \tTraining Loss: 0.408621\n",
            "Epoch: 1690 \tTraining Loss: 0.408560\n",
            "Epoch: 1691 \tTraining Loss: 0.408498\n",
            "Epoch: 1692 \tTraining Loss: 0.408436\n",
            "Epoch: 1693 \tTraining Loss: 0.408374\n",
            "Epoch: 1694 \tTraining Loss: 0.408312\n",
            "Epoch: 1695 \tTraining Loss: 0.408250\n",
            "Epoch: 1696 \tTraining Loss: 0.408188\n",
            "Epoch: 1697 \tTraining Loss: 0.408126\n",
            "Epoch: 1698 \tTraining Loss: 0.408065\n",
            "Epoch: 1699 \tTraining Loss: 0.408003\n",
            "Epoch: 1700 \tTraining Loss: 0.407941\n",
            "Epoch: 1701 \tTraining Loss: 0.407879\n",
            "Epoch: 1702 \tTraining Loss: 0.407817\n",
            "Epoch: 1703 \tTraining Loss: 0.407755\n",
            "Epoch: 1704 \tTraining Loss: 0.407693\n",
            "Epoch: 1705 \tTraining Loss: 0.407631\n",
            "Epoch: 1706 \tTraining Loss: 0.407569\n",
            "Epoch: 1707 \tTraining Loss: 0.407507\n",
            "Epoch: 1708 \tTraining Loss: 0.407445\n",
            "Epoch: 1709 \tTraining Loss: 0.407384\n",
            "Epoch: 1710 \tTraining Loss: 0.407322\n",
            "Epoch: 1711 \tTraining Loss: 0.407260\n",
            "Epoch: 1712 \tTraining Loss: 0.407198\n",
            "Epoch: 1713 \tTraining Loss: 0.407136\n",
            "Epoch: 1714 \tTraining Loss: 0.407074\n",
            "Epoch: 1715 \tTraining Loss: 0.407012\n",
            "Epoch: 1716 \tTraining Loss: 0.406951\n",
            "Epoch: 1717 \tTraining Loss: 0.406889\n",
            "Epoch: 1718 \tTraining Loss: 0.406827\n",
            "Epoch: 1719 \tTraining Loss: 0.406766\n",
            "Epoch: 1720 \tTraining Loss: 0.406704\n",
            "Epoch: 1721 \tTraining Loss: 0.406642\n",
            "Epoch: 1722 \tTraining Loss: 0.406580\n",
            "Epoch: 1723 \tTraining Loss: 0.406519\n",
            "Epoch: 1724 \tTraining Loss: 0.406457\n",
            "Epoch: 1725 \tTraining Loss: 0.406395\n",
            "Epoch: 1726 \tTraining Loss: 0.406334\n",
            "Epoch: 1727 \tTraining Loss: 0.406272\n",
            "Epoch: 1728 \tTraining Loss: 0.406210\n",
            "Epoch: 1729 \tTraining Loss: 0.406149\n",
            "Epoch: 1730 \tTraining Loss: 0.406087\n",
            "Epoch: 1731 \tTraining Loss: 0.406025\n",
            "Epoch: 1732 \tTraining Loss: 0.405963\n",
            "Epoch: 1733 \tTraining Loss: 0.405901\n",
            "Epoch: 1734 \tTraining Loss: 0.405839\n",
            "Epoch: 1735 \tTraining Loss: 0.405777\n",
            "Epoch: 1736 \tTraining Loss: 0.405715\n",
            "Epoch: 1737 \tTraining Loss: 0.405653\n",
            "Epoch: 1738 \tTraining Loss: 0.405591\n",
            "Epoch: 1739 \tTraining Loss: 0.405528\n",
            "Epoch: 1740 \tTraining Loss: 0.405466\n",
            "Epoch: 1741 \tTraining Loss: 0.405403\n",
            "Epoch: 1742 \tTraining Loss: 0.405340\n",
            "Epoch: 1743 \tTraining Loss: 0.405277\n",
            "Epoch: 1744 \tTraining Loss: 0.405213\n",
            "Epoch: 1745 \tTraining Loss: 0.405149\n",
            "Epoch: 1746 \tTraining Loss: 0.405085\n",
            "Epoch: 1747 \tTraining Loss: 0.405021\n",
            "Epoch: 1748 \tTraining Loss: 0.404957\n",
            "Epoch: 1749 \tTraining Loss: 0.404894\n",
            "Epoch: 1750 \tTraining Loss: 0.404831\n",
            "Epoch: 1751 \tTraining Loss: 0.404768\n",
            "Epoch: 1752 \tTraining Loss: 0.404705\n",
            "Epoch: 1753 \tTraining Loss: 0.404642\n",
            "Epoch: 1754 \tTraining Loss: 0.404579\n",
            "Epoch: 1755 \tTraining Loss: 0.404516\n",
            "Epoch: 1756 \tTraining Loss: 0.404453\n",
            "Epoch: 1757 \tTraining Loss: 0.404390\n",
            "Epoch: 1758 \tTraining Loss: 0.404328\n",
            "Epoch: 1759 \tTraining Loss: 0.404264\n",
            "Epoch: 1760 \tTraining Loss: 0.404202\n",
            "Epoch: 1761 \tTraining Loss: 0.404139\n",
            "Epoch: 1762 \tTraining Loss: 0.404081\n",
            "Epoch: 1763 \tTraining Loss: 0.404022\n",
            "Epoch: 1764 \tTraining Loss: 0.403972\n",
            "Epoch: 1765 \tTraining Loss: 0.403922\n",
            "Epoch: 1766 \tTraining Loss: 0.403889\n",
            "Epoch: 1767 \tTraining Loss: 0.403879\n",
            "Epoch: 1768 \tTraining Loss: 0.403890\n",
            "Epoch: 1769 \tTraining Loss: 0.403904\n",
            "Epoch: 1770 \tTraining Loss: 0.403964\n",
            "Epoch: 1771 \tTraining Loss: 0.403911\n",
            "Epoch: 1772 \tTraining Loss: 0.403964\n",
            "Epoch: 1773 \tTraining Loss: 0.403876\n",
            "Epoch: 1774 \tTraining Loss: 0.403928\n",
            "Epoch: 1775 \tTraining Loss: 0.403777\n",
            "Epoch: 1776 \tTraining Loss: 0.403751\n",
            "Epoch: 1777 \tTraining Loss: 0.403521\n",
            "Epoch: 1778 \tTraining Loss: 0.403357\n",
            "Epoch: 1779 \tTraining Loss: 0.403109\n",
            "Epoch: 1780 \tTraining Loss: 0.402935\n",
            "Epoch: 1781 \tTraining Loss: 0.402771\n",
            "Epoch: 1782 \tTraining Loss: 0.402648\n",
            "Epoch: 1783 \tTraining Loss: 0.402567\n",
            "Epoch: 1784 \tTraining Loss: 0.402518\n",
            "Epoch: 1785 \tTraining Loss: 0.402488\n",
            "Epoch: 1786 \tTraining Loss: 0.402464\n",
            "Epoch: 1787 \tTraining Loss: 0.402462\n",
            "Epoch: 1788 \tTraining Loss: 0.402454\n",
            "Epoch: 1789 \tTraining Loss: 0.402507\n",
            "Epoch: 1790 \tTraining Loss: 0.402482\n",
            "Epoch: 1791 \tTraining Loss: 0.402544\n",
            "Epoch: 1792 \tTraining Loss: 0.402504\n",
            "Epoch: 1793 \tTraining Loss: 0.402616\n",
            "Epoch: 1794 \tTraining Loss: 0.402528\n",
            "Epoch: 1795 \tTraining Loss: 0.402629\n",
            "Epoch: 1796 \tTraining Loss: 0.402448\n",
            "Epoch: 1797 \tTraining Loss: 0.402434\n",
            "Epoch: 1798 \tTraining Loss: 0.402134\n",
            "Epoch: 1799 \tTraining Loss: 0.401949\n",
            "Epoch: 1800 \tTraining Loss: 0.401635\n",
            "Epoch: 1801 \tTraining Loss: 0.401419\n",
            "Epoch: 1802 \tTraining Loss: 0.401241\n",
            "Epoch: 1803 \tTraining Loss: 0.401127\n",
            "Epoch: 1804 \tTraining Loss: 0.401065\n",
            "Epoch: 1805 \tTraining Loss: 0.401035\n",
            "Epoch: 1806 \tTraining Loss: 0.401030\n",
            "Epoch: 1807 \tTraining Loss: 0.401040\n",
            "Epoch: 1808 \tTraining Loss: 0.401131\n",
            "Epoch: 1809 \tTraining Loss: 0.401184\n",
            "Epoch: 1810 \tTraining Loss: 0.401396\n",
            "Epoch: 1811 \tTraining Loss: 0.401472\n",
            "Epoch: 1812 \tTraining Loss: 0.401924\n",
            "Epoch: 1813 \tTraining Loss: 0.401957\n",
            "Epoch: 1814 \tTraining Loss: 0.402618\n",
            "Epoch: 1815 \tTraining Loss: 0.402260\n",
            "Epoch: 1816 \tTraining Loss: 0.402531\n",
            "Epoch: 1817 \tTraining Loss: 0.401480\n",
            "Epoch: 1818 \tTraining Loss: 0.400700\n",
            "Epoch: 1819 \tTraining Loss: 0.400046\n",
            "Epoch: 1820 \tTraining Loss: 0.399943\n",
            "Epoch: 1821 \tTraining Loss: 0.400203\n",
            "Epoch: 1822 \tTraining Loss: 0.400553\n",
            "Epoch: 1823 \tTraining Loss: 0.401159\n",
            "Epoch: 1824 \tTraining Loss: 0.400895\n",
            "Epoch: 1825 \tTraining Loss: 0.400768\n",
            "Epoch: 1826 \tTraining Loss: 0.399919\n",
            "Epoch: 1827 \tTraining Loss: 0.399542\n",
            "Epoch: 1828 \tTraining Loss: 0.399474\n",
            "Epoch: 1829 \tTraining Loss: 0.399459\n",
            "Epoch: 1830 \tTraining Loss: 0.399814\n",
            "Epoch: 1831 \tTraining Loss: 0.399808\n",
            "Epoch: 1832 \tTraining Loss: 0.399859\n",
            "Epoch: 1833 \tTraining Loss: 0.399814\n",
            "Epoch: 1834 \tTraining Loss: 0.399620\n",
            "Epoch: 1835 \tTraining Loss: 0.399011\n",
            "Epoch: 1836 \tTraining Loss: 0.398973\n",
            "Epoch: 1837 \tTraining Loss: 0.398703\n",
            "Epoch: 1838 \tTraining Loss: 0.398956\n",
            "Epoch: 1839 \tTraining Loss: 0.399208\n",
            "Epoch: 1840 \tTraining Loss: 0.399334\n",
            "Epoch: 1841 \tTraining Loss: 0.399923\n",
            "Epoch: 1842 \tTraining Loss: 0.399654\n",
            "Epoch: 1843 \tTraining Loss: 0.400360\n",
            "Epoch: 1844 \tTraining Loss: 0.399906\n",
            "Epoch: 1845 \tTraining Loss: 0.399492\n",
            "Epoch: 1846 \tTraining Loss: 0.398419\n",
            "Epoch: 1847 \tTraining Loss: 0.398107\n",
            "Epoch: 1848 \tTraining Loss: 0.398662\n",
            "Epoch: 1849 \tTraining Loss: 0.398552\n",
            "Epoch: 1850 \tTraining Loss: 0.399124\n",
            "Epoch: 1851 \tTraining Loss: 0.398281\n",
            "Epoch: 1852 \tTraining Loss: 0.398119\n",
            "Epoch: 1853 \tTraining Loss: 0.397682\n",
            "Epoch: 1854 \tTraining Loss: 0.397628\n",
            "Epoch: 1855 \tTraining Loss: 0.398143\n",
            "Epoch: 1856 \tTraining Loss: 0.397841\n",
            "Epoch: 1857 \tTraining Loss: 0.397666\n",
            "Epoch: 1858 \tTraining Loss: 0.397360\n",
            "Epoch: 1859 \tTraining Loss: 0.397054\n",
            "Epoch: 1860 \tTraining Loss: 0.397030\n",
            "Epoch: 1861 \tTraining Loss: 0.396890\n",
            "Epoch: 1862 \tTraining Loss: 0.396939\n",
            "Epoch: 1863 \tTraining Loss: 0.396945\n",
            "Epoch: 1864 \tTraining Loss: 0.396869\n",
            "Epoch: 1865 \tTraining Loss: 0.396778\n",
            "Epoch: 1866 \tTraining Loss: 0.396722\n",
            "Epoch: 1867 \tTraining Loss: 0.396535\n",
            "Epoch: 1868 \tTraining Loss: 0.396477\n",
            "Epoch: 1869 \tTraining Loss: 0.396313\n",
            "Epoch: 1870 \tTraining Loss: 0.396184\n",
            "Epoch: 1871 \tTraining Loss: 0.396201\n",
            "Epoch: 1872 \tTraining Loss: 0.396123\n",
            "Epoch: 1873 \tTraining Loss: 0.396049\n",
            "Epoch: 1874 \tTraining Loss: 0.396060\n",
            "Epoch: 1875 \tTraining Loss: 0.396013\n",
            "Epoch: 1876 \tTraining Loss: 0.395979\n",
            "Epoch: 1877 \tTraining Loss: 0.396043\n",
            "Epoch: 1878 \tTraining Loss: 0.395812\n",
            "Epoch: 1879 \tTraining Loss: 0.395802\n",
            "Epoch: 1880 \tTraining Loss: 0.395730\n",
            "Epoch: 1881 \tTraining Loss: 0.395580\n",
            "Epoch: 1882 \tTraining Loss: 0.395537\n",
            "Epoch: 1883 \tTraining Loss: 0.395527\n",
            "Epoch: 1884 \tTraining Loss: 0.395457\n",
            "Epoch: 1885 \tTraining Loss: 0.395492\n",
            "Epoch: 1886 \tTraining Loss: 0.395321\n",
            "Epoch: 1887 \tTraining Loss: 0.395406\n",
            "Epoch: 1888 \tTraining Loss: 0.395282\n",
            "Epoch: 1889 \tTraining Loss: 0.395156\n",
            "Epoch: 1890 \tTraining Loss: 0.395102\n",
            "Epoch: 1891 \tTraining Loss: 0.395135\n",
            "Epoch: 1892 \tTraining Loss: 0.395054\n",
            "Epoch: 1893 \tTraining Loss: 0.395094\n",
            "Epoch: 1894 \tTraining Loss: 0.394651\n",
            "Epoch: 1895 \tTraining Loss: 0.394695\n",
            "Epoch: 1896 \tTraining Loss: 0.394586\n",
            "Epoch: 1897 \tTraining Loss: 0.394281\n",
            "Epoch: 1898 \tTraining Loss: 0.394345\n",
            "Epoch: 1899 \tTraining Loss: 0.394292\n",
            "Epoch: 1900 \tTraining Loss: 0.394158\n",
            "Epoch: 1901 \tTraining Loss: 0.394128\n",
            "Epoch: 1902 \tTraining Loss: 0.393941\n",
            "Epoch: 1903 \tTraining Loss: 0.394164\n",
            "Epoch: 1904 \tTraining Loss: 0.394195\n",
            "Epoch: 1905 \tTraining Loss: 0.393972\n",
            "Epoch: 1906 \tTraining Loss: 0.393919\n",
            "Epoch: 1907 \tTraining Loss: 0.393737\n",
            "Epoch: 1908 \tTraining Loss: 0.393845\n",
            "Epoch: 1909 \tTraining Loss: 0.393682\n",
            "Epoch: 1910 \tTraining Loss: 0.393468\n",
            "Epoch: 1911 \tTraining Loss: 0.393661\n",
            "Epoch: 1912 \tTraining Loss: 0.393662\n",
            "Epoch: 1913 \tTraining Loss: 0.393722\n",
            "Epoch: 1914 \tTraining Loss: 0.394166\n",
            "Epoch: 1915 \tTraining Loss: 0.394322\n",
            "Epoch: 1916 \tTraining Loss: 0.395627\n",
            "Epoch: 1917 \tTraining Loss: 0.395728\n",
            "Epoch: 1918 \tTraining Loss: 0.398273\n",
            "Epoch: 1919 \tTraining Loss: 0.399776\n",
            "Epoch: 1920 \tTraining Loss: 0.406747\n",
            "Epoch: 1921 \tTraining Loss: 0.411454\n",
            "Epoch: 1922 \tTraining Loss: 0.418397\n",
            "Epoch: 1923 \tTraining Loss: 0.404435\n",
            "Epoch: 1924 \tTraining Loss: 0.395038\n",
            "Epoch: 1925 \tTraining Loss: 0.398489\n",
            "Epoch: 1926 \tTraining Loss: 0.398750\n",
            "Epoch: 1927 \tTraining Loss: 0.396079\n",
            "Epoch: 1928 \tTraining Loss: 0.396209\n",
            "Epoch: 1929 \tTraining Loss: 0.396676\n",
            "Epoch: 1930 \tTraining Loss: 0.396262\n",
            "Epoch: 1931 \tTraining Loss: 0.394747\n",
            "Epoch: 1932 \tTraining Loss: 0.394688\n",
            "Epoch: 1933 \tTraining Loss: 0.394959\n",
            "Epoch: 1934 \tTraining Loss: 0.394985\n",
            "Epoch: 1935 \tTraining Loss: 0.393849\n",
            "Epoch: 1936 \tTraining Loss: 0.393748\n",
            "Epoch: 1937 \tTraining Loss: 0.394797\n",
            "Epoch: 1938 \tTraining Loss: 0.393305\n",
            "Epoch: 1939 \tTraining Loss: 0.392841\n",
            "Epoch: 1940 \tTraining Loss: 0.393261\n",
            "Epoch: 1941 \tTraining Loss: 0.392830\n",
            "Epoch: 1942 \tTraining Loss: 0.392373\n",
            "Epoch: 1943 \tTraining Loss: 0.392924\n",
            "Epoch: 1944 \tTraining Loss: 0.392133\n",
            "Epoch: 1945 \tTraining Loss: 0.392352\n",
            "Epoch: 1946 \tTraining Loss: 0.391863\n",
            "Epoch: 1947 \tTraining Loss: 0.391996\n",
            "Epoch: 1948 \tTraining Loss: 0.391765\n",
            "Epoch: 1949 \tTraining Loss: 0.391676\n",
            "Epoch: 1950 \tTraining Loss: 0.391852\n",
            "Epoch: 1951 \tTraining Loss: 0.391462\n",
            "Epoch: 1952 \tTraining Loss: 0.391378\n",
            "Epoch: 1953 \tTraining Loss: 0.391345\n",
            "Epoch: 1954 \tTraining Loss: 0.391267\n",
            "Epoch: 1955 \tTraining Loss: 0.390980\n",
            "Epoch: 1956 \tTraining Loss: 0.391195\n",
            "Epoch: 1957 \tTraining Loss: 0.390851\n",
            "Epoch: 1958 \tTraining Loss: 0.390875\n",
            "Epoch: 1959 \tTraining Loss: 0.390792\n",
            "Epoch: 1960 \tTraining Loss: 0.390710\n",
            "Epoch: 1961 \tTraining Loss: 0.390624\n",
            "Epoch: 1962 \tTraining Loss: 0.390561\n",
            "Epoch: 1963 \tTraining Loss: 0.390525\n",
            "Epoch: 1964 \tTraining Loss: 0.390382\n",
            "Epoch: 1965 \tTraining Loss: 0.390356\n",
            "Epoch: 1966 \tTraining Loss: 0.390212\n",
            "Epoch: 1967 \tTraining Loss: 0.390197\n",
            "Epoch: 1968 \tTraining Loss: 0.390112\n",
            "Epoch: 1969 \tTraining Loss: 0.390018\n",
            "Epoch: 1970 \tTraining Loss: 0.390000\n",
            "Epoch: 1971 \tTraining Loss: 0.389900\n",
            "Epoch: 1972 \tTraining Loss: 0.389839\n",
            "Epoch: 1973 \tTraining Loss: 0.389777\n",
            "Epoch: 1974 \tTraining Loss: 0.389706\n",
            "Epoch: 1975 \tTraining Loss: 0.389661\n",
            "Epoch: 1976 \tTraining Loss: 0.389590\n",
            "Epoch: 1977 \tTraining Loss: 0.389531\n",
            "Epoch: 1978 \tTraining Loss: 0.389454\n",
            "Epoch: 1979 \tTraining Loss: 0.389405\n",
            "Epoch: 1980 \tTraining Loss: 0.389360\n",
            "Epoch: 1981 \tTraining Loss: 0.389269\n",
            "Epoch: 1982 \tTraining Loss: 0.389225\n",
            "Epoch: 1983 \tTraining Loss: 0.389163\n",
            "Epoch: 1984 \tTraining Loss: 0.389103\n",
            "Epoch: 1985 \tTraining Loss: 0.389048\n",
            "Epoch: 1986 \tTraining Loss: 0.388984\n",
            "Epoch: 1987 \tTraining Loss: 0.388924\n",
            "Epoch: 1988 \tTraining Loss: 0.388862\n",
            "Epoch: 1989 \tTraining Loss: 0.388811\n",
            "Epoch: 1990 \tTraining Loss: 0.388755\n",
            "Epoch: 1991 \tTraining Loss: 0.388683\n",
            "Epoch: 1992 \tTraining Loss: 0.388632\n",
            "Epoch: 1993 \tTraining Loss: 0.388572\n",
            "Epoch: 1994 \tTraining Loss: 0.388512\n",
            "Epoch: 1995 \tTraining Loss: 0.388458\n",
            "Epoch: 1996 \tTraining Loss: 0.388395\n",
            "Epoch: 1997 \tTraining Loss: 0.388337\n",
            "Epoch: 1998 \tTraining Loss: 0.388279\n",
            "Epoch: 1999 \tTraining Loss: 0.388222\n",
            "Epoch: 2000 \tTraining Loss: 0.388166\n",
            "Epoch: 2001 \tTraining Loss: 0.388105\n",
            "Epoch: 2002 \tTraining Loss: 0.388047\n",
            "Epoch: 2003 \tTraining Loss: 0.387991\n",
            "Epoch: 2004 \tTraining Loss: 0.387931\n",
            "Epoch: 2005 \tTraining Loss: 0.387874\n",
            "Epoch: 2006 \tTraining Loss: 0.387817\n",
            "Epoch: 2007 \tTraining Loss: 0.387759\n",
            "Epoch: 2008 \tTraining Loss: 0.387701\n",
            "Epoch: 2009 \tTraining Loss: 0.387643\n",
            "Epoch: 2010 \tTraining Loss: 0.387585\n",
            "Epoch: 2011 \tTraining Loss: 0.387528\n",
            "Epoch: 2012 \tTraining Loss: 0.387470\n",
            "Epoch: 2013 \tTraining Loss: 0.387413\n",
            "Epoch: 2014 \tTraining Loss: 0.387356\n",
            "Epoch: 2015 \tTraining Loss: 0.387298\n",
            "Epoch: 2016 \tTraining Loss: 0.387240\n",
            "Epoch: 2017 \tTraining Loss: 0.387183\n",
            "Epoch: 2018 \tTraining Loss: 0.387125\n",
            "Epoch: 2019 \tTraining Loss: 0.387068\n",
            "Epoch: 2020 \tTraining Loss: 0.387010\n",
            "Epoch: 2021 \tTraining Loss: 0.386953\n",
            "Epoch: 2022 \tTraining Loss: 0.386896\n",
            "Epoch: 2023 \tTraining Loss: 0.386839\n",
            "Epoch: 2024 \tTraining Loss: 0.386781\n",
            "Epoch: 2025 \tTraining Loss: 0.386724\n",
            "Epoch: 2026 \tTraining Loss: 0.386666\n",
            "Epoch: 2027 \tTraining Loss: 0.386609\n",
            "Epoch: 2028 \tTraining Loss: 0.386551\n",
            "Epoch: 2029 \tTraining Loss: 0.386494\n",
            "Epoch: 2030 \tTraining Loss: 0.386437\n",
            "Epoch: 2031 \tTraining Loss: 0.386380\n",
            "Epoch: 2032 \tTraining Loss: 0.386322\n",
            "Epoch: 2033 \tTraining Loss: 0.386265\n",
            "Epoch: 2034 \tTraining Loss: 0.386208\n",
            "Epoch: 2035 \tTraining Loss: 0.386150\n",
            "Epoch: 2036 \tTraining Loss: 0.386093\n",
            "Epoch: 2037 \tTraining Loss: 0.386036\n",
            "Epoch: 2038 \tTraining Loss: 0.385979\n",
            "Epoch: 2039 \tTraining Loss: 0.385921\n",
            "Epoch: 2040 \tTraining Loss: 0.385864\n",
            "Epoch: 2041 \tTraining Loss: 0.385807\n",
            "Epoch: 2042 \tTraining Loss: 0.385749\n",
            "Epoch: 2043 \tTraining Loss: 0.385692\n",
            "Epoch: 2044 \tTraining Loss: 0.385635\n",
            "Epoch: 2045 \tTraining Loss: 0.385578\n",
            "Epoch: 2046 \tTraining Loss: 0.385521\n",
            "Epoch: 2047 \tTraining Loss: 0.385464\n",
            "Epoch: 2048 \tTraining Loss: 0.385407\n",
            "Epoch: 2049 \tTraining Loss: 0.385350\n",
            "Epoch: 2050 \tTraining Loss: 0.385295\n",
            "Epoch: 2051 \tTraining Loss: 0.385240\n",
            "Epoch: 2052 \tTraining Loss: 0.385187\n",
            "Epoch: 2053 \tTraining Loss: 0.385138\n",
            "Epoch: 2054 \tTraining Loss: 0.385095\n",
            "Epoch: 2055 \tTraining Loss: 0.385056\n",
            "Epoch: 2056 \tTraining Loss: 0.385035\n",
            "Epoch: 2057 \tTraining Loss: 0.385008\n",
            "Epoch: 2058 \tTraining Loss: 0.385000\n",
            "Epoch: 2059 \tTraining Loss: 0.384949\n",
            "Epoch: 2060 \tTraining Loss: 0.384906\n",
            "Epoch: 2061 \tTraining Loss: 0.384812\n",
            "Epoch: 2062 \tTraining Loss: 0.384728\n",
            "Epoch: 2063 \tTraining Loss: 0.384625\n",
            "Epoch: 2064 \tTraining Loss: 0.384540\n",
            "Epoch: 2065 \tTraining Loss: 0.384463\n",
            "Epoch: 2066 \tTraining Loss: 0.384397\n",
            "Epoch: 2067 \tTraining Loss: 0.384332\n",
            "Epoch: 2068 \tTraining Loss: 0.384268\n",
            "Epoch: 2069 \tTraining Loss: 0.384207\n",
            "Epoch: 2070 \tTraining Loss: 0.384149\n",
            "Epoch: 2071 \tTraining Loss: 0.384092\n",
            "Epoch: 2072 \tTraining Loss: 0.384034\n",
            "Epoch: 2073 \tTraining Loss: 0.383975\n",
            "Epoch: 2074 \tTraining Loss: 0.383917\n",
            "Epoch: 2075 \tTraining Loss: 0.383861\n",
            "Epoch: 2076 \tTraining Loss: 0.383806\n",
            "Epoch: 2077 \tTraining Loss: 0.383752\n",
            "Epoch: 2078 \tTraining Loss: 0.383701\n",
            "Epoch: 2079 \tTraining Loss: 0.383657\n",
            "Epoch: 2080 \tTraining Loss: 0.383630\n",
            "Epoch: 2081 \tTraining Loss: 0.383627\n",
            "Epoch: 2082 \tTraining Loss: 0.383696\n",
            "Epoch: 2083 \tTraining Loss: 0.383783\n",
            "Epoch: 2084 \tTraining Loss: 0.384022\n",
            "Epoch: 2085 \tTraining Loss: 0.384074\n",
            "Epoch: 2086 \tTraining Loss: 0.384253\n",
            "Epoch: 2087 \tTraining Loss: 0.384105\n",
            "Epoch: 2088 \tTraining Loss: 0.384107\n",
            "Epoch: 2089 \tTraining Loss: 0.383697\n",
            "Epoch: 2090 \tTraining Loss: 0.383431\n",
            "Epoch: 2091 \tTraining Loss: 0.383197\n",
            "Epoch: 2092 \tTraining Loss: 0.383104\n",
            "Epoch: 2093 \tTraining Loss: 0.382925\n",
            "Epoch: 2094 \tTraining Loss: 0.382795\n",
            "Epoch: 2095 \tTraining Loss: 0.382747\n",
            "Epoch: 2096 \tTraining Loss: 0.382750\n",
            "Epoch: 2097 \tTraining Loss: 0.382792\n",
            "Epoch: 2098 \tTraining Loss: 0.382760\n",
            "Epoch: 2099 \tTraining Loss: 0.382777\n",
            "Epoch: 2100 \tTraining Loss: 0.382794\n",
            "Epoch: 2101 \tTraining Loss: 0.382899\n",
            "Epoch: 2102 \tTraining Loss: 0.382827\n",
            "Epoch: 2103 \tTraining Loss: 0.382832\n",
            "Epoch: 2104 \tTraining Loss: 0.382752\n",
            "Epoch: 2105 \tTraining Loss: 0.382793\n",
            "Epoch: 2106 \tTraining Loss: 0.382534\n",
            "Epoch: 2107 \tTraining Loss: 0.382342\n",
            "Epoch: 2108 \tTraining Loss: 0.382162\n",
            "Epoch: 2109 \tTraining Loss: 0.382046\n",
            "Epoch: 2110 \tTraining Loss: 0.381934\n",
            "Epoch: 2111 \tTraining Loss: 0.381834\n",
            "Epoch: 2112 \tTraining Loss: 0.381786\n",
            "Epoch: 2113 \tTraining Loss: 0.381747\n",
            "Epoch: 2114 \tTraining Loss: 0.381694\n",
            "Epoch: 2115 \tTraining Loss: 0.381623\n",
            "Epoch: 2116 \tTraining Loss: 0.381584\n",
            "Epoch: 2117 \tTraining Loss: 0.381554\n",
            "Epoch: 2118 \tTraining Loss: 0.381547\n",
            "Epoch: 2119 \tTraining Loss: 0.381542\n",
            "Epoch: 2120 \tTraining Loss: 0.381627\n",
            "Epoch: 2121 \tTraining Loss: 0.381633\n",
            "Epoch: 2122 \tTraining Loss: 0.381805\n",
            "Epoch: 2123 \tTraining Loss: 0.381973\n",
            "Epoch: 2124 \tTraining Loss: 0.382474\n",
            "Epoch: 2125 \tTraining Loss: 0.382552\n",
            "Epoch: 2126 \tTraining Loss: 0.382992\n",
            "Epoch: 2127 \tTraining Loss: 0.382748\n",
            "Epoch: 2128 \tTraining Loss: 0.382975\n",
            "Epoch: 2129 \tTraining Loss: 0.382082\n",
            "Epoch: 2130 \tTraining Loss: 0.381456\n",
            "Epoch: 2131 \tTraining Loss: 0.380854\n",
            "Epoch: 2132 \tTraining Loss: 0.380781\n",
            "Epoch: 2133 \tTraining Loss: 0.381184\n",
            "Epoch: 2134 \tTraining Loss: 0.381492\n",
            "Epoch: 2135 \tTraining Loss: 0.382082\n",
            "Epoch: 2136 \tTraining Loss: 0.381829\n",
            "Epoch: 2137 \tTraining Loss: 0.381535\n",
            "Epoch: 2138 \tTraining Loss: 0.380927\n",
            "Epoch: 2139 \tTraining Loss: 0.380494\n",
            "Epoch: 2140 \tTraining Loss: 0.380281\n",
            "Epoch: 2141 \tTraining Loss: 0.380218\n",
            "Epoch: 2142 \tTraining Loss: 0.380381\n",
            "Epoch: 2143 \tTraining Loss: 0.380554\n",
            "Epoch: 2144 \tTraining Loss: 0.380905\n",
            "Epoch: 2145 \tTraining Loss: 0.380678\n",
            "Epoch: 2146 \tTraining Loss: 0.380566\n",
            "Epoch: 2147 \tTraining Loss: 0.380093\n",
            "Epoch: 2148 \tTraining Loss: 0.380049\n",
            "Epoch: 2149 \tTraining Loss: 0.380141\n",
            "Epoch: 2150 \tTraining Loss: 0.380070\n",
            "Epoch: 2151 \tTraining Loss: 0.380315\n",
            "Epoch: 2152 \tTraining Loss: 0.380193\n",
            "Epoch: 2153 \tTraining Loss: 0.379848\n",
            "Epoch: 2154 \tTraining Loss: 0.379827\n",
            "Epoch: 2155 \tTraining Loss: 0.379880\n",
            "Epoch: 2156 \tTraining Loss: 0.379423\n",
            "Epoch: 2157 \tTraining Loss: 0.379456\n",
            "Epoch: 2158 \tTraining Loss: 0.379482\n",
            "Epoch: 2159 \tTraining Loss: 0.379381\n",
            "Epoch: 2160 \tTraining Loss: 0.379574\n",
            "Epoch: 2161 \tTraining Loss: 0.379241\n",
            "Epoch: 2162 \tTraining Loss: 0.379803\n",
            "Epoch: 2163 \tTraining Loss: 0.379676\n",
            "Epoch: 2164 \tTraining Loss: 0.379532\n",
            "Epoch: 2165 \tTraining Loss: 0.379465\n",
            "Epoch: 2166 \tTraining Loss: 0.379230\n",
            "Epoch: 2167 \tTraining Loss: 0.379641\n",
            "Epoch: 2168 \tTraining Loss: 0.379281\n",
            "Epoch: 2169 \tTraining Loss: 0.379263\n",
            "Epoch: 2170 \tTraining Loss: 0.379109\n",
            "Epoch: 2171 \tTraining Loss: 0.378645\n",
            "Epoch: 2172 \tTraining Loss: 0.378887\n",
            "Epoch: 2173 \tTraining Loss: 0.378794\n",
            "Epoch: 2174 \tTraining Loss: 0.378562\n",
            "Epoch: 2175 \tTraining Loss: 0.379060\n",
            "Epoch: 2176 \tTraining Loss: 0.378732\n",
            "Epoch: 2177 \tTraining Loss: 0.378996\n",
            "Epoch: 2178 \tTraining Loss: 0.378937\n",
            "Epoch: 2179 \tTraining Loss: 0.378589\n",
            "Epoch: 2180 \tTraining Loss: 0.379057\n",
            "Epoch: 2181 \tTraining Loss: 0.378567\n",
            "Epoch: 2182 \tTraining Loss: 0.378798\n",
            "Epoch: 2183 \tTraining Loss: 0.378452\n",
            "Epoch: 2184 \tTraining Loss: 0.378043\n",
            "Epoch: 2185 \tTraining Loss: 0.378562\n",
            "Epoch: 2186 \tTraining Loss: 0.378064\n",
            "Epoch: 2187 \tTraining Loss: 0.378190\n",
            "Epoch: 2188 \tTraining Loss: 0.378275\n",
            "Epoch: 2189 \tTraining Loss: 0.378088\n",
            "Epoch: 2190 \tTraining Loss: 0.378341\n",
            "Epoch: 2191 \tTraining Loss: 0.378064\n",
            "Epoch: 2192 \tTraining Loss: 0.377760\n",
            "Epoch: 2193 \tTraining Loss: 0.377985\n",
            "Epoch: 2194 \tTraining Loss: 0.377514\n",
            "Epoch: 2195 \tTraining Loss: 0.377710\n",
            "Epoch: 2196 \tTraining Loss: 0.377495\n",
            "Epoch: 2197 \tTraining Loss: 0.377408\n",
            "Epoch: 2198 \tTraining Loss: 0.377704\n",
            "Epoch: 2199 \tTraining Loss: 0.377446\n",
            "Epoch: 2200 \tTraining Loss: 0.377464\n",
            "Epoch: 2201 \tTraining Loss: 0.377570\n",
            "Epoch: 2202 \tTraining Loss: 0.377603\n",
            "Epoch: 2203 \tTraining Loss: 0.377664\n",
            "Epoch: 2204 \tTraining Loss: 0.377639\n",
            "Epoch: 2205 \tTraining Loss: 0.377327\n",
            "Epoch: 2206 \tTraining Loss: 0.377398\n",
            "Epoch: 2207 \tTraining Loss: 0.376937\n",
            "Epoch: 2208 \tTraining Loss: 0.376843\n",
            "Epoch: 2209 \tTraining Loss: 0.376731\n",
            "Epoch: 2210 \tTraining Loss: 0.376597\n",
            "Epoch: 2211 \tTraining Loss: 0.376519\n",
            "Epoch: 2212 \tTraining Loss: 0.376452\n",
            "Epoch: 2213 \tTraining Loss: 0.376283\n",
            "Epoch: 2214 \tTraining Loss: 0.376400\n",
            "Epoch: 2215 \tTraining Loss: 0.376354\n",
            "Epoch: 2216 \tTraining Loss: 0.376229\n",
            "Epoch: 2217 \tTraining Loss: 0.376302\n",
            "Epoch: 2218 \tTraining Loss: 0.376154\n",
            "Epoch: 2219 \tTraining Loss: 0.376173\n",
            "Epoch: 2220 \tTraining Loss: 0.376174\n",
            "Epoch: 2221 \tTraining Loss: 0.375926\n",
            "Epoch: 2222 \tTraining Loss: 0.376024\n",
            "Epoch: 2223 \tTraining Loss: 0.376080\n",
            "Epoch: 2224 \tTraining Loss: 0.375870\n",
            "Epoch: 2225 \tTraining Loss: 0.376048\n",
            "Epoch: 2226 \tTraining Loss: 0.375983\n",
            "Epoch: 2227 \tTraining Loss: 0.376236\n",
            "Epoch: 2228 \tTraining Loss: 0.376275\n",
            "Epoch: 2229 \tTraining Loss: 0.376425\n",
            "Epoch: 2230 \tTraining Loss: 0.376977\n",
            "Epoch: 2231 \tTraining Loss: 0.378218\n",
            "Epoch: 2232 \tTraining Loss: 0.378957\n",
            "Epoch: 2233 \tTraining Loss: 0.380839\n",
            "Epoch: 2234 \tTraining Loss: 0.381184\n",
            "Epoch: 2235 \tTraining Loss: 0.387101\n",
            "Epoch: 2236 \tTraining Loss: 0.392453\n",
            "Epoch: 2237 \tTraining Loss: 0.431844\n",
            "Epoch: 2238 \tTraining Loss: 0.457462\n",
            "Epoch: 2239 \tTraining Loss: 0.443445\n",
            "Epoch: 2240 \tTraining Loss: 0.400304\n",
            "Epoch: 2241 \tTraining Loss: 0.464063\n",
            "Epoch: 2242 \tTraining Loss: 0.433469\n",
            "Epoch: 2243 \tTraining Loss: 0.412647\n",
            "Epoch: 2244 \tTraining Loss: 0.434961\n",
            "Epoch: 2245 \tTraining Loss: 0.389190\n",
            "Epoch: 2246 \tTraining Loss: 0.414266\n",
            "Epoch: 2247 \tTraining Loss: 0.395948\n",
            "Epoch: 2248 \tTraining Loss: 0.406286\n",
            "Epoch: 2249 \tTraining Loss: 0.393513\n",
            "Epoch: 2250 \tTraining Loss: 0.408731\n",
            "Epoch: 2251 \tTraining Loss: 0.394030\n",
            "Epoch: 2252 \tTraining Loss: 0.392269\n",
            "Epoch: 2253 \tTraining Loss: 0.391373\n",
            "Epoch: 2254 \tTraining Loss: 0.391461\n",
            "Epoch: 2255 \tTraining Loss: 0.391871\n",
            "Epoch: 2256 \tTraining Loss: 0.387804\n",
            "Epoch: 2257 \tTraining Loss: 0.387160\n",
            "Epoch: 2258 \tTraining Loss: 0.385116\n",
            "Epoch: 2259 \tTraining Loss: 0.388298\n",
            "Epoch: 2260 \tTraining Loss: 0.385245\n",
            "Epoch: 2261 \tTraining Loss: 0.383911\n",
            "Epoch: 2262 \tTraining Loss: 0.383951\n",
            "Epoch: 2263 \tTraining Loss: 0.382978\n",
            "Epoch: 2264 \tTraining Loss: 0.382281\n",
            "Epoch: 2265 \tTraining Loss: 0.382777\n",
            "Epoch: 2266 \tTraining Loss: 0.379672\n",
            "Epoch: 2267 \tTraining Loss: 0.379901\n",
            "Epoch: 2268 \tTraining Loss: 0.380072\n",
            "Epoch: 2269 \tTraining Loss: 0.379368\n",
            "Epoch: 2270 \tTraining Loss: 0.378277\n",
            "Epoch: 2271 \tTraining Loss: 0.377214\n",
            "Epoch: 2272 \tTraining Loss: 0.378606\n",
            "Epoch: 2273 \tTraining Loss: 0.377650\n",
            "Epoch: 2274 \tTraining Loss: 0.377171\n",
            "Epoch: 2275 \tTraining Loss: 0.376413\n",
            "Epoch: 2276 \tTraining Loss: 0.376976\n",
            "Epoch: 2277 \tTraining Loss: 0.376577\n",
            "Epoch: 2278 \tTraining Loss: 0.376129\n",
            "Epoch: 2279 \tTraining Loss: 0.375849\n",
            "Epoch: 2280 \tTraining Loss: 0.375632\n",
            "Epoch: 2281 \tTraining Loss: 0.375627\n",
            "Epoch: 2282 \tTraining Loss: 0.375460\n",
            "Epoch: 2283 \tTraining Loss: 0.375226\n",
            "Epoch: 2284 \tTraining Loss: 0.374927\n",
            "Epoch: 2285 \tTraining Loss: 0.375161\n",
            "Epoch: 2286 \tTraining Loss: 0.374875\n",
            "Epoch: 2287 \tTraining Loss: 0.374771\n",
            "Epoch: 2288 \tTraining Loss: 0.374700\n",
            "Epoch: 2289 \tTraining Loss: 0.374544\n",
            "Epoch: 2290 \tTraining Loss: 0.374363\n",
            "Epoch: 2291 \tTraining Loss: 0.374364\n",
            "Epoch: 2292 \tTraining Loss: 0.374298\n",
            "Epoch: 2293 \tTraining Loss: 0.374159\n",
            "Epoch: 2294 \tTraining Loss: 0.374105\n",
            "Epoch: 2295 \tTraining Loss: 0.374038\n",
            "Epoch: 2296 \tTraining Loss: 0.373852\n",
            "Epoch: 2297 \tTraining Loss: 0.373878\n",
            "Epoch: 2298 \tTraining Loss: 0.373808\n",
            "Epoch: 2299 \tTraining Loss: 0.373743\n",
            "Epoch: 2300 \tTraining Loss: 0.373660\n",
            "Epoch: 2301 \tTraining Loss: 0.373604\n",
            "Epoch: 2302 \tTraining Loss: 0.373528\n",
            "Epoch: 2303 \tTraining Loss: 0.373515\n",
            "Epoch: 2304 \tTraining Loss: 0.373435\n",
            "Epoch: 2305 \tTraining Loss: 0.373393\n",
            "Epoch: 2306 \tTraining Loss: 0.373331\n",
            "Epoch: 2307 \tTraining Loss: 0.373280\n",
            "Epoch: 2308 \tTraining Loss: 0.373235\n",
            "Epoch: 2309 \tTraining Loss: 0.373190\n",
            "Epoch: 2310 \tTraining Loss: 0.373128\n",
            "Epoch: 2311 \tTraining Loss: 0.373100\n",
            "Epoch: 2312 \tTraining Loss: 0.373046\n",
            "Epoch: 2313 \tTraining Loss: 0.373000\n",
            "Epoch: 2314 \tTraining Loss: 0.372956\n",
            "Epoch: 2315 \tTraining Loss: 0.372925\n",
            "Epoch: 2316 \tTraining Loss: 0.372870\n",
            "Epoch: 2317 \tTraining Loss: 0.372829\n",
            "Epoch: 2318 \tTraining Loss: 0.372786\n",
            "Epoch: 2319 \tTraining Loss: 0.372752\n",
            "Epoch: 2320 \tTraining Loss: 0.372712\n",
            "Epoch: 2321 \tTraining Loss: 0.372671\n",
            "Epoch: 2322 \tTraining Loss: 0.372632\n",
            "Epoch: 2323 \tTraining Loss: 0.372593\n",
            "Epoch: 2324 \tTraining Loss: 0.372550\n",
            "Epoch: 2325 \tTraining Loss: 0.372514\n",
            "Epoch: 2326 \tTraining Loss: 0.372478\n",
            "Epoch: 2327 \tTraining Loss: 0.372438\n",
            "Epoch: 2328 \tTraining Loss: 0.372400\n",
            "Epoch: 2329 \tTraining Loss: 0.372364\n",
            "Epoch: 2330 \tTraining Loss: 0.372326\n",
            "Epoch: 2331 \tTraining Loss: 0.372290\n",
            "Epoch: 2332 \tTraining Loss: 0.372253\n",
            "Epoch: 2333 \tTraining Loss: 0.372214\n",
            "Epoch: 2334 \tTraining Loss: 0.372178\n",
            "Epoch: 2335 \tTraining Loss: 0.372141\n",
            "Epoch: 2336 \tTraining Loss: 0.372106\n",
            "Epoch: 2337 \tTraining Loss: 0.372069\n",
            "Epoch: 2338 \tTraining Loss: 0.372032\n",
            "Epoch: 2339 \tTraining Loss: 0.371997\n",
            "Epoch: 2340 \tTraining Loss: 0.371961\n",
            "Epoch: 2341 \tTraining Loss: 0.371924\n",
            "Epoch: 2342 \tTraining Loss: 0.371889\n",
            "Epoch: 2343 \tTraining Loss: 0.371853\n",
            "Epoch: 2344 \tTraining Loss: 0.371818\n",
            "Epoch: 2345 \tTraining Loss: 0.371782\n",
            "Epoch: 2346 \tTraining Loss: 0.371746\n",
            "Epoch: 2347 \tTraining Loss: 0.371711\n",
            "Epoch: 2348 \tTraining Loss: 0.371676\n",
            "Epoch: 2349 \tTraining Loss: 0.371640\n",
            "Epoch: 2350 \tTraining Loss: 0.371605\n",
            "Epoch: 2351 \tTraining Loss: 0.371569\n",
            "Epoch: 2352 \tTraining Loss: 0.371534\n",
            "Epoch: 2353 \tTraining Loss: 0.371499\n",
            "Epoch: 2354 \tTraining Loss: 0.371464\n",
            "Epoch: 2355 \tTraining Loss: 0.371428\n",
            "Epoch: 2356 \tTraining Loss: 0.371394\n",
            "Epoch: 2357 \tTraining Loss: 0.371358\n",
            "Epoch: 2358 \tTraining Loss: 0.371323\n",
            "Epoch: 2359 \tTraining Loss: 0.371288\n",
            "Epoch: 2360 \tTraining Loss: 0.371253\n",
            "Epoch: 2361 \tTraining Loss: 0.371219\n",
            "Epoch: 2362 \tTraining Loss: 0.371184\n",
            "Epoch: 2363 \tTraining Loss: 0.371149\n",
            "Epoch: 2364 \tTraining Loss: 0.371114\n",
            "Epoch: 2365 \tTraining Loss: 0.371079\n",
            "Epoch: 2366 \tTraining Loss: 0.371044\n",
            "Epoch: 2367 \tTraining Loss: 0.371009\n",
            "Epoch: 2368 \tTraining Loss: 0.370975\n",
            "Epoch: 2369 \tTraining Loss: 0.370940\n",
            "Epoch: 2370 \tTraining Loss: 0.370905\n",
            "Epoch: 2371 \tTraining Loss: 0.370870\n",
            "Epoch: 2372 \tTraining Loss: 0.370836\n",
            "Epoch: 2373 \tTraining Loss: 0.370801\n",
            "Epoch: 2374 \tTraining Loss: 0.370766\n",
            "Epoch: 2375 \tTraining Loss: 0.370732\n",
            "Epoch: 2376 \tTraining Loss: 0.370697\n",
            "Epoch: 2377 \tTraining Loss: 0.370663\n",
            "Epoch: 2378 \tTraining Loss: 0.370628\n",
            "Epoch: 2379 \tTraining Loss: 0.370593\n",
            "Epoch: 2380 \tTraining Loss: 0.370559\n",
            "Epoch: 2381 \tTraining Loss: 0.370524\n",
            "Epoch: 2382 \tTraining Loss: 0.370490\n",
            "Epoch: 2383 \tTraining Loss: 0.370455\n",
            "Epoch: 2384 \tTraining Loss: 0.370421\n",
            "Epoch: 2385 \tTraining Loss: 0.370386\n",
            "Epoch: 2386 \tTraining Loss: 0.370352\n",
            "Epoch: 2387 \tTraining Loss: 0.370317\n",
            "Epoch: 2388 \tTraining Loss: 0.370283\n",
            "Epoch: 2389 \tTraining Loss: 0.370249\n",
            "Epoch: 2390 \tTraining Loss: 0.370214\n",
            "Epoch: 2391 \tTraining Loss: 0.370180\n",
            "Epoch: 2392 \tTraining Loss: 0.370145\n",
            "Epoch: 2393 \tTraining Loss: 0.370111\n",
            "Epoch: 2394 \tTraining Loss: 0.370077\n",
            "Epoch: 2395 \tTraining Loss: 0.370042\n",
            "Epoch: 2396 \tTraining Loss: 0.370008\n",
            "Epoch: 2397 \tTraining Loss: 0.369974\n",
            "Epoch: 2398 \tTraining Loss: 0.369939\n",
            "Epoch: 2399 \tTraining Loss: 0.369905\n",
            "Epoch: 2400 \tTraining Loss: 0.369871\n",
            "Epoch: 2401 \tTraining Loss: 0.369837\n",
            "Epoch: 2402 \tTraining Loss: 0.369802\n",
            "Epoch: 2403 \tTraining Loss: 0.369768\n",
            "Epoch: 2404 \tTraining Loss: 0.369734\n",
            "Epoch: 2405 \tTraining Loss: 0.369700\n",
            "Epoch: 2406 \tTraining Loss: 0.369665\n",
            "Epoch: 2407 \tTraining Loss: 0.369631\n",
            "Epoch: 2408 \tTraining Loss: 0.369597\n",
            "Epoch: 2409 \tTraining Loss: 0.369563\n",
            "Epoch: 2410 \tTraining Loss: 0.369529\n",
            "Epoch: 2411 \tTraining Loss: 0.369494\n",
            "Epoch: 2412 \tTraining Loss: 0.369460\n",
            "Epoch: 2413 \tTraining Loss: 0.369426\n",
            "Epoch: 2414 \tTraining Loss: 0.369392\n",
            "Epoch: 2415 \tTraining Loss: 0.369358\n",
            "Epoch: 2416 \tTraining Loss: 0.369324\n",
            "Epoch: 2417 \tTraining Loss: 0.369289\n",
            "Epoch: 2418 \tTraining Loss: 0.369255\n",
            "Epoch: 2419 \tTraining Loss: 0.369221\n",
            "Epoch: 2420 \tTraining Loss: 0.369187\n",
            "Epoch: 2421 \tTraining Loss: 0.369153\n",
            "Epoch: 2422 \tTraining Loss: 0.369119\n",
            "Epoch: 2423 \tTraining Loss: 0.369085\n",
            "Epoch: 2424 \tTraining Loss: 0.369051\n",
            "Epoch: 2425 \tTraining Loss: 0.369016\n",
            "Epoch: 2426 \tTraining Loss: 0.368982\n",
            "Epoch: 2427 \tTraining Loss: 0.368948\n",
            "Epoch: 2428 \tTraining Loss: 0.368914\n",
            "Epoch: 2429 \tTraining Loss: 0.368880\n",
            "Epoch: 2430 \tTraining Loss: 0.368846\n",
            "Epoch: 2431 \tTraining Loss: 0.368812\n",
            "Epoch: 2432 \tTraining Loss: 0.368778\n",
            "Epoch: 2433 \tTraining Loss: 0.368744\n",
            "Epoch: 2434 \tTraining Loss: 0.368710\n",
            "Epoch: 2435 \tTraining Loss: 0.368676\n",
            "Epoch: 2436 \tTraining Loss: 0.368642\n",
            "Epoch: 2437 \tTraining Loss: 0.368608\n",
            "Epoch: 2438 \tTraining Loss: 0.368573\n",
            "Epoch: 2439 \tTraining Loss: 0.368540\n",
            "Epoch: 2440 \tTraining Loss: 0.368505\n",
            "Epoch: 2441 \tTraining Loss: 0.368471\n",
            "Epoch: 2442 \tTraining Loss: 0.368437\n",
            "Epoch: 2443 \tTraining Loss: 0.368403\n",
            "Epoch: 2444 \tTraining Loss: 0.368369\n",
            "Epoch: 2445 \tTraining Loss: 0.368335\n",
            "Epoch: 2446 \tTraining Loss: 0.368301\n",
            "Epoch: 2447 \tTraining Loss: 0.368267\n",
            "Epoch: 2448 \tTraining Loss: 0.368233\n",
            "Epoch: 2449 \tTraining Loss: 0.368199\n",
            "Epoch: 2450 \tTraining Loss: 0.368165\n",
            "Epoch: 2451 \tTraining Loss: 0.368131\n",
            "Epoch: 2452 \tTraining Loss: 0.368097\n",
            "Epoch: 2453 \tTraining Loss: 0.368063\n",
            "Epoch: 2454 \tTraining Loss: 0.368029\n",
            "Epoch: 2455 \tTraining Loss: 0.367995\n",
            "Epoch: 2456 \tTraining Loss: 0.367961\n",
            "Epoch: 2457 \tTraining Loss: 0.367927\n",
            "Epoch: 2458 \tTraining Loss: 0.367893\n",
            "Epoch: 2459 \tTraining Loss: 0.367859\n",
            "Epoch: 2460 \tTraining Loss: 0.367825\n",
            "Epoch: 2461 \tTraining Loss: 0.367791\n",
            "Epoch: 2462 \tTraining Loss: 0.367757\n",
            "Epoch: 2463 \tTraining Loss: 0.367723\n",
            "Epoch: 2464 \tTraining Loss: 0.367689\n",
            "Epoch: 2465 \tTraining Loss: 0.367655\n",
            "Epoch: 2466 \tTraining Loss: 0.367621\n",
            "Epoch: 2467 \tTraining Loss: 0.367587\n",
            "Epoch: 2468 \tTraining Loss: 0.367553\n",
            "Epoch: 2469 \tTraining Loss: 0.367519\n",
            "Epoch: 2470 \tTraining Loss: 0.367485\n",
            "Epoch: 2471 \tTraining Loss: 0.367451\n",
            "Epoch: 2472 \tTraining Loss: 0.367417\n",
            "Epoch: 2473 \tTraining Loss: 0.367383\n",
            "Epoch: 2474 \tTraining Loss: 0.367349\n",
            "Epoch: 2475 \tTraining Loss: 0.367315\n",
            "Epoch: 2476 \tTraining Loss: 0.367281\n",
            "Epoch: 2477 \tTraining Loss: 0.367247\n",
            "Epoch: 2478 \tTraining Loss: 0.367213\n",
            "Epoch: 2479 \tTraining Loss: 0.367179\n",
            "Epoch: 2480 \tTraining Loss: 0.367145\n",
            "Epoch: 2481 \tTraining Loss: 0.367111\n",
            "Epoch: 2482 \tTraining Loss: 0.367077\n",
            "Epoch: 2483 \tTraining Loss: 0.367043\n",
            "Epoch: 2484 \tTraining Loss: 0.367009\n",
            "Epoch: 2485 \tTraining Loss: 0.366975\n",
            "Epoch: 2486 \tTraining Loss: 0.366941\n",
            "Epoch: 2487 \tTraining Loss: 0.366907\n",
            "Epoch: 2488 \tTraining Loss: 0.366873\n",
            "Epoch: 2489 \tTraining Loss: 0.366839\n",
            "Epoch: 2490 \tTraining Loss: 0.366805\n",
            "Epoch: 2491 \tTraining Loss: 0.366771\n",
            "Epoch: 2492 \tTraining Loss: 0.366737\n",
            "Epoch: 2493 \tTraining Loss: 0.366704\n",
            "Epoch: 2494 \tTraining Loss: 0.366670\n",
            "Epoch: 2495 \tTraining Loss: 0.366636\n",
            "Epoch: 2496 \tTraining Loss: 0.366602\n",
            "Epoch: 2497 \tTraining Loss: 0.366568\n",
            "Epoch: 2498 \tTraining Loss: 0.366534\n",
            "Epoch: 2499 \tTraining Loss: 0.366500\n",
            "Epoch: 2500 \tTraining Loss: 0.366466\n",
            "Epoch: 2501 \tTraining Loss: 0.366432\n",
            "Epoch: 2502 \tTraining Loss: 0.366398\n",
            "Epoch: 2503 \tTraining Loss: 0.366364\n",
            "Epoch: 2504 \tTraining Loss: 0.366330\n",
            "Epoch: 2505 \tTraining Loss: 0.366296\n",
            "Epoch: 2506 \tTraining Loss: 0.366262\n",
            "Epoch: 2507 \tTraining Loss: 0.366228\n",
            "Epoch: 2508 \tTraining Loss: 0.366194\n",
            "Epoch: 2509 \tTraining Loss: 0.366160\n",
            "Epoch: 2510 \tTraining Loss: 0.366126\n",
            "Epoch: 2511 \tTraining Loss: 0.366092\n",
            "Epoch: 2512 \tTraining Loss: 0.366058\n",
            "Epoch: 2513 \tTraining Loss: 0.366024\n",
            "Epoch: 2514 \tTraining Loss: 0.365990\n",
            "Epoch: 2515 \tTraining Loss: 0.365956\n",
            "Epoch: 2516 \tTraining Loss: 0.365922\n",
            "Epoch: 2517 \tTraining Loss: 0.365888\n",
            "Epoch: 2518 \tTraining Loss: 0.365854\n",
            "Epoch: 2519 \tTraining Loss: 0.365820\n",
            "Epoch: 2520 \tTraining Loss: 0.365786\n",
            "Epoch: 2521 \tTraining Loss: 0.365752\n",
            "Epoch: 2522 \tTraining Loss: 0.365718\n",
            "Epoch: 2523 \tTraining Loss: 0.365684\n",
            "Epoch: 2524 \tTraining Loss: 0.365650\n",
            "Epoch: 2525 \tTraining Loss: 0.365616\n",
            "Epoch: 2526 \tTraining Loss: 0.365582\n",
            "Epoch: 2527 \tTraining Loss: 0.365548\n",
            "Epoch: 2528 \tTraining Loss: 0.365514\n",
            "Epoch: 2529 \tTraining Loss: 0.365481\n",
            "Epoch: 2530 \tTraining Loss: 0.365447\n",
            "Epoch: 2531 \tTraining Loss: 0.365413\n",
            "Epoch: 2532 \tTraining Loss: 0.365379\n",
            "Epoch: 2533 \tTraining Loss: 0.365345\n",
            "Epoch: 2534 \tTraining Loss: 0.365311\n",
            "Epoch: 2535 \tTraining Loss: 0.365277\n",
            "Epoch: 2536 \tTraining Loss: 0.365243\n",
            "Epoch: 2537 \tTraining Loss: 0.365209\n",
            "Epoch: 2538 \tTraining Loss: 0.365175\n",
            "Epoch: 2539 \tTraining Loss: 0.365141\n",
            "Epoch: 2540 \tTraining Loss: 0.365107\n",
            "Epoch: 2541 \tTraining Loss: 0.365073\n",
            "Epoch: 2542 \tTraining Loss: 0.365039\n",
            "Epoch: 2543 \tTraining Loss: 0.365005\n",
            "Epoch: 2544 \tTraining Loss: 0.364971\n",
            "Epoch: 2545 \tTraining Loss: 0.364937\n",
            "Epoch: 2546 \tTraining Loss: 0.364903\n",
            "Epoch: 2547 \tTraining Loss: 0.364869\n",
            "Epoch: 2548 \tTraining Loss: 0.364835\n",
            "Epoch: 2549 \tTraining Loss: 0.364801\n",
            "Epoch: 2550 \tTraining Loss: 0.364767\n",
            "Epoch: 2551 \tTraining Loss: 0.364733\n",
            "Epoch: 2552 \tTraining Loss: 0.364699\n",
            "Epoch: 2553 \tTraining Loss: 0.364665\n",
            "Epoch: 2554 \tTraining Loss: 0.364631\n",
            "Epoch: 2555 \tTraining Loss: 0.364597\n",
            "Epoch: 2556 \tTraining Loss: 0.364563\n",
            "Epoch: 2557 \tTraining Loss: 0.364529\n",
            "Epoch: 2558 \tTraining Loss: 0.364495\n",
            "Epoch: 2559 \tTraining Loss: 0.364461\n",
            "Epoch: 2560 \tTraining Loss: 0.364427\n",
            "Epoch: 2561 \tTraining Loss: 0.364393\n",
            "Epoch: 2562 \tTraining Loss: 0.364359\n",
            "Epoch: 2563 \tTraining Loss: 0.364325\n",
            "Epoch: 2564 \tTraining Loss: 0.364291\n",
            "Epoch: 2565 \tTraining Loss: 0.364257\n",
            "Epoch: 2566 \tTraining Loss: 0.364223\n",
            "Epoch: 2567 \tTraining Loss: 0.364189\n",
            "Epoch: 2568 \tTraining Loss: 0.364155\n",
            "Epoch: 2569 \tTraining Loss: 0.364121\n",
            "Epoch: 2570 \tTraining Loss: 0.364087\n",
            "Epoch: 2571 \tTraining Loss: 0.364053\n",
            "Epoch: 2572 \tTraining Loss: 0.364019\n",
            "Epoch: 2573 \tTraining Loss: 0.363985\n",
            "Epoch: 2574 \tTraining Loss: 0.363951\n",
            "Epoch: 2575 \tTraining Loss: 0.363917\n",
            "Epoch: 2576 \tTraining Loss: 0.363883\n",
            "Epoch: 2577 \tTraining Loss: 0.363849\n",
            "Epoch: 2578 \tTraining Loss: 0.363815\n",
            "Epoch: 2579 \tTraining Loss: 0.363781\n",
            "Epoch: 2580 \tTraining Loss: 0.363746\n",
            "Epoch: 2581 \tTraining Loss: 0.363712\n",
            "Epoch: 2582 \tTraining Loss: 0.363678\n",
            "Epoch: 2583 \tTraining Loss: 0.363644\n",
            "Epoch: 2584 \tTraining Loss: 0.363610\n",
            "Epoch: 2585 \tTraining Loss: 0.363576\n",
            "Epoch: 2586 \tTraining Loss: 0.363542\n",
            "Epoch: 2587 \tTraining Loss: 0.363508\n",
            "Epoch: 2588 \tTraining Loss: 0.363474\n",
            "Epoch: 2589 \tTraining Loss: 0.363440\n",
            "Epoch: 2590 \tTraining Loss: 0.363406\n",
            "Epoch: 2591 \tTraining Loss: 0.363372\n",
            "Epoch: 2592 \tTraining Loss: 0.363338\n",
            "Epoch: 2593 \tTraining Loss: 0.363304\n",
            "Epoch: 2594 \tTraining Loss: 0.363270\n",
            "Epoch: 2595 \tTraining Loss: 0.363236\n",
            "Epoch: 2596 \tTraining Loss: 0.363202\n",
            "Epoch: 2597 \tTraining Loss: 0.363169\n",
            "Epoch: 2598 \tTraining Loss: 0.363136\n",
            "Epoch: 2599 \tTraining Loss: 0.363103\n",
            "Epoch: 2600 \tTraining Loss: 0.363072\n",
            "Epoch: 2601 \tTraining Loss: 0.363044\n",
            "Epoch: 2602 \tTraining Loss: 0.363020\n",
            "Epoch: 2603 \tTraining Loss: 0.363002\n",
            "Epoch: 2604 \tTraining Loss: 0.362988\n",
            "Epoch: 2605 \tTraining Loss: 0.362970\n",
            "Epoch: 2606 \tTraining Loss: 0.362933\n",
            "Epoch: 2607 \tTraining Loss: 0.362873\n",
            "Epoch: 2608 \tTraining Loss: 0.362808\n",
            "Epoch: 2609 \tTraining Loss: 0.362762\n",
            "Epoch: 2610 \tTraining Loss: 0.362738\n",
            "Epoch: 2611 \tTraining Loss: 0.362728\n",
            "Epoch: 2612 \tTraining Loss: 0.362722\n",
            "Epoch: 2613 \tTraining Loss: 0.362713\n",
            "Epoch: 2614 \tTraining Loss: 0.362685\n",
            "Epoch: 2615 \tTraining Loss: 0.362625\n",
            "Epoch: 2616 \tTraining Loss: 0.362549\n",
            "Epoch: 2617 \tTraining Loss: 0.362497\n",
            "Epoch: 2618 \tTraining Loss: 0.362473\n",
            "Epoch: 2619 \tTraining Loss: 0.362461\n",
            "Epoch: 2620 \tTraining Loss: 0.362435\n",
            "Epoch: 2621 \tTraining Loss: 0.362392\n",
            "Epoch: 2622 \tTraining Loss: 0.362342\n",
            "Epoch: 2623 \tTraining Loss: 0.362299\n",
            "Epoch: 2624 \tTraining Loss: 0.362256\n",
            "Epoch: 2625 \tTraining Loss: 0.362216\n",
            "Epoch: 2626 \tTraining Loss: 0.362184\n",
            "Epoch: 2627 \tTraining Loss: 0.362153\n",
            "Epoch: 2628 \tTraining Loss: 0.362119\n",
            "Epoch: 2629 \tTraining Loss: 0.362088\n",
            "Epoch: 2630 \tTraining Loss: 0.362057\n",
            "Epoch: 2631 \tTraining Loss: 0.362021\n",
            "Epoch: 2632 \tTraining Loss: 0.361989\n",
            "Epoch: 2633 \tTraining Loss: 0.361960\n",
            "Epoch: 2634 \tTraining Loss: 0.361928\n",
            "Epoch: 2635 \tTraining Loss: 0.361899\n",
            "Epoch: 2636 \tTraining Loss: 0.361872\n",
            "Epoch: 2637 \tTraining Loss: 0.361843\n",
            "Epoch: 2638 \tTraining Loss: 0.361809\n",
            "Epoch: 2639 \tTraining Loss: 0.361772\n",
            "Epoch: 2640 \tTraining Loss: 0.361729\n",
            "Epoch: 2641 \tTraining Loss: 0.361685\n",
            "Epoch: 2642 \tTraining Loss: 0.361642\n",
            "Epoch: 2643 \tTraining Loss: 0.361603\n",
            "Epoch: 2644 \tTraining Loss: 0.361566\n",
            "Epoch: 2645 \tTraining Loss: 0.361532\n",
            "Epoch: 2646 \tTraining Loss: 0.361501\n",
            "Epoch: 2647 \tTraining Loss: 0.361471\n",
            "Epoch: 2648 \tTraining Loss: 0.361444\n",
            "Epoch: 2649 \tTraining Loss: 0.361423\n",
            "Epoch: 2650 \tTraining Loss: 0.361412\n",
            "Epoch: 2651 \tTraining Loss: 0.361401\n",
            "Epoch: 2652 \tTraining Loss: 0.361385\n",
            "Epoch: 2653 \tTraining Loss: 0.361347\n",
            "Epoch: 2654 \tTraining Loss: 0.361280\n",
            "Epoch: 2655 \tTraining Loss: 0.361212\n",
            "Epoch: 2656 \tTraining Loss: 0.361159\n",
            "Epoch: 2657 \tTraining Loss: 0.361130\n",
            "Epoch: 2658 \tTraining Loss: 0.361118\n",
            "Epoch: 2659 \tTraining Loss: 0.361118\n",
            "Epoch: 2660 \tTraining Loss: 0.361139\n",
            "Epoch: 2661 \tTraining Loss: 0.361174\n",
            "Epoch: 2662 \tTraining Loss: 0.361138\n",
            "Epoch: 2663 \tTraining Loss: 0.361035\n",
            "Epoch: 2664 \tTraining Loss: 0.360948\n",
            "Epoch: 2665 \tTraining Loss: 0.360879\n",
            "Epoch: 2666 \tTraining Loss: 0.360866\n",
            "Epoch: 2667 \tTraining Loss: 0.360898\n",
            "Epoch: 2668 \tTraining Loss: 0.360845\n",
            "Epoch: 2669 \tTraining Loss: 0.360792\n",
            "Epoch: 2670 \tTraining Loss: 0.360772\n",
            "Epoch: 2671 \tTraining Loss: 0.360732\n",
            "Epoch: 2672 \tTraining Loss: 0.360667\n",
            "Epoch: 2673 \tTraining Loss: 0.360611\n",
            "Epoch: 2674 \tTraining Loss: 0.360580\n",
            "Epoch: 2675 \tTraining Loss: 0.360533\n",
            "Epoch: 2676 \tTraining Loss: 0.360485\n",
            "Epoch: 2677 \tTraining Loss: 0.360467\n",
            "Epoch: 2678 \tTraining Loss: 0.360443\n",
            "Epoch: 2679 \tTraining Loss: 0.360395\n",
            "Epoch: 2680 \tTraining Loss: 0.360368\n",
            "Epoch: 2681 \tTraining Loss: 0.360365\n",
            "Epoch: 2682 \tTraining Loss: 0.360342\n",
            "Epoch: 2683 \tTraining Loss: 0.360319\n",
            "Epoch: 2684 \tTraining Loss: 0.360307\n",
            "Epoch: 2685 \tTraining Loss: 0.360265\n",
            "Epoch: 2686 \tTraining Loss: 0.360200\n",
            "Epoch: 2687 \tTraining Loss: 0.360120\n",
            "Epoch: 2688 \tTraining Loss: 0.360076\n",
            "Epoch: 2689 \tTraining Loss: 0.360052\n",
            "Epoch: 2690 \tTraining Loss: 0.360023\n",
            "Epoch: 2691 \tTraining Loss: 0.360013\n",
            "Epoch: 2692 \tTraining Loss: 0.360049\n",
            "Epoch: 2693 \tTraining Loss: 0.360082\n",
            "Epoch: 2694 \tTraining Loss: 0.360076\n",
            "Epoch: 2695 \tTraining Loss: 0.360026\n",
            "Epoch: 2696 \tTraining Loss: 0.359917\n",
            "Epoch: 2697 \tTraining Loss: 0.359814\n",
            "Epoch: 2698 \tTraining Loss: 0.359742\n",
            "Epoch: 2699 \tTraining Loss: 0.359764\n",
            "Epoch: 2700 \tTraining Loss: 0.359803\n",
            "Epoch: 2701 \tTraining Loss: 0.359758\n",
            "Epoch: 2702 \tTraining Loss: 0.359785\n",
            "Epoch: 2703 \tTraining Loss: 0.359859\n",
            "Epoch: 2704 \tTraining Loss: 0.359724\n",
            "Epoch: 2705 \tTraining Loss: 0.359609\n",
            "Epoch: 2706 \tTraining Loss: 0.359594\n",
            "Epoch: 2707 \tTraining Loss: 0.359480\n",
            "Epoch: 2708 \tTraining Loss: 0.359445\n",
            "Epoch: 2709 \tTraining Loss: 0.359477\n",
            "Epoch: 2710 \tTraining Loss: 0.359389\n",
            "Epoch: 2711 \tTraining Loss: 0.359361\n",
            "Epoch: 2712 \tTraining Loss: 0.359380\n",
            "Epoch: 2713 \tTraining Loss: 0.359335\n",
            "Epoch: 2714 \tTraining Loss: 0.359292\n",
            "Epoch: 2715 \tTraining Loss: 0.359263\n",
            "Epoch: 2716 \tTraining Loss: 0.359208\n",
            "Epoch: 2717 \tTraining Loss: 0.359149\n",
            "Epoch: 2718 \tTraining Loss: 0.359081\n",
            "Epoch: 2719 \tTraining Loss: 0.359035\n",
            "Epoch: 2720 \tTraining Loss: 0.359008\n",
            "Epoch: 2721 \tTraining Loss: 0.358988\n",
            "Epoch: 2722 \tTraining Loss: 0.358982\n",
            "Epoch: 2723 \tTraining Loss: 0.358995\n",
            "Epoch: 2724 \tTraining Loss: 0.359005\n",
            "Epoch: 2725 \tTraining Loss: 0.359009\n",
            "Epoch: 2726 \tTraining Loss: 0.358979\n",
            "Epoch: 2727 \tTraining Loss: 0.358873\n",
            "Epoch: 2728 \tTraining Loss: 0.358787\n",
            "Epoch: 2729 \tTraining Loss: 0.358697\n",
            "Epoch: 2730 \tTraining Loss: 0.358653\n",
            "Epoch: 2731 \tTraining Loss: 0.358664\n",
            "Epoch: 2732 \tTraining Loss: 0.358667\n",
            "Epoch: 2733 \tTraining Loss: 0.358687\n",
            "Epoch: 2734 \tTraining Loss: 0.358751\n",
            "Epoch: 2735 \tTraining Loss: 0.358776\n",
            "Epoch: 2736 \tTraining Loss: 0.358696\n",
            "Epoch: 2737 \tTraining Loss: 0.358625\n",
            "Epoch: 2738 \tTraining Loss: 0.358470\n",
            "Epoch: 2739 \tTraining Loss: 0.358377\n",
            "Epoch: 2740 \tTraining Loss: 0.358360\n",
            "Epoch: 2741 \tTraining Loss: 0.358361\n",
            "Epoch: 2742 \tTraining Loss: 0.358391\n",
            "Epoch: 2743 \tTraining Loss: 0.358388\n",
            "Epoch: 2744 \tTraining Loss: 0.358414\n",
            "Epoch: 2745 \tTraining Loss: 0.358437\n",
            "Epoch: 2746 \tTraining Loss: 0.358330\n",
            "Epoch: 2747 \tTraining Loss: 0.358215\n",
            "Epoch: 2748 \tTraining Loss: 0.358157\n",
            "Epoch: 2749 \tTraining Loss: 0.358026\n",
            "Epoch: 2750 \tTraining Loss: 0.358016\n",
            "Epoch: 2751 \tTraining Loss: 0.358038\n",
            "Epoch: 2752 \tTraining Loss: 0.357963\n",
            "Epoch: 2753 \tTraining Loss: 0.357977\n",
            "Epoch: 2754 \tTraining Loss: 0.358027\n",
            "Epoch: 2755 \tTraining Loss: 0.357962\n",
            "Epoch: 2756 \tTraining Loss: 0.357912\n",
            "Epoch: 2757 \tTraining Loss: 0.357914\n",
            "Epoch: 2758 \tTraining Loss: 0.357789\n",
            "Epoch: 2759 \tTraining Loss: 0.357682\n",
            "Epoch: 2760 \tTraining Loss: 0.357630\n",
            "Epoch: 2761 \tTraining Loss: 0.357611\n",
            "Epoch: 2762 \tTraining Loss: 0.357588\n",
            "Epoch: 2763 \tTraining Loss: 0.357577\n",
            "Epoch: 2764 \tTraining Loss: 0.357607\n",
            "Epoch: 2765 \tTraining Loss: 0.357639\n",
            "Epoch: 2766 \tTraining Loss: 0.357618\n",
            "Epoch: 2767 \tTraining Loss: 0.357593\n",
            "Epoch: 2768 \tTraining Loss: 0.357594\n",
            "Epoch: 2769 \tTraining Loss: 0.357424\n",
            "Epoch: 2770 \tTraining Loss: 0.357305\n",
            "Epoch: 2771 \tTraining Loss: 0.357244\n",
            "Epoch: 2772 \tTraining Loss: 0.357214\n",
            "Epoch: 2773 \tTraining Loss: 0.357210\n",
            "Epoch: 2774 \tTraining Loss: 0.357236\n",
            "Epoch: 2775 \tTraining Loss: 0.357293\n",
            "Epoch: 2776 \tTraining Loss: 0.357358\n",
            "Epoch: 2777 \tTraining Loss: 0.357377\n",
            "Epoch: 2778 \tTraining Loss: 0.357309\n",
            "Epoch: 2779 \tTraining Loss: 0.357265\n",
            "Epoch: 2780 \tTraining Loss: 0.357030\n",
            "Epoch: 2781 \tTraining Loss: 0.356903\n",
            "Epoch: 2782 \tTraining Loss: 0.356873\n",
            "Epoch: 2783 \tTraining Loss: 0.356878\n",
            "Epoch: 2784 \tTraining Loss: 0.356941\n",
            "Epoch: 2785 \tTraining Loss: 0.357032\n",
            "Epoch: 2786 \tTraining Loss: 0.357119\n",
            "Epoch: 2787 \tTraining Loss: 0.357130\n",
            "Epoch: 2788 \tTraining Loss: 0.357091\n",
            "Epoch: 2789 \tTraining Loss: 0.356837\n",
            "Epoch: 2790 \tTraining Loss: 0.356655\n",
            "Epoch: 2791 \tTraining Loss: 0.356527\n",
            "Epoch: 2792 \tTraining Loss: 0.356526\n",
            "Epoch: 2793 \tTraining Loss: 0.356597\n",
            "Epoch: 2794 \tTraining Loss: 0.356617\n",
            "Epoch: 2795 \tTraining Loss: 0.356696\n",
            "Epoch: 2796 \tTraining Loss: 0.356761\n",
            "Epoch: 2797 \tTraining Loss: 0.356683\n",
            "Epoch: 2798 \tTraining Loss: 0.356537\n",
            "Epoch: 2799 \tTraining Loss: 0.356446\n",
            "Epoch: 2800 \tTraining Loss: 0.356243\n",
            "Epoch: 2801 \tTraining Loss: 0.356153\n",
            "Epoch: 2802 \tTraining Loss: 0.356175\n",
            "Epoch: 2803 \tTraining Loss: 0.356184\n",
            "Epoch: 2804 \tTraining Loss: 0.356229\n",
            "Epoch: 2805 \tTraining Loss: 0.356280\n",
            "Epoch: 2806 \tTraining Loss: 0.356280\n",
            "Epoch: 2807 \tTraining Loss: 0.356222\n",
            "Epoch: 2808 \tTraining Loss: 0.356148\n",
            "Epoch: 2809 \tTraining Loss: 0.356010\n",
            "Epoch: 2810 \tTraining Loss: 0.355892\n",
            "Epoch: 2811 \tTraining Loss: 0.355792\n",
            "Epoch: 2812 \tTraining Loss: 0.355766\n",
            "Epoch: 2813 \tTraining Loss: 0.355764\n",
            "Epoch: 2814 \tTraining Loss: 0.355755\n",
            "Epoch: 2815 \tTraining Loss: 0.355785\n",
            "Epoch: 2816 \tTraining Loss: 0.355834\n",
            "Epoch: 2817 \tTraining Loss: 0.355818\n",
            "Epoch: 2818 \tTraining Loss: 0.355783\n",
            "Epoch: 2819 \tTraining Loss: 0.355771\n",
            "Epoch: 2820 \tTraining Loss: 0.355635\n",
            "Epoch: 2821 \tTraining Loss: 0.355523\n",
            "Epoch: 2822 \tTraining Loss: 0.355431\n",
            "Epoch: 2823 \tTraining Loss: 0.355371\n",
            "Epoch: 2824 \tTraining Loss: 0.355330\n",
            "Epoch: 2825 \tTraining Loss: 0.355318\n",
            "Epoch: 2826 \tTraining Loss: 0.355335\n",
            "Epoch: 2827 \tTraining Loss: 0.355364\n",
            "Epoch: 2828 \tTraining Loss: 0.355388\n",
            "Epoch: 2829 \tTraining Loss: 0.355417\n",
            "Epoch: 2830 \tTraining Loss: 0.355467\n",
            "Epoch: 2831 \tTraining Loss: 0.355382\n",
            "Epoch: 2832 \tTraining Loss: 0.355316\n",
            "Epoch: 2833 \tTraining Loss: 0.355185\n",
            "Epoch: 2834 \tTraining Loss: 0.355076\n",
            "Epoch: 2835 \tTraining Loss: 0.354973\n",
            "Epoch: 2836 \tTraining Loss: 0.354904\n",
            "Epoch: 2837 \tTraining Loss: 0.354872\n",
            "Epoch: 2838 \tTraining Loss: 0.354866\n",
            "Epoch: 2839 \tTraining Loss: 0.354871\n",
            "Epoch: 2840 \tTraining Loss: 0.354889\n",
            "Epoch: 2841 \tTraining Loss: 0.354937\n",
            "Epoch: 2842 \tTraining Loss: 0.354997\n",
            "Epoch: 2843 \tTraining Loss: 0.355061\n",
            "Epoch: 2844 \tTraining Loss: 0.355014\n",
            "Epoch: 2845 \tTraining Loss: 0.355002\n",
            "Epoch: 2846 \tTraining Loss: 0.354860\n",
            "Epoch: 2847 \tTraining Loss: 0.354755\n",
            "Epoch: 2848 \tTraining Loss: 0.354613\n",
            "Epoch: 2849 \tTraining Loss: 0.354504\n",
            "Epoch: 2850 \tTraining Loss: 0.354437\n",
            "Epoch: 2851 \tTraining Loss: 0.354402\n",
            "Epoch: 2852 \tTraining Loss: 0.354370\n",
            "Epoch: 2853 \tTraining Loss: 0.354328\n",
            "Epoch: 2854 \tTraining Loss: 0.354295\n",
            "Epoch: 2855 \tTraining Loss: 0.354284\n",
            "Epoch: 2856 \tTraining Loss: 0.354286\n",
            "Epoch: 2857 \tTraining Loss: 0.354280\n",
            "Epoch: 2858 \tTraining Loss: 0.354272\n",
            "Epoch: 2859 \tTraining Loss: 0.354262\n",
            "Epoch: 2860 \tTraining Loss: 0.354277\n",
            "Epoch: 2861 \tTraining Loss: 0.354272\n",
            "Epoch: 2862 \tTraining Loss: 0.354294\n",
            "Epoch: 2863 \tTraining Loss: 0.354296\n",
            "Epoch: 2864 \tTraining Loss: 0.354351\n",
            "Epoch: 2865 \tTraining Loss: 0.354384\n",
            "Epoch: 2866 \tTraining Loss: 0.354460\n",
            "Epoch: 2867 \tTraining Loss: 0.354357\n",
            "Epoch: 2868 \tTraining Loss: 0.354335\n",
            "Epoch: 2869 \tTraining Loss: 0.354173\n",
            "Epoch: 2870 \tTraining Loss: 0.354073\n",
            "Epoch: 2871 \tTraining Loss: 0.353904\n",
            "Epoch: 2872 \tTraining Loss: 0.353773\n",
            "Epoch: 2873 \tTraining Loss: 0.353679\n",
            "Epoch: 2874 \tTraining Loss: 0.353626\n",
            "Epoch: 2875 \tTraining Loss: 0.353582\n",
            "Epoch: 2876 \tTraining Loss: 0.353534\n",
            "Epoch: 2877 \tTraining Loss: 0.353489\n",
            "Epoch: 2878 \tTraining Loss: 0.353458\n",
            "Epoch: 2879 \tTraining Loss: 0.353440\n",
            "Epoch: 2880 \tTraining Loss: 0.353422\n",
            "Epoch: 2881 \tTraining Loss: 0.353398\n",
            "Epoch: 2882 \tTraining Loss: 0.353377\n",
            "Epoch: 2883 \tTraining Loss: 0.353370\n",
            "Epoch: 2884 \tTraining Loss: 0.353372\n",
            "Epoch: 2885 \tTraining Loss: 0.353395\n",
            "Epoch: 2886 \tTraining Loss: 0.353417\n",
            "Epoch: 2887 \tTraining Loss: 0.353494\n",
            "Epoch: 2888 \tTraining Loss: 0.353601\n",
            "Epoch: 2889 \tTraining Loss: 0.353795\n",
            "Epoch: 2890 \tTraining Loss: 0.353861\n",
            "Epoch: 2891 \tTraining Loss: 0.354096\n",
            "Epoch: 2892 \tTraining Loss: 0.353886\n",
            "Epoch: 2893 \tTraining Loss: 0.353884\n",
            "Epoch: 2894 \tTraining Loss: 0.353587\n",
            "Epoch: 2895 \tTraining Loss: 0.353402\n",
            "Epoch: 2896 \tTraining Loss: 0.353128\n",
            "Epoch: 2897 \tTraining Loss: 0.352993\n",
            "Epoch: 2898 \tTraining Loss: 0.352910\n",
            "Epoch: 2899 \tTraining Loss: 0.352838\n",
            "Epoch: 2900 \tTraining Loss: 0.352751\n",
            "Epoch: 2901 \tTraining Loss: 0.352674\n",
            "Epoch: 2902 \tTraining Loss: 0.352646\n",
            "Epoch: 2903 \tTraining Loss: 0.352643\n",
            "Epoch: 2904 \tTraining Loss: 0.352612\n",
            "Epoch: 2905 \tTraining Loss: 0.352566\n",
            "Epoch: 2906 \tTraining Loss: 0.352538\n",
            "Epoch: 2907 \tTraining Loss: 0.352528\n",
            "Epoch: 2908 \tTraining Loss: 0.352527\n",
            "Epoch: 2909 \tTraining Loss: 0.352507\n",
            "Epoch: 2910 \tTraining Loss: 0.352500\n",
            "Epoch: 2911 \tTraining Loss: 0.352520\n",
            "Epoch: 2912 \tTraining Loss: 0.352577\n",
            "Epoch: 2913 \tTraining Loss: 0.352656\n",
            "Epoch: 2914 \tTraining Loss: 0.352806\n",
            "Epoch: 2915 \tTraining Loss: 0.352848\n",
            "Epoch: 2916 \tTraining Loss: 0.353057\n",
            "Epoch: 2917 \tTraining Loss: 0.353066\n",
            "Epoch: 2918 \tTraining Loss: 0.353286\n",
            "Epoch: 2919 \tTraining Loss: 0.353124\n",
            "Epoch: 2920 \tTraining Loss: 0.353190\n",
            "Epoch: 2921 \tTraining Loss: 0.352906\n",
            "Epoch: 2922 \tTraining Loss: 0.352803\n",
            "Epoch: 2923 \tTraining Loss: 0.352488\n",
            "Epoch: 2924 \tTraining Loss: 0.352244\n",
            "Epoch: 2925 \tTraining Loss: 0.352006\n",
            "Epoch: 2926 \tTraining Loss: 0.351869\n",
            "Epoch: 2927 \tTraining Loss: 0.351807\n",
            "Epoch: 2928 \tTraining Loss: 0.351799\n",
            "Epoch: 2929 \tTraining Loss: 0.351832\n",
            "Epoch: 2930 \tTraining Loss: 0.351892\n",
            "Epoch: 2931 \tTraining Loss: 0.351990\n",
            "Epoch: 2932 \tTraining Loss: 0.352064\n",
            "Epoch: 2933 \tTraining Loss: 0.352172\n",
            "Epoch: 2934 \tTraining Loss: 0.352190\n",
            "Epoch: 2935 \tTraining Loss: 0.352277\n",
            "Epoch: 2936 \tTraining Loss: 0.352140\n",
            "Epoch: 2937 \tTraining Loss: 0.352071\n",
            "Epoch: 2938 \tTraining Loss: 0.351859\n",
            "Epoch: 2939 \tTraining Loss: 0.351701\n",
            "Epoch: 2940 \tTraining Loss: 0.351516\n",
            "Epoch: 2941 \tTraining Loss: 0.351383\n",
            "Epoch: 2942 \tTraining Loss: 0.351298\n",
            "Epoch: 2943 \tTraining Loss: 0.351261\n",
            "Epoch: 2944 \tTraining Loss: 0.351255\n",
            "Epoch: 2945 \tTraining Loss: 0.351259\n",
            "Epoch: 2946 \tTraining Loss: 0.351282\n",
            "Epoch: 2947 \tTraining Loss: 0.351320\n",
            "Epoch: 2948 \tTraining Loss: 0.351374\n",
            "Epoch: 2949 \tTraining Loss: 0.351413\n",
            "Epoch: 2950 \tTraining Loss: 0.351465\n",
            "Epoch: 2951 \tTraining Loss: 0.351444\n",
            "Epoch: 2952 \tTraining Loss: 0.351480\n",
            "Epoch: 2953 \tTraining Loss: 0.351402\n",
            "Epoch: 2954 \tTraining Loss: 0.351383\n",
            "Epoch: 2955 \tTraining Loss: 0.351275\n",
            "Epoch: 2956 \tTraining Loss: 0.351207\n",
            "Epoch: 2957 \tTraining Loss: 0.351115\n",
            "Epoch: 2958 \tTraining Loss: 0.351059\n",
            "Epoch: 2959 \tTraining Loss: 0.350967\n",
            "Epoch: 2960 \tTraining Loss: 0.350891\n",
            "Epoch: 2961 \tTraining Loss: 0.350813\n",
            "Epoch: 2962 \tTraining Loss: 0.350749\n",
            "Epoch: 2963 \tTraining Loss: 0.350679\n",
            "Epoch: 2964 \tTraining Loss: 0.350618\n",
            "Epoch: 2965 \tTraining Loss: 0.350555\n",
            "Epoch: 2966 \tTraining Loss: 0.350506\n",
            "Epoch: 2967 \tTraining Loss: 0.350470\n",
            "Epoch: 2968 \tTraining Loss: 0.350433\n",
            "Epoch: 2969 \tTraining Loss: 0.350394\n",
            "Epoch: 2970 \tTraining Loss: 0.350354\n",
            "Epoch: 2971 \tTraining Loss: 0.350319\n",
            "Epoch: 2972 \tTraining Loss: 0.350287\n",
            "Epoch: 2973 \tTraining Loss: 0.350254\n",
            "Epoch: 2974 \tTraining Loss: 0.350219\n",
            "Epoch: 2975 \tTraining Loss: 0.350183\n",
            "Epoch: 2976 \tTraining Loss: 0.350149\n",
            "Epoch: 2977 \tTraining Loss: 0.350117\n",
            "Epoch: 2978 \tTraining Loss: 0.350085\n",
            "Epoch: 2979 \tTraining Loss: 0.350051\n",
            "Epoch: 2980 \tTraining Loss: 0.350018\n",
            "Epoch: 2981 \tTraining Loss: 0.349986\n",
            "Epoch: 2982 \tTraining Loss: 0.349958\n",
            "Epoch: 2983 \tTraining Loss: 0.349933\n",
            "Epoch: 2984 \tTraining Loss: 0.349915\n",
            "Epoch: 2985 \tTraining Loss: 0.349905\n",
            "Epoch: 2986 \tTraining Loss: 0.349916\n",
            "Epoch: 2987 \tTraining Loss: 0.349960\n",
            "Epoch: 2988 \tTraining Loss: 0.350063\n",
            "Epoch: 2989 \tTraining Loss: 0.350249\n",
            "Epoch: 2990 \tTraining Loss: 0.350641\n",
            "Epoch: 2991 \tTraining Loss: 0.351107\n",
            "Epoch: 2992 \tTraining Loss: 0.352301\n",
            "Epoch: 2993 \tTraining Loss: 0.352593\n",
            "Epoch: 2994 \tTraining Loss: 0.354499\n",
            "Epoch: 2995 \tTraining Loss: 0.355054\n",
            "Epoch: 2996 \tTraining Loss: 0.362379\n",
            "Epoch: 2997 \tTraining Loss: 0.380681\n",
            "Epoch: 2998 \tTraining Loss: 0.488167\n",
            "Epoch: 2999 \tTraining Loss: 0.467197\n",
            "Epoch: 3000 \tTraining Loss: 0.479602\n",
            "Epoch: 3001 \tTraining Loss: 0.412633\n",
            "Epoch: 3002 \tTraining Loss: 0.498038\n",
            "Epoch: 3003 \tTraining Loss: 0.482536\n",
            "Epoch: 3004 \tTraining Loss: 0.421407\n",
            "Epoch: 3005 \tTraining Loss: 0.569386\n",
            "Epoch: 3006 \tTraining Loss: 0.464857\n",
            "Epoch: 3007 \tTraining Loss: 0.487413\n",
            "Epoch: 3008 \tTraining Loss: 0.425380\n",
            "Epoch: 3009 \tTraining Loss: 0.462010\n",
            "Epoch: 3010 \tTraining Loss: 0.433613\n",
            "Epoch: 3011 \tTraining Loss: 0.421367\n",
            "Epoch: 3012 \tTraining Loss: 0.405545\n",
            "Epoch: 3013 \tTraining Loss: 0.409035\n",
            "Epoch: 3014 \tTraining Loss: 0.401729\n",
            "Epoch: 3015 \tTraining Loss: 0.405221\n",
            "Epoch: 3016 \tTraining Loss: 0.405628\n",
            "Epoch: 3017 \tTraining Loss: 0.381717\n",
            "Epoch: 3018 \tTraining Loss: 0.387690\n",
            "Epoch: 3019 \tTraining Loss: 0.394739\n",
            "Epoch: 3020 \tTraining Loss: 0.374207\n",
            "Epoch: 3021 \tTraining Loss: 0.379585\n",
            "Epoch: 3022 \tTraining Loss: 0.376789\n",
            "Epoch: 3023 \tTraining Loss: 0.375783\n",
            "Epoch: 3024 \tTraining Loss: 0.375976\n",
            "Epoch: 3025 \tTraining Loss: 0.364441\n",
            "Epoch: 3026 \tTraining Loss: 0.370259\n",
            "Epoch: 3027 \tTraining Loss: 0.365570\n",
            "Epoch: 3028 \tTraining Loss: 0.364411\n",
            "Epoch: 3029 \tTraining Loss: 0.363554\n",
            "Epoch: 3030 \tTraining Loss: 0.364008\n",
            "Epoch: 3031 \tTraining Loss: 0.360405\n",
            "Epoch: 3032 \tTraining Loss: 0.359764\n",
            "Epoch: 3033 \tTraining Loss: 0.359060\n",
            "Epoch: 3034 \tTraining Loss: 0.359129\n",
            "Epoch: 3035 \tTraining Loss: 0.357854\n",
            "Epoch: 3036 \tTraining Loss: 0.357577\n",
            "Epoch: 3037 \tTraining Loss: 0.355968\n",
            "Epoch: 3038 \tTraining Loss: 0.355343\n",
            "Epoch: 3039 \tTraining Loss: 0.355923\n",
            "Epoch: 3040 \tTraining Loss: 0.353954\n",
            "Epoch: 3041 \tTraining Loss: 0.354381\n",
            "Epoch: 3042 \tTraining Loss: 0.353700\n",
            "Epoch: 3043 \tTraining Loss: 0.353504\n",
            "Epoch: 3044 \tTraining Loss: 0.352783\n",
            "Epoch: 3045 \tTraining Loss: 0.353074\n",
            "Epoch: 3046 \tTraining Loss: 0.352115\n",
            "Epoch: 3047 \tTraining Loss: 0.351854\n",
            "Epoch: 3048 \tTraining Loss: 0.352121\n",
            "Epoch: 3049 \tTraining Loss: 0.351563\n",
            "Epoch: 3050 \tTraining Loss: 0.351240\n",
            "Epoch: 3051 \tTraining Loss: 0.351107\n",
            "Epoch: 3052 \tTraining Loss: 0.351226\n",
            "Epoch: 3053 \tTraining Loss: 0.350735\n",
            "Epoch: 3054 \tTraining Loss: 0.350554\n",
            "Epoch: 3055 \tTraining Loss: 0.350455\n",
            "Epoch: 3056 \tTraining Loss: 0.350375\n",
            "Epoch: 3057 \tTraining Loss: 0.350289\n",
            "Epoch: 3058 \tTraining Loss: 0.350133\n",
            "Epoch: 3059 \tTraining Loss: 0.350004\n",
            "Epoch: 3060 \tTraining Loss: 0.349896\n",
            "Epoch: 3061 \tTraining Loss: 0.349791\n",
            "Epoch: 3062 \tTraining Loss: 0.349705\n",
            "Epoch: 3063 \tTraining Loss: 0.349572\n",
            "Epoch: 3064 \tTraining Loss: 0.349537\n",
            "Epoch: 3065 \tTraining Loss: 0.349466\n",
            "Epoch: 3066 \tTraining Loss: 0.349417\n",
            "Epoch: 3067 \tTraining Loss: 0.349329\n",
            "Epoch: 3068 \tTraining Loss: 0.349245\n",
            "Epoch: 3069 \tTraining Loss: 0.349201\n",
            "Epoch: 3070 \tTraining Loss: 0.349138\n",
            "Epoch: 3071 \tTraining Loss: 0.349123\n",
            "Epoch: 3072 \tTraining Loss: 0.349015\n",
            "Epoch: 3073 \tTraining Loss: 0.348979\n",
            "Epoch: 3074 \tTraining Loss: 0.348915\n",
            "Epoch: 3075 \tTraining Loss: 0.348897\n",
            "Epoch: 3076 \tTraining Loss: 0.348817\n",
            "Epoch: 3077 \tTraining Loss: 0.348793\n",
            "Epoch: 3078 \tTraining Loss: 0.348738\n",
            "Epoch: 3079 \tTraining Loss: 0.348696\n",
            "Epoch: 3080 \tTraining Loss: 0.348659\n",
            "Epoch: 3081 \tTraining Loss: 0.348610\n",
            "Epoch: 3082 \tTraining Loss: 0.348571\n",
            "Epoch: 3083 \tTraining Loss: 0.348530\n",
            "Epoch: 3084 \tTraining Loss: 0.348501\n",
            "Epoch: 3085 \tTraining Loss: 0.348454\n",
            "Epoch: 3086 \tTraining Loss: 0.348425\n",
            "Epoch: 3087 \tTraining Loss: 0.348388\n",
            "Epoch: 3088 \tTraining Loss: 0.348353\n",
            "Epoch: 3089 \tTraining Loss: 0.348320\n",
            "Epoch: 3090 \tTraining Loss: 0.348282\n",
            "Epoch: 3091 \tTraining Loss: 0.348248\n",
            "Epoch: 3092 \tTraining Loss: 0.348220\n",
            "Epoch: 3093 \tTraining Loss: 0.348187\n",
            "Epoch: 3094 \tTraining Loss: 0.348153\n",
            "Epoch: 3095 \tTraining Loss: 0.348125\n",
            "Epoch: 3096 \tTraining Loss: 0.348091\n",
            "Epoch: 3097 \tTraining Loss: 0.348063\n",
            "Epoch: 3098 \tTraining Loss: 0.348032\n",
            "Epoch: 3099 \tTraining Loss: 0.348002\n",
            "Epoch: 3100 \tTraining Loss: 0.347973\n",
            "Epoch: 3101 \tTraining Loss: 0.347943\n",
            "Epoch: 3102 \tTraining Loss: 0.347914\n",
            "Epoch: 3103 \tTraining Loss: 0.347885\n",
            "Epoch: 3104 \tTraining Loss: 0.347856\n",
            "Epoch: 3105 \tTraining Loss: 0.347827\n",
            "Epoch: 3106 \tTraining Loss: 0.347799\n",
            "Epoch: 3107 \tTraining Loss: 0.347769\n",
            "Epoch: 3108 \tTraining Loss: 0.347742\n",
            "Epoch: 3109 \tTraining Loss: 0.347713\n",
            "Epoch: 3110 \tTraining Loss: 0.347685\n",
            "Epoch: 3111 \tTraining Loss: 0.347656\n",
            "Epoch: 3112 \tTraining Loss: 0.347628\n",
            "Epoch: 3113 \tTraining Loss: 0.347599\n",
            "Epoch: 3114 \tTraining Loss: 0.347569\n",
            "Epoch: 3115 \tTraining Loss: 0.347538\n",
            "Epoch: 3116 \tTraining Loss: 0.347503\n",
            "Epoch: 3117 \tTraining Loss: 0.347464\n",
            "Epoch: 3118 \tTraining Loss: 0.347412\n",
            "Epoch: 3119 \tTraining Loss: 0.347337\n",
            "Epoch: 3120 \tTraining Loss: 0.347241\n",
            "Epoch: 3121 \tTraining Loss: 0.347166\n",
            "Epoch: 3122 \tTraining Loss: 0.347116\n",
            "Epoch: 3123 \tTraining Loss: 0.347076\n",
            "Epoch: 3124 \tTraining Loss: 0.347039\n",
            "Epoch: 3125 \tTraining Loss: 0.347004\n",
            "Epoch: 3126 \tTraining Loss: 0.346970\n",
            "Epoch: 3127 \tTraining Loss: 0.346936\n",
            "Epoch: 3128 \tTraining Loss: 0.346907\n",
            "Epoch: 3129 \tTraining Loss: 0.346883\n",
            "Epoch: 3130 \tTraining Loss: 0.346861\n",
            "Epoch: 3131 \tTraining Loss: 0.346838\n",
            "Epoch: 3132 \tTraining Loss: 0.346815\n",
            "Epoch: 3133 \tTraining Loss: 0.346790\n",
            "Epoch: 3134 \tTraining Loss: 0.346764\n",
            "Epoch: 3135 \tTraining Loss: 0.346739\n",
            "Epoch: 3136 \tTraining Loss: 0.346714\n",
            "Epoch: 3137 \tTraining Loss: 0.346690\n",
            "Epoch: 3138 \tTraining Loss: 0.346665\n",
            "Epoch: 3139 \tTraining Loss: 0.346641\n",
            "Epoch: 3140 \tTraining Loss: 0.346616\n",
            "Epoch: 3141 \tTraining Loss: 0.346591\n",
            "Epoch: 3142 \tTraining Loss: 0.346566\n",
            "Epoch: 3143 \tTraining Loss: 0.346540\n",
            "Epoch: 3144 \tTraining Loss: 0.346515\n",
            "Epoch: 3145 \tTraining Loss: 0.346489\n",
            "Epoch: 3146 \tTraining Loss: 0.346464\n",
            "Epoch: 3147 \tTraining Loss: 0.346439\n",
            "Epoch: 3148 \tTraining Loss: 0.346414\n",
            "Epoch: 3149 \tTraining Loss: 0.346389\n",
            "Epoch: 3150 \tTraining Loss: 0.346364\n",
            "Epoch: 3151 \tTraining Loss: 0.346339\n",
            "Epoch: 3152 \tTraining Loss: 0.346313\n",
            "Epoch: 3153 \tTraining Loss: 0.346287\n",
            "Epoch: 3154 \tTraining Loss: 0.346261\n",
            "Epoch: 3155 \tTraining Loss: 0.346235\n",
            "Epoch: 3156 \tTraining Loss: 0.346210\n",
            "Epoch: 3157 \tTraining Loss: 0.346184\n",
            "Epoch: 3158 \tTraining Loss: 0.346160\n",
            "Epoch: 3159 \tTraining Loss: 0.346136\n",
            "Epoch: 3160 \tTraining Loss: 0.346112\n",
            "Epoch: 3161 \tTraining Loss: 0.346089\n",
            "Epoch: 3162 \tTraining Loss: 0.346066\n",
            "Epoch: 3163 \tTraining Loss: 0.346043\n",
            "Epoch: 3164 \tTraining Loss: 0.346020\n",
            "Epoch: 3165 \tTraining Loss: 0.345996\n",
            "Epoch: 3166 \tTraining Loss: 0.345973\n",
            "Epoch: 3167 \tTraining Loss: 0.345949\n",
            "Epoch: 3168 \tTraining Loss: 0.345925\n",
            "Epoch: 3169 \tTraining Loss: 0.345901\n",
            "Epoch: 3170 \tTraining Loss: 0.345878\n",
            "Epoch: 3171 \tTraining Loss: 0.345854\n",
            "Epoch: 3172 \tTraining Loss: 0.345831\n",
            "Epoch: 3173 \tTraining Loss: 0.345807\n",
            "Epoch: 3174 \tTraining Loss: 0.345784\n",
            "Epoch: 3175 \tTraining Loss: 0.345761\n",
            "Epoch: 3176 \tTraining Loss: 0.345738\n",
            "Epoch: 3177 \tTraining Loss: 0.345715\n",
            "Epoch: 3178 \tTraining Loss: 0.345692\n",
            "Epoch: 3179 \tTraining Loss: 0.345669\n",
            "Epoch: 3180 \tTraining Loss: 0.345646\n",
            "Epoch: 3181 \tTraining Loss: 0.345623\n",
            "Epoch: 3182 \tTraining Loss: 0.345600\n",
            "Epoch: 3183 \tTraining Loss: 0.345577\n",
            "Epoch: 3184 \tTraining Loss: 0.345554\n",
            "Epoch: 3185 \tTraining Loss: 0.345531\n",
            "Epoch: 3186 \tTraining Loss: 0.345508\n",
            "Epoch: 3187 \tTraining Loss: 0.345485\n",
            "Epoch: 3188 \tTraining Loss: 0.345462\n",
            "Epoch: 3189 \tTraining Loss: 0.345440\n",
            "Epoch: 3190 \tTraining Loss: 0.345417\n",
            "Epoch: 3191 \tTraining Loss: 0.345394\n",
            "Epoch: 3192 \tTraining Loss: 0.345371\n",
            "Epoch: 3193 \tTraining Loss: 0.345349\n",
            "Epoch: 3194 \tTraining Loss: 0.345326\n",
            "Epoch: 3195 \tTraining Loss: 0.345303\n",
            "Epoch: 3196 \tTraining Loss: 0.345281\n",
            "Epoch: 3197 \tTraining Loss: 0.345258\n",
            "Epoch: 3198 \tTraining Loss: 0.345236\n",
            "Epoch: 3199 \tTraining Loss: 0.345213\n",
            "Epoch: 3200 \tTraining Loss: 0.345190\n",
            "Epoch: 3201 \tTraining Loss: 0.345168\n",
            "Epoch: 3202 \tTraining Loss: 0.345145\n",
            "Epoch: 3203 \tTraining Loss: 0.345123\n",
            "Epoch: 3204 \tTraining Loss: 0.345100\n",
            "Epoch: 3205 \tTraining Loss: 0.345078\n",
            "Epoch: 3206 \tTraining Loss: 0.345055\n",
            "Epoch: 3207 \tTraining Loss: 0.345033\n",
            "Epoch: 3208 \tTraining Loss: 0.345011\n",
            "Epoch: 3209 \tTraining Loss: 0.344988\n",
            "Epoch: 3210 \tTraining Loss: 0.344966\n",
            "Epoch: 3211 \tTraining Loss: 0.344944\n",
            "Epoch: 3212 \tTraining Loss: 0.344921\n",
            "Epoch: 3213 \tTraining Loss: 0.344899\n",
            "Epoch: 3214 \tTraining Loss: 0.344877\n",
            "Epoch: 3215 \tTraining Loss: 0.344854\n",
            "Epoch: 3216 \tTraining Loss: 0.344832\n",
            "Epoch: 3217 \tTraining Loss: 0.344810\n",
            "Epoch: 3218 \tTraining Loss: 0.344788\n",
            "Epoch: 3219 \tTraining Loss: 0.344765\n",
            "Epoch: 3220 \tTraining Loss: 0.344743\n",
            "Epoch: 3221 \tTraining Loss: 0.344721\n",
            "Epoch: 3222 \tTraining Loss: 0.344699\n",
            "Epoch: 3223 \tTraining Loss: 0.344677\n",
            "Epoch: 3224 \tTraining Loss: 0.344654\n",
            "Epoch: 3225 \tTraining Loss: 0.344632\n",
            "Epoch: 3226 \tTraining Loss: 0.344610\n",
            "Epoch: 3227 \tTraining Loss: 0.344588\n",
            "Epoch: 3228 \tTraining Loss: 0.344566\n",
            "Epoch: 3229 \tTraining Loss: 0.344544\n",
            "Epoch: 3230 \tTraining Loss: 0.344522\n",
            "Epoch: 3231 \tTraining Loss: 0.344500\n",
            "Epoch: 3232 \tTraining Loss: 0.344478\n",
            "Epoch: 3233 \tTraining Loss: 0.344455\n",
            "Epoch: 3234 \tTraining Loss: 0.344434\n",
            "Epoch: 3235 \tTraining Loss: 0.344411\n",
            "Epoch: 3236 \tTraining Loss: 0.344389\n",
            "Epoch: 3237 \tTraining Loss: 0.344367\n",
            "Epoch: 3238 \tTraining Loss: 0.344345\n",
            "Epoch: 3239 \tTraining Loss: 0.344324\n",
            "Epoch: 3240 \tTraining Loss: 0.344301\n",
            "Epoch: 3241 \tTraining Loss: 0.344280\n",
            "Epoch: 3242 \tTraining Loss: 0.344258\n",
            "Epoch: 3243 \tTraining Loss: 0.344236\n",
            "Epoch: 3244 \tTraining Loss: 0.344214\n",
            "Epoch: 3245 \tTraining Loss: 0.344192\n",
            "Epoch: 3246 \tTraining Loss: 0.344170\n",
            "Epoch: 3247 \tTraining Loss: 0.344148\n",
            "Epoch: 3248 \tTraining Loss: 0.344126\n",
            "Epoch: 3249 \tTraining Loss: 0.344104\n",
            "Epoch: 3250 \tTraining Loss: 0.344082\n",
            "Epoch: 3251 \tTraining Loss: 0.344061\n",
            "Epoch: 3252 \tTraining Loss: 0.344039\n",
            "Epoch: 3253 \tTraining Loss: 0.344017\n",
            "Epoch: 3254 \tTraining Loss: 0.343995\n",
            "Epoch: 3255 \tTraining Loss: 0.343973\n",
            "Epoch: 3256 \tTraining Loss: 0.343951\n",
            "Epoch: 3257 \tTraining Loss: 0.343930\n",
            "Epoch: 3258 \tTraining Loss: 0.343908\n",
            "Epoch: 3259 \tTraining Loss: 0.343886\n",
            "Epoch: 3260 \tTraining Loss: 0.343864\n",
            "Epoch: 3261 \tTraining Loss: 0.343842\n",
            "Epoch: 3262 \tTraining Loss: 0.343821\n",
            "Epoch: 3263 \tTraining Loss: 0.343799\n",
            "Epoch: 3264 \tTraining Loss: 0.343777\n",
            "Epoch: 3265 \tTraining Loss: 0.343755\n",
            "Epoch: 3266 \tTraining Loss: 0.343734\n",
            "Epoch: 3267 \tTraining Loss: 0.343712\n",
            "Epoch: 3268 \tTraining Loss: 0.343690\n",
            "Epoch: 3269 \tTraining Loss: 0.343669\n",
            "Epoch: 3270 \tTraining Loss: 0.343647\n",
            "Epoch: 3271 \tTraining Loss: 0.343625\n",
            "Epoch: 3272 \tTraining Loss: 0.343604\n",
            "Epoch: 3273 \tTraining Loss: 0.343582\n",
            "Epoch: 3274 \tTraining Loss: 0.343560\n",
            "Epoch: 3275 \tTraining Loss: 0.343539\n",
            "Epoch: 3276 \tTraining Loss: 0.343517\n",
            "Epoch: 3277 \tTraining Loss: 0.343495\n",
            "Epoch: 3278 \tTraining Loss: 0.343474\n",
            "Epoch: 3279 \tTraining Loss: 0.343452\n",
            "Epoch: 3280 \tTraining Loss: 0.343430\n",
            "Epoch: 3281 \tTraining Loss: 0.343409\n",
            "Epoch: 3282 \tTraining Loss: 0.343387\n",
            "Epoch: 3283 \tTraining Loss: 0.343365\n",
            "Epoch: 3284 \tTraining Loss: 0.343344\n",
            "Epoch: 3285 \tTraining Loss: 0.343322\n",
            "Epoch: 3286 \tTraining Loss: 0.343301\n",
            "Epoch: 3287 \tTraining Loss: 0.343279\n",
            "Epoch: 3288 \tTraining Loss: 0.343257\n",
            "Epoch: 3289 \tTraining Loss: 0.343236\n",
            "Epoch: 3290 \tTraining Loss: 0.343214\n",
            "Epoch: 3291 \tTraining Loss: 0.343193\n",
            "Epoch: 3292 \tTraining Loss: 0.343171\n",
            "Epoch: 3293 \tTraining Loss: 0.343150\n",
            "Epoch: 3294 \tTraining Loss: 0.343128\n",
            "Epoch: 3295 \tTraining Loss: 0.343106\n",
            "Epoch: 3296 \tTraining Loss: 0.343085\n",
            "Epoch: 3297 \tTraining Loss: 0.343063\n",
            "Epoch: 3298 \tTraining Loss: 0.343042\n",
            "Epoch: 3299 \tTraining Loss: 0.343020\n",
            "Epoch: 3300 \tTraining Loss: 0.342999\n",
            "Epoch: 3301 \tTraining Loss: 0.342977\n",
            "Epoch: 3302 \tTraining Loss: 0.342956\n",
            "Epoch: 3303 \tTraining Loss: 0.342934\n",
            "Epoch: 3304 \tTraining Loss: 0.342913\n",
            "Epoch: 3305 \tTraining Loss: 0.342891\n",
            "Epoch: 3306 \tTraining Loss: 0.342870\n",
            "Epoch: 3307 \tTraining Loss: 0.342848\n",
            "Epoch: 3308 \tTraining Loss: 0.342827\n",
            "Epoch: 3309 \tTraining Loss: 0.342805\n",
            "Epoch: 3310 \tTraining Loss: 0.342784\n",
            "Epoch: 3311 \tTraining Loss: 0.342762\n",
            "Epoch: 3312 \tTraining Loss: 0.342741\n",
            "Epoch: 3313 \tTraining Loss: 0.342719\n",
            "Epoch: 3314 \tTraining Loss: 0.342698\n",
            "Epoch: 3315 \tTraining Loss: 0.342676\n",
            "Epoch: 3316 \tTraining Loss: 0.342655\n",
            "Epoch: 3317 \tTraining Loss: 0.342634\n",
            "Epoch: 3318 \tTraining Loss: 0.342612\n",
            "Epoch: 3319 \tTraining Loss: 0.342591\n",
            "Epoch: 3320 \tTraining Loss: 0.342569\n",
            "Epoch: 3321 \tTraining Loss: 0.342548\n",
            "Epoch: 3322 \tTraining Loss: 0.342526\n",
            "Epoch: 3323 \tTraining Loss: 0.342505\n",
            "Epoch: 3324 \tTraining Loss: 0.342483\n",
            "Epoch: 3325 \tTraining Loss: 0.342462\n",
            "Epoch: 3326 \tTraining Loss: 0.342441\n",
            "Epoch: 3327 \tTraining Loss: 0.342419\n",
            "Epoch: 3328 \tTraining Loss: 0.342398\n",
            "Epoch: 3329 \tTraining Loss: 0.342376\n",
            "Epoch: 3330 \tTraining Loss: 0.342355\n",
            "Epoch: 3331 \tTraining Loss: 0.342334\n",
            "Epoch: 3332 \tTraining Loss: 0.342312\n",
            "Epoch: 3333 \tTraining Loss: 0.342291\n",
            "Epoch: 3334 \tTraining Loss: 0.342269\n",
            "Epoch: 3335 \tTraining Loss: 0.342248\n",
            "Epoch: 3336 \tTraining Loss: 0.342227\n",
            "Epoch: 3337 \tTraining Loss: 0.342205\n",
            "Epoch: 3338 \tTraining Loss: 0.342184\n",
            "Epoch: 3339 \tTraining Loss: 0.342162\n",
            "Epoch: 3340 \tTraining Loss: 0.342141\n",
            "Epoch: 3341 \tTraining Loss: 0.342120\n",
            "Epoch: 3342 \tTraining Loss: 0.342098\n",
            "Epoch: 3343 \tTraining Loss: 0.342077\n",
            "Epoch: 3344 \tTraining Loss: 0.342056\n",
            "Epoch: 3345 \tTraining Loss: 0.342034\n",
            "Epoch: 3346 \tTraining Loss: 0.342013\n",
            "Epoch: 3347 \tTraining Loss: 0.341991\n",
            "Epoch: 3348 \tTraining Loss: 0.341970\n",
            "Epoch: 3349 \tTraining Loss: 0.341949\n",
            "Epoch: 3350 \tTraining Loss: 0.341927\n",
            "Epoch: 3351 \tTraining Loss: 0.341906\n",
            "Epoch: 3352 \tTraining Loss: 0.341885\n",
            "Epoch: 3353 \tTraining Loss: 0.341863\n",
            "Epoch: 3354 \tTraining Loss: 0.341842\n",
            "Epoch: 3355 \tTraining Loss: 0.341820\n",
            "Epoch: 3356 \tTraining Loss: 0.341799\n",
            "Epoch: 3357 \tTraining Loss: 0.341778\n",
            "Epoch: 3358 \tTraining Loss: 0.341756\n",
            "Epoch: 3359 \tTraining Loss: 0.341735\n",
            "Epoch: 3360 \tTraining Loss: 0.341714\n",
            "Epoch: 3361 \tTraining Loss: 0.341692\n",
            "Epoch: 3362 \tTraining Loss: 0.341671\n",
            "Epoch: 3363 \tTraining Loss: 0.341650\n",
            "Epoch: 3364 \tTraining Loss: 0.341628\n",
            "Epoch: 3365 \tTraining Loss: 0.341607\n",
            "Epoch: 3366 \tTraining Loss: 0.341586\n",
            "Epoch: 3367 \tTraining Loss: 0.341564\n",
            "Epoch: 3368 \tTraining Loss: 0.341543\n",
            "Epoch: 3369 \tTraining Loss: 0.341521\n",
            "Epoch: 3370 \tTraining Loss: 0.341500\n",
            "Epoch: 3371 \tTraining Loss: 0.341479\n",
            "Epoch: 3372 \tTraining Loss: 0.341457\n",
            "Epoch: 3373 \tTraining Loss: 0.341436\n",
            "Epoch: 3374 \tTraining Loss: 0.341415\n",
            "Epoch: 3375 \tTraining Loss: 0.341393\n",
            "Epoch: 3376 \tTraining Loss: 0.341372\n",
            "Epoch: 3377 \tTraining Loss: 0.341351\n",
            "Epoch: 3378 \tTraining Loss: 0.341329\n",
            "Epoch: 3379 \tTraining Loss: 0.341308\n",
            "Epoch: 3380 \tTraining Loss: 0.341287\n",
            "Epoch: 3381 \tTraining Loss: 0.341265\n",
            "Epoch: 3382 \tTraining Loss: 0.341244\n",
            "Epoch: 3383 \tTraining Loss: 0.341223\n",
            "Epoch: 3384 \tTraining Loss: 0.341201\n",
            "Epoch: 3385 \tTraining Loss: 0.341180\n",
            "Epoch: 3386 \tTraining Loss: 0.341159\n",
            "Epoch: 3387 \tTraining Loss: 0.341138\n",
            "Epoch: 3388 \tTraining Loss: 0.341116\n",
            "Epoch: 3389 \tTraining Loss: 0.341095\n",
            "Epoch: 3390 \tTraining Loss: 0.341074\n",
            "Epoch: 3391 \tTraining Loss: 0.341052\n",
            "Epoch: 3392 \tTraining Loss: 0.341031\n",
            "Epoch: 3393 \tTraining Loss: 0.341010\n",
            "Epoch: 3394 \tTraining Loss: 0.340988\n",
            "Epoch: 3395 \tTraining Loss: 0.340967\n",
            "Epoch: 3396 \tTraining Loss: 0.340946\n",
            "Epoch: 3397 \tTraining Loss: 0.340925\n",
            "Epoch: 3398 \tTraining Loss: 0.340903\n",
            "Epoch: 3399 \tTraining Loss: 0.340882\n",
            "Epoch: 3400 \tTraining Loss: 0.340861\n",
            "Epoch: 3401 \tTraining Loss: 0.340839\n",
            "Epoch: 3402 \tTraining Loss: 0.340818\n",
            "Epoch: 3403 \tTraining Loss: 0.340797\n",
            "Epoch: 3404 \tTraining Loss: 0.340775\n",
            "Epoch: 3405 \tTraining Loss: 0.340754\n",
            "Epoch: 3406 \tTraining Loss: 0.340733\n",
            "Epoch: 3407 \tTraining Loss: 0.340712\n",
            "Epoch: 3408 \tTraining Loss: 0.340690\n",
            "Epoch: 3409 \tTraining Loss: 0.340669\n",
            "Epoch: 3410 \tTraining Loss: 0.340648\n",
            "Epoch: 3411 \tTraining Loss: 0.340626\n",
            "Epoch: 3412 \tTraining Loss: 0.340605\n",
            "Epoch: 3413 \tTraining Loss: 0.340584\n",
            "Epoch: 3414 \tTraining Loss: 0.340562\n",
            "Epoch: 3415 \tTraining Loss: 0.340541\n",
            "Epoch: 3416 \tTraining Loss: 0.340520\n",
            "Epoch: 3417 \tTraining Loss: 0.340499\n",
            "Epoch: 3418 \tTraining Loss: 0.340477\n",
            "Epoch: 3419 \tTraining Loss: 0.340456\n",
            "Epoch: 3420 \tTraining Loss: 0.340435\n",
            "Epoch: 3421 \tTraining Loss: 0.340413\n",
            "Epoch: 3422 \tTraining Loss: 0.340392\n",
            "Epoch: 3423 \tTraining Loss: 0.340371\n",
            "Epoch: 3424 \tTraining Loss: 0.340349\n",
            "Epoch: 3425 \tTraining Loss: 0.340328\n",
            "Epoch: 3426 \tTraining Loss: 0.340307\n",
            "Epoch: 3427 \tTraining Loss: 0.340285\n",
            "Epoch: 3428 \tTraining Loss: 0.340264\n",
            "Epoch: 3429 \tTraining Loss: 0.340243\n",
            "Epoch: 3430 \tTraining Loss: 0.340221\n",
            "Epoch: 3431 \tTraining Loss: 0.340200\n",
            "Epoch: 3432 \tTraining Loss: 0.340179\n",
            "Epoch: 3433 \tTraining Loss: 0.340158\n",
            "Epoch: 3434 \tTraining Loss: 0.340136\n",
            "Epoch: 3435 \tTraining Loss: 0.340115\n",
            "Epoch: 3436 \tTraining Loss: 0.340094\n",
            "Epoch: 3437 \tTraining Loss: 0.340072\n",
            "Epoch: 3438 \tTraining Loss: 0.340051\n",
            "Epoch: 3439 \tTraining Loss: 0.340030\n",
            "Epoch: 3440 \tTraining Loss: 0.340008\n",
            "Epoch: 3441 \tTraining Loss: 0.339987\n",
            "Epoch: 3442 \tTraining Loss: 0.339966\n",
            "Epoch: 3443 \tTraining Loss: 0.339944\n",
            "Epoch: 3444 \tTraining Loss: 0.339923\n",
            "Epoch: 3445 \tTraining Loss: 0.339902\n",
            "Epoch: 3446 \tTraining Loss: 0.339880\n",
            "Epoch: 3447 \tTraining Loss: 0.339859\n",
            "Epoch: 3448 \tTraining Loss: 0.339838\n",
            "Epoch: 3449 \tTraining Loss: 0.339816\n",
            "Epoch: 3450 \tTraining Loss: 0.339795\n",
            "Epoch: 3451 \tTraining Loss: 0.339774\n",
            "Epoch: 3452 \tTraining Loss: 0.339752\n",
            "Epoch: 3453 \tTraining Loss: 0.339731\n",
            "Epoch: 3454 \tTraining Loss: 0.339710\n",
            "Epoch: 3455 \tTraining Loss: 0.339688\n",
            "Epoch: 3456 \tTraining Loss: 0.339667\n",
            "Epoch: 3457 \tTraining Loss: 0.339646\n",
            "Epoch: 3458 \tTraining Loss: 0.339624\n",
            "Epoch: 3459 \tTraining Loss: 0.339603\n",
            "Epoch: 3460 \tTraining Loss: 0.339582\n",
            "Epoch: 3461 \tTraining Loss: 0.339560\n",
            "Epoch: 3462 \tTraining Loss: 0.339539\n",
            "Epoch: 3463 \tTraining Loss: 0.339517\n",
            "Epoch: 3464 \tTraining Loss: 0.339496\n",
            "Epoch: 3465 \tTraining Loss: 0.339475\n",
            "Epoch: 3466 \tTraining Loss: 0.339453\n",
            "Epoch: 3467 \tTraining Loss: 0.339432\n",
            "Epoch: 3468 \tTraining Loss: 0.339410\n",
            "Epoch: 3469 \tTraining Loss: 0.339389\n",
            "Epoch: 3470 \tTraining Loss: 0.339367\n",
            "Epoch: 3471 \tTraining Loss: 0.339346\n",
            "Epoch: 3472 \tTraining Loss: 0.339324\n",
            "Epoch: 3473 \tTraining Loss: 0.339303\n",
            "Epoch: 3474 \tTraining Loss: 0.339281\n",
            "Epoch: 3475 \tTraining Loss: 0.339260\n",
            "Epoch: 3476 \tTraining Loss: 0.339238\n",
            "Epoch: 3477 \tTraining Loss: 0.339216\n",
            "Epoch: 3478 \tTraining Loss: 0.339195\n",
            "Epoch: 3479 \tTraining Loss: 0.339173\n",
            "Epoch: 3480 \tTraining Loss: 0.339152\n",
            "Epoch: 3481 \tTraining Loss: 0.339130\n",
            "Epoch: 3482 \tTraining Loss: 0.339108\n",
            "Epoch: 3483 \tTraining Loss: 0.339087\n",
            "Epoch: 3484 \tTraining Loss: 0.339065\n",
            "Epoch: 3485 \tTraining Loss: 0.339043\n",
            "Epoch: 3486 \tTraining Loss: 0.339021\n",
            "Epoch: 3487 \tTraining Loss: 0.339000\n",
            "Epoch: 3488 \tTraining Loss: 0.338978\n",
            "Epoch: 3489 \tTraining Loss: 0.338956\n",
            "Epoch: 3490 \tTraining Loss: 0.338935\n",
            "Epoch: 3491 \tTraining Loss: 0.338913\n",
            "Epoch: 3492 \tTraining Loss: 0.338891\n",
            "Epoch: 3493 \tTraining Loss: 0.338870\n",
            "Epoch: 3494 \tTraining Loss: 0.338848\n",
            "Epoch: 3495 \tTraining Loss: 0.338826\n",
            "Epoch: 3496 \tTraining Loss: 0.338804\n",
            "Epoch: 3497 \tTraining Loss: 0.338783\n",
            "Epoch: 3498 \tTraining Loss: 0.338761\n",
            "Epoch: 3499 \tTraining Loss: 0.338739\n",
            "Epoch: 3500 \tTraining Loss: 0.338718\n",
            "Epoch: 3501 \tTraining Loss: 0.338696\n",
            "Epoch: 3502 \tTraining Loss: 0.338674\n",
            "Epoch: 3503 \tTraining Loss: 0.338653\n",
            "Epoch: 3504 \tTraining Loss: 0.338631\n",
            "Epoch: 3505 \tTraining Loss: 0.338609\n",
            "Epoch: 3506 \tTraining Loss: 0.338588\n",
            "Epoch: 3507 \tTraining Loss: 0.338566\n",
            "Epoch: 3508 \tTraining Loss: 0.338544\n",
            "Epoch: 3509 \tTraining Loss: 0.338523\n",
            "Epoch: 3510 \tTraining Loss: 0.338501\n",
            "Epoch: 3511 \tTraining Loss: 0.338479\n",
            "Epoch: 3512 \tTraining Loss: 0.338457\n",
            "Epoch: 3513 \tTraining Loss: 0.338436\n",
            "Epoch: 3514 \tTraining Loss: 0.338414\n",
            "Epoch: 3515 \tTraining Loss: 0.338392\n",
            "Epoch: 3516 \tTraining Loss: 0.338371\n",
            "Epoch: 3517 \tTraining Loss: 0.338349\n",
            "Epoch: 3518 \tTraining Loss: 0.338327\n",
            "Epoch: 3519 \tTraining Loss: 0.338306\n",
            "Epoch: 3520 \tTraining Loss: 0.338284\n",
            "Epoch: 3521 \tTraining Loss: 0.338262\n",
            "Epoch: 3522 \tTraining Loss: 0.338240\n",
            "Epoch: 3523 \tTraining Loss: 0.338219\n",
            "Epoch: 3524 \tTraining Loss: 0.338197\n",
            "Epoch: 3525 \tTraining Loss: 0.338175\n",
            "Epoch: 3526 \tTraining Loss: 0.338154\n",
            "Epoch: 3527 \tTraining Loss: 0.338132\n",
            "Epoch: 3528 \tTraining Loss: 0.338110\n",
            "Epoch: 3529 \tTraining Loss: 0.338088\n",
            "Epoch: 3530 \tTraining Loss: 0.338067\n",
            "Epoch: 3531 \tTraining Loss: 0.338045\n",
            "Epoch: 3532 \tTraining Loss: 0.338023\n",
            "Epoch: 3533 \tTraining Loss: 0.338001\n",
            "Epoch: 3534 \tTraining Loss: 0.337980\n",
            "Epoch: 3535 \tTraining Loss: 0.337958\n",
            "Epoch: 3536 \tTraining Loss: 0.337936\n",
            "Epoch: 3537 \tTraining Loss: 0.337914\n",
            "Epoch: 3538 \tTraining Loss: 0.337893\n",
            "Epoch: 3539 \tTraining Loss: 0.337871\n",
            "Epoch: 3540 \tTraining Loss: 0.337849\n",
            "Epoch: 3541 \tTraining Loss: 0.337827\n",
            "Epoch: 3542 \tTraining Loss: 0.337805\n",
            "Epoch: 3543 \tTraining Loss: 0.337784\n",
            "Epoch: 3544 \tTraining Loss: 0.337762\n",
            "Epoch: 3545 \tTraining Loss: 0.337740\n",
            "Epoch: 3546 \tTraining Loss: 0.337718\n",
            "Epoch: 3547 \tTraining Loss: 0.337696\n",
            "Epoch: 3548 \tTraining Loss: 0.337675\n",
            "Epoch: 3549 \tTraining Loss: 0.337653\n",
            "Epoch: 3550 \tTraining Loss: 0.337631\n",
            "Epoch: 3551 \tTraining Loss: 0.337609\n",
            "Epoch: 3552 \tTraining Loss: 0.337587\n",
            "Epoch: 3553 \tTraining Loss: 0.337566\n",
            "Epoch: 3554 \tTraining Loss: 0.337544\n",
            "Epoch: 3555 \tTraining Loss: 0.337522\n",
            "Epoch: 3556 \tTraining Loss: 0.337500\n",
            "Epoch: 3557 \tTraining Loss: 0.337478\n",
            "Epoch: 3558 \tTraining Loss: 0.337456\n",
            "Epoch: 3559 \tTraining Loss: 0.337435\n",
            "Epoch: 3560 \tTraining Loss: 0.337413\n",
            "Epoch: 3561 \tTraining Loss: 0.337391\n",
            "Epoch: 3562 \tTraining Loss: 0.337369\n",
            "Epoch: 3563 \tTraining Loss: 0.337347\n",
            "Epoch: 3564 \tTraining Loss: 0.337325\n",
            "Epoch: 3565 \tTraining Loss: 0.337303\n",
            "Epoch: 3566 \tTraining Loss: 0.337281\n",
            "Epoch: 3567 \tTraining Loss: 0.337260\n",
            "Epoch: 3568 \tTraining Loss: 0.337238\n",
            "Epoch: 3569 \tTraining Loss: 0.337216\n",
            "Epoch: 3570 \tTraining Loss: 0.337194\n",
            "Epoch: 3571 \tTraining Loss: 0.337172\n",
            "Epoch: 3572 \tTraining Loss: 0.337150\n",
            "Epoch: 3573 \tTraining Loss: 0.337128\n",
            "Epoch: 3574 \tTraining Loss: 0.337106\n",
            "Epoch: 3575 \tTraining Loss: 0.337085\n",
            "Epoch: 3576 \tTraining Loss: 0.337063\n",
            "Epoch: 3577 \tTraining Loss: 0.337041\n",
            "Epoch: 3578 \tTraining Loss: 0.337019\n",
            "Epoch: 3579 \tTraining Loss: 0.336997\n",
            "Epoch: 3580 \tTraining Loss: 0.336975\n",
            "Epoch: 3581 \tTraining Loss: 0.336953\n",
            "Epoch: 3582 \tTraining Loss: 0.336931\n",
            "Epoch: 3583 \tTraining Loss: 0.336909\n",
            "Epoch: 3584 \tTraining Loss: 0.336887\n",
            "Epoch: 3585 \tTraining Loss: 0.336865\n",
            "Epoch: 3586 \tTraining Loss: 0.336843\n",
            "Epoch: 3587 \tTraining Loss: 0.336821\n",
            "Epoch: 3588 \tTraining Loss: 0.336799\n",
            "Epoch: 3589 \tTraining Loss: 0.336777\n",
            "Epoch: 3590 \tTraining Loss: 0.336756\n",
            "Epoch: 3591 \tTraining Loss: 0.336734\n",
            "Epoch: 3592 \tTraining Loss: 0.336712\n",
            "Epoch: 3593 \tTraining Loss: 0.336690\n",
            "Epoch: 3594 \tTraining Loss: 0.336668\n",
            "Epoch: 3595 \tTraining Loss: 0.336646\n",
            "Epoch: 3596 \tTraining Loss: 0.336624\n",
            "Epoch: 3597 \tTraining Loss: 0.336602\n",
            "Epoch: 3598 \tTraining Loss: 0.336580\n",
            "Epoch: 3599 \tTraining Loss: 0.336558\n",
            "Epoch: 3600 \tTraining Loss: 0.336535\n",
            "Epoch: 3601 \tTraining Loss: 0.336513\n",
            "Epoch: 3602 \tTraining Loss: 0.336491\n",
            "Epoch: 3603 \tTraining Loss: 0.336469\n",
            "Epoch: 3604 \tTraining Loss: 0.336447\n",
            "Epoch: 3605 \tTraining Loss: 0.336425\n",
            "Epoch: 3606 \tTraining Loss: 0.336403\n",
            "Epoch: 3607 \tTraining Loss: 0.336381\n",
            "Epoch: 3608 \tTraining Loss: 0.336359\n",
            "Epoch: 3609 \tTraining Loss: 0.336337\n",
            "Epoch: 3610 \tTraining Loss: 0.336315\n",
            "Epoch: 3611 \tTraining Loss: 0.336293\n",
            "Epoch: 3612 \tTraining Loss: 0.336271\n",
            "Epoch: 3613 \tTraining Loss: 0.336249\n",
            "Epoch: 3614 \tTraining Loss: 0.336227\n",
            "Epoch: 3615 \tTraining Loss: 0.336205\n",
            "Epoch: 3616 \tTraining Loss: 0.336183\n",
            "Epoch: 3617 \tTraining Loss: 0.336161\n",
            "Epoch: 3618 \tTraining Loss: 0.336139\n",
            "Epoch: 3619 \tTraining Loss: 0.336117\n",
            "Epoch: 3620 \tTraining Loss: 0.336095\n",
            "Epoch: 3621 \tTraining Loss: 0.336072\n",
            "Epoch: 3622 \tTraining Loss: 0.336050\n",
            "Epoch: 3623 \tTraining Loss: 0.336028\n",
            "Epoch: 3624 \tTraining Loss: 0.336006\n",
            "Epoch: 3625 \tTraining Loss: 0.335985\n",
            "Epoch: 3626 \tTraining Loss: 0.335963\n",
            "Epoch: 3627 \tTraining Loss: 0.335941\n",
            "Epoch: 3628 \tTraining Loss: 0.335920\n",
            "Epoch: 3629 \tTraining Loss: 0.335898\n",
            "Epoch: 3630 \tTraining Loss: 0.335877\n",
            "Epoch: 3631 \tTraining Loss: 0.335856\n",
            "Epoch: 3632 \tTraining Loss: 0.335836\n",
            "Epoch: 3633 \tTraining Loss: 0.335816\n",
            "Epoch: 3634 \tTraining Loss: 0.335799\n",
            "Epoch: 3635 \tTraining Loss: 0.335779\n",
            "Epoch: 3636 \tTraining Loss: 0.335765\n",
            "Epoch: 3637 \tTraining Loss: 0.335746\n",
            "Epoch: 3638 \tTraining Loss: 0.335735\n",
            "Epoch: 3639 \tTraining Loss: 0.335713\n",
            "Epoch: 3640 \tTraining Loss: 0.335701\n",
            "Epoch: 3641 \tTraining Loss: 0.335671\n",
            "Epoch: 3642 \tTraining Loss: 0.335650\n",
            "Epoch: 3643 \tTraining Loss: 0.335616\n",
            "Epoch: 3644 \tTraining Loss: 0.335588\n",
            "Epoch: 3645 \tTraining Loss: 0.335555\n",
            "Epoch: 3646 \tTraining Loss: 0.335528\n",
            "Epoch: 3647 \tTraining Loss: 0.335501\n",
            "Epoch: 3648 \tTraining Loss: 0.335477\n",
            "Epoch: 3649 \tTraining Loss: 0.335453\n",
            "Epoch: 3650 \tTraining Loss: 0.335430\n",
            "Epoch: 3651 \tTraining Loss: 0.335408\n",
            "Epoch: 3652 \tTraining Loss: 0.335385\n",
            "Epoch: 3653 \tTraining Loss: 0.335363\n",
            "Epoch: 3654 \tTraining Loss: 0.335342\n",
            "Epoch: 3655 \tTraining Loss: 0.335320\n",
            "Epoch: 3656 \tTraining Loss: 0.335299\n",
            "Epoch: 3657 \tTraining Loss: 0.335278\n",
            "Epoch: 3658 \tTraining Loss: 0.335257\n",
            "Epoch: 3659 \tTraining Loss: 0.335238\n",
            "Epoch: 3660 \tTraining Loss: 0.335218\n",
            "Epoch: 3661 \tTraining Loss: 0.335202\n",
            "Epoch: 3662 \tTraining Loss: 0.335185\n",
            "Epoch: 3663 \tTraining Loss: 0.335175\n",
            "Epoch: 3664 \tTraining Loss: 0.335160\n",
            "Epoch: 3665 \tTraining Loss: 0.335160\n",
            "Epoch: 3666 \tTraining Loss: 0.335140\n",
            "Epoch: 3667 \tTraining Loss: 0.335138\n",
            "Epoch: 3668 \tTraining Loss: 0.335104\n",
            "Epoch: 3669 \tTraining Loss: 0.335082\n",
            "Epoch: 3670 \tTraining Loss: 0.335038\n",
            "Epoch: 3671 \tTraining Loss: 0.335001\n",
            "Epoch: 3672 \tTraining Loss: 0.334960\n",
            "Epoch: 3673 \tTraining Loss: 0.334929\n",
            "Epoch: 3674 \tTraining Loss: 0.334900\n",
            "Epoch: 3675 \tTraining Loss: 0.334874\n",
            "Epoch: 3676 \tTraining Loss: 0.334851\n",
            "Epoch: 3677 \tTraining Loss: 0.334829\n",
            "Epoch: 3678 \tTraining Loss: 0.334809\n",
            "Epoch: 3679 \tTraining Loss: 0.334789\n",
            "Epoch: 3680 \tTraining Loss: 0.334772\n",
            "Epoch: 3681 \tTraining Loss: 0.334755\n",
            "Epoch: 3682 \tTraining Loss: 0.334743\n",
            "Epoch: 3683 \tTraining Loss: 0.334729\n",
            "Epoch: 3684 \tTraining Loss: 0.334723\n",
            "Epoch: 3685 \tTraining Loss: 0.334705\n",
            "Epoch: 3686 \tTraining Loss: 0.334702\n",
            "Epoch: 3687 \tTraining Loss: 0.334676\n",
            "Epoch: 3688 \tTraining Loss: 0.334666\n",
            "Epoch: 3689 \tTraining Loss: 0.334632\n",
            "Epoch: 3690 \tTraining Loss: 0.334610\n",
            "Epoch: 3691 \tTraining Loss: 0.334572\n",
            "Epoch: 3692 \tTraining Loss: 0.334545\n",
            "Epoch: 3693 \tTraining Loss: 0.334508\n",
            "Epoch: 3694 \tTraining Loss: 0.334480\n",
            "Epoch: 3695 \tTraining Loss: 0.334447\n",
            "Epoch: 3696 \tTraining Loss: 0.334421\n",
            "Epoch: 3697 \tTraining Loss: 0.334393\n",
            "Epoch: 3698 \tTraining Loss: 0.334368\n",
            "Epoch: 3699 \tTraining Loss: 0.334344\n",
            "Epoch: 3700 \tTraining Loss: 0.334321\n",
            "Epoch: 3701 \tTraining Loss: 0.334298\n",
            "Epoch: 3702 \tTraining Loss: 0.334278\n",
            "Epoch: 3703 \tTraining Loss: 0.334256\n",
            "Epoch: 3704 \tTraining Loss: 0.334238\n",
            "Epoch: 3705 \tTraining Loss: 0.334218\n",
            "Epoch: 3706 \tTraining Loss: 0.334203\n",
            "Epoch: 3707 \tTraining Loss: 0.334187\n",
            "Epoch: 3708 \tTraining Loss: 0.334179\n",
            "Epoch: 3709 \tTraining Loss: 0.334165\n",
            "Epoch: 3710 \tTraining Loss: 0.334168\n",
            "Epoch: 3711 \tTraining Loss: 0.334153\n",
            "Epoch: 3712 \tTraining Loss: 0.334161\n",
            "Epoch: 3713 \tTraining Loss: 0.334144\n",
            "Epoch: 3714 \tTraining Loss: 0.334149\n",
            "Epoch: 3715 \tTraining Loss: 0.334133\n",
            "Epoch: 3716 \tTraining Loss: 0.334134\n",
            "Epoch: 3717 \tTraining Loss: 0.334119\n",
            "Epoch: 3718 \tTraining Loss: 0.334123\n",
            "Epoch: 3719 \tTraining Loss: 0.334109\n",
            "Epoch: 3720 \tTraining Loss: 0.334116\n",
            "Epoch: 3721 \tTraining Loss: 0.334114\n",
            "Epoch: 3722 \tTraining Loss: 0.334128\n",
            "Epoch: 3723 \tTraining Loss: 0.334168\n",
            "Epoch: 3724 \tTraining Loss: 0.334226\n",
            "Epoch: 3725 \tTraining Loss: 0.334396\n",
            "Epoch: 3726 \tTraining Loss: 0.334567\n",
            "Epoch: 3727 \tTraining Loss: 0.335055\n",
            "Epoch: 3728 \tTraining Loss: 0.334786\n",
            "Epoch: 3729 \tTraining Loss: 0.334618\n",
            "Epoch: 3730 \tTraining Loss: 0.334150\n",
            "Epoch: 3731 \tTraining Loss: 0.333831\n",
            "Epoch: 3732 \tTraining Loss: 0.333624\n",
            "Epoch: 3733 \tTraining Loss: 0.333667\n",
            "Epoch: 3734 \tTraining Loss: 0.333892\n",
            "Epoch: 3735 \tTraining Loss: 0.334075\n",
            "Epoch: 3736 \tTraining Loss: 0.334402\n",
            "Epoch: 3737 \tTraining Loss: 0.334723\n",
            "Epoch: 3738 \tTraining Loss: 0.334768\n",
            "Epoch: 3739 \tTraining Loss: 0.334297\n",
            "Epoch: 3740 \tTraining Loss: 0.333997\n",
            "Epoch: 3741 \tTraining Loss: 0.333527\n",
            "Epoch: 3742 \tTraining Loss: 0.333584\n",
            "Epoch: 3743 \tTraining Loss: 0.334035\n",
            "Epoch: 3744 \tTraining Loss: 0.333942\n",
            "Epoch: 3745 \tTraining Loss: 0.334025\n",
            "Epoch: 3746 \tTraining Loss: 0.334289\n",
            "Epoch: 3747 \tTraining Loss: 0.333669\n",
            "Epoch: 3748 \tTraining Loss: 0.333376\n",
            "Epoch: 3749 \tTraining Loss: 0.333601\n",
            "Epoch: 3750 \tTraining Loss: 0.333519\n",
            "Epoch: 3751 \tTraining Loss: 0.333602\n",
            "Epoch: 3752 \tTraining Loss: 0.333510\n",
            "Epoch: 3753 \tTraining Loss: 0.333218\n",
            "Epoch: 3754 \tTraining Loss: 0.333353\n",
            "Epoch: 3755 \tTraining Loss: 0.333375\n",
            "Epoch: 3756 \tTraining Loss: 0.333152\n",
            "Epoch: 3757 \tTraining Loss: 0.333355\n",
            "Epoch: 3758 \tTraining Loss: 0.333589\n",
            "Epoch: 3759 \tTraining Loss: 0.333122\n",
            "Epoch: 3760 \tTraining Loss: 0.333437\n",
            "Epoch: 3761 \tTraining Loss: 0.333252\n",
            "Epoch: 3762 \tTraining Loss: 0.333137\n",
            "Epoch: 3763 \tTraining Loss: 0.333289\n",
            "Epoch: 3764 \tTraining Loss: 0.333105\n",
            "Epoch: 3765 \tTraining Loss: 0.333110\n",
            "Epoch: 3766 \tTraining Loss: 0.333112\n",
            "Epoch: 3767 \tTraining Loss: 0.332925\n",
            "Epoch: 3768 \tTraining Loss: 0.332912\n",
            "Epoch: 3769 \tTraining Loss: 0.332957\n",
            "Epoch: 3770 \tTraining Loss: 0.332956\n",
            "Epoch: 3771 \tTraining Loss: 0.332864\n",
            "Epoch: 3772 \tTraining Loss: 0.332913\n",
            "Epoch: 3773 \tTraining Loss: 0.332753\n",
            "Epoch: 3774 \tTraining Loss: 0.332747\n",
            "Epoch: 3775 \tTraining Loss: 0.332823\n",
            "Epoch: 3776 \tTraining Loss: 0.332750\n",
            "Epoch: 3777 \tTraining Loss: 0.332665\n",
            "Epoch: 3778 \tTraining Loss: 0.332664\n",
            "Epoch: 3779 \tTraining Loss: 0.332585\n",
            "Epoch: 3780 \tTraining Loss: 0.332575\n",
            "Epoch: 3781 \tTraining Loss: 0.332599\n",
            "Epoch: 3782 \tTraining Loss: 0.332559\n",
            "Epoch: 3783 \tTraining Loss: 0.332486\n",
            "Epoch: 3784 \tTraining Loss: 0.332508\n",
            "Epoch: 3785 \tTraining Loss: 0.332492\n",
            "Epoch: 3786 \tTraining Loss: 0.332453\n",
            "Epoch: 3787 \tTraining Loss: 0.332476\n",
            "Epoch: 3788 \tTraining Loss: 0.332401\n",
            "Epoch: 3789 \tTraining Loss: 0.332331\n",
            "Epoch: 3790 \tTraining Loss: 0.332361\n",
            "Epoch: 3791 \tTraining Loss: 0.332337\n",
            "Epoch: 3792 \tTraining Loss: 0.332273\n",
            "Epoch: 3793 \tTraining Loss: 0.332289\n",
            "Epoch: 3794 \tTraining Loss: 0.332256\n",
            "Epoch: 3795 \tTraining Loss: 0.332184\n",
            "Epoch: 3796 \tTraining Loss: 0.332204\n",
            "Epoch: 3797 \tTraining Loss: 0.332216\n",
            "Epoch: 3798 \tTraining Loss: 0.332127\n",
            "Epoch: 3799 \tTraining Loss: 0.332158\n",
            "Epoch: 3800 \tTraining Loss: 0.332121\n",
            "Epoch: 3801 \tTraining Loss: 0.332049\n",
            "Epoch: 3802 \tTraining Loss: 0.332078\n",
            "Epoch: 3803 \tTraining Loss: 0.332066\n",
            "Epoch: 3804 \tTraining Loss: 0.331974\n",
            "Epoch: 3805 \tTraining Loss: 0.331999\n",
            "Epoch: 3806 \tTraining Loss: 0.331993\n",
            "Epoch: 3807 \tTraining Loss: 0.331903\n",
            "Epoch: 3808 \tTraining Loss: 0.331921\n",
            "Epoch: 3809 \tTraining Loss: 0.331936\n",
            "Epoch: 3810 \tTraining Loss: 0.331836\n",
            "Epoch: 3811 \tTraining Loss: 0.331857\n",
            "Epoch: 3812 \tTraining Loss: 0.331856\n",
            "Epoch: 3813 \tTraining Loss: 0.331765\n",
            "Epoch: 3814 \tTraining Loss: 0.331786\n",
            "Epoch: 3815 \tTraining Loss: 0.331793\n",
            "Epoch: 3816 \tTraining Loss: 0.331696\n",
            "Epoch: 3817 \tTraining Loss: 0.331711\n",
            "Epoch: 3818 \tTraining Loss: 0.331729\n",
            "Epoch: 3819 \tTraining Loss: 0.331627\n",
            "Epoch: 3820 \tTraining Loss: 0.331636\n",
            "Epoch: 3821 \tTraining Loss: 0.331653\n",
            "Epoch: 3822 \tTraining Loss: 0.331558\n",
            "Epoch: 3823 \tTraining Loss: 0.331559\n",
            "Epoch: 3824 \tTraining Loss: 0.331576\n",
            "Epoch: 3825 \tTraining Loss: 0.331489\n",
            "Epoch: 3826 \tTraining Loss: 0.331485\n",
            "Epoch: 3827 \tTraining Loss: 0.331502\n",
            "Epoch: 3828 \tTraining Loss: 0.331421\n",
            "Epoch: 3829 \tTraining Loss: 0.331410\n",
            "Epoch: 3830 \tTraining Loss: 0.331424\n",
            "Epoch: 3831 \tTraining Loss: 0.331352\n",
            "Epoch: 3832 \tTraining Loss: 0.331333\n",
            "Epoch: 3833 \tTraining Loss: 0.331341\n",
            "Epoch: 3834 \tTraining Loss: 0.331283\n",
            "Epoch: 3835 \tTraining Loss: 0.331255\n",
            "Epoch: 3836 \tTraining Loss: 0.331256\n",
            "Epoch: 3837 \tTraining Loss: 0.331213\n",
            "Epoch: 3838 \tTraining Loss: 0.331180\n",
            "Epoch: 3839 \tTraining Loss: 0.331174\n",
            "Epoch: 3840 \tTraining Loss: 0.331142\n",
            "Epoch: 3841 \tTraining Loss: 0.331108\n",
            "Epoch: 3842 \tTraining Loss: 0.331093\n",
            "Epoch: 3843 \tTraining Loss: 0.331071\n",
            "Epoch: 3844 \tTraining Loss: 0.331039\n",
            "Epoch: 3845 \tTraining Loss: 0.331017\n",
            "Epoch: 3846 \tTraining Loss: 0.330998\n",
            "Epoch: 3847 \tTraining Loss: 0.330970\n",
            "Epoch: 3848 \tTraining Loss: 0.330944\n",
            "Epoch: 3849 \tTraining Loss: 0.330924\n",
            "Epoch: 3850 \tTraining Loss: 0.330901\n",
            "Epoch: 3851 \tTraining Loss: 0.330873\n",
            "Epoch: 3852 \tTraining Loss: 0.330851\n",
            "Epoch: 3853 \tTraining Loss: 0.330830\n",
            "Epoch: 3854 \tTraining Loss: 0.330803\n",
            "Epoch: 3855 \tTraining Loss: 0.330778\n",
            "Epoch: 3856 \tTraining Loss: 0.330757\n",
            "Epoch: 3857 \tTraining Loss: 0.330732\n",
            "Epoch: 3858 \tTraining Loss: 0.330706\n",
            "Epoch: 3859 \tTraining Loss: 0.330684\n",
            "Epoch: 3860 \tTraining Loss: 0.330661\n",
            "Epoch: 3861 \tTraining Loss: 0.330635\n",
            "Epoch: 3862 \tTraining Loss: 0.330611\n",
            "Epoch: 3863 \tTraining Loss: 0.330588\n",
            "Epoch: 3864 \tTraining Loss: 0.330565\n",
            "Epoch: 3865 \tTraining Loss: 0.330542\n",
            "Epoch: 3866 \tTraining Loss: 0.330518\n",
            "Epoch: 3867 \tTraining Loss: 0.330495\n",
            "Epoch: 3868 \tTraining Loss: 0.330471\n",
            "Epoch: 3869 \tTraining Loss: 0.330447\n",
            "Epoch: 3870 \tTraining Loss: 0.330425\n",
            "Epoch: 3871 \tTraining Loss: 0.330401\n",
            "Epoch: 3872 \tTraining Loss: 0.330377\n",
            "Epoch: 3873 \tTraining Loss: 0.330354\n",
            "Epoch: 3874 \tTraining Loss: 0.330330\n",
            "Epoch: 3875 \tTraining Loss: 0.330306\n",
            "Epoch: 3876 \tTraining Loss: 0.330282\n",
            "Epoch: 3877 \tTraining Loss: 0.330259\n",
            "Epoch: 3878 \tTraining Loss: 0.330236\n",
            "Epoch: 3879 \tTraining Loss: 0.330212\n",
            "Epoch: 3880 \tTraining Loss: 0.330188\n",
            "Epoch: 3881 \tTraining Loss: 0.330165\n",
            "Epoch: 3882 \tTraining Loss: 0.330141\n",
            "Epoch: 3883 \tTraining Loss: 0.330118\n",
            "Epoch: 3884 \tTraining Loss: 0.330095\n",
            "Epoch: 3885 \tTraining Loss: 0.330071\n",
            "Epoch: 3886 \tTraining Loss: 0.330048\n",
            "Epoch: 3887 \tTraining Loss: 0.330025\n",
            "Epoch: 3888 \tTraining Loss: 0.330003\n",
            "Epoch: 3889 \tTraining Loss: 0.329980\n",
            "Epoch: 3890 \tTraining Loss: 0.329958\n",
            "Epoch: 3891 \tTraining Loss: 0.329937\n",
            "Epoch: 3892 \tTraining Loss: 0.329917\n",
            "Epoch: 3893 \tTraining Loss: 0.329898\n",
            "Epoch: 3894 \tTraining Loss: 0.329882\n",
            "Epoch: 3895 \tTraining Loss: 0.329869\n",
            "Epoch: 3896 \tTraining Loss: 0.329861\n",
            "Epoch: 3897 \tTraining Loss: 0.329862\n",
            "Epoch: 3898 \tTraining Loss: 0.329881\n",
            "Epoch: 3899 \tTraining Loss: 0.329926\n",
            "Epoch: 3900 \tTraining Loss: 0.330037\n",
            "Epoch: 3901 \tTraining Loss: 0.330314\n",
            "Epoch: 3902 \tTraining Loss: 0.330994\n",
            "Epoch: 3903 \tTraining Loss: 0.332893\n",
            "Epoch: 3904 \tTraining Loss: 0.331750\n",
            "Epoch: 3905 \tTraining Loss: 0.330739\n",
            "Epoch: 3906 \tTraining Loss: 0.329807\n",
            "Epoch: 3907 \tTraining Loss: 0.330644\n",
            "Epoch: 3908 \tTraining Loss: 0.331149\n",
            "Epoch: 3909 \tTraining Loss: 0.331462\n",
            "Epoch: 3910 \tTraining Loss: 0.332493\n",
            "Epoch: 3911 \tTraining Loss: 0.330038\n",
            "Epoch: 3912 \tTraining Loss: 0.333089\n",
            "Epoch: 3913 \tTraining Loss: 0.332380\n",
            "Epoch: 3914 \tTraining Loss: 0.331754\n",
            "Epoch: 3915 \tTraining Loss: 0.331250\n",
            "Epoch: 3916 \tTraining Loss: 0.330229\n",
            "Epoch: 3917 \tTraining Loss: 0.331286\n",
            "Epoch: 3918 \tTraining Loss: 0.331363\n",
            "Epoch: 3919 \tTraining Loss: 0.331002\n",
            "Epoch: 3920 \tTraining Loss: 0.330952\n",
            "Epoch: 3921 \tTraining Loss: 0.330712\n",
            "Epoch: 3922 \tTraining Loss: 0.329722\n",
            "Epoch: 3923 \tTraining Loss: 0.330660\n",
            "Epoch: 3924 \tTraining Loss: 0.330553\n",
            "Epoch: 3925 \tTraining Loss: 0.330457\n",
            "Epoch: 3926 \tTraining Loss: 0.330921\n",
            "Epoch: 3927 \tTraining Loss: 0.330959\n",
            "Epoch: 3928 \tTraining Loss: 0.329892\n",
            "Epoch: 3929 \tTraining Loss: 0.330063\n",
            "Epoch: 3930 \tTraining Loss: 0.330003\n",
            "Epoch: 3931 \tTraining Loss: 0.329586\n",
            "Epoch: 3932 \tTraining Loss: 0.330430\n",
            "Epoch: 3933 \tTraining Loss: 0.330113\n",
            "Epoch: 3934 \tTraining Loss: 0.329476\n",
            "Epoch: 3935 \tTraining Loss: 0.329781\n",
            "Epoch: 3936 \tTraining Loss: 0.329479\n",
            "Epoch: 3937 \tTraining Loss: 0.329091\n",
            "Epoch: 3938 \tTraining Loss: 0.329658\n",
            "Epoch: 3939 \tTraining Loss: 0.329596\n",
            "Epoch: 3940 \tTraining Loss: 0.329381\n",
            "Epoch: 3941 \tTraining Loss: 0.329589\n",
            "Epoch: 3942 \tTraining Loss: 0.329271\n",
            "Epoch: 3943 \tTraining Loss: 0.329077\n",
            "Epoch: 3944 \tTraining Loss: 0.329461\n",
            "Epoch: 3945 \tTraining Loss: 0.329150\n",
            "Epoch: 3946 \tTraining Loss: 0.329064\n",
            "Epoch: 3947 \tTraining Loss: 0.329138\n",
            "Epoch: 3948 \tTraining Loss: 0.328912\n",
            "Epoch: 3949 \tTraining Loss: 0.328856\n",
            "Epoch: 3950 \tTraining Loss: 0.328897\n",
            "Epoch: 3951 \tTraining Loss: 0.328791\n",
            "Epoch: 3952 \tTraining Loss: 0.328678\n",
            "Epoch: 3953 \tTraining Loss: 0.328767\n",
            "Epoch: 3954 \tTraining Loss: 0.328619\n",
            "Epoch: 3955 \tTraining Loss: 0.328608\n",
            "Epoch: 3956 \tTraining Loss: 0.328658\n",
            "Epoch: 3957 \tTraining Loss: 0.328491\n",
            "Epoch: 3958 \tTraining Loss: 0.328518\n",
            "Epoch: 3959 \tTraining Loss: 0.328488\n",
            "Epoch: 3960 \tTraining Loss: 0.328410\n",
            "Epoch: 3961 \tTraining Loss: 0.328434\n",
            "Epoch: 3962 \tTraining Loss: 0.328388\n",
            "Epoch: 3963 \tTraining Loss: 0.328325\n",
            "Epoch: 3964 \tTraining Loss: 0.328320\n",
            "Epoch: 3965 \tTraining Loss: 0.328290\n",
            "Epoch: 3966 \tTraining Loss: 0.328244\n",
            "Epoch: 3967 \tTraining Loss: 0.328250\n",
            "Epoch: 3968 \tTraining Loss: 0.328203\n",
            "Epoch: 3969 \tTraining Loss: 0.328168\n",
            "Epoch: 3970 \tTraining Loss: 0.328165\n",
            "Epoch: 3971 \tTraining Loss: 0.328115\n",
            "Epoch: 3972 \tTraining Loss: 0.328101\n",
            "Epoch: 3973 \tTraining Loss: 0.328086\n",
            "Epoch: 3974 \tTraining Loss: 0.328046\n",
            "Epoch: 3975 \tTraining Loss: 0.328028\n",
            "Epoch: 3976 \tTraining Loss: 0.328004\n",
            "Epoch: 3977 \tTraining Loss: 0.327971\n",
            "Epoch: 3978 \tTraining Loss: 0.327954\n",
            "Epoch: 3979 \tTraining Loss: 0.327933\n",
            "Epoch: 3980 \tTraining Loss: 0.327900\n",
            "Epoch: 3981 \tTraining Loss: 0.327883\n",
            "Epoch: 3982 \tTraining Loss: 0.327857\n",
            "Epoch: 3983 \tTraining Loss: 0.327827\n",
            "Epoch: 3984 \tTraining Loss: 0.327811\n",
            "Epoch: 3985 \tTraining Loss: 0.327784\n",
            "Epoch: 3986 \tTraining Loss: 0.327759\n",
            "Epoch: 3987 \tTraining Loss: 0.327739\n",
            "Epoch: 3988 \tTraining Loss: 0.327713\n",
            "Epoch: 3989 \tTraining Loss: 0.327688\n",
            "Epoch: 3990 \tTraining Loss: 0.327667\n",
            "Epoch: 3991 \tTraining Loss: 0.327642\n",
            "Epoch: 3992 \tTraining Loss: 0.327618\n",
            "Epoch: 3993 \tTraining Loss: 0.327597\n",
            "Epoch: 3994 \tTraining Loss: 0.327572\n",
            "Epoch: 3995 \tTraining Loss: 0.327548\n",
            "Epoch: 3996 \tTraining Loss: 0.327526\n",
            "Epoch: 3997 \tTraining Loss: 0.327501\n",
            "Epoch: 3998 \tTraining Loss: 0.327478\n",
            "Epoch: 3999 \tTraining Loss: 0.327455\n",
            "Epoch: 4000 \tTraining Loss: 0.327432\n",
            "Epoch: 4001 \tTraining Loss: 0.327408\n",
            "Epoch: 4002 \tTraining Loss: 0.327385\n",
            "Epoch: 4003 \tTraining Loss: 0.327361\n",
            "Epoch: 4004 \tTraining Loss: 0.327338\n",
            "Epoch: 4005 \tTraining Loss: 0.327315\n",
            "Epoch: 4006 \tTraining Loss: 0.327291\n",
            "Epoch: 4007 \tTraining Loss: 0.327268\n",
            "Epoch: 4008 \tTraining Loss: 0.327245\n",
            "Epoch: 4009 \tTraining Loss: 0.327221\n",
            "Epoch: 4010 \tTraining Loss: 0.327198\n",
            "Epoch: 4011 \tTraining Loss: 0.327175\n",
            "Epoch: 4012 \tTraining Loss: 0.327151\n",
            "Epoch: 4013 \tTraining Loss: 0.327127\n",
            "Epoch: 4014 \tTraining Loss: 0.327104\n",
            "Epoch: 4015 \tTraining Loss: 0.327080\n",
            "Epoch: 4016 \tTraining Loss: 0.327057\n",
            "Epoch: 4017 \tTraining Loss: 0.327033\n",
            "Epoch: 4018 \tTraining Loss: 0.327008\n",
            "Epoch: 4019 \tTraining Loss: 0.326983\n",
            "Epoch: 4020 \tTraining Loss: 0.326959\n",
            "Epoch: 4021 \tTraining Loss: 0.326938\n",
            "Epoch: 4022 \tTraining Loss: 0.326931\n",
            "Epoch: 4023 \tTraining Loss: 0.326917\n",
            "Epoch: 4024 \tTraining Loss: 0.326897\n",
            "Epoch: 4025 \tTraining Loss: 0.326889\n",
            "Epoch: 4026 \tTraining Loss: 0.326883\n",
            "Epoch: 4027 \tTraining Loss: 0.326858\n",
            "Epoch: 4028 \tTraining Loss: 0.326842\n",
            "Epoch: 4029 \tTraining Loss: 0.326845\n",
            "Epoch: 4030 \tTraining Loss: 0.326846\n",
            "Epoch: 4031 \tTraining Loss: 0.326822\n",
            "Epoch: 4032 \tTraining Loss: 0.326815\n",
            "Epoch: 4033 \tTraining Loss: 0.326818\n",
            "Epoch: 4034 \tTraining Loss: 0.326814\n",
            "Epoch: 4035 \tTraining Loss: 0.326834\n",
            "Epoch: 4036 \tTraining Loss: 0.326891\n",
            "Epoch: 4037 \tTraining Loss: 0.327020\n",
            "Epoch: 4038 \tTraining Loss: 0.327202\n",
            "Epoch: 4039 \tTraining Loss: 0.327690\n",
            "Epoch: 4040 \tTraining Loss: 0.328511\n",
            "Epoch: 4041 \tTraining Loss: 0.329904\n",
            "Epoch: 4042 \tTraining Loss: 0.328427\n",
            "Epoch: 4043 \tTraining Loss: 0.327442\n",
            "Epoch: 4044 \tTraining Loss: 0.326660\n",
            "Epoch: 4045 \tTraining Loss: 0.327358\n",
            "Epoch: 4046 \tTraining Loss: 0.328198\n",
            "Epoch: 4047 \tTraining Loss: 0.328383\n",
            "Epoch: 4048 \tTraining Loss: 0.329311\n",
            "Epoch: 4049 \tTraining Loss: 0.326874\n",
            "Epoch: 4050 \tTraining Loss: 0.327164\n",
            "Epoch: 4051 \tTraining Loss: 0.329422\n",
            "Epoch: 4052 \tTraining Loss: 0.328905\n",
            "Epoch: 4053 \tTraining Loss: 0.328748\n",
            "Epoch: 4054 \tTraining Loss: 0.326742\n",
            "Epoch: 4055 \tTraining Loss: 0.326982\n",
            "Epoch: 4056 \tTraining Loss: 0.327110\n",
            "Epoch: 4057 \tTraining Loss: 0.327692\n",
            "Epoch: 4058 \tTraining Loss: 0.327847\n",
            "Epoch: 4059 \tTraining Loss: 0.327175\n",
            "Epoch: 4060 \tTraining Loss: 0.326586\n",
            "Epoch: 4061 \tTraining Loss: 0.326586\n",
            "Epoch: 4062 \tTraining Loss: 0.327256\n",
            "Epoch: 4063 \tTraining Loss: 0.327175\n",
            "Epoch: 4064 \tTraining Loss: 0.326901\n",
            "Epoch: 4065 \tTraining Loss: 0.326905\n",
            "Epoch: 4066 \tTraining Loss: 0.326269\n",
            "Epoch: 4067 \tTraining Loss: 0.326108\n",
            "Epoch: 4068 \tTraining Loss: 0.326218\n",
            "Epoch: 4069 \tTraining Loss: 0.326098\n",
            "Epoch: 4070 \tTraining Loss: 0.326238\n",
            "Epoch: 4071 \tTraining Loss: 0.325872\n",
            "Epoch: 4072 \tTraining Loss: 0.325892\n",
            "Epoch: 4073 \tTraining Loss: 0.325867\n",
            "Epoch: 4074 \tTraining Loss: 0.325920\n",
            "Epoch: 4075 \tTraining Loss: 0.326115\n",
            "Epoch: 4076 \tTraining Loss: 0.325877\n",
            "Epoch: 4077 \tTraining Loss: 0.325804\n",
            "Epoch: 4078 \tTraining Loss: 0.325687\n",
            "Epoch: 4079 \tTraining Loss: 0.325644\n",
            "Epoch: 4080 \tTraining Loss: 0.325703\n",
            "Epoch: 4081 \tTraining Loss: 0.325707\n",
            "Epoch: 4082 \tTraining Loss: 0.325587\n",
            "Epoch: 4083 \tTraining Loss: 0.325529\n",
            "Epoch: 4084 \tTraining Loss: 0.325431\n",
            "Epoch: 4085 \tTraining Loss: 0.325461\n",
            "Epoch: 4086 \tTraining Loss: 0.325449\n",
            "Epoch: 4087 \tTraining Loss: 0.325387\n",
            "Epoch: 4088 \tTraining Loss: 0.325404\n",
            "Epoch: 4089 \tTraining Loss: 0.325268\n",
            "Epoch: 4090 \tTraining Loss: 0.325318\n",
            "Epoch: 4091 \tTraining Loss: 0.325295\n",
            "Epoch: 4092 \tTraining Loss: 0.325256\n",
            "Epoch: 4093 \tTraining Loss: 0.325274\n",
            "Epoch: 4094 \tTraining Loss: 0.325146\n",
            "Epoch: 4095 \tTraining Loss: 0.325137\n",
            "Epoch: 4096 \tTraining Loss: 0.325098\n",
            "Epoch: 4097 \tTraining Loss: 0.325055\n",
            "Epoch: 4098 \tTraining Loss: 0.325070\n",
            "Epoch: 4099 \tTraining Loss: 0.324998\n",
            "Epoch: 4100 \tTraining Loss: 0.324984\n",
            "Epoch: 4101 \tTraining Loss: 0.324951\n",
            "Epoch: 4102 \tTraining Loss: 0.324916\n",
            "Epoch: 4103 \tTraining Loss: 0.324921\n",
            "Epoch: 4104 \tTraining Loss: 0.324879\n",
            "Epoch: 4105 \tTraining Loss: 0.324849\n",
            "Epoch: 4106 \tTraining Loss: 0.324822\n",
            "Epoch: 4107 \tTraining Loss: 0.324780\n",
            "Epoch: 4108 \tTraining Loss: 0.324760\n",
            "Epoch: 4109 \tTraining Loss: 0.324740\n",
            "Epoch: 4110 \tTraining Loss: 0.324708\n",
            "Epoch: 4111 \tTraining Loss: 0.324692\n",
            "Epoch: 4112 \tTraining Loss: 0.324655\n",
            "Epoch: 4113 \tTraining Loss: 0.324627\n",
            "Epoch: 4114 \tTraining Loss: 0.324603\n",
            "Epoch: 4115 \tTraining Loss: 0.324571\n",
            "Epoch: 4116 \tTraining Loss: 0.324553\n",
            "Epoch: 4117 \tTraining Loss: 0.324527\n",
            "Epoch: 4118 \tTraining Loss: 0.324499\n",
            "Epoch: 4119 \tTraining Loss: 0.324476\n",
            "Epoch: 4120 \tTraining Loss: 0.324445\n",
            "Epoch: 4121 \tTraining Loss: 0.324419\n",
            "Epoch: 4122 \tTraining Loss: 0.324395\n",
            "Epoch: 4123 \tTraining Loss: 0.324368\n",
            "Epoch: 4124 \tTraining Loss: 0.324345\n",
            "Epoch: 4125 \tTraining Loss: 0.324320\n",
            "Epoch: 4126 \tTraining Loss: 0.324292\n",
            "Epoch: 4127 \tTraining Loss: 0.324268\n",
            "Epoch: 4128 \tTraining Loss: 0.324241\n",
            "Epoch: 4129 \tTraining Loss: 0.324214\n",
            "Epoch: 4130 \tTraining Loss: 0.324190\n",
            "Epoch: 4131 \tTraining Loss: 0.324164\n",
            "Epoch: 4132 \tTraining Loss: 0.324139\n",
            "Epoch: 4133 \tTraining Loss: 0.324114\n",
            "Epoch: 4134 \tTraining Loss: 0.324088\n",
            "Epoch: 4135 \tTraining Loss: 0.324063\n",
            "Epoch: 4136 \tTraining Loss: 0.324037\n",
            "Epoch: 4137 \tTraining Loss: 0.324011\n",
            "Epoch: 4138 \tTraining Loss: 0.323986\n",
            "Epoch: 4139 \tTraining Loss: 0.323960\n",
            "Epoch: 4140 \tTraining Loss: 0.323935\n",
            "Epoch: 4141 \tTraining Loss: 0.323910\n",
            "Epoch: 4142 \tTraining Loss: 0.323884\n",
            "Epoch: 4143 \tTraining Loss: 0.323859\n",
            "Epoch: 4144 \tTraining Loss: 0.323834\n",
            "Epoch: 4145 \tTraining Loss: 0.323808\n",
            "Epoch: 4146 \tTraining Loss: 0.323783\n",
            "Epoch: 4147 \tTraining Loss: 0.323757\n",
            "Epoch: 4148 \tTraining Loss: 0.323731\n",
            "Epoch: 4149 \tTraining Loss: 0.323706\n",
            "Epoch: 4150 \tTraining Loss: 0.323681\n",
            "Epoch: 4151 \tTraining Loss: 0.323655\n",
            "Epoch: 4152 \tTraining Loss: 0.323630\n",
            "Epoch: 4153 \tTraining Loss: 0.323604\n",
            "Epoch: 4154 \tTraining Loss: 0.323579\n",
            "Epoch: 4155 \tTraining Loss: 0.323553\n",
            "Epoch: 4156 \tTraining Loss: 0.323528\n",
            "Epoch: 4157 \tTraining Loss: 0.323502\n",
            "Epoch: 4158 \tTraining Loss: 0.323477\n",
            "Epoch: 4159 \tTraining Loss: 0.323451\n",
            "Epoch: 4160 \tTraining Loss: 0.323426\n",
            "Epoch: 4161 \tTraining Loss: 0.323401\n",
            "Epoch: 4162 \tTraining Loss: 0.323375\n",
            "Epoch: 4163 \tTraining Loss: 0.323350\n",
            "Epoch: 4164 \tTraining Loss: 0.323324\n",
            "Epoch: 4165 \tTraining Loss: 0.323299\n",
            "Epoch: 4166 \tTraining Loss: 0.323274\n",
            "Epoch: 4167 \tTraining Loss: 0.323248\n",
            "Epoch: 4168 \tTraining Loss: 0.323223\n",
            "Epoch: 4169 \tTraining Loss: 0.323198\n",
            "Epoch: 4170 \tTraining Loss: 0.323173\n",
            "Epoch: 4171 \tTraining Loss: 0.323148\n",
            "Epoch: 4172 \tTraining Loss: 0.323124\n",
            "Epoch: 4173 \tTraining Loss: 0.323100\n",
            "Epoch: 4174 \tTraining Loss: 0.323078\n",
            "Epoch: 4175 \tTraining Loss: 0.323057\n",
            "Epoch: 4176 \tTraining Loss: 0.323038\n",
            "Epoch: 4177 \tTraining Loss: 0.323024\n",
            "Epoch: 4178 \tTraining Loss: 0.323016\n",
            "Epoch: 4179 \tTraining Loss: 0.323019\n",
            "Epoch: 4180 \tTraining Loss: 0.323042\n",
            "Epoch: 4181 \tTraining Loss: 0.323089\n",
            "Epoch: 4182 \tTraining Loss: 0.323197\n",
            "Epoch: 4183 \tTraining Loss: 0.323376\n",
            "Epoch: 4184 \tTraining Loss: 0.323811\n",
            "Epoch: 4185 \tTraining Loss: 0.324688\n",
            "Epoch: 4186 \tTraining Loss: 0.327181\n",
            "Epoch: 4187 \tTraining Loss: 0.326880\n",
            "Epoch: 4188 \tTraining Loss: 0.327165\n",
            "Epoch: 4189 \tTraining Loss: 0.326666\n",
            "Epoch: 4190 \tTraining Loss: 0.332142\n",
            "Epoch: 4191 \tTraining Loss: 0.332701\n",
            "Epoch: 4192 \tTraining Loss: 0.347061\n",
            "Epoch: 4193 \tTraining Loss: 0.413928\n",
            "Epoch: 4194 \tTraining Loss: 0.592458\n",
            "Epoch: 4195 \tTraining Loss: 0.552811\n",
            "Epoch: 4196 \tTraining Loss: 0.481960\n",
            "Epoch: 4197 \tTraining Loss: 0.481488\n",
            "Epoch: 4198 \tTraining Loss: 0.496495\n",
            "Epoch: 4199 \tTraining Loss: 0.460360\n",
            "Epoch: 4200 \tTraining Loss: 0.451182\n",
            "Epoch: 4201 \tTraining Loss: 0.461380\n",
            "Epoch: 4202 \tTraining Loss: 0.433666\n",
            "Epoch: 4203 \tTraining Loss: 0.413194\n",
            "Epoch: 4204 \tTraining Loss: 0.444621\n",
            "Epoch: 4205 \tTraining Loss: 0.414929\n",
            "Epoch: 4206 \tTraining Loss: 0.376332\n",
            "Epoch: 4207 \tTraining Loss: 0.380030\n",
            "Epoch: 4208 \tTraining Loss: 0.376194\n",
            "Epoch: 4209 \tTraining Loss: 0.380368\n",
            "Epoch: 4210 \tTraining Loss: 0.379467\n",
            "Epoch: 4211 \tTraining Loss: 0.362892\n",
            "Epoch: 4212 \tTraining Loss: 0.360138\n",
            "Epoch: 4213 \tTraining Loss: 0.362170\n",
            "Epoch: 4214 \tTraining Loss: 0.359842\n",
            "Epoch: 4215 \tTraining Loss: 0.351323\n",
            "Epoch: 4216 \tTraining Loss: 0.364350\n",
            "Epoch: 4217 \tTraining Loss: 0.353969\n",
            "Epoch: 4218 \tTraining Loss: 0.350608\n",
            "Epoch: 4219 \tTraining Loss: 0.360186\n",
            "Epoch: 4220 \tTraining Loss: 0.346372\n",
            "Epoch: 4221 \tTraining Loss: 0.347221\n",
            "Epoch: 4222 \tTraining Loss: 0.349244\n",
            "Epoch: 4223 \tTraining Loss: 0.340472\n",
            "Epoch: 4224 \tTraining Loss: 0.353832\n",
            "Epoch: 4225 \tTraining Loss: 0.340945\n",
            "Epoch: 4226 \tTraining Loss: 0.337351\n",
            "Epoch: 4227 \tTraining Loss: 0.340562\n",
            "Epoch: 4228 \tTraining Loss: 0.336235\n",
            "Epoch: 4229 \tTraining Loss: 0.337701\n",
            "Epoch: 4230 \tTraining Loss: 0.337154\n",
            "Epoch: 4231 \tTraining Loss: 0.334889\n",
            "Epoch: 4232 \tTraining Loss: 0.331800\n",
            "Epoch: 4233 \tTraining Loss: 0.334257\n",
            "Epoch: 4234 \tTraining Loss: 0.331104\n",
            "Epoch: 4235 \tTraining Loss: 0.331052\n",
            "Epoch: 4236 \tTraining Loss: 0.332718\n",
            "Epoch: 4237 \tTraining Loss: 0.328704\n",
            "Epoch: 4238 \tTraining Loss: 0.328363\n",
            "Epoch: 4239 \tTraining Loss: 0.330703\n",
            "Epoch: 4240 \tTraining Loss: 0.329177\n",
            "Epoch: 4241 \tTraining Loss: 0.327015\n",
            "Epoch: 4242 \tTraining Loss: 0.329760\n",
            "Epoch: 4243 \tTraining Loss: 0.327851\n",
            "Epoch: 4244 \tTraining Loss: 0.326149\n",
            "Epoch: 4245 \tTraining Loss: 0.327678\n",
            "Epoch: 4246 \tTraining Loss: 0.327426\n",
            "Epoch: 4247 \tTraining Loss: 0.325944\n",
            "Epoch: 4248 \tTraining Loss: 0.326069\n",
            "Epoch: 4249 \tTraining Loss: 0.326246\n",
            "Epoch: 4250 \tTraining Loss: 0.325256\n",
            "Epoch: 4251 \tTraining Loss: 0.325061\n",
            "Epoch: 4252 \tTraining Loss: 0.325501\n",
            "Epoch: 4253 \tTraining Loss: 0.324828\n",
            "Epoch: 4254 \tTraining Loss: 0.324569\n",
            "Epoch: 4255 \tTraining Loss: 0.324556\n",
            "Epoch: 4256 \tTraining Loss: 0.324727\n",
            "Epoch: 4257 \tTraining Loss: 0.324519\n",
            "Epoch: 4258 \tTraining Loss: 0.324122\n",
            "Epoch: 4259 \tTraining Loss: 0.323922\n",
            "Epoch: 4260 \tTraining Loss: 0.324058\n",
            "Epoch: 4261 \tTraining Loss: 0.323897\n",
            "Epoch: 4262 \tTraining Loss: 0.323788\n",
            "Epoch: 4263 \tTraining Loss: 0.323699\n",
            "Epoch: 4264 \tTraining Loss: 0.323648\n",
            "Epoch: 4265 \tTraining Loss: 0.323483\n",
            "Epoch: 4266 \tTraining Loss: 0.323519\n",
            "Epoch: 4267 \tTraining Loss: 0.323416\n",
            "Epoch: 4268 \tTraining Loss: 0.323327\n",
            "Epoch: 4269 \tTraining Loss: 0.323251\n",
            "Epoch: 4270 \tTraining Loss: 0.323144\n",
            "Epoch: 4271 \tTraining Loss: 0.323038\n",
            "Epoch: 4272 \tTraining Loss: 0.323000\n",
            "Epoch: 4273 \tTraining Loss: 0.323023\n",
            "Epoch: 4274 \tTraining Loss: 0.322915\n",
            "Epoch: 4275 \tTraining Loss: 0.322837\n",
            "Epoch: 4276 \tTraining Loss: 0.322804\n",
            "Epoch: 4277 \tTraining Loss: 0.322785\n",
            "Epoch: 4278 \tTraining Loss: 0.322710\n",
            "Epoch: 4279 \tTraining Loss: 0.322669\n",
            "Epoch: 4280 \tTraining Loss: 0.322647\n",
            "Epoch: 4281 \tTraining Loss: 0.322604\n",
            "Epoch: 4282 \tTraining Loss: 0.322564\n",
            "Epoch: 4283 \tTraining Loss: 0.322531\n",
            "Epoch: 4284 \tTraining Loss: 0.322489\n",
            "Epoch: 4285 \tTraining Loss: 0.322451\n",
            "Epoch: 4286 \tTraining Loss: 0.322433\n",
            "Epoch: 4287 \tTraining Loss: 0.322405\n",
            "Epoch: 4288 \tTraining Loss: 0.322372\n",
            "Epoch: 4289 \tTraining Loss: 0.322340\n",
            "Epoch: 4290 \tTraining Loss: 0.322312\n",
            "Epoch: 4291 \tTraining Loss: 0.322288\n",
            "Epoch: 4292 \tTraining Loss: 0.322264\n",
            "Epoch: 4293 \tTraining Loss: 0.322235\n",
            "Epoch: 4294 \tTraining Loss: 0.322209\n",
            "Epoch: 4295 \tTraining Loss: 0.322186\n",
            "Epoch: 4296 \tTraining Loss: 0.322157\n",
            "Epoch: 4297 \tTraining Loss: 0.322131\n",
            "Epoch: 4298 \tTraining Loss: 0.322108\n",
            "Epoch: 4299 \tTraining Loss: 0.322086\n",
            "Epoch: 4300 \tTraining Loss: 0.322058\n",
            "Epoch: 4301 \tTraining Loss: 0.322037\n",
            "Epoch: 4302 \tTraining Loss: 0.322014\n",
            "Epoch: 4303 \tTraining Loss: 0.321988\n",
            "Epoch: 4304 \tTraining Loss: 0.321965\n",
            "Epoch: 4305 \tTraining Loss: 0.321943\n",
            "Epoch: 4306 \tTraining Loss: 0.321919\n",
            "Epoch: 4307 \tTraining Loss: 0.321896\n",
            "Epoch: 4308 \tTraining Loss: 0.321875\n",
            "Epoch: 4309 \tTraining Loss: 0.321853\n",
            "Epoch: 4310 \tTraining Loss: 0.321830\n",
            "Epoch: 4311 \tTraining Loss: 0.321808\n",
            "Epoch: 4312 \tTraining Loss: 0.321786\n",
            "Epoch: 4313 \tTraining Loss: 0.321764\n",
            "Epoch: 4314 \tTraining Loss: 0.321743\n",
            "Epoch: 4315 \tTraining Loss: 0.321721\n",
            "Epoch: 4316 \tTraining Loss: 0.321699\n",
            "Epoch: 4317 \tTraining Loss: 0.321676\n",
            "Epoch: 4318 \tTraining Loss: 0.321653\n",
            "Epoch: 4319 \tTraining Loss: 0.321629\n",
            "Epoch: 4320 \tTraining Loss: 0.321605\n",
            "Epoch: 4321 \tTraining Loss: 0.321580\n",
            "Epoch: 4322 \tTraining Loss: 0.321555\n",
            "Epoch: 4323 \tTraining Loss: 0.321533\n",
            "Epoch: 4324 \tTraining Loss: 0.321511\n",
            "Epoch: 4325 \tTraining Loss: 0.321490\n",
            "Epoch: 4326 \tTraining Loss: 0.321470\n",
            "Epoch: 4327 \tTraining Loss: 0.321450\n",
            "Epoch: 4328 \tTraining Loss: 0.321430\n",
            "Epoch: 4329 \tTraining Loss: 0.321410\n",
            "Epoch: 4330 \tTraining Loss: 0.321391\n",
            "Epoch: 4331 \tTraining Loss: 0.321371\n",
            "Epoch: 4332 \tTraining Loss: 0.321352\n",
            "Epoch: 4333 \tTraining Loss: 0.321333\n",
            "Epoch: 4334 \tTraining Loss: 0.321314\n",
            "Epoch: 4335 \tTraining Loss: 0.321295\n",
            "Epoch: 4336 \tTraining Loss: 0.321276\n",
            "Epoch: 4337 \tTraining Loss: 0.321257\n",
            "Epoch: 4338 \tTraining Loss: 0.321238\n",
            "Epoch: 4339 \tTraining Loss: 0.321219\n",
            "Epoch: 4340 \tTraining Loss: 0.321200\n",
            "Epoch: 4341 \tTraining Loss: 0.321181\n",
            "Epoch: 4342 \tTraining Loss: 0.321163\n",
            "Epoch: 4343 \tTraining Loss: 0.321144\n",
            "Epoch: 4344 \tTraining Loss: 0.321125\n",
            "Epoch: 4345 \tTraining Loss: 0.321107\n",
            "Epoch: 4346 \tTraining Loss: 0.321088\n",
            "Epoch: 4347 \tTraining Loss: 0.321070\n",
            "Epoch: 4348 \tTraining Loss: 0.321051\n",
            "Epoch: 4349 \tTraining Loss: 0.321033\n",
            "Epoch: 4350 \tTraining Loss: 0.321015\n",
            "Epoch: 4351 \tTraining Loss: 0.320996\n",
            "Epoch: 4352 \tTraining Loss: 0.320978\n",
            "Epoch: 4353 \tTraining Loss: 0.320960\n",
            "Epoch: 4354 \tTraining Loss: 0.320941\n",
            "Epoch: 4355 \tTraining Loss: 0.320923\n",
            "Epoch: 4356 \tTraining Loss: 0.320905\n",
            "Epoch: 4357 \tTraining Loss: 0.320887\n",
            "Epoch: 4358 \tTraining Loss: 0.320869\n",
            "Epoch: 4359 \tTraining Loss: 0.320850\n",
            "Epoch: 4360 \tTraining Loss: 0.320832\n",
            "Epoch: 4361 \tTraining Loss: 0.320814\n",
            "Epoch: 4362 \tTraining Loss: 0.320796\n",
            "Epoch: 4363 \tTraining Loss: 0.320778\n",
            "Epoch: 4364 \tTraining Loss: 0.320760\n",
            "Epoch: 4365 \tTraining Loss: 0.320742\n",
            "Epoch: 4366 \tTraining Loss: 0.320725\n",
            "Epoch: 4367 \tTraining Loss: 0.320707\n",
            "Epoch: 4368 \tTraining Loss: 0.320689\n",
            "Epoch: 4369 \tTraining Loss: 0.320671\n",
            "Epoch: 4370 \tTraining Loss: 0.320653\n",
            "Epoch: 4371 \tTraining Loss: 0.320635\n",
            "Epoch: 4372 \tTraining Loss: 0.320618\n",
            "Epoch: 4373 \tTraining Loss: 0.320600\n",
            "Epoch: 4374 \tTraining Loss: 0.320582\n",
            "Epoch: 4375 \tTraining Loss: 0.320564\n",
            "Epoch: 4376 \tTraining Loss: 0.320547\n",
            "Epoch: 4377 \tTraining Loss: 0.320529\n",
            "Epoch: 4378 \tTraining Loss: 0.320511\n",
            "Epoch: 4379 \tTraining Loss: 0.320494\n",
            "Epoch: 4380 \tTraining Loss: 0.320476\n",
            "Epoch: 4381 \tTraining Loss: 0.320458\n",
            "Epoch: 4382 \tTraining Loss: 0.320441\n",
            "Epoch: 4383 \tTraining Loss: 0.320423\n",
            "Epoch: 4384 \tTraining Loss: 0.320406\n",
            "Epoch: 4385 \tTraining Loss: 0.320388\n",
            "Epoch: 4386 \tTraining Loss: 0.320370\n",
            "Epoch: 4387 \tTraining Loss: 0.320353\n",
            "Epoch: 4388 \tTraining Loss: 0.320335\n",
            "Epoch: 4389 \tTraining Loss: 0.320318\n",
            "Epoch: 4390 \tTraining Loss: 0.320300\n",
            "Epoch: 4391 \tTraining Loss: 0.320283\n",
            "Epoch: 4392 \tTraining Loss: 0.320265\n",
            "Epoch: 4393 \tTraining Loss: 0.320248\n",
            "Epoch: 4394 \tTraining Loss: 0.320230\n",
            "Epoch: 4395 \tTraining Loss: 0.320213\n",
            "Epoch: 4396 \tTraining Loss: 0.320195\n",
            "Epoch: 4397 \tTraining Loss: 0.320178\n",
            "Epoch: 4398 \tTraining Loss: 0.320160\n",
            "Epoch: 4399 \tTraining Loss: 0.320143\n",
            "Epoch: 4400 \tTraining Loss: 0.320126\n",
            "Epoch: 4401 \tTraining Loss: 0.320108\n",
            "Epoch: 4402 \tTraining Loss: 0.320091\n",
            "Epoch: 4403 \tTraining Loss: 0.320073\n",
            "Epoch: 4404 \tTraining Loss: 0.320056\n",
            "Epoch: 4405 \tTraining Loss: 0.320039\n",
            "Epoch: 4406 \tTraining Loss: 0.320021\n",
            "Epoch: 4407 \tTraining Loss: 0.320004\n",
            "Epoch: 4408 \tTraining Loss: 0.319986\n",
            "Epoch: 4409 \tTraining Loss: 0.319969\n",
            "Epoch: 4410 \tTraining Loss: 0.319952\n",
            "Epoch: 4411 \tTraining Loss: 0.319934\n",
            "Epoch: 4412 \tTraining Loss: 0.319917\n",
            "Epoch: 4413 \tTraining Loss: 0.319899\n",
            "Epoch: 4414 \tTraining Loss: 0.319882\n",
            "Epoch: 4415 \tTraining Loss: 0.319865\n",
            "Epoch: 4416 \tTraining Loss: 0.319847\n",
            "Epoch: 4417 \tTraining Loss: 0.319830\n",
            "Epoch: 4418 \tTraining Loss: 0.319813\n",
            "Epoch: 4419 \tTraining Loss: 0.319795\n",
            "Epoch: 4420 \tTraining Loss: 0.319778\n",
            "Epoch: 4421 \tTraining Loss: 0.319761\n",
            "Epoch: 4422 \tTraining Loss: 0.319743\n",
            "Epoch: 4423 \tTraining Loss: 0.319726\n",
            "Epoch: 4424 \tTraining Loss: 0.319709\n",
            "Epoch: 4425 \tTraining Loss: 0.319691\n",
            "Epoch: 4426 \tTraining Loss: 0.319674\n",
            "Epoch: 4427 \tTraining Loss: 0.319657\n",
            "Epoch: 4428 \tTraining Loss: 0.319639\n",
            "Epoch: 4429 \tTraining Loss: 0.319622\n",
            "Epoch: 4430 \tTraining Loss: 0.319605\n",
            "Epoch: 4431 \tTraining Loss: 0.319588\n",
            "Epoch: 4432 \tTraining Loss: 0.319570\n",
            "Epoch: 4433 \tTraining Loss: 0.319553\n",
            "Epoch: 4434 \tTraining Loss: 0.319536\n",
            "Epoch: 4435 \tTraining Loss: 0.319518\n",
            "Epoch: 4436 \tTraining Loss: 0.319501\n",
            "Epoch: 4437 \tTraining Loss: 0.319484\n",
            "Epoch: 4438 \tTraining Loss: 0.319466\n",
            "Epoch: 4439 \tTraining Loss: 0.319449\n",
            "Epoch: 4440 \tTraining Loss: 0.319432\n",
            "Epoch: 4441 \tTraining Loss: 0.319415\n",
            "Epoch: 4442 \tTraining Loss: 0.319397\n",
            "Epoch: 4443 \tTraining Loss: 0.319380\n",
            "Epoch: 4444 \tTraining Loss: 0.319363\n",
            "Epoch: 4445 \tTraining Loss: 0.319345\n",
            "Epoch: 4446 \tTraining Loss: 0.319328\n",
            "Epoch: 4447 \tTraining Loss: 0.319311\n",
            "Epoch: 4448 \tTraining Loss: 0.319293\n",
            "Epoch: 4449 \tTraining Loss: 0.319276\n",
            "Epoch: 4450 \tTraining Loss: 0.319259\n",
            "Epoch: 4451 \tTraining Loss: 0.319242\n",
            "Epoch: 4452 \tTraining Loss: 0.319224\n",
            "Epoch: 4453 \tTraining Loss: 0.319207\n",
            "Epoch: 4454 \tTraining Loss: 0.319190\n",
            "Epoch: 4455 \tTraining Loss: 0.319172\n",
            "Epoch: 4456 \tTraining Loss: 0.319155\n",
            "Epoch: 4457 \tTraining Loss: 0.319138\n",
            "Epoch: 4458 \tTraining Loss: 0.319120\n",
            "Epoch: 4459 \tTraining Loss: 0.319103\n",
            "Epoch: 4460 \tTraining Loss: 0.319086\n",
            "Epoch: 4461 \tTraining Loss: 0.319068\n",
            "Epoch: 4462 \tTraining Loss: 0.319051\n",
            "Epoch: 4463 \tTraining Loss: 0.319034\n",
            "Epoch: 4464 \tTraining Loss: 0.319016\n",
            "Epoch: 4465 \tTraining Loss: 0.318999\n",
            "Epoch: 4466 \tTraining Loss: 0.318982\n",
            "Epoch: 4467 \tTraining Loss: 0.318965\n",
            "Epoch: 4468 \tTraining Loss: 0.318947\n",
            "Epoch: 4469 \tTraining Loss: 0.318930\n",
            "Epoch: 4470 \tTraining Loss: 0.318913\n",
            "Epoch: 4471 \tTraining Loss: 0.318895\n",
            "Epoch: 4472 \tTraining Loss: 0.318878\n",
            "Epoch: 4473 \tTraining Loss: 0.318861\n",
            "Epoch: 4474 \tTraining Loss: 0.318843\n",
            "Epoch: 4475 \tTraining Loss: 0.318826\n",
            "Epoch: 4476 \tTraining Loss: 0.318809\n",
            "Epoch: 4477 \tTraining Loss: 0.318791\n",
            "Epoch: 4478 \tTraining Loss: 0.318774\n",
            "Epoch: 4479 \tTraining Loss: 0.318757\n",
            "Epoch: 4480 \tTraining Loss: 0.318739\n",
            "Epoch: 4481 \tTraining Loss: 0.318722\n",
            "Epoch: 4482 \tTraining Loss: 0.318705\n",
            "Epoch: 4483 \tTraining Loss: 0.318687\n",
            "Epoch: 4484 \tTraining Loss: 0.318670\n",
            "Epoch: 4485 \tTraining Loss: 0.318652\n",
            "Epoch: 4486 \tTraining Loss: 0.318635\n",
            "Epoch: 4487 \tTraining Loss: 0.318618\n",
            "Epoch: 4488 \tTraining Loss: 0.318600\n",
            "Epoch: 4489 \tTraining Loss: 0.318583\n",
            "Epoch: 4490 \tTraining Loss: 0.318566\n",
            "Epoch: 4491 \tTraining Loss: 0.318548\n",
            "Epoch: 4492 \tTraining Loss: 0.318531\n",
            "Epoch: 4493 \tTraining Loss: 0.318514\n",
            "Epoch: 4494 \tTraining Loss: 0.318496\n",
            "Epoch: 4495 \tTraining Loss: 0.318479\n",
            "Epoch: 4496 \tTraining Loss: 0.318462\n",
            "Epoch: 4497 \tTraining Loss: 0.318444\n",
            "Epoch: 4498 \tTraining Loss: 0.318427\n",
            "Epoch: 4499 \tTraining Loss: 0.318409\n",
            "Epoch: 4500 \tTraining Loss: 0.318392\n",
            "Epoch: 4501 \tTraining Loss: 0.318375\n",
            "Epoch: 4502 \tTraining Loss: 0.318357\n",
            "Epoch: 4503 \tTraining Loss: 0.318340\n",
            "Epoch: 4504 \tTraining Loss: 0.318322\n",
            "Epoch: 4505 \tTraining Loss: 0.318305\n",
            "Epoch: 4506 \tTraining Loss: 0.318288\n",
            "Epoch: 4507 \tTraining Loss: 0.318270\n",
            "Epoch: 4508 \tTraining Loss: 0.318253\n",
            "Epoch: 4509 \tTraining Loss: 0.318236\n",
            "Epoch: 4510 \tTraining Loss: 0.318218\n",
            "Epoch: 4511 \tTraining Loss: 0.318201\n",
            "Epoch: 4512 \tTraining Loss: 0.318183\n",
            "Epoch: 4513 \tTraining Loss: 0.318166\n",
            "Epoch: 4514 \tTraining Loss: 0.318149\n",
            "Epoch: 4515 \tTraining Loss: 0.318131\n",
            "Epoch: 4516 \tTraining Loss: 0.318114\n",
            "Epoch: 4517 \tTraining Loss: 0.318096\n",
            "Epoch: 4518 \tTraining Loss: 0.318079\n",
            "Epoch: 4519 \tTraining Loss: 0.318062\n",
            "Epoch: 4520 \tTraining Loss: 0.318044\n",
            "Epoch: 4521 \tTraining Loss: 0.318027\n",
            "Epoch: 4522 \tTraining Loss: 0.318009\n",
            "Epoch: 4523 \tTraining Loss: 0.317992\n",
            "Epoch: 4524 \tTraining Loss: 0.317975\n",
            "Epoch: 4525 \tTraining Loss: 0.317957\n",
            "Epoch: 4526 \tTraining Loss: 0.317940\n",
            "Epoch: 4527 \tTraining Loss: 0.317922\n",
            "Epoch: 4528 \tTraining Loss: 0.317905\n",
            "Epoch: 4529 \tTraining Loss: 0.317888\n",
            "Epoch: 4530 \tTraining Loss: 0.317870\n",
            "Epoch: 4531 \tTraining Loss: 0.317853\n",
            "Epoch: 4532 \tTraining Loss: 0.317835\n",
            "Epoch: 4533 \tTraining Loss: 0.317818\n",
            "Epoch: 4534 \tTraining Loss: 0.317801\n",
            "Epoch: 4535 \tTraining Loss: 0.317783\n",
            "Epoch: 4536 \tTraining Loss: 0.317766\n",
            "Epoch: 4537 \tTraining Loss: 0.317749\n",
            "Epoch: 4538 \tTraining Loss: 0.317731\n",
            "Epoch: 4539 \tTraining Loss: 0.317714\n",
            "Epoch: 4540 \tTraining Loss: 0.317696\n",
            "Epoch: 4541 \tTraining Loss: 0.317679\n",
            "Epoch: 4542 \tTraining Loss: 0.317662\n",
            "Epoch: 4543 \tTraining Loss: 0.317644\n",
            "Epoch: 4544 \tTraining Loss: 0.317627\n",
            "Epoch: 4545 \tTraining Loss: 0.317609\n",
            "Epoch: 4546 \tTraining Loss: 0.317592\n",
            "Epoch: 4547 \tTraining Loss: 0.317575\n",
            "Epoch: 4548 \tTraining Loss: 0.317557\n",
            "Epoch: 4549 \tTraining Loss: 0.317540\n",
            "Epoch: 4550 \tTraining Loss: 0.317523\n",
            "Epoch: 4551 \tTraining Loss: 0.317505\n",
            "Epoch: 4552 \tTraining Loss: 0.317488\n",
            "Epoch: 4553 \tTraining Loss: 0.317470\n",
            "Epoch: 4554 \tTraining Loss: 0.317453\n",
            "Epoch: 4555 \tTraining Loss: 0.317436\n",
            "Epoch: 4556 \tTraining Loss: 0.317418\n",
            "Epoch: 4557 \tTraining Loss: 0.317401\n",
            "Epoch: 4558 \tTraining Loss: 0.317384\n",
            "Epoch: 4559 \tTraining Loss: 0.317366\n",
            "Epoch: 4560 \tTraining Loss: 0.317349\n",
            "Epoch: 4561 \tTraining Loss: 0.317332\n",
            "Epoch: 4562 \tTraining Loss: 0.317314\n",
            "Epoch: 4563 \tTraining Loss: 0.317297\n",
            "Epoch: 4564 \tTraining Loss: 0.317280\n",
            "Epoch: 4565 \tTraining Loss: 0.317262\n",
            "Epoch: 4566 \tTraining Loss: 0.317245\n",
            "Epoch: 4567 \tTraining Loss: 0.317228\n",
            "Epoch: 4568 \tTraining Loss: 0.317210\n",
            "Epoch: 4569 \tTraining Loss: 0.317193\n",
            "Epoch: 4570 \tTraining Loss: 0.317176\n",
            "Epoch: 4571 \tTraining Loss: 0.317158\n",
            "Epoch: 4572 \tTraining Loss: 0.317141\n",
            "Epoch: 4573 \tTraining Loss: 0.317124\n",
            "Epoch: 4574 \tTraining Loss: 0.317106\n",
            "Epoch: 4575 \tTraining Loss: 0.317089\n",
            "Epoch: 4576 \tTraining Loss: 0.317072\n",
            "Epoch: 4577 \tTraining Loss: 0.317054\n",
            "Epoch: 4578 \tTraining Loss: 0.317037\n",
            "Epoch: 4579 \tTraining Loss: 0.317020\n",
            "Epoch: 4580 \tTraining Loss: 0.317002\n",
            "Epoch: 4581 \tTraining Loss: 0.316985\n",
            "Epoch: 4582 \tTraining Loss: 0.316968\n",
            "Epoch: 4583 \tTraining Loss: 0.316951\n",
            "Epoch: 4584 \tTraining Loss: 0.316933\n",
            "Epoch: 4585 \tTraining Loss: 0.316916\n",
            "Epoch: 4586 \tTraining Loss: 0.316899\n",
            "Epoch: 4587 \tTraining Loss: 0.316881\n",
            "Epoch: 4588 \tTraining Loss: 0.316864\n",
            "Epoch: 4589 \tTraining Loss: 0.316847\n",
            "Epoch: 4590 \tTraining Loss: 0.316829\n",
            "Epoch: 4591 \tTraining Loss: 0.316812\n",
            "Epoch: 4592 \tTraining Loss: 0.316795\n",
            "Epoch: 4593 \tTraining Loss: 0.316778\n",
            "Epoch: 4594 \tTraining Loss: 0.316760\n",
            "Epoch: 4595 \tTraining Loss: 0.316743\n",
            "Epoch: 4596 \tTraining Loss: 0.316726\n",
            "Epoch: 4597 \tTraining Loss: 0.316708\n",
            "Epoch: 4598 \tTraining Loss: 0.316691\n",
            "Epoch: 4599 \tTraining Loss: 0.316674\n",
            "Epoch: 4600 \tTraining Loss: 0.316657\n",
            "Epoch: 4601 \tTraining Loss: 0.316639\n",
            "Epoch: 4602 \tTraining Loss: 0.316622\n",
            "Epoch: 4603 \tTraining Loss: 0.316605\n",
            "Epoch: 4604 \tTraining Loss: 0.316588\n",
            "Epoch: 4605 \tTraining Loss: 0.316570\n",
            "Epoch: 4606 \tTraining Loss: 0.316553\n",
            "Epoch: 4607 \tTraining Loss: 0.316536\n",
            "Epoch: 4608 \tTraining Loss: 0.316518\n",
            "Epoch: 4609 \tTraining Loss: 0.316501\n",
            "Epoch: 4610 \tTraining Loss: 0.316484\n",
            "Epoch: 4611 \tTraining Loss: 0.316467\n",
            "Epoch: 4612 \tTraining Loss: 0.316449\n",
            "Epoch: 4613 \tTraining Loss: 0.316432\n",
            "Epoch: 4614 \tTraining Loss: 0.316415\n",
            "Epoch: 4615 \tTraining Loss: 0.316398\n",
            "Epoch: 4616 \tTraining Loss: 0.316380\n",
            "Epoch: 4617 \tTraining Loss: 0.316363\n",
            "Epoch: 4618 \tTraining Loss: 0.316346\n",
            "Epoch: 4619 \tTraining Loss: 0.316329\n",
            "Epoch: 4620 \tTraining Loss: 0.316311\n",
            "Epoch: 4621 \tTraining Loss: 0.316294\n",
            "Epoch: 4622 \tTraining Loss: 0.316277\n",
            "Epoch: 4623 \tTraining Loss: 0.316260\n",
            "Epoch: 4624 \tTraining Loss: 0.316242\n",
            "Epoch: 4625 \tTraining Loss: 0.316225\n",
            "Epoch: 4626 \tTraining Loss: 0.316208\n",
            "Epoch: 4627 \tTraining Loss: 0.316191\n",
            "Epoch: 4628 \tTraining Loss: 0.316173\n",
            "Epoch: 4629 \tTraining Loss: 0.316156\n",
            "Epoch: 4630 \tTraining Loss: 0.316139\n",
            "Epoch: 4631 \tTraining Loss: 0.316121\n",
            "Epoch: 4632 \tTraining Loss: 0.316104\n",
            "Epoch: 4633 \tTraining Loss: 0.316087\n",
            "Epoch: 4634 \tTraining Loss: 0.316070\n",
            "Epoch: 4635 \tTraining Loss: 0.316052\n",
            "Epoch: 4636 \tTraining Loss: 0.316035\n",
            "Epoch: 4637 \tTraining Loss: 0.316018\n",
            "Epoch: 4638 \tTraining Loss: 0.316001\n",
            "Epoch: 4639 \tTraining Loss: 0.315983\n",
            "Epoch: 4640 \tTraining Loss: 0.315966\n",
            "Epoch: 4641 \tTraining Loss: 0.315949\n",
            "Epoch: 4642 \tTraining Loss: 0.315932\n",
            "Epoch: 4643 \tTraining Loss: 0.315914\n",
            "Epoch: 4644 \tTraining Loss: 0.315897\n",
            "Epoch: 4645 \tTraining Loss: 0.315880\n",
            "Epoch: 4646 \tTraining Loss: 0.315863\n",
            "Epoch: 4647 \tTraining Loss: 0.315845\n",
            "Epoch: 4648 \tTraining Loss: 0.315828\n",
            "Epoch: 4649 \tTraining Loss: 0.315811\n",
            "Epoch: 4650 \tTraining Loss: 0.315793\n",
            "Epoch: 4651 \tTraining Loss: 0.315776\n",
            "Epoch: 4652 \tTraining Loss: 0.315759\n",
            "Epoch: 4653 \tTraining Loss: 0.315742\n",
            "Epoch: 4654 \tTraining Loss: 0.315724\n",
            "Epoch: 4655 \tTraining Loss: 0.315707\n",
            "Epoch: 4656 \tTraining Loss: 0.315690\n",
            "Epoch: 4657 \tTraining Loss: 0.315673\n",
            "Epoch: 4658 \tTraining Loss: 0.315655\n",
            "Epoch: 4659 \tTraining Loss: 0.315638\n",
            "Epoch: 4660 \tTraining Loss: 0.315621\n",
            "Epoch: 4661 \tTraining Loss: 0.315603\n",
            "Epoch: 4662 \tTraining Loss: 0.315586\n",
            "Epoch: 4663 \tTraining Loss: 0.315569\n",
            "Epoch: 4664 \tTraining Loss: 0.315552\n",
            "Epoch: 4665 \tTraining Loss: 0.315534\n",
            "Epoch: 4666 \tTraining Loss: 0.315517\n",
            "Epoch: 4667 \tTraining Loss: 0.315500\n",
            "Epoch: 4668 \tTraining Loss: 0.315482\n",
            "Epoch: 4669 \tTraining Loss: 0.315465\n",
            "Epoch: 4670 \tTraining Loss: 0.315448\n",
            "Epoch: 4671 \tTraining Loss: 0.315431\n",
            "Epoch: 4672 \tTraining Loss: 0.315413\n",
            "Epoch: 4673 \tTraining Loss: 0.315396\n",
            "Epoch: 4674 \tTraining Loss: 0.315379\n",
            "Epoch: 4675 \tTraining Loss: 0.315361\n",
            "Epoch: 4676 \tTraining Loss: 0.315344\n",
            "Epoch: 4677 \tTraining Loss: 0.315327\n",
            "Epoch: 4678 \tTraining Loss: 0.315309\n",
            "Epoch: 4679 \tTraining Loss: 0.315292\n",
            "Epoch: 4680 \tTraining Loss: 0.315275\n",
            "Epoch: 4681 \tTraining Loss: 0.315257\n",
            "Epoch: 4682 \tTraining Loss: 0.315240\n",
            "Epoch: 4683 \tTraining Loss: 0.315223\n",
            "Epoch: 4684 \tTraining Loss: 0.315205\n",
            "Epoch: 4685 \tTraining Loss: 0.315188\n",
            "Epoch: 4686 \tTraining Loss: 0.315171\n",
            "Epoch: 4687 \tTraining Loss: 0.315153\n",
            "Epoch: 4688 \tTraining Loss: 0.315136\n",
            "Epoch: 4689 \tTraining Loss: 0.315119\n",
            "Epoch: 4690 \tTraining Loss: 0.315101\n",
            "Epoch: 4691 \tTraining Loss: 0.315084\n",
            "Epoch: 4692 \tTraining Loss: 0.315067\n",
            "Epoch: 4693 \tTraining Loss: 0.315049\n",
            "Epoch: 4694 \tTraining Loss: 0.315032\n",
            "Epoch: 4695 \tTraining Loss: 0.315015\n",
            "Epoch: 4696 \tTraining Loss: 0.314997\n",
            "Epoch: 4697 \tTraining Loss: 0.314980\n",
            "Epoch: 4698 \tTraining Loss: 0.314963\n",
            "Epoch: 4699 \tTraining Loss: 0.314945\n",
            "Epoch: 4700 \tTraining Loss: 0.314928\n",
            "Epoch: 4701 \tTraining Loss: 0.314911\n",
            "Epoch: 4702 \tTraining Loss: 0.314893\n",
            "Epoch: 4703 \tTraining Loss: 0.314876\n",
            "Epoch: 4704 \tTraining Loss: 0.314859\n",
            "Epoch: 4705 \tTraining Loss: 0.314841\n",
            "Epoch: 4706 \tTraining Loss: 0.314824\n",
            "Epoch: 4707 \tTraining Loss: 0.314806\n",
            "Epoch: 4708 \tTraining Loss: 0.314789\n",
            "Epoch: 4709 \tTraining Loss: 0.314772\n",
            "Epoch: 4710 \tTraining Loss: 0.314754\n",
            "Epoch: 4711 \tTraining Loss: 0.314737\n",
            "Epoch: 4712 \tTraining Loss: 0.314719\n",
            "Epoch: 4713 \tTraining Loss: 0.314702\n",
            "Epoch: 4714 \tTraining Loss: 0.314685\n",
            "Epoch: 4715 \tTraining Loss: 0.314667\n",
            "Epoch: 4716 \tTraining Loss: 0.314650\n",
            "Epoch: 4717 \tTraining Loss: 0.314632\n",
            "Epoch: 4718 \tTraining Loss: 0.314615\n",
            "Epoch: 4719 \tTraining Loss: 0.314598\n",
            "Epoch: 4720 \tTraining Loss: 0.314580\n",
            "Epoch: 4721 \tTraining Loss: 0.314563\n",
            "Epoch: 4722 \tTraining Loss: 0.314545\n",
            "Epoch: 4723 \tTraining Loss: 0.314528\n",
            "Epoch: 4724 \tTraining Loss: 0.314511\n",
            "Epoch: 4725 \tTraining Loss: 0.314493\n",
            "Epoch: 4726 \tTraining Loss: 0.314476\n",
            "Epoch: 4727 \tTraining Loss: 0.314458\n",
            "Epoch: 4728 \tTraining Loss: 0.314441\n",
            "Epoch: 4729 \tTraining Loss: 0.314423\n",
            "Epoch: 4730 \tTraining Loss: 0.314406\n",
            "Epoch: 4731 \tTraining Loss: 0.314389\n",
            "Epoch: 4732 \tTraining Loss: 0.314371\n",
            "Epoch: 4733 \tTraining Loss: 0.314354\n",
            "Epoch: 4734 \tTraining Loss: 0.314336\n",
            "Epoch: 4735 \tTraining Loss: 0.314319\n",
            "Epoch: 4736 \tTraining Loss: 0.314301\n",
            "Epoch: 4737 \tTraining Loss: 0.314284\n",
            "Epoch: 4738 \tTraining Loss: 0.314266\n",
            "Epoch: 4739 \tTraining Loss: 0.314249\n",
            "Epoch: 4740 \tTraining Loss: 0.314231\n",
            "Epoch: 4741 \tTraining Loss: 0.314214\n",
            "Epoch: 4742 \tTraining Loss: 0.314197\n",
            "Epoch: 4743 \tTraining Loss: 0.314179\n",
            "Epoch: 4744 \tTraining Loss: 0.314162\n",
            "Epoch: 4745 \tTraining Loss: 0.314144\n",
            "Epoch: 4746 \tTraining Loss: 0.314127\n",
            "Epoch: 4747 \tTraining Loss: 0.314109\n",
            "Epoch: 4748 \tTraining Loss: 0.314092\n",
            "Epoch: 4749 \tTraining Loss: 0.314074\n",
            "Epoch: 4750 \tTraining Loss: 0.314057\n",
            "Epoch: 4751 \tTraining Loss: 0.314039\n",
            "Epoch: 4752 \tTraining Loss: 0.314022\n",
            "Epoch: 4753 \tTraining Loss: 0.314004\n",
            "Epoch: 4754 \tTraining Loss: 0.313987\n",
            "Epoch: 4755 \tTraining Loss: 0.313969\n",
            "Epoch: 4756 \tTraining Loss: 0.313952\n",
            "Epoch: 4757 \tTraining Loss: 0.313934\n",
            "Epoch: 4758 \tTraining Loss: 0.313917\n",
            "Epoch: 4759 \tTraining Loss: 0.313899\n",
            "Epoch: 4760 \tTraining Loss: 0.313882\n",
            "Epoch: 4761 \tTraining Loss: 0.313864\n",
            "Epoch: 4762 \tTraining Loss: 0.313847\n",
            "Epoch: 4763 \tTraining Loss: 0.313829\n",
            "Epoch: 4764 \tTraining Loss: 0.313812\n",
            "Epoch: 4765 \tTraining Loss: 0.313794\n",
            "Epoch: 4766 \tTraining Loss: 0.313776\n",
            "Epoch: 4767 \tTraining Loss: 0.313759\n",
            "Epoch: 4768 \tTraining Loss: 0.313741\n",
            "Epoch: 4769 \tTraining Loss: 0.313724\n",
            "Epoch: 4770 \tTraining Loss: 0.313706\n",
            "Epoch: 4771 \tTraining Loss: 0.313689\n",
            "Epoch: 4772 \tTraining Loss: 0.313671\n",
            "Epoch: 4773 \tTraining Loss: 0.313654\n",
            "Epoch: 4774 \tTraining Loss: 0.313636\n",
            "Epoch: 4775 \tTraining Loss: 0.313618\n",
            "Epoch: 4776 \tTraining Loss: 0.313601\n",
            "Epoch: 4777 \tTraining Loss: 0.313583\n",
            "Epoch: 4778 \tTraining Loss: 0.313566\n",
            "Epoch: 4779 \tTraining Loss: 0.313548\n",
            "Epoch: 4780 \tTraining Loss: 0.313531\n",
            "Epoch: 4781 \tTraining Loss: 0.313513\n",
            "Epoch: 4782 \tTraining Loss: 0.313495\n",
            "Epoch: 4783 \tTraining Loss: 0.313478\n",
            "Epoch: 4784 \tTraining Loss: 0.313460\n",
            "Epoch: 4785 \tTraining Loss: 0.313443\n",
            "Epoch: 4786 \tTraining Loss: 0.313425\n",
            "Epoch: 4787 \tTraining Loss: 0.313407\n",
            "Epoch: 4788 \tTraining Loss: 0.313390\n",
            "Epoch: 4789 \tTraining Loss: 0.313372\n",
            "Epoch: 4790 \tTraining Loss: 0.313355\n",
            "Epoch: 4791 \tTraining Loss: 0.313337\n",
            "Epoch: 4792 \tTraining Loss: 0.313319\n",
            "Epoch: 4793 \tTraining Loss: 0.313302\n",
            "Epoch: 4794 \tTraining Loss: 0.313284\n",
            "Epoch: 4795 \tTraining Loss: 0.313267\n",
            "Epoch: 4796 \tTraining Loss: 0.313249\n",
            "Epoch: 4797 \tTraining Loss: 0.313231\n",
            "Epoch: 4798 \tTraining Loss: 0.313214\n",
            "Epoch: 4799 \tTraining Loss: 0.313196\n",
            "Epoch: 4800 \tTraining Loss: 0.313178\n",
            "Epoch: 4801 \tTraining Loss: 0.313161\n",
            "Epoch: 4802 \tTraining Loss: 0.313143\n",
            "Epoch: 4803 \tTraining Loss: 0.313125\n",
            "Epoch: 4804 \tTraining Loss: 0.313108\n",
            "Epoch: 4805 \tTraining Loss: 0.313090\n",
            "Epoch: 4806 \tTraining Loss: 0.313072\n",
            "Epoch: 4807 \tTraining Loss: 0.313055\n",
            "Epoch: 4808 \tTraining Loss: 0.313037\n",
            "Epoch: 4809 \tTraining Loss: 0.313019\n",
            "Epoch: 4810 \tTraining Loss: 0.313002\n",
            "Epoch: 4811 \tTraining Loss: 0.312984\n",
            "Epoch: 4812 \tTraining Loss: 0.312966\n",
            "Epoch: 4813 \tTraining Loss: 0.312949\n",
            "Epoch: 4814 \tTraining Loss: 0.312931\n",
            "Epoch: 4815 \tTraining Loss: 0.312913\n",
            "Epoch: 4816 \tTraining Loss: 0.312896\n",
            "Epoch: 4817 \tTraining Loss: 0.312878\n",
            "Epoch: 4818 \tTraining Loss: 0.312860\n",
            "Epoch: 4819 \tTraining Loss: 0.312843\n",
            "Epoch: 4820 \tTraining Loss: 0.312825\n",
            "Epoch: 4821 \tTraining Loss: 0.312807\n",
            "Epoch: 4822 \tTraining Loss: 0.312789\n",
            "Epoch: 4823 \tTraining Loss: 0.312772\n",
            "Epoch: 4824 \tTraining Loss: 0.312754\n",
            "Epoch: 4825 \tTraining Loss: 0.312736\n",
            "Epoch: 4826 \tTraining Loss: 0.312719\n",
            "Epoch: 4827 \tTraining Loss: 0.312701\n",
            "Epoch: 4828 \tTraining Loss: 0.312683\n",
            "Epoch: 4829 \tTraining Loss: 0.312665\n",
            "Epoch: 4830 \tTraining Loss: 0.312648\n",
            "Epoch: 4831 \tTraining Loss: 0.312630\n",
            "Epoch: 4832 \tTraining Loss: 0.312612\n",
            "Epoch: 4833 \tTraining Loss: 0.312594\n",
            "Epoch: 4834 \tTraining Loss: 0.312577\n",
            "Epoch: 4835 \tTraining Loss: 0.312559\n",
            "Epoch: 4836 \tTraining Loss: 0.312541\n",
            "Epoch: 4837 \tTraining Loss: 0.312523\n",
            "Epoch: 4838 \tTraining Loss: 0.312506\n",
            "Epoch: 4839 \tTraining Loss: 0.312488\n",
            "Epoch: 4840 \tTraining Loss: 0.312470\n",
            "Epoch: 4841 \tTraining Loss: 0.312452\n",
            "Epoch: 4842 \tTraining Loss: 0.312435\n",
            "Epoch: 4843 \tTraining Loss: 0.312417\n",
            "Epoch: 4844 \tTraining Loss: 0.312399\n",
            "Epoch: 4845 \tTraining Loss: 0.312381\n",
            "Epoch: 4846 \tTraining Loss: 0.312363\n",
            "Epoch: 4847 \tTraining Loss: 0.312346\n",
            "Epoch: 4848 \tTraining Loss: 0.312328\n",
            "Epoch: 4849 \tTraining Loss: 0.312310\n",
            "Epoch: 4850 \tTraining Loss: 0.312292\n",
            "Epoch: 4851 \tTraining Loss: 0.312274\n",
            "Epoch: 4852 \tTraining Loss: 0.312257\n",
            "Epoch: 4853 \tTraining Loss: 0.312239\n",
            "Epoch: 4854 \tTraining Loss: 0.312221\n",
            "Epoch: 4855 \tTraining Loss: 0.312203\n",
            "Epoch: 4856 \tTraining Loss: 0.312185\n",
            "Epoch: 4857 \tTraining Loss: 0.312167\n",
            "Epoch: 4858 \tTraining Loss: 0.312150\n",
            "Epoch: 4859 \tTraining Loss: 0.312132\n",
            "Epoch: 4860 \tTraining Loss: 0.312114\n",
            "Epoch: 4861 \tTraining Loss: 0.312096\n",
            "Epoch: 4862 \tTraining Loss: 0.312078\n",
            "Epoch: 4863 \tTraining Loss: 0.312060\n",
            "Epoch: 4864 \tTraining Loss: 0.312043\n",
            "Epoch: 4865 \tTraining Loss: 0.312025\n",
            "Epoch: 4866 \tTraining Loss: 0.312007\n",
            "Epoch: 4867 \tTraining Loss: 0.311989\n",
            "Epoch: 4868 \tTraining Loss: 0.311971\n",
            "Epoch: 4869 \tTraining Loss: 0.311953\n",
            "Epoch: 4870 \tTraining Loss: 0.311935\n",
            "Epoch: 4871 \tTraining Loss: 0.311918\n",
            "Epoch: 4872 \tTraining Loss: 0.311900\n",
            "Epoch: 4873 \tTraining Loss: 0.311882\n",
            "Epoch: 4874 \tTraining Loss: 0.311864\n",
            "Epoch: 4875 \tTraining Loss: 0.311846\n",
            "Epoch: 4876 \tTraining Loss: 0.311828\n",
            "Epoch: 4877 \tTraining Loss: 0.311810\n",
            "Epoch: 4878 \tTraining Loss: 0.311792\n",
            "Epoch: 4879 \tTraining Loss: 0.311774\n",
            "Epoch: 4880 \tTraining Loss: 0.311756\n",
            "Epoch: 4881 \tTraining Loss: 0.311739\n",
            "Epoch: 4882 \tTraining Loss: 0.311721\n",
            "Epoch: 4883 \tTraining Loss: 0.311703\n",
            "Epoch: 4884 \tTraining Loss: 0.311685\n",
            "Epoch: 4885 \tTraining Loss: 0.311667\n",
            "Epoch: 4886 \tTraining Loss: 0.311649\n",
            "Epoch: 4887 \tTraining Loss: 0.311631\n",
            "Epoch: 4888 \tTraining Loss: 0.311613\n",
            "Epoch: 4889 \tTraining Loss: 0.311595\n",
            "Epoch: 4890 \tTraining Loss: 0.311577\n",
            "Epoch: 4891 \tTraining Loss: 0.311559\n",
            "Epoch: 4892 \tTraining Loss: 0.311541\n",
            "Epoch: 4893 \tTraining Loss: 0.311523\n",
            "Epoch: 4894 \tTraining Loss: 0.311505\n",
            "Epoch: 4895 \tTraining Loss: 0.311487\n",
            "Epoch: 4896 \tTraining Loss: 0.311469\n",
            "Epoch: 4897 \tTraining Loss: 0.311451\n",
            "Epoch: 4898 \tTraining Loss: 0.311434\n",
            "Epoch: 4899 \tTraining Loss: 0.311416\n",
            "Epoch: 4900 \tTraining Loss: 0.311398\n",
            "Epoch: 4901 \tTraining Loss: 0.311380\n",
            "Epoch: 4902 \tTraining Loss: 0.311362\n",
            "Epoch: 4903 \tTraining Loss: 0.311344\n",
            "Epoch: 4904 \tTraining Loss: 0.311326\n",
            "Epoch: 4905 \tTraining Loss: 0.311308\n",
            "Epoch: 4906 \tTraining Loss: 0.311290\n",
            "Epoch: 4907 \tTraining Loss: 0.311272\n",
            "Epoch: 4908 \tTraining Loss: 0.311254\n",
            "Epoch: 4909 \tTraining Loss: 0.311236\n",
            "Epoch: 4910 \tTraining Loss: 0.311218\n",
            "Epoch: 4911 \tTraining Loss: 0.311200\n",
            "Epoch: 4912 \tTraining Loss: 0.311182\n",
            "Epoch: 4913 \tTraining Loss: 0.311164\n",
            "Epoch: 4914 \tTraining Loss: 0.311146\n",
            "Epoch: 4915 \tTraining Loss: 0.311127\n",
            "Epoch: 4916 \tTraining Loss: 0.311109\n",
            "Epoch: 4917 \tTraining Loss: 0.311091\n",
            "Epoch: 4918 \tTraining Loss: 0.311073\n",
            "Epoch: 4919 \tTraining Loss: 0.311055\n",
            "Epoch: 4920 \tTraining Loss: 0.311037\n",
            "Epoch: 4921 \tTraining Loss: 0.311019\n",
            "Epoch: 4922 \tTraining Loss: 0.311001\n",
            "Epoch: 4923 \tTraining Loss: 0.310983\n",
            "Epoch: 4924 \tTraining Loss: 0.310965\n",
            "Epoch: 4925 \tTraining Loss: 0.310947\n",
            "Epoch: 4926 \tTraining Loss: 0.310929\n",
            "Epoch: 4927 \tTraining Loss: 0.310911\n",
            "Epoch: 4928 \tTraining Loss: 0.310893\n",
            "Epoch: 4929 \tTraining Loss: 0.310875\n",
            "Epoch: 4930 \tTraining Loss: 0.310857\n",
            "Epoch: 4931 \tTraining Loss: 0.310838\n",
            "Epoch: 4932 \tTraining Loss: 0.310820\n",
            "Epoch: 4933 \tTraining Loss: 0.310802\n",
            "Epoch: 4934 \tTraining Loss: 0.310784\n",
            "Epoch: 4935 \tTraining Loss: 0.310766\n",
            "Epoch: 4936 \tTraining Loss: 0.310748\n",
            "Epoch: 4937 \tTraining Loss: 0.310730\n",
            "Epoch: 4938 \tTraining Loss: 0.310712\n",
            "Epoch: 4939 \tTraining Loss: 0.310694\n",
            "Epoch: 4940 \tTraining Loss: 0.310676\n",
            "Epoch: 4941 \tTraining Loss: 0.310657\n",
            "Epoch: 4942 \tTraining Loss: 0.310639\n",
            "Epoch: 4943 \tTraining Loss: 0.310621\n",
            "Epoch: 4944 \tTraining Loss: 0.310603\n",
            "Epoch: 4945 \tTraining Loss: 0.310585\n",
            "Epoch: 4946 \tTraining Loss: 0.310567\n",
            "Epoch: 4947 \tTraining Loss: 0.310549\n",
            "Epoch: 4948 \tTraining Loss: 0.310531\n",
            "Epoch: 4949 \tTraining Loss: 0.310513\n",
            "Epoch: 4950 \tTraining Loss: 0.310496\n",
            "Epoch: 4951 \tTraining Loss: 0.310479\n",
            "Epoch: 4952 \tTraining Loss: 0.310463\n",
            "Epoch: 4953 \tTraining Loss: 0.310449\n",
            "Epoch: 4954 \tTraining Loss: 0.310437\n",
            "Epoch: 4955 \tTraining Loss: 0.310431\n",
            "Epoch: 4956 \tTraining Loss: 0.310438\n",
            "Epoch: 4957 \tTraining Loss: 0.310468\n",
            "Epoch: 4958 \tTraining Loss: 0.310583\n",
            "Epoch: 4959 \tTraining Loss: 0.310991\n",
            "Epoch: 4960 \tTraining Loss: 0.313522\n",
            "Epoch: 4961 \tTraining Loss: 0.315580\n",
            "Epoch: 4962 \tTraining Loss: 0.317599\n",
            "Epoch: 4963 \tTraining Loss: 0.316073\n",
            "Epoch: 4964 \tTraining Loss: 0.315013\n",
            "Epoch: 4965 \tTraining Loss: 0.311816\n",
            "Epoch: 4966 \tTraining Loss: 0.311410\n",
            "Epoch: 4967 \tTraining Loss: 0.314489\n",
            "Epoch: 4968 \tTraining Loss: 0.315419\n",
            "Epoch: 4969 \tTraining Loss: 0.311952\n",
            "Epoch: 4970 \tTraining Loss: 0.315282\n",
            "Epoch: 4971 \tTraining Loss: 0.315335\n",
            "Epoch: 4972 \tTraining Loss: 0.314640\n",
            "Epoch: 4973 \tTraining Loss: 0.314255\n",
            "Epoch: 4974 \tTraining Loss: 0.321370\n",
            "Epoch: 4975 \tTraining Loss: 0.317627\n",
            "Epoch: 4976 \tTraining Loss: 0.313885\n",
            "Epoch: 4977 \tTraining Loss: 0.313165\n",
            "Epoch: 4978 \tTraining Loss: 0.312611\n",
            "Epoch: 4979 \tTraining Loss: 0.313312\n",
            "Epoch: 4980 \tTraining Loss: 0.312299\n",
            "Epoch: 4981 \tTraining Loss: 0.311651\n",
            "Epoch: 4982 \tTraining Loss: 0.312504\n",
            "Epoch: 4983 \tTraining Loss: 0.312764\n",
            "Epoch: 4984 \tTraining Loss: 0.311780\n",
            "Epoch: 4985 \tTraining Loss: 0.312176\n",
            "Epoch: 4986 \tTraining Loss: 0.310420\n",
            "Epoch: 4987 \tTraining Loss: 0.311694\n",
            "Epoch: 4988 \tTraining Loss: 0.311919\n",
            "Epoch: 4989 \tTraining Loss: 0.311711\n",
            "Epoch: 4990 \tTraining Loss: 0.311382\n",
            "Epoch: 4991 \tTraining Loss: 0.310482\n",
            "Epoch: 4992 \tTraining Loss: 0.310955\n",
            "Epoch: 4993 \tTraining Loss: 0.311365\n",
            "Epoch: 4994 \tTraining Loss: 0.311305\n",
            "Epoch: 4995 \tTraining Loss: 0.310831\n",
            "Epoch: 4996 \tTraining Loss: 0.310131\n",
            "Epoch: 4997 \tTraining Loss: 0.310371\n",
            "Epoch: 4998 \tTraining Loss: 0.310995\n",
            "Epoch: 4999 \tTraining Loss: 0.310802\n",
            "Epoch: 5000 \tTraining Loss: 0.310288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Test"
      ],
      "metadata": {
        "id": "yTM49l0YER7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test.float())\n",
        "    predicted_labels = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "predicted_labels = np.where(predicted_labels == 0 ,-1,1)\n",
        "y_test = np.where(y_test == 0 ,-1,1)\n",
        "\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40b0a2f-73c3-4323-d525-697d08c44228",
        "id": "0OMla1suER7S"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1\n",
            "  1  1  1  1  1 -1 -1 -1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1\n",
            "  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# a = np.array(*predicted_labels)\n",
        "test_accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f'Test Accuracy: {100 * test_accuracy:.2f}%')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
        "import seaborn as sns\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d7618ca7-5b37-4dea-d1f9-5688986bb781",
        "id": "JnIJjSUkER7S"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 50.62%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with use شاخص کل(هم وزن)"
      ],
      "metadata": {
        "id": "yrRo434K1I4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Add indicators to Features"
      ],
      "metadata": {
        "id": "DV5IFP-81I4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = op.join('/content/drive/My Drive/','DL','DL_HW05','فولاد-ت.csv')  # Path to the Data folder\n",
        "df = pd.read_csv(dir)\n",
        "aroon = df.ta.aroon(inplace=True)\n",
        "aroon = aroon['AROONU_14'].values[indices_Foolad].astype(np.float64)\n",
        "macd = df.ta.macd(inplace=True)\n",
        "macd = macd['MACD_12_26_9'].values[indices_Foolad].astype(np.float64)\n",
        "rsi = df.ta.rsi(inplace=True)\n",
        "rsi = rsi[indices_Foolad].astype(np.float64)\n",
        "\n",
        "adjClose = df['adjClose'].values[indices_Foolad].astype(np.float64)\n",
        "\n",
        "df = HamVazn\n",
        "aroon_HamVazn = df.ta.aroon(inplace=True)\n",
        "aroon_HamVazn = aroon_HamVazn['AROONU_14'].values[indices_HamVazn].astype(np.float64)\n",
        "macd_HamVazn = df.ta.macd(inplace=True)\n",
        "macd_HamVazn = macd_HamVazn['MACD_12_26_9'].values[indices_HamVazn].astype(np.float64)\n",
        "rsi_HamVazn = df.ta.rsi(inplace=True)\n",
        "rsi_HamVazn = rsi_HamVazn[indices_HamVazn].astype(np.float64)\n",
        "\n",
        "close_HamVazn = df['close'].values[indices_HamVazn].astype(np.float64)\n",
        "\n",
        "features = np.vstack((adjClose[1:],aroon[1:],macd[1:],rsi[1:],close_HamVazn[1:],aroon_HamVazn[1:],macd_HamVazn[1:],rsi_HamVazn[1:]))\n",
        "\n",
        "labels = np.zeros(2006)\n",
        "labels = np.where(merged_Foolad[1:] >= merged_Foolad[:-1], 1, 0)"
      ],
      "metadata": {
        "id": "Eg-HoPdK1I40"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f83004-319d-4d06-d597-244fcc968f63",
        "id": "E59m2UpV1I40"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 2006)"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Split data and adjust window size"
      ],
      "metadata": {
        "id": "KbijX3Sr1I40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 5\n",
        "X, y = create_windows(np.hstack((features.T, labels.reshape(-1, 1))), window_size)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_size = int(len(X) * 0.80)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ],
      "metadata": {
        "id": "av9ow_QT1I40"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define RNN Model"
      ],
      "metadata": {
        "id": "Tou7lh3d1I40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "O2CgUvYk1I40"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 8  # 8 features\n",
        "hidden_size = 50\n",
        "output_size = 2  # Two classes\n",
        "model = RNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)"
      ],
      "metadata": {
        "id": "wMjE7uep1I40"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Training"
      ],
      "metadata": {
        "id": "OpuJmqmY1I41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    outputs = model(X_train.float())\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        loss.item()\n",
        "        ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a751f8-0456-41b5-d37c-d97727eae328",
        "id": "eN1u8xzo1I41"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.714487\n",
            "Epoch: 2 \tTraining Loss: 0.709596\n",
            "Epoch: 3 \tTraining Loss: 0.705013\n",
            "Epoch: 4 \tTraining Loss: 0.700436\n",
            "Epoch: 5 \tTraining Loss: 0.695977\n",
            "Epoch: 6 \tTraining Loss: 0.691858\n",
            "Epoch: 7 \tTraining Loss: 0.688051\n",
            "Epoch: 8 \tTraining Loss: 0.684340\n",
            "Epoch: 9 \tTraining Loss: 0.680759\n",
            "Epoch: 10 \tTraining Loss: 0.677253\n",
            "Epoch: 11 \tTraining Loss: 0.673803\n",
            "Epoch: 12 \tTraining Loss: 0.670220\n",
            "Epoch: 13 \tTraining Loss: 0.666331\n",
            "Epoch: 14 \tTraining Loss: 0.662433\n",
            "Epoch: 15 \tTraining Loss: 0.659353\n",
            "Epoch: 16 \tTraining Loss: 0.657338\n",
            "Epoch: 17 \tTraining Loss: 0.656022\n",
            "Epoch: 18 \tTraining Loss: 0.655154\n",
            "Epoch: 19 \tTraining Loss: 0.654380\n",
            "Epoch: 20 \tTraining Loss: 0.653563\n",
            "Epoch: 21 \tTraining Loss: 0.652709\n",
            "Epoch: 22 \tTraining Loss: 0.651893\n",
            "Epoch: 23 \tTraining Loss: 0.651107\n",
            "Epoch: 24 \tTraining Loss: 0.650383\n",
            "Epoch: 25 \tTraining Loss: 0.649669\n",
            "Epoch: 26 \tTraining Loss: 0.648969\n",
            "Epoch: 27 \tTraining Loss: 0.648348\n",
            "Epoch: 28 \tTraining Loss: 0.647724\n",
            "Epoch: 29 \tTraining Loss: 0.647012\n",
            "Epoch: 30 \tTraining Loss: 0.646104\n",
            "Epoch: 31 \tTraining Loss: 0.645338\n",
            "Epoch: 32 \tTraining Loss: 0.644636\n",
            "Epoch: 33 \tTraining Loss: 0.643864\n",
            "Epoch: 34 \tTraining Loss: 0.643211\n",
            "Epoch: 35 \tTraining Loss: 0.642624\n",
            "Epoch: 36 \tTraining Loss: 0.642257\n",
            "Epoch: 37 \tTraining Loss: 0.641861\n",
            "Epoch: 38 \tTraining Loss: 0.641651\n",
            "Epoch: 39 \tTraining Loss: 0.641364\n",
            "Epoch: 40 \tTraining Loss: 0.641080\n",
            "Epoch: 41 \tTraining Loss: 0.640824\n",
            "Epoch: 42 \tTraining Loss: 0.640364\n",
            "Epoch: 43 \tTraining Loss: 0.640043\n",
            "Epoch: 44 \tTraining Loss: 0.639692\n",
            "Epoch: 45 \tTraining Loss: 0.639412\n",
            "Epoch: 46 \tTraining Loss: 0.639138\n",
            "Epoch: 47 \tTraining Loss: 0.638836\n",
            "Epoch: 48 \tTraining Loss: 0.638535\n",
            "Epoch: 49 \tTraining Loss: 0.638244\n",
            "Epoch: 50 \tTraining Loss: 0.637999\n",
            "Epoch: 51 \tTraining Loss: 0.637702\n",
            "Epoch: 52 \tTraining Loss: 0.637347\n",
            "Epoch: 53 \tTraining Loss: 0.637076\n",
            "Epoch: 54 \tTraining Loss: 0.636991\n",
            "Epoch: 55 \tTraining Loss: 0.636696\n",
            "Epoch: 56 \tTraining Loss: 0.636576\n",
            "Epoch: 57 \tTraining Loss: 0.636471\n",
            "Epoch: 58 \tTraining Loss: 0.636352\n",
            "Epoch: 59 \tTraining Loss: 0.636034\n",
            "Epoch: 60 \tTraining Loss: 0.635784\n",
            "Epoch: 61 \tTraining Loss: 0.635591\n",
            "Epoch: 62 \tTraining Loss: 0.635361\n",
            "Epoch: 63 \tTraining Loss: 0.635115\n",
            "Epoch: 64 \tTraining Loss: 0.634957\n",
            "Epoch: 65 \tTraining Loss: 0.634792\n",
            "Epoch: 66 \tTraining Loss: 0.634549\n",
            "Epoch: 67 \tTraining Loss: 0.634346\n",
            "Epoch: 68 \tTraining Loss: 0.634198\n",
            "Epoch: 69 \tTraining Loss: 0.634083\n",
            "Epoch: 70 \tTraining Loss: 0.633911\n",
            "Epoch: 71 \tTraining Loss: 0.633721\n",
            "Epoch: 72 \tTraining Loss: 0.633577\n",
            "Epoch: 73 \tTraining Loss: 0.633466\n",
            "Epoch: 74 \tTraining Loss: 0.633306\n",
            "Epoch: 75 \tTraining Loss: 0.633152\n",
            "Epoch: 76 \tTraining Loss: 0.633021\n",
            "Epoch: 77 \tTraining Loss: 0.632904\n",
            "Epoch: 78 \tTraining Loss: 0.632784\n",
            "Epoch: 79 \tTraining Loss: 0.632671\n",
            "Epoch: 80 \tTraining Loss: 0.632567\n",
            "Epoch: 81 \tTraining Loss: 0.632468\n",
            "Epoch: 82 \tTraining Loss: 0.632363\n",
            "Epoch: 83 \tTraining Loss: 0.632231\n",
            "Epoch: 84 \tTraining Loss: 0.632118\n",
            "Epoch: 85 \tTraining Loss: 0.632010\n",
            "Epoch: 86 \tTraining Loss: 0.631872\n",
            "Epoch: 87 \tTraining Loss: 0.631742\n",
            "Epoch: 88 \tTraining Loss: 0.631639\n",
            "Epoch: 89 \tTraining Loss: 0.631528\n",
            "Epoch: 90 \tTraining Loss: 0.631416\n",
            "Epoch: 91 \tTraining Loss: 0.631313\n",
            "Epoch: 92 \tTraining Loss: 0.631191\n",
            "Epoch: 93 \tTraining Loss: 0.631074\n",
            "Epoch: 94 \tTraining Loss: 0.630961\n",
            "Epoch: 95 \tTraining Loss: 0.630839\n",
            "Epoch: 96 \tTraining Loss: 0.630722\n",
            "Epoch: 97 \tTraining Loss: 0.630614\n",
            "Epoch: 98 \tTraining Loss: 0.630519\n",
            "Epoch: 99 \tTraining Loss: 0.630427\n",
            "Epoch: 100 \tTraining Loss: 0.630337\n",
            "Epoch: 101 \tTraining Loss: 0.630224\n",
            "Epoch: 102 \tTraining Loss: 0.630123\n",
            "Epoch: 103 \tTraining Loss: 0.630052\n",
            "Epoch: 104 \tTraining Loss: 0.629931\n",
            "Epoch: 105 \tTraining Loss: 0.629918\n",
            "Epoch: 106 \tTraining Loss: 0.629833\n",
            "Epoch: 107 \tTraining Loss: 0.629773\n",
            "Epoch: 108 \tTraining Loss: 0.629543\n",
            "Epoch: 109 \tTraining Loss: 0.629537\n",
            "Epoch: 110 \tTraining Loss: 0.629285\n",
            "Epoch: 111 \tTraining Loss: 0.629187\n",
            "Epoch: 112 \tTraining Loss: 0.629156\n",
            "Epoch: 113 \tTraining Loss: 0.629069\n",
            "Epoch: 114 \tTraining Loss: 0.628870\n",
            "Epoch: 115 \tTraining Loss: 0.628897\n",
            "Epoch: 116 \tTraining Loss: 0.628746\n",
            "Epoch: 117 \tTraining Loss: 0.628503\n",
            "Epoch: 118 \tTraining Loss: 0.628495\n",
            "Epoch: 119 \tTraining Loss: 0.628408\n",
            "Epoch: 120 \tTraining Loss: 0.628255\n",
            "Epoch: 121 \tTraining Loss: 0.628158\n",
            "Epoch: 122 \tTraining Loss: 0.628058\n",
            "Epoch: 123 \tTraining Loss: 0.627952\n",
            "Epoch: 124 \tTraining Loss: 0.627880\n",
            "Epoch: 125 \tTraining Loss: 0.627748\n",
            "Epoch: 126 \tTraining Loss: 0.627607\n",
            "Epoch: 127 \tTraining Loss: 0.627597\n",
            "Epoch: 128 \tTraining Loss: 0.627399\n",
            "Epoch: 129 \tTraining Loss: 0.627350\n",
            "Epoch: 130 \tTraining Loss: 0.627255\n",
            "Epoch: 131 \tTraining Loss: 0.627104\n",
            "Epoch: 132 \tTraining Loss: 0.627016\n",
            "Epoch: 133 \tTraining Loss: 0.626879\n",
            "Epoch: 134 \tTraining Loss: 0.626778\n",
            "Epoch: 135 \tTraining Loss: 0.626671\n",
            "Epoch: 136 \tTraining Loss: 0.626524\n",
            "Epoch: 137 \tTraining Loss: 0.626433\n",
            "Epoch: 138 \tTraining Loss: 0.626268\n",
            "Epoch: 139 \tTraining Loss: 0.626161\n",
            "Epoch: 140 \tTraining Loss: 0.626020\n",
            "Epoch: 141 \tTraining Loss: 0.625923\n",
            "Epoch: 142 \tTraining Loss: 0.625790\n",
            "Epoch: 143 \tTraining Loss: 0.625702\n",
            "Epoch: 144 \tTraining Loss: 0.625576\n",
            "Epoch: 145 \tTraining Loss: 0.625492\n",
            "Epoch: 146 \tTraining Loss: 0.625371\n",
            "Epoch: 147 \tTraining Loss: 0.625267\n",
            "Epoch: 148 \tTraining Loss: 0.625160\n",
            "Epoch: 149 \tTraining Loss: 0.625051\n",
            "Epoch: 150 \tTraining Loss: 0.624948\n",
            "Epoch: 151 \tTraining Loss: 0.624821\n",
            "Epoch: 152 \tTraining Loss: 0.624711\n",
            "Epoch: 153 \tTraining Loss: 0.624577\n",
            "Epoch: 154 \tTraining Loss: 0.624457\n",
            "Epoch: 155 \tTraining Loss: 0.624335\n",
            "Epoch: 156 \tTraining Loss: 0.624198\n",
            "Epoch: 157 \tTraining Loss: 0.624078\n",
            "Epoch: 158 \tTraining Loss: 0.623961\n",
            "Epoch: 159 \tTraining Loss: 0.623851\n",
            "Epoch: 160 \tTraining Loss: 0.623734\n",
            "Epoch: 161 \tTraining Loss: 0.623605\n",
            "Epoch: 162 \tTraining Loss: 0.623482\n",
            "Epoch: 163 \tTraining Loss: 0.623359\n",
            "Epoch: 164 \tTraining Loss: 0.623242\n",
            "Epoch: 165 \tTraining Loss: 0.623128\n",
            "Epoch: 166 \tTraining Loss: 0.622997\n",
            "Epoch: 167 \tTraining Loss: 0.622859\n",
            "Epoch: 168 \tTraining Loss: 0.622748\n",
            "Epoch: 169 \tTraining Loss: 0.622635\n",
            "Epoch: 170 \tTraining Loss: 0.622502\n",
            "Epoch: 171 \tTraining Loss: 0.622370\n",
            "Epoch: 172 \tTraining Loss: 0.622256\n",
            "Epoch: 173 \tTraining Loss: 0.622144\n",
            "Epoch: 174 \tTraining Loss: 0.622017\n",
            "Epoch: 175 \tTraining Loss: 0.621881\n",
            "Epoch: 176 \tTraining Loss: 0.621757\n",
            "Epoch: 177 \tTraining Loss: 0.621641\n",
            "Epoch: 178 \tTraining Loss: 0.621513\n",
            "Epoch: 179 \tTraining Loss: 0.621373\n",
            "Epoch: 180 \tTraining Loss: 0.621243\n",
            "Epoch: 181 \tTraining Loss: 0.621116\n",
            "Epoch: 182 \tTraining Loss: 0.620992\n",
            "Epoch: 183 \tTraining Loss: 0.620855\n",
            "Epoch: 184 \tTraining Loss: 0.620727\n",
            "Epoch: 185 \tTraining Loss: 0.620598\n",
            "Epoch: 186 \tTraining Loss: 0.620465\n",
            "Epoch: 187 \tTraining Loss: 0.620335\n",
            "Epoch: 188 \tTraining Loss: 0.620200\n",
            "Epoch: 189 \tTraining Loss: 0.620060\n",
            "Epoch: 190 \tTraining Loss: 0.619921\n",
            "Epoch: 191 \tTraining Loss: 0.619788\n",
            "Epoch: 192 \tTraining Loss: 0.619656\n",
            "Epoch: 193 \tTraining Loss: 0.619517\n",
            "Epoch: 194 \tTraining Loss: 0.619376\n",
            "Epoch: 195 \tTraining Loss: 0.619230\n",
            "Epoch: 196 \tTraining Loss: 0.619075\n",
            "Epoch: 197 \tTraining Loss: 0.618907\n",
            "Epoch: 198 \tTraining Loss: 0.618723\n",
            "Epoch: 199 \tTraining Loss: 0.618546\n",
            "Epoch: 200 \tTraining Loss: 0.618333\n",
            "Epoch: 201 \tTraining Loss: 0.618093\n",
            "Epoch: 202 \tTraining Loss: 0.617858\n",
            "Epoch: 203 \tTraining Loss: 0.617616\n",
            "Epoch: 204 \tTraining Loss: 0.617391\n",
            "Epoch: 205 \tTraining Loss: 0.617175\n",
            "Epoch: 206 \tTraining Loss: 0.616975\n",
            "Epoch: 207 \tTraining Loss: 0.616805\n",
            "Epoch: 208 \tTraining Loss: 0.616629\n",
            "Epoch: 209 \tTraining Loss: 0.616410\n",
            "Epoch: 210 \tTraining Loss: 0.616176\n",
            "Epoch: 211 \tTraining Loss: 0.615992\n",
            "Epoch: 212 \tTraining Loss: 0.615798\n",
            "Epoch: 213 \tTraining Loss: 0.615602\n",
            "Epoch: 214 \tTraining Loss: 0.615437\n",
            "Epoch: 215 \tTraining Loss: 0.615261\n",
            "Epoch: 216 \tTraining Loss: 0.615078\n",
            "Epoch: 217 \tTraining Loss: 0.614851\n",
            "Epoch: 218 \tTraining Loss: 0.614653\n",
            "Epoch: 219 \tTraining Loss: 0.614424\n",
            "Epoch: 220 \tTraining Loss: 0.614228\n",
            "Epoch: 221 \tTraining Loss: 0.614037\n",
            "Epoch: 222 \tTraining Loss: 0.613826\n",
            "Epoch: 223 \tTraining Loss: 0.613603\n",
            "Epoch: 224 \tTraining Loss: 0.613391\n",
            "Epoch: 225 \tTraining Loss: 0.613176\n",
            "Epoch: 226 \tTraining Loss: 0.612943\n",
            "Epoch: 227 \tTraining Loss: 0.612732\n",
            "Epoch: 228 \tTraining Loss: 0.612502\n",
            "Epoch: 229 \tTraining Loss: 0.612241\n",
            "Epoch: 230 \tTraining Loss: 0.611991\n",
            "Epoch: 231 \tTraining Loss: 0.611771\n",
            "Epoch: 232 \tTraining Loss: 0.611544\n",
            "Epoch: 233 \tTraining Loss: 0.611300\n",
            "Epoch: 234 \tTraining Loss: 0.611033\n",
            "Epoch: 235 \tTraining Loss: 0.610743\n",
            "Epoch: 236 \tTraining Loss: 0.610470\n",
            "Epoch: 237 \tTraining Loss: 0.610164\n",
            "Epoch: 238 \tTraining Loss: 0.609680\n",
            "Epoch: 239 \tTraining Loss: 0.609458\n",
            "Epoch: 240 \tTraining Loss: 0.608826\n",
            "Epoch: 241 \tTraining Loss: 0.608842\n",
            "Epoch: 242 \tTraining Loss: 0.608332\n",
            "Epoch: 243 \tTraining Loss: 0.607816\n",
            "Epoch: 244 \tTraining Loss: 0.607661\n",
            "Epoch: 245 \tTraining Loss: 0.607262\n",
            "Epoch: 246 \tTraining Loss: 0.606929\n",
            "Epoch: 247 \tTraining Loss: 0.607160\n",
            "Epoch: 248 \tTraining Loss: 0.606632\n",
            "Epoch: 249 \tTraining Loss: 0.605992\n",
            "Epoch: 250 \tTraining Loss: 0.605448\n",
            "Epoch: 251 \tTraining Loss: 0.605209\n",
            "Epoch: 252 \tTraining Loss: 0.604947\n",
            "Epoch: 253 \tTraining Loss: 0.604639\n",
            "Epoch: 254 \tTraining Loss: 0.604566\n",
            "Epoch: 255 \tTraining Loss: 0.604092\n",
            "Epoch: 256 \tTraining Loss: 0.603722\n",
            "Epoch: 257 \tTraining Loss: 0.603662\n",
            "Epoch: 258 \tTraining Loss: 0.603283\n",
            "Epoch: 259 \tTraining Loss: 0.603033\n",
            "Epoch: 260 \tTraining Loss: 0.602683\n",
            "Epoch: 261 \tTraining Loss: 0.602776\n",
            "Epoch: 262 \tTraining Loss: 0.602562\n",
            "Epoch: 263 \tTraining Loss: 0.602172\n",
            "Epoch: 264 \tTraining Loss: 0.601505\n",
            "Epoch: 265 \tTraining Loss: 0.601431\n",
            "Epoch: 266 \tTraining Loss: 0.601106\n",
            "Epoch: 267 \tTraining Loss: 0.600819\n",
            "Epoch: 268 \tTraining Loss: 0.600484\n",
            "Epoch: 269 \tTraining Loss: 0.600270\n",
            "Epoch: 270 \tTraining Loss: 0.599997\n",
            "Epoch: 271 \tTraining Loss: 0.599662\n",
            "Epoch: 272 \tTraining Loss: 0.599421\n",
            "Epoch: 273 \tTraining Loss: 0.599234\n",
            "Epoch: 274 \tTraining Loss: 0.598889\n",
            "Epoch: 275 \tTraining Loss: 0.598621\n",
            "Epoch: 276 \tTraining Loss: 0.598409\n",
            "Epoch: 277 \tTraining Loss: 0.598083\n",
            "Epoch: 278 \tTraining Loss: 0.597778\n",
            "Epoch: 279 \tTraining Loss: 0.597623\n",
            "Epoch: 280 \tTraining Loss: 0.597343\n",
            "Epoch: 281 \tTraining Loss: 0.597074\n",
            "Epoch: 282 \tTraining Loss: 0.596782\n",
            "Epoch: 283 \tTraining Loss: 0.596491\n",
            "Epoch: 284 \tTraining Loss: 0.596202\n",
            "Epoch: 285 \tTraining Loss: 0.595824\n",
            "Epoch: 286 \tTraining Loss: 0.595631\n",
            "Epoch: 287 \tTraining Loss: 0.595349\n",
            "Epoch: 288 \tTraining Loss: 0.595007\n",
            "Epoch: 289 \tTraining Loss: 0.594826\n",
            "Epoch: 290 \tTraining Loss: 0.594458\n",
            "Epoch: 291 \tTraining Loss: 0.594244\n",
            "Epoch: 292 \tTraining Loss: 0.593895\n",
            "Epoch: 293 \tTraining Loss: 0.593659\n",
            "Epoch: 294 \tTraining Loss: 0.593328\n",
            "Epoch: 295 \tTraining Loss: 0.593104\n",
            "Epoch: 296 \tTraining Loss: 0.592760\n",
            "Epoch: 297 \tTraining Loss: 0.592504\n",
            "Epoch: 298 \tTraining Loss: 0.592200\n",
            "Epoch: 299 \tTraining Loss: 0.591968\n",
            "Epoch: 300 \tTraining Loss: 0.591690\n",
            "Epoch: 301 \tTraining Loss: 0.591470\n",
            "Epoch: 302 \tTraining Loss: 0.591152\n",
            "Epoch: 303 \tTraining Loss: 0.590788\n",
            "Epoch: 304 \tTraining Loss: 0.590430\n",
            "Epoch: 305 \tTraining Loss: 0.590110\n",
            "Epoch: 306 \tTraining Loss: 0.589835\n",
            "Epoch: 307 \tTraining Loss: 0.589569\n",
            "Epoch: 308 \tTraining Loss: 0.589296\n",
            "Epoch: 309 \tTraining Loss: 0.588930\n",
            "Epoch: 310 \tTraining Loss: 0.588560\n",
            "Epoch: 311 \tTraining Loss: 0.588219\n",
            "Epoch: 312 \tTraining Loss: 0.587905\n",
            "Epoch: 313 \tTraining Loss: 0.587629\n",
            "Epoch: 314 \tTraining Loss: 0.587381\n",
            "Epoch: 315 \tTraining Loss: 0.587150\n",
            "Epoch: 316 \tTraining Loss: 0.586800\n",
            "Epoch: 317 \tTraining Loss: 0.586345\n",
            "Epoch: 318 \tTraining Loss: 0.585924\n",
            "Epoch: 319 \tTraining Loss: 0.585609\n",
            "Epoch: 320 \tTraining Loss: 0.585371\n",
            "Epoch: 321 \tTraining Loss: 0.585091\n",
            "Epoch: 322 \tTraining Loss: 0.584679\n",
            "Epoch: 323 \tTraining Loss: 0.584234\n",
            "Epoch: 324 \tTraining Loss: 0.583837\n",
            "Epoch: 325 \tTraining Loss: 0.583572\n",
            "Epoch: 326 \tTraining Loss: 0.583363\n",
            "Epoch: 327 \tTraining Loss: 0.583106\n",
            "Epoch: 328 \tTraining Loss: 0.582726\n",
            "Epoch: 329 \tTraining Loss: 0.582195\n",
            "Epoch: 330 \tTraining Loss: 0.581702\n",
            "Epoch: 331 \tTraining Loss: 0.581358\n",
            "Epoch: 332 \tTraining Loss: 0.581091\n",
            "Epoch: 333 \tTraining Loss: 0.580875\n",
            "Epoch: 334 \tTraining Loss: 0.580563\n",
            "Epoch: 335 \tTraining Loss: 0.580089\n",
            "Epoch: 336 \tTraining Loss: 0.579451\n",
            "Epoch: 337 \tTraining Loss: 0.578915\n",
            "Epoch: 338 \tTraining Loss: 0.578503\n",
            "Epoch: 339 \tTraining Loss: 0.578214\n",
            "Epoch: 340 \tTraining Loss: 0.577998\n",
            "Epoch: 341 \tTraining Loss: 0.577448\n",
            "Epoch: 342 \tTraining Loss: 0.576797\n",
            "Epoch: 343 \tTraining Loss: 0.575736\n",
            "Epoch: 344 \tTraining Loss: 0.575337\n",
            "Epoch: 345 \tTraining Loss: 0.574592\n",
            "Epoch: 346 \tTraining Loss: 0.574878\n",
            "Epoch: 347 \tTraining Loss: 0.574408\n",
            "Epoch: 348 \tTraining Loss: 0.573430\n",
            "Epoch: 349 \tTraining Loss: 0.572914\n",
            "Epoch: 350 \tTraining Loss: 0.572542\n",
            "Epoch: 351 \tTraining Loss: 0.572097\n",
            "Epoch: 352 \tTraining Loss: 0.571445\n",
            "Epoch: 353 \tTraining Loss: 0.570928\n",
            "Epoch: 354 \tTraining Loss: 0.570313\n",
            "Epoch: 355 \tTraining Loss: 0.570118\n",
            "Epoch: 356 \tTraining Loss: 0.569727\n",
            "Epoch: 357 \tTraining Loss: 0.569609\n",
            "Epoch: 358 \tTraining Loss: 0.567943\n",
            "Epoch: 359 \tTraining Loss: 0.567829\n",
            "Epoch: 360 \tTraining Loss: 0.568306\n",
            "Epoch: 361 \tTraining Loss: 0.567894\n",
            "Epoch: 362 \tTraining Loss: 0.567421\n",
            "Epoch: 363 \tTraining Loss: 0.567747\n",
            "Epoch: 364 \tTraining Loss: 0.566565\n",
            "Epoch: 365 \tTraining Loss: 0.566550\n",
            "Epoch: 366 \tTraining Loss: 0.566308\n",
            "Epoch: 367 \tTraining Loss: 0.565247\n",
            "Epoch: 368 \tTraining Loss: 0.564368\n",
            "Epoch: 369 \tTraining Loss: 0.564037\n",
            "Epoch: 370 \tTraining Loss: 0.563556\n",
            "Epoch: 371 \tTraining Loss: 0.563368\n",
            "Epoch: 372 \tTraining Loss: 0.562615\n",
            "Epoch: 373 \tTraining Loss: 0.561982\n",
            "Epoch: 374 \tTraining Loss: 0.561560\n",
            "Epoch: 375 \tTraining Loss: 0.560936\n",
            "Epoch: 376 \tTraining Loss: 0.560659\n",
            "Epoch: 377 \tTraining Loss: 0.560085\n",
            "Epoch: 378 \tTraining Loss: 0.559573\n",
            "Epoch: 379 \tTraining Loss: 0.559234\n",
            "Epoch: 380 \tTraining Loss: 0.558872\n",
            "Epoch: 381 \tTraining Loss: 0.558647\n",
            "Epoch: 382 \tTraining Loss: 0.558252\n",
            "Epoch: 383 \tTraining Loss: 0.557394\n",
            "Epoch: 384 \tTraining Loss: 0.557752\n",
            "Epoch: 385 \tTraining Loss: 0.556867\n",
            "Epoch: 386 \tTraining Loss: 0.556722\n",
            "Epoch: 387 \tTraining Loss: 0.556352\n",
            "Epoch: 388 \tTraining Loss: 0.556036\n",
            "Epoch: 389 \tTraining Loss: 0.555306\n",
            "Epoch: 390 \tTraining Loss: 0.554873\n",
            "Epoch: 391 \tTraining Loss: 0.554291\n",
            "Epoch: 392 \tTraining Loss: 0.554111\n",
            "Epoch: 393 \tTraining Loss: 0.553559\n",
            "Epoch: 394 \tTraining Loss: 0.553232\n",
            "Epoch: 395 \tTraining Loss: 0.552786\n",
            "Epoch: 396 \tTraining Loss: 0.552172\n",
            "Epoch: 397 \tTraining Loss: 0.551868\n",
            "Epoch: 398 \tTraining Loss: 0.551466\n",
            "Epoch: 399 \tTraining Loss: 0.551131\n",
            "Epoch: 400 \tTraining Loss: 0.550931\n",
            "Epoch: 401 \tTraining Loss: 0.550535\n",
            "Epoch: 402 \tTraining Loss: 0.550101\n",
            "Epoch: 403 \tTraining Loss: 0.549764\n",
            "Epoch: 404 \tTraining Loss: 0.549466\n",
            "Epoch: 405 \tTraining Loss: 0.548574\n",
            "Epoch: 406 \tTraining Loss: 0.548248\n",
            "Epoch: 407 \tTraining Loss: 0.547754\n",
            "Epoch: 408 \tTraining Loss: 0.547150\n",
            "Epoch: 409 \tTraining Loss: 0.546953\n",
            "Epoch: 410 \tTraining Loss: 0.546294\n",
            "Epoch: 411 \tTraining Loss: 0.546185\n",
            "Epoch: 412 \tTraining Loss: 0.546251\n",
            "Epoch: 413 \tTraining Loss: 0.545861\n",
            "Epoch: 414 \tTraining Loss: 0.545978\n",
            "Epoch: 415 \tTraining Loss: 0.545606\n",
            "Epoch: 416 \tTraining Loss: 0.545370\n",
            "Epoch: 417 \tTraining Loss: 0.544357\n",
            "Epoch: 418 \tTraining Loss: 0.543282\n",
            "Epoch: 419 \tTraining Loss: 0.543950\n",
            "Epoch: 420 \tTraining Loss: 0.543025\n",
            "Epoch: 421 \tTraining Loss: 0.543082\n",
            "Epoch: 422 \tTraining Loss: 0.542241\n",
            "Epoch: 423 \tTraining Loss: 0.541629\n",
            "Epoch: 424 \tTraining Loss: 0.541624\n",
            "Epoch: 425 \tTraining Loss: 0.541243\n",
            "Epoch: 426 \tTraining Loss: 0.540829\n",
            "Epoch: 427 \tTraining Loss: 0.540220\n",
            "Epoch: 428 \tTraining Loss: 0.539703\n",
            "Epoch: 429 \tTraining Loss: 0.540326\n",
            "Epoch: 430 \tTraining Loss: 0.539364\n",
            "Epoch: 431 \tTraining Loss: 0.539294\n",
            "Epoch: 432 \tTraining Loss: 0.538359\n",
            "Epoch: 433 \tTraining Loss: 0.538372\n",
            "Epoch: 434 \tTraining Loss: 0.537996\n",
            "Epoch: 435 \tTraining Loss: 0.536815\n",
            "Epoch: 436 \tTraining Loss: 0.536854\n",
            "Epoch: 437 \tTraining Loss: 0.536311\n",
            "Epoch: 438 \tTraining Loss: 0.535800\n",
            "Epoch: 439 \tTraining Loss: 0.535568\n",
            "Epoch: 440 \tTraining Loss: 0.535210\n",
            "Epoch: 441 \tTraining Loss: 0.534798\n",
            "Epoch: 442 \tTraining Loss: 0.534476\n",
            "Epoch: 443 \tTraining Loss: 0.534102\n",
            "Epoch: 444 \tTraining Loss: 0.533664\n",
            "Epoch: 445 \tTraining Loss: 0.533200\n",
            "Epoch: 446 \tTraining Loss: 0.533082\n",
            "Epoch: 447 \tTraining Loss: 0.532547\n",
            "Epoch: 448 \tTraining Loss: 0.532389\n",
            "Epoch: 449 \tTraining Loss: 0.531963\n",
            "Epoch: 450 \tTraining Loss: 0.531679\n",
            "Epoch: 451 \tTraining Loss: 0.531459\n",
            "Epoch: 452 \tTraining Loss: 0.531553\n",
            "Epoch: 453 \tTraining Loss: 0.531408\n",
            "Epoch: 454 \tTraining Loss: 0.530639\n",
            "Epoch: 455 \tTraining Loss: 0.530497\n",
            "Epoch: 456 \tTraining Loss: 0.529849\n",
            "Epoch: 457 \tTraining Loss: 0.529155\n",
            "Epoch: 458 \tTraining Loss: 0.529473\n",
            "Epoch: 459 \tTraining Loss: 0.529822\n",
            "Epoch: 460 \tTraining Loss: 0.529210\n",
            "Epoch: 461 \tTraining Loss: 0.527894\n",
            "Epoch: 462 \tTraining Loss: 0.527849\n",
            "Epoch: 463 \tTraining Loss: 0.527492\n",
            "Epoch: 464 \tTraining Loss: 0.527385\n",
            "Epoch: 465 \tTraining Loss: 0.526886\n",
            "Epoch: 466 \tTraining Loss: 0.526981\n",
            "Epoch: 467 \tTraining Loss: 0.527066\n",
            "Epoch: 468 \tTraining Loss: 0.526680\n",
            "Epoch: 469 \tTraining Loss: 0.526386\n",
            "Epoch: 470 \tTraining Loss: 0.525923\n",
            "Epoch: 471 \tTraining Loss: 0.525490\n",
            "Epoch: 472 \tTraining Loss: 0.524983\n",
            "Epoch: 473 \tTraining Loss: 0.524295\n",
            "Epoch: 474 \tTraining Loss: 0.523429\n",
            "Epoch: 475 \tTraining Loss: 0.524098\n",
            "Epoch: 476 \tTraining Loss: 0.523899\n",
            "Epoch: 477 \tTraining Loss: 0.523424\n",
            "Epoch: 478 \tTraining Loss: 0.522898\n",
            "Epoch: 479 \tTraining Loss: 0.522509\n",
            "Epoch: 480 \tTraining Loss: 0.521962\n",
            "Epoch: 481 \tTraining Loss: 0.521185\n",
            "Epoch: 482 \tTraining Loss: 0.520962\n",
            "Epoch: 483 \tTraining Loss: 0.520809\n",
            "Epoch: 484 \tTraining Loss: 0.520473\n",
            "Epoch: 485 \tTraining Loss: 0.520231\n",
            "Epoch: 486 \tTraining Loss: 0.519765\n",
            "Epoch: 487 \tTraining Loss: 0.519376\n",
            "Epoch: 488 \tTraining Loss: 0.518874\n",
            "Epoch: 489 \tTraining Loss: 0.518468\n",
            "Epoch: 490 \tTraining Loss: 0.518069\n",
            "Epoch: 491 \tTraining Loss: 0.517534\n",
            "Epoch: 492 \tTraining Loss: 0.517083\n",
            "Epoch: 493 \tTraining Loss: 0.516819\n",
            "Epoch: 494 \tTraining Loss: 0.516586\n",
            "Epoch: 495 \tTraining Loss: 0.516414\n",
            "Epoch: 496 \tTraining Loss: 0.516145\n",
            "Epoch: 497 \tTraining Loss: 0.515837\n",
            "Epoch: 498 \tTraining Loss: 0.515456\n",
            "Epoch: 499 \tTraining Loss: 0.514838\n",
            "Epoch: 500 \tTraining Loss: 0.514339\n",
            "Epoch: 501 \tTraining Loss: 0.513939\n",
            "Epoch: 502 \tTraining Loss: 0.513699\n",
            "Epoch: 503 \tTraining Loss: 0.513459\n",
            "Epoch: 504 \tTraining Loss: 0.513219\n",
            "Epoch: 505 \tTraining Loss: 0.512963\n",
            "Epoch: 506 \tTraining Loss: 0.512621\n",
            "Epoch: 507 \tTraining Loss: 0.512232\n",
            "Epoch: 508 \tTraining Loss: 0.511822\n",
            "Epoch: 509 \tTraining Loss: 0.511390\n",
            "Epoch: 510 \tTraining Loss: 0.510905\n",
            "Epoch: 511 \tTraining Loss: 0.510604\n",
            "Epoch: 512 \tTraining Loss: 0.510212\n",
            "Epoch: 513 \tTraining Loss: 0.510016\n",
            "Epoch: 514 \tTraining Loss: 0.509676\n",
            "Epoch: 515 \tTraining Loss: 0.509468\n",
            "Epoch: 516 \tTraining Loss: 0.509247\n",
            "Epoch: 517 \tTraining Loss: 0.509218\n",
            "Epoch: 518 \tTraining Loss: 0.509184\n",
            "Epoch: 519 \tTraining Loss: 0.508961\n",
            "Epoch: 520 \tTraining Loss: 0.508522\n",
            "Epoch: 521 \tTraining Loss: 0.507709\n",
            "Epoch: 522 \tTraining Loss: 0.507067\n",
            "Epoch: 523 \tTraining Loss: 0.506809\n",
            "Epoch: 524 \tTraining Loss: 0.506731\n",
            "Epoch: 525 \tTraining Loss: 0.506619\n",
            "Epoch: 526 \tTraining Loss: 0.506300\n",
            "Epoch: 527 \tTraining Loss: 0.505715\n",
            "Epoch: 528 \tTraining Loss: 0.505227\n",
            "Epoch: 529 \tTraining Loss: 0.504922\n",
            "Epoch: 530 \tTraining Loss: 0.504796\n",
            "Epoch: 531 \tTraining Loss: 0.504650\n",
            "Epoch: 532 \tTraining Loss: 0.504476\n",
            "Epoch: 533 \tTraining Loss: 0.504056\n",
            "Epoch: 534 \tTraining Loss: 0.503622\n",
            "Epoch: 535 \tTraining Loss: 0.503169\n",
            "Epoch: 536 \tTraining Loss: 0.502818\n",
            "Epoch: 537 \tTraining Loss: 0.502558\n",
            "Epoch: 538 \tTraining Loss: 0.502398\n",
            "Epoch: 539 \tTraining Loss: 0.502208\n",
            "Epoch: 540 \tTraining Loss: 0.501977\n",
            "Epoch: 541 \tTraining Loss: 0.501764\n",
            "Epoch: 542 \tTraining Loss: 0.501398\n",
            "Epoch: 543 \tTraining Loss: 0.500992\n",
            "Epoch: 544 \tTraining Loss: 0.500583\n",
            "Epoch: 545 \tTraining Loss: 0.500210\n",
            "Epoch: 546 \tTraining Loss: 0.499915\n",
            "Epoch: 547 \tTraining Loss: 0.499688\n",
            "Epoch: 548 \tTraining Loss: 0.499485\n",
            "Epoch: 549 \tTraining Loss: 0.499290\n",
            "Epoch: 550 \tTraining Loss: 0.499137\n",
            "Epoch: 551 \tTraining Loss: 0.498893\n",
            "Epoch: 552 \tTraining Loss: 0.498687\n",
            "Epoch: 553 \tTraining Loss: 0.498327\n",
            "Epoch: 554 \tTraining Loss: 0.497947\n",
            "Epoch: 555 \tTraining Loss: 0.497531\n",
            "Epoch: 556 \tTraining Loss: 0.497126\n",
            "Epoch: 557 \tTraining Loss: 0.496819\n",
            "Epoch: 558 \tTraining Loss: 0.496585\n",
            "Epoch: 559 \tTraining Loss: 0.496375\n",
            "Epoch: 560 \tTraining Loss: 0.496205\n",
            "Epoch: 561 \tTraining Loss: 0.496136\n",
            "Epoch: 562 \tTraining Loss: 0.495998\n",
            "Epoch: 563 \tTraining Loss: 0.496020\n",
            "Epoch: 564 \tTraining Loss: 0.495655\n",
            "Epoch: 565 \tTraining Loss: 0.495212\n",
            "Epoch: 566 \tTraining Loss: 0.494575\n",
            "Epoch: 567 \tTraining Loss: 0.494078\n",
            "Epoch: 568 \tTraining Loss: 0.493894\n",
            "Epoch: 569 \tTraining Loss: 0.493879\n",
            "Epoch: 570 \tTraining Loss: 0.493827\n",
            "Epoch: 571 \tTraining Loss: 0.493545\n",
            "Epoch: 572 \tTraining Loss: 0.493250\n",
            "Epoch: 573 \tTraining Loss: 0.492621\n",
            "Epoch: 574 \tTraining Loss: 0.492139\n",
            "Epoch: 575 \tTraining Loss: 0.491883\n",
            "Epoch: 576 \tTraining Loss: 0.491753\n",
            "Epoch: 577 \tTraining Loss: 0.491674\n",
            "Epoch: 578 \tTraining Loss: 0.491434\n",
            "Epoch: 579 \tTraining Loss: 0.491144\n",
            "Epoch: 580 \tTraining Loss: 0.490678\n",
            "Epoch: 581 \tTraining Loss: 0.490215\n",
            "Epoch: 582 \tTraining Loss: 0.489855\n",
            "Epoch: 583 \tTraining Loss: 0.489622\n",
            "Epoch: 584 \tTraining Loss: 0.489455\n",
            "Epoch: 585 \tTraining Loss: 0.489294\n",
            "Epoch: 586 \tTraining Loss: 0.489183\n",
            "Epoch: 587 \tTraining Loss: 0.489004\n",
            "Epoch: 588 \tTraining Loss: 0.489179\n",
            "Epoch: 589 \tTraining Loss: 0.488777\n",
            "Epoch: 590 \tTraining Loss: 0.488055\n",
            "Epoch: 591 \tTraining Loss: 0.487630\n",
            "Epoch: 592 \tTraining Loss: 0.487414\n",
            "Epoch: 593 \tTraining Loss: 0.487361\n",
            "Epoch: 594 \tTraining Loss: 0.487177\n",
            "Epoch: 595 \tTraining Loss: 0.487013\n",
            "Epoch: 596 \tTraining Loss: 0.486622\n",
            "Epoch: 597 \tTraining Loss: 0.486283\n",
            "Epoch: 598 \tTraining Loss: 0.485731\n",
            "Epoch: 599 \tTraining Loss: 0.485340\n",
            "Epoch: 600 \tTraining Loss: 0.485172\n",
            "Epoch: 601 \tTraining Loss: 0.484965\n",
            "Epoch: 602 \tTraining Loss: 0.484826\n",
            "Epoch: 603 \tTraining Loss: 0.484538\n",
            "Epoch: 604 \tTraining Loss: 0.484285\n",
            "Epoch: 605 \tTraining Loss: 0.483848\n",
            "Epoch: 606 \tTraining Loss: 0.483621\n",
            "Epoch: 607 \tTraining Loss: 0.483114\n",
            "Epoch: 608 \tTraining Loss: 0.482823\n",
            "Epoch: 609 \tTraining Loss: 0.482692\n",
            "Epoch: 610 \tTraining Loss: 0.482607\n",
            "Epoch: 611 \tTraining Loss: 0.482847\n",
            "Epoch: 612 \tTraining Loss: 0.482284\n",
            "Epoch: 613 \tTraining Loss: 0.481737\n",
            "Epoch: 614 \tTraining Loss: 0.481333\n",
            "Epoch: 615 \tTraining Loss: 0.480915\n",
            "Epoch: 616 \tTraining Loss: 0.480775\n",
            "Epoch: 617 \tTraining Loss: 0.480650\n",
            "Epoch: 618 \tTraining Loss: 0.480365\n",
            "Epoch: 619 \tTraining Loss: 0.479855\n",
            "Epoch: 620 \tTraining Loss: 0.479562\n",
            "Epoch: 621 \tTraining Loss: 0.479241\n",
            "Epoch: 622 \tTraining Loss: 0.479109\n",
            "Epoch: 623 \tTraining Loss: 0.478853\n",
            "Epoch: 624 \tTraining Loss: 0.478636\n",
            "Epoch: 625 \tTraining Loss: 0.478700\n",
            "Epoch: 626 \tTraining Loss: 0.478589\n",
            "Epoch: 627 \tTraining Loss: 0.478567\n",
            "Epoch: 628 \tTraining Loss: 0.478059\n",
            "Epoch: 629 \tTraining Loss: 0.477448\n",
            "Epoch: 630 \tTraining Loss: 0.477194\n",
            "Epoch: 631 \tTraining Loss: 0.476939\n",
            "Epoch: 632 \tTraining Loss: 0.476984\n",
            "Epoch: 633 \tTraining Loss: 0.476827\n",
            "Epoch: 634 \tTraining Loss: 0.476722\n",
            "Epoch: 635 \tTraining Loss: 0.476224\n",
            "Epoch: 636 \tTraining Loss: 0.475725\n",
            "Epoch: 637 \tTraining Loss: 0.475273\n",
            "Epoch: 638 \tTraining Loss: 0.475068\n",
            "Epoch: 639 \tTraining Loss: 0.475024\n",
            "Epoch: 640 \tTraining Loss: 0.474905\n",
            "Epoch: 641 \tTraining Loss: 0.474745\n",
            "Epoch: 642 \tTraining Loss: 0.474269\n",
            "Epoch: 643 \tTraining Loss: 0.473860\n",
            "Epoch: 644 \tTraining Loss: 0.473541\n",
            "Epoch: 645 \tTraining Loss: 0.473345\n",
            "Epoch: 646 \tTraining Loss: 0.473282\n",
            "Epoch: 647 \tTraining Loss: 0.473156\n",
            "Epoch: 648 \tTraining Loss: 0.473048\n",
            "Epoch: 649 \tTraining Loss: 0.472704\n",
            "Epoch: 650 \tTraining Loss: 0.472353\n",
            "Epoch: 651 \tTraining Loss: 0.471907\n",
            "Epoch: 652 \tTraining Loss: 0.471522\n",
            "Epoch: 653 \tTraining Loss: 0.471386\n",
            "Epoch: 654 \tTraining Loss: 0.471527\n",
            "Epoch: 655 \tTraining Loss: 0.471703\n",
            "Epoch: 656 \tTraining Loss: 0.471598\n",
            "Epoch: 657 \tTraining Loss: 0.471387\n",
            "Epoch: 658 \tTraining Loss: 0.471048\n",
            "Epoch: 659 \tTraining Loss: 0.470200\n",
            "Epoch: 660 \tTraining Loss: 0.470128\n",
            "Epoch: 661 \tTraining Loss: 0.469886\n",
            "Epoch: 662 \tTraining Loss: 0.470215\n",
            "Epoch: 663 \tTraining Loss: 0.469500\n",
            "Epoch: 664 \tTraining Loss: 0.469154\n",
            "Epoch: 665 \tTraining Loss: 0.468749\n",
            "Epoch: 666 \tTraining Loss: 0.468701\n",
            "Epoch: 667 \tTraining Loss: 0.468629\n",
            "Epoch: 668 \tTraining Loss: 0.468679\n",
            "Epoch: 669 \tTraining Loss: 0.468675\n",
            "Epoch: 670 \tTraining Loss: 0.467918\n",
            "Epoch: 671 \tTraining Loss: 0.467302\n",
            "Epoch: 672 \tTraining Loss: 0.467329\n",
            "Epoch: 673 \tTraining Loss: 0.467198\n",
            "Epoch: 674 \tTraining Loss: 0.467215\n",
            "Epoch: 675 \tTraining Loss: 0.466830\n",
            "Epoch: 676 \tTraining Loss: 0.466551\n",
            "Epoch: 677 \tTraining Loss: 0.466139\n",
            "Epoch: 678 \tTraining Loss: 0.465778\n",
            "Epoch: 679 \tTraining Loss: 0.465560\n",
            "Epoch: 680 \tTraining Loss: 0.465423\n",
            "Epoch: 681 \tTraining Loss: 0.465475\n",
            "Epoch: 682 \tTraining Loss: 0.465280\n",
            "Epoch: 683 \tTraining Loss: 0.465116\n",
            "Epoch: 684 \tTraining Loss: 0.464732\n",
            "Epoch: 685 \tTraining Loss: 0.464493\n",
            "Epoch: 686 \tTraining Loss: 0.464197\n",
            "Epoch: 687 \tTraining Loss: 0.463958\n",
            "Epoch: 688 \tTraining Loss: 0.463640\n",
            "Epoch: 689 \tTraining Loss: 0.463493\n",
            "Epoch: 690 \tTraining Loss: 0.463210\n",
            "Epoch: 691 \tTraining Loss: 0.462950\n",
            "Epoch: 692 \tTraining Loss: 0.462906\n",
            "Epoch: 693 \tTraining Loss: 0.462741\n",
            "Epoch: 694 \tTraining Loss: 0.462573\n",
            "Epoch: 695 \tTraining Loss: 0.462426\n",
            "Epoch: 696 \tTraining Loss: 0.462297\n",
            "Epoch: 697 \tTraining Loss: 0.462129\n",
            "Epoch: 698 \tTraining Loss: 0.461956\n",
            "Epoch: 699 \tTraining Loss: 0.461701\n",
            "Epoch: 700 \tTraining Loss: 0.461515\n",
            "Epoch: 701 \tTraining Loss: 0.460983\n",
            "Epoch: 702 \tTraining Loss: 0.460715\n",
            "Epoch: 703 \tTraining Loss: 0.460484\n",
            "Epoch: 704 \tTraining Loss: 0.460241\n",
            "Epoch: 705 \tTraining Loss: 0.459918\n",
            "Epoch: 706 \tTraining Loss: 0.459896\n",
            "Epoch: 707 \tTraining Loss: 0.459833\n",
            "Epoch: 708 \tTraining Loss: 0.459650\n",
            "Epoch: 709 \tTraining Loss: 0.459880\n",
            "Epoch: 710 \tTraining Loss: 0.459694\n",
            "Epoch: 711 \tTraining Loss: 0.460058\n",
            "Epoch: 712 \tTraining Loss: 0.459781\n",
            "Epoch: 713 \tTraining Loss: 0.459602\n",
            "Epoch: 714 \tTraining Loss: 0.458967\n",
            "Epoch: 715 \tTraining Loss: 0.458256\n",
            "Epoch: 716 \tTraining Loss: 0.458507\n",
            "Epoch: 717 \tTraining Loss: 0.458892\n",
            "Epoch: 718 \tTraining Loss: 0.459112\n",
            "Epoch: 719 \tTraining Loss: 0.458756\n",
            "Epoch: 720 \tTraining Loss: 0.458736\n",
            "Epoch: 721 \tTraining Loss: 0.457459\n",
            "Epoch: 722 \tTraining Loss: 0.457173\n",
            "Epoch: 723 \tTraining Loss: 0.457724\n",
            "Epoch: 724 \tTraining Loss: 0.457286\n",
            "Epoch: 725 \tTraining Loss: 0.456845\n",
            "Epoch: 726 \tTraining Loss: 0.456383\n",
            "Epoch: 727 \tTraining Loss: 0.455990\n",
            "Epoch: 728 \tTraining Loss: 0.456424\n",
            "Epoch: 729 \tTraining Loss: 0.456098\n",
            "Epoch: 730 \tTraining Loss: 0.455997\n",
            "Epoch: 731 \tTraining Loss: 0.455209\n",
            "Epoch: 732 \tTraining Loss: 0.454842\n",
            "Epoch: 733 \tTraining Loss: 0.455232\n",
            "Epoch: 734 \tTraining Loss: 0.454902\n",
            "Epoch: 735 \tTraining Loss: 0.454549\n",
            "Epoch: 736 \tTraining Loss: 0.454133\n",
            "Epoch: 737 \tTraining Loss: 0.453897\n",
            "Epoch: 738 \tTraining Loss: 0.453822\n",
            "Epoch: 739 \tTraining Loss: 0.453792\n",
            "Epoch: 740 \tTraining Loss: 0.453303\n",
            "Epoch: 741 \tTraining Loss: 0.453077\n",
            "Epoch: 742 \tTraining Loss: 0.452526\n",
            "Epoch: 743 \tTraining Loss: 0.452293\n",
            "Epoch: 744 \tTraining Loss: 0.452084\n",
            "Epoch: 745 \tTraining Loss: 0.452200\n",
            "Epoch: 746 \tTraining Loss: 0.451894\n",
            "Epoch: 747 \tTraining Loss: 0.451724\n",
            "Epoch: 748 \tTraining Loss: 0.451442\n",
            "Epoch: 749 \tTraining Loss: 0.451255\n",
            "Epoch: 750 \tTraining Loss: 0.450889\n",
            "Epoch: 751 \tTraining Loss: 0.450806\n",
            "Epoch: 752 \tTraining Loss: 0.450517\n",
            "Epoch: 753 \tTraining Loss: 0.450176\n",
            "Epoch: 754 \tTraining Loss: 0.450294\n",
            "Epoch: 755 \tTraining Loss: 0.449918\n",
            "Epoch: 756 \tTraining Loss: 0.449785\n",
            "Epoch: 757 \tTraining Loss: 0.449670\n",
            "Epoch: 758 \tTraining Loss: 0.449581\n",
            "Epoch: 759 \tTraining Loss: 0.449453\n",
            "Epoch: 760 \tTraining Loss: 0.449322\n",
            "Epoch: 761 \tTraining Loss: 0.449218\n",
            "Epoch: 762 \tTraining Loss: 0.449233\n",
            "Epoch: 763 \tTraining Loss: 0.449011\n",
            "Epoch: 764 \tTraining Loss: 0.449180\n",
            "Epoch: 765 \tTraining Loss: 0.448144\n",
            "Epoch: 766 \tTraining Loss: 0.447722\n",
            "Epoch: 767 \tTraining Loss: 0.446719\n",
            "Epoch: 768 \tTraining Loss: 0.446825\n",
            "Epoch: 769 \tTraining Loss: 0.446915\n",
            "Epoch: 770 \tTraining Loss: 0.447164\n",
            "Epoch: 771 \tTraining Loss: 0.446748\n",
            "Epoch: 772 \tTraining Loss: 0.445741\n",
            "Epoch: 773 \tTraining Loss: 0.445381\n",
            "Epoch: 774 \tTraining Loss: 0.445444\n",
            "Epoch: 775 \tTraining Loss: 0.445071\n",
            "Epoch: 776 \tTraining Loss: 0.445108\n",
            "Epoch: 777 \tTraining Loss: 0.444595\n",
            "Epoch: 778 \tTraining Loss: 0.444237\n",
            "Epoch: 779 \tTraining Loss: 0.444235\n",
            "Epoch: 780 \tTraining Loss: 0.443608\n",
            "Epoch: 781 \tTraining Loss: 0.443670\n",
            "Epoch: 782 \tTraining Loss: 0.444598\n",
            "Epoch: 783 \tTraining Loss: 0.444727\n",
            "Epoch: 784 \tTraining Loss: 0.443454\n",
            "Epoch: 785 \tTraining Loss: 0.443813\n",
            "Epoch: 786 \tTraining Loss: 0.442345\n",
            "Epoch: 787 \tTraining Loss: 0.443533\n",
            "Epoch: 788 \tTraining Loss: 0.444216\n",
            "Epoch: 789 \tTraining Loss: 0.444395\n",
            "Epoch: 790 \tTraining Loss: 0.444064\n",
            "Epoch: 791 \tTraining Loss: 0.442662\n",
            "Epoch: 792 \tTraining Loss: 0.444074\n",
            "Epoch: 793 \tTraining Loss: 0.445751\n",
            "Epoch: 794 \tTraining Loss: 0.441847\n",
            "Epoch: 795 \tTraining Loss: 0.442960\n",
            "Epoch: 796 \tTraining Loss: 0.444438\n",
            "Epoch: 797 \tTraining Loss: 0.444772\n",
            "Epoch: 798 \tTraining Loss: 0.444169\n",
            "Epoch: 799 \tTraining Loss: 0.443893\n",
            "Epoch: 800 \tTraining Loss: 0.443695\n",
            "Epoch: 801 \tTraining Loss: 0.443788\n",
            "Epoch: 802 \tTraining Loss: 0.444174\n",
            "Epoch: 803 \tTraining Loss: 0.443753\n",
            "Epoch: 804 \tTraining Loss: 0.443107\n",
            "Epoch: 805 \tTraining Loss: 0.442403\n",
            "Epoch: 806 \tTraining Loss: 0.442029\n",
            "Epoch: 807 \tTraining Loss: 0.441902\n",
            "Epoch: 808 \tTraining Loss: 0.441852\n",
            "Epoch: 809 \tTraining Loss: 0.442018\n",
            "Epoch: 810 \tTraining Loss: 0.441519\n",
            "Epoch: 811 \tTraining Loss: 0.440769\n",
            "Epoch: 812 \tTraining Loss: 0.439952\n",
            "Epoch: 813 \tTraining Loss: 0.439026\n",
            "Epoch: 814 \tTraining Loss: 0.437841\n",
            "Epoch: 815 \tTraining Loss: 0.438250\n",
            "Epoch: 816 \tTraining Loss: 0.440329\n",
            "Epoch: 817 \tTraining Loss: 0.437483\n",
            "Epoch: 818 \tTraining Loss: 0.436255\n",
            "Epoch: 819 \tTraining Loss: 0.436893\n",
            "Epoch: 820 \tTraining Loss: 0.437250\n",
            "Epoch: 821 \tTraining Loss: 0.437194\n",
            "Epoch: 822 \tTraining Loss: 0.436592\n",
            "Epoch: 823 \tTraining Loss: 0.435672\n",
            "Epoch: 824 \tTraining Loss: 0.435102\n",
            "Epoch: 825 \tTraining Loss: 0.434600\n",
            "Epoch: 826 \tTraining Loss: 0.434451\n",
            "Epoch: 827 \tTraining Loss: 0.434465\n",
            "Epoch: 828 \tTraining Loss: 0.433958\n",
            "Epoch: 829 \tTraining Loss: 0.433831\n",
            "Epoch: 830 \tTraining Loss: 0.433416\n",
            "Epoch: 831 \tTraining Loss: 0.433637\n",
            "Epoch: 832 \tTraining Loss: 0.433690\n",
            "Epoch: 833 \tTraining Loss: 0.433787\n",
            "Epoch: 834 \tTraining Loss: 0.434679\n",
            "Epoch: 835 \tTraining Loss: 0.433974\n",
            "Epoch: 836 \tTraining Loss: 0.433391\n",
            "Epoch: 837 \tTraining Loss: 0.431998\n",
            "Epoch: 838 \tTraining Loss: 0.432275\n",
            "Epoch: 839 \tTraining Loss: 0.432627\n",
            "Epoch: 840 \tTraining Loss: 0.432650\n",
            "Epoch: 841 \tTraining Loss: 0.432475\n",
            "Epoch: 842 \tTraining Loss: 0.431193\n",
            "Epoch: 843 \tTraining Loss: 0.430663\n",
            "Epoch: 844 \tTraining Loss: 0.431306\n",
            "Epoch: 845 \tTraining Loss: 0.431537\n",
            "Epoch: 846 \tTraining Loss: 0.431623\n",
            "Epoch: 847 \tTraining Loss: 0.430897\n",
            "Epoch: 848 \tTraining Loss: 0.430398\n",
            "Epoch: 849 \tTraining Loss: 0.430699\n",
            "Epoch: 850 \tTraining Loss: 0.431519\n",
            "Epoch: 851 \tTraining Loss: 0.430922\n",
            "Epoch: 852 \tTraining Loss: 0.432611\n",
            "Epoch: 853 \tTraining Loss: 0.432295\n",
            "Epoch: 854 \tTraining Loss: 0.431138\n",
            "Epoch: 855 \tTraining Loss: 0.429020\n",
            "Epoch: 856 \tTraining Loss: 0.430824\n",
            "Epoch: 857 \tTraining Loss: 0.430813\n",
            "Epoch: 858 \tTraining Loss: 0.428947\n",
            "Epoch: 859 \tTraining Loss: 0.430307\n",
            "Epoch: 860 \tTraining Loss: 0.430767\n",
            "Epoch: 861 \tTraining Loss: 0.428029\n",
            "Epoch: 862 \tTraining Loss: 0.428968\n",
            "Epoch: 863 \tTraining Loss: 0.429710\n",
            "Epoch: 864 \tTraining Loss: 0.428983\n",
            "Epoch: 865 \tTraining Loss: 0.427441\n",
            "Epoch: 866 \tTraining Loss: 0.429303\n",
            "Epoch: 867 \tTraining Loss: 0.430086\n",
            "Epoch: 868 \tTraining Loss: 0.429678\n",
            "Epoch: 869 \tTraining Loss: 0.429364\n",
            "Epoch: 870 \tTraining Loss: 0.429310\n",
            "Epoch: 871 \tTraining Loss: 0.427886\n",
            "Epoch: 872 \tTraining Loss: 0.428704\n",
            "Epoch: 873 \tTraining Loss: 0.429726\n",
            "Epoch: 874 \tTraining Loss: 0.430035\n",
            "Epoch: 875 \tTraining Loss: 0.429178\n",
            "Epoch: 876 \tTraining Loss: 0.428671\n",
            "Epoch: 877 \tTraining Loss: 0.427324\n",
            "Epoch: 878 \tTraining Loss: 0.425940\n",
            "Epoch: 879 \tTraining Loss: 0.425761\n",
            "Epoch: 880 \tTraining Loss: 0.426318\n",
            "Epoch: 881 \tTraining Loss: 0.425647\n",
            "Epoch: 882 \tTraining Loss: 0.425546\n",
            "Epoch: 883 \tTraining Loss: 0.425188\n",
            "Epoch: 884 \tTraining Loss: 0.424620\n",
            "Epoch: 885 \tTraining Loss: 0.424879\n",
            "Epoch: 886 \tTraining Loss: 0.423698\n",
            "Epoch: 887 \tTraining Loss: 0.423200\n",
            "Epoch: 888 \tTraining Loss: 0.422771\n",
            "Epoch: 889 \tTraining Loss: 0.422473\n",
            "Epoch: 890 \tTraining Loss: 0.422545\n",
            "Epoch: 891 \tTraining Loss: 0.422397\n",
            "Epoch: 892 \tTraining Loss: 0.422852\n",
            "Epoch: 893 \tTraining Loss: 0.422817\n",
            "Epoch: 894 \tTraining Loss: 0.423531\n",
            "Epoch: 895 \tTraining Loss: 0.424035\n",
            "Epoch: 896 \tTraining Loss: 0.425005\n",
            "Epoch: 897 \tTraining Loss: 0.423237\n",
            "Epoch: 898 \tTraining Loss: 0.421908\n",
            "Epoch: 899 \tTraining Loss: 0.420151\n",
            "Epoch: 900 \tTraining Loss: 0.421180\n",
            "Epoch: 901 \tTraining Loss: 0.422298\n",
            "Epoch: 902 \tTraining Loss: 0.421017\n",
            "Epoch: 903 \tTraining Loss: 0.419879\n",
            "Epoch: 904 \tTraining Loss: 0.419794\n",
            "Epoch: 905 \tTraining Loss: 0.420222\n",
            "Epoch: 906 \tTraining Loss: 0.419663\n",
            "Epoch: 907 \tTraining Loss: 0.418873\n",
            "Epoch: 908 \tTraining Loss: 0.419170\n",
            "Epoch: 909 \tTraining Loss: 0.419358\n",
            "Epoch: 910 \tTraining Loss: 0.418868\n",
            "Epoch: 911 \tTraining Loss: 0.418326\n",
            "Epoch: 912 \tTraining Loss: 0.418061\n",
            "Epoch: 913 \tTraining Loss: 0.417894\n",
            "Epoch: 914 \tTraining Loss: 0.418088\n",
            "Epoch: 915 \tTraining Loss: 0.417945\n",
            "Epoch: 916 \tTraining Loss: 0.417851\n",
            "Epoch: 917 \tTraining Loss: 0.417397\n",
            "Epoch: 918 \tTraining Loss: 0.417254\n",
            "Epoch: 919 \tTraining Loss: 0.416652\n",
            "Epoch: 920 \tTraining Loss: 0.416214\n",
            "Epoch: 921 \tTraining Loss: 0.416162\n",
            "Epoch: 922 \tTraining Loss: 0.415961\n",
            "Epoch: 923 \tTraining Loss: 0.415935\n",
            "Epoch: 924 \tTraining Loss: 0.415735\n",
            "Epoch: 925 \tTraining Loss: 0.416199\n",
            "Epoch: 926 \tTraining Loss: 0.416272\n",
            "Epoch: 927 \tTraining Loss: 0.417055\n",
            "Epoch: 928 \tTraining Loss: 0.417447\n",
            "Epoch: 929 \tTraining Loss: 0.419298\n",
            "Epoch: 930 \tTraining Loss: 0.418310\n",
            "Epoch: 931 \tTraining Loss: 0.417476\n",
            "Epoch: 932 \tTraining Loss: 0.414790\n",
            "Epoch: 933 \tTraining Loss: 0.414438\n",
            "Epoch: 934 \tTraining Loss: 0.415878\n",
            "Epoch: 935 \tTraining Loss: 0.415538\n",
            "Epoch: 936 \tTraining Loss: 0.414244\n",
            "Epoch: 937 \tTraining Loss: 0.413796\n",
            "Epoch: 938 \tTraining Loss: 0.414407\n",
            "Epoch: 939 \tTraining Loss: 0.414178\n",
            "Epoch: 940 \tTraining Loss: 0.413102\n",
            "Epoch: 941 \tTraining Loss: 0.413178\n",
            "Epoch: 942 \tTraining Loss: 0.413660\n",
            "Epoch: 943 \tTraining Loss: 0.413176\n",
            "Epoch: 944 \tTraining Loss: 0.412476\n",
            "Epoch: 945 \tTraining Loss: 0.412178\n",
            "Epoch: 946 \tTraining Loss: 0.412322\n",
            "Epoch: 947 \tTraining Loss: 0.412351\n",
            "Epoch: 948 \tTraining Loss: 0.412443\n",
            "Epoch: 949 \tTraining Loss: 0.412070\n",
            "Epoch: 950 \tTraining Loss: 0.411573\n",
            "Epoch: 951 \tTraining Loss: 0.411192\n",
            "Epoch: 952 \tTraining Loss: 0.411235\n",
            "Epoch: 953 \tTraining Loss: 0.411100\n",
            "Epoch: 954 \tTraining Loss: 0.411153\n",
            "Epoch: 955 \tTraining Loss: 0.410784\n",
            "Epoch: 956 \tTraining Loss: 0.410948\n",
            "Epoch: 957 \tTraining Loss: 0.410824\n",
            "Epoch: 958 \tTraining Loss: 0.410647\n",
            "Epoch: 959 \tTraining Loss: 0.410776\n",
            "Epoch: 960 \tTraining Loss: 0.410671\n",
            "Epoch: 961 \tTraining Loss: 0.411155\n",
            "Epoch: 962 \tTraining Loss: 0.411825\n",
            "Epoch: 963 \tTraining Loss: 0.412166\n",
            "Epoch: 964 \tTraining Loss: 0.412509\n",
            "Epoch: 965 \tTraining Loss: 0.410984\n",
            "Epoch: 966 \tTraining Loss: 0.409405\n",
            "Epoch: 967 \tTraining Loss: 0.409222\n",
            "Epoch: 968 \tTraining Loss: 0.409349\n",
            "Epoch: 969 \tTraining Loss: 0.410379\n",
            "Epoch: 970 \tTraining Loss: 0.409742\n",
            "Epoch: 971 \tTraining Loss: 0.408820\n",
            "Epoch: 972 \tTraining Loss: 0.408514\n",
            "Epoch: 973 \tTraining Loss: 0.408819\n",
            "Epoch: 974 \tTraining Loss: 0.408936\n",
            "Epoch: 975 \tTraining Loss: 0.408442\n",
            "Epoch: 976 \tTraining Loss: 0.407911\n",
            "Epoch: 977 \tTraining Loss: 0.407580\n",
            "Epoch: 978 \tTraining Loss: 0.407640\n",
            "Epoch: 979 \tTraining Loss: 0.407649\n",
            "Epoch: 980 \tTraining Loss: 0.407544\n",
            "Epoch: 981 \tTraining Loss: 0.407205\n",
            "Epoch: 982 \tTraining Loss: 0.407049\n",
            "Epoch: 983 \tTraining Loss: 0.406651\n",
            "Epoch: 984 \tTraining Loss: 0.406414\n",
            "Epoch: 985 \tTraining Loss: 0.406209\n",
            "Epoch: 986 \tTraining Loss: 0.406009\n",
            "Epoch: 987 \tTraining Loss: 0.405705\n",
            "Epoch: 988 \tTraining Loss: 0.405609\n",
            "Epoch: 989 \tTraining Loss: 0.405630\n",
            "Epoch: 990 \tTraining Loss: 0.405359\n",
            "Epoch: 991 \tTraining Loss: 0.405643\n",
            "Epoch: 992 \tTraining Loss: 0.405760\n",
            "Epoch: 993 \tTraining Loss: 0.406189\n",
            "Epoch: 994 \tTraining Loss: 0.407979\n",
            "Epoch: 995 \tTraining Loss: 0.409649\n",
            "Epoch: 996 \tTraining Loss: 0.413716\n",
            "Epoch: 997 \tTraining Loss: 0.411079\n",
            "Epoch: 998 \tTraining Loss: 0.408041\n",
            "Epoch: 999 \tTraining Loss: 0.404410\n",
            "Epoch: 1000 \tTraining Loss: 0.407137\n",
            "Epoch: 1001 \tTraining Loss: 0.410073\n",
            "Epoch: 1002 \tTraining Loss: 0.404880\n",
            "Epoch: 1003 \tTraining Loss: 0.405786\n",
            "Epoch: 1004 \tTraining Loss: 0.408913\n",
            "Epoch: 1005 \tTraining Loss: 0.404837\n",
            "Epoch: 1006 \tTraining Loss: 0.405167\n",
            "Epoch: 1007 \tTraining Loss: 0.407990\n",
            "Epoch: 1008 \tTraining Loss: 0.404012\n",
            "Epoch: 1009 \tTraining Loss: 0.405034\n",
            "Epoch: 1010 \tTraining Loss: 0.406755\n",
            "Epoch: 1011 \tTraining Loss: 0.402941\n",
            "Epoch: 1012 \tTraining Loss: 0.404711\n",
            "Epoch: 1013 \tTraining Loss: 0.405293\n",
            "Epoch: 1014 \tTraining Loss: 0.402526\n",
            "Epoch: 1015 \tTraining Loss: 0.404415\n",
            "Epoch: 1016 \tTraining Loss: 0.404150\n",
            "Epoch: 1017 \tTraining Loss: 0.402424\n",
            "Epoch: 1018 \tTraining Loss: 0.403699\n",
            "Epoch: 1019 \tTraining Loss: 0.403428\n",
            "Epoch: 1020 \tTraining Loss: 0.401698\n",
            "Epoch: 1021 \tTraining Loss: 0.403238\n",
            "Epoch: 1022 \tTraining Loss: 0.403065\n",
            "Epoch: 1023 \tTraining Loss: 0.401465\n",
            "Epoch: 1024 \tTraining Loss: 0.402493\n",
            "Epoch: 1025 \tTraining Loss: 0.402393\n",
            "Epoch: 1026 \tTraining Loss: 0.400905\n",
            "Epoch: 1027 \tTraining Loss: 0.401883\n",
            "Epoch: 1028 \tTraining Loss: 0.401897\n",
            "Epoch: 1029 \tTraining Loss: 0.400390\n",
            "Epoch: 1030 \tTraining Loss: 0.401187\n",
            "Epoch: 1031 \tTraining Loss: 0.401481\n",
            "Epoch: 1032 \tTraining Loss: 0.399999\n",
            "Epoch: 1033 \tTraining Loss: 0.400520\n",
            "Epoch: 1034 \tTraining Loss: 0.400769\n",
            "Epoch: 1035 \tTraining Loss: 0.400162\n",
            "Epoch: 1036 \tTraining Loss: 0.399621\n",
            "Epoch: 1037 \tTraining Loss: 0.400111\n",
            "Epoch: 1038 \tTraining Loss: 0.399564\n",
            "Epoch: 1039 \tTraining Loss: 0.398995\n",
            "Epoch: 1040 \tTraining Loss: 0.399090\n",
            "Epoch: 1041 \tTraining Loss: 0.399219\n",
            "Epoch: 1042 \tTraining Loss: 0.398943\n",
            "Epoch: 1043 \tTraining Loss: 0.398644\n",
            "Epoch: 1044 \tTraining Loss: 0.398528\n",
            "Epoch: 1045 \tTraining Loss: 0.398803\n",
            "Epoch: 1046 \tTraining Loss: 0.398196\n",
            "Epoch: 1047 \tTraining Loss: 0.398043\n",
            "Epoch: 1048 \tTraining Loss: 0.397997\n",
            "Epoch: 1049 \tTraining Loss: 0.397861\n",
            "Epoch: 1050 \tTraining Loss: 0.397604\n",
            "Epoch: 1051 \tTraining Loss: 0.397481\n",
            "Epoch: 1052 \tTraining Loss: 0.397366\n",
            "Epoch: 1053 \tTraining Loss: 0.397198\n",
            "Epoch: 1054 \tTraining Loss: 0.397348\n",
            "Epoch: 1055 \tTraining Loss: 0.397102\n",
            "Epoch: 1056 \tTraining Loss: 0.396948\n",
            "Epoch: 1057 \tTraining Loss: 0.396665\n",
            "Epoch: 1058 \tTraining Loss: 0.396662\n",
            "Epoch: 1059 \tTraining Loss: 0.396700\n",
            "Epoch: 1060 \tTraining Loss: 0.396524\n",
            "Epoch: 1061 \tTraining Loss: 0.396266\n",
            "Epoch: 1062 \tTraining Loss: 0.396300\n",
            "Epoch: 1063 \tTraining Loss: 0.395899\n",
            "Epoch: 1064 \tTraining Loss: 0.395864\n",
            "Epoch: 1065 \tTraining Loss: 0.395675\n",
            "Epoch: 1066 \tTraining Loss: 0.395642\n",
            "Epoch: 1067 \tTraining Loss: 0.395347\n",
            "Epoch: 1068 \tTraining Loss: 0.395350\n",
            "Epoch: 1069 \tTraining Loss: 0.395358\n",
            "Epoch: 1070 \tTraining Loss: 0.395120\n",
            "Epoch: 1071 \tTraining Loss: 0.395146\n",
            "Epoch: 1072 \tTraining Loss: 0.394928\n",
            "Epoch: 1073 \tTraining Loss: 0.394772\n",
            "Epoch: 1074 \tTraining Loss: 0.395048\n",
            "Epoch: 1075 \tTraining Loss: 0.395206\n",
            "Epoch: 1076 \tTraining Loss: 0.395644\n",
            "Epoch: 1077 \tTraining Loss: 0.396660\n",
            "Epoch: 1078 \tTraining Loss: 0.397881\n",
            "Epoch: 1079 \tTraining Loss: 0.399557\n",
            "Epoch: 1080 \tTraining Loss: 0.398154\n",
            "Epoch: 1081 \tTraining Loss: 0.396629\n",
            "Epoch: 1082 \tTraining Loss: 0.394267\n",
            "Epoch: 1083 \tTraining Loss: 0.394093\n",
            "Epoch: 1084 \tTraining Loss: 0.395870\n",
            "Epoch: 1085 \tTraining Loss: 0.394730\n",
            "Epoch: 1086 \tTraining Loss: 0.393342\n",
            "Epoch: 1087 \tTraining Loss: 0.393518\n",
            "Epoch: 1088 \tTraining Loss: 0.394051\n",
            "Epoch: 1089 \tTraining Loss: 0.393707\n",
            "Epoch: 1090 \tTraining Loss: 0.392620\n",
            "Epoch: 1091 \tTraining Loss: 0.393153\n",
            "Epoch: 1092 \tTraining Loss: 0.393801\n",
            "Epoch: 1093 \tTraining Loss: 0.393008\n",
            "Epoch: 1094 \tTraining Loss: 0.392342\n",
            "Epoch: 1095 \tTraining Loss: 0.392606\n",
            "Epoch: 1096 \tTraining Loss: 0.392595\n",
            "Epoch: 1097 \tTraining Loss: 0.392163\n",
            "Epoch: 1098 \tTraining Loss: 0.391762\n",
            "Epoch: 1099 \tTraining Loss: 0.391793\n",
            "Epoch: 1100 \tTraining Loss: 0.391677\n",
            "Epoch: 1101 \tTraining Loss: 0.391878\n",
            "Epoch: 1102 \tTraining Loss: 0.391526\n",
            "Epoch: 1103 \tTraining Loss: 0.391259\n",
            "Epoch: 1104 \tTraining Loss: 0.391105\n",
            "Epoch: 1105 \tTraining Loss: 0.391040\n",
            "Epoch: 1106 \tTraining Loss: 0.390897\n",
            "Epoch: 1107 \tTraining Loss: 0.390645\n",
            "Epoch: 1108 \tTraining Loss: 0.390565\n",
            "Epoch: 1109 \tTraining Loss: 0.390464\n",
            "Epoch: 1110 \tTraining Loss: 0.390132\n",
            "Epoch: 1111 \tTraining Loss: 0.390309\n",
            "Epoch: 1112 \tTraining Loss: 0.390166\n",
            "Epoch: 1113 \tTraining Loss: 0.390034\n",
            "Epoch: 1114 \tTraining Loss: 0.389928\n",
            "Epoch: 1115 \tTraining Loss: 0.390022\n",
            "Epoch: 1116 \tTraining Loss: 0.390045\n",
            "Epoch: 1117 \tTraining Loss: 0.390001\n",
            "Epoch: 1118 \tTraining Loss: 0.390107\n",
            "Epoch: 1119 \tTraining Loss: 0.390120\n",
            "Epoch: 1120 \tTraining Loss: 0.390072\n",
            "Epoch: 1121 \tTraining Loss: 0.390257\n",
            "Epoch: 1122 \tTraining Loss: 0.390592\n",
            "Epoch: 1123 \tTraining Loss: 0.390288\n",
            "Epoch: 1124 \tTraining Loss: 0.390077\n",
            "Epoch: 1125 \tTraining Loss: 0.389600\n",
            "Epoch: 1126 \tTraining Loss: 0.388717\n",
            "Epoch: 1127 \tTraining Loss: 0.388414\n",
            "Epoch: 1128 \tTraining Loss: 0.388169\n",
            "Epoch: 1129 \tTraining Loss: 0.388602\n",
            "Epoch: 1130 \tTraining Loss: 0.388385\n",
            "Epoch: 1131 \tTraining Loss: 0.388432\n",
            "Epoch: 1132 \tTraining Loss: 0.387993\n",
            "Epoch: 1133 \tTraining Loss: 0.387747\n",
            "Epoch: 1134 \tTraining Loss: 0.387366\n",
            "Epoch: 1135 \tTraining Loss: 0.387224\n",
            "Epoch: 1136 \tTraining Loss: 0.387281\n",
            "Epoch: 1137 \tTraining Loss: 0.387092\n",
            "Epoch: 1138 \tTraining Loss: 0.387352\n",
            "Epoch: 1139 \tTraining Loss: 0.387321\n",
            "Epoch: 1140 \tTraining Loss: 0.387216\n",
            "Epoch: 1141 \tTraining Loss: 0.387097\n",
            "Epoch: 1142 \tTraining Loss: 0.387028\n",
            "Epoch: 1143 \tTraining Loss: 0.387042\n",
            "Epoch: 1144 \tTraining Loss: 0.387012\n",
            "Epoch: 1145 \tTraining Loss: 0.386907\n",
            "Epoch: 1146 \tTraining Loss: 0.387197\n",
            "Epoch: 1147 \tTraining Loss: 0.386916\n",
            "Epoch: 1148 \tTraining Loss: 0.386977\n",
            "Epoch: 1149 \tTraining Loss: 0.386385\n",
            "Epoch: 1150 \tTraining Loss: 0.385866\n",
            "Epoch: 1151 \tTraining Loss: 0.385705\n",
            "Epoch: 1152 \tTraining Loss: 0.385412\n",
            "Epoch: 1153 \tTraining Loss: 0.385183\n",
            "Epoch: 1154 \tTraining Loss: 0.385109\n",
            "Epoch: 1155 \tTraining Loss: 0.384956\n",
            "Epoch: 1156 \tTraining Loss: 0.384952\n",
            "Epoch: 1157 \tTraining Loss: 0.385097\n",
            "Epoch: 1158 \tTraining Loss: 0.384998\n",
            "Epoch: 1159 \tTraining Loss: 0.384997\n",
            "Epoch: 1160 \tTraining Loss: 0.385209\n",
            "Epoch: 1161 \tTraining Loss: 0.385277\n",
            "Epoch: 1162 \tTraining Loss: 0.385343\n",
            "Epoch: 1163 \tTraining Loss: 0.385440\n",
            "Epoch: 1164 \tTraining Loss: 0.385498\n",
            "Epoch: 1165 \tTraining Loss: 0.385613\n",
            "Epoch: 1166 \tTraining Loss: 0.385204\n",
            "Epoch: 1167 \tTraining Loss: 0.384643\n",
            "Epoch: 1168 \tTraining Loss: 0.384068\n",
            "Epoch: 1169 \tTraining Loss: 0.383312\n",
            "Epoch: 1170 \tTraining Loss: 0.383229\n",
            "Epoch: 1171 \tTraining Loss: 0.383169\n",
            "Epoch: 1172 \tTraining Loss: 0.383417\n",
            "Epoch: 1173 \tTraining Loss: 0.383455\n",
            "Epoch: 1174 \tTraining Loss: 0.383486\n",
            "Epoch: 1175 \tTraining Loss: 0.383175\n",
            "Epoch: 1176 \tTraining Loss: 0.382815\n",
            "Epoch: 1177 \tTraining Loss: 0.382449\n",
            "Epoch: 1178 \tTraining Loss: 0.382199\n",
            "Epoch: 1179 \tTraining Loss: 0.382000\n",
            "Epoch: 1180 \tTraining Loss: 0.381933\n",
            "Epoch: 1181 \tTraining Loss: 0.381931\n",
            "Epoch: 1182 \tTraining Loss: 0.381890\n",
            "Epoch: 1183 \tTraining Loss: 0.381989\n",
            "Epoch: 1184 \tTraining Loss: 0.382067\n",
            "Epoch: 1185 \tTraining Loss: 0.382403\n",
            "Epoch: 1186 \tTraining Loss: 0.382718\n",
            "Epoch: 1187 \tTraining Loss: 0.383615\n",
            "Epoch: 1188 \tTraining Loss: 0.383748\n",
            "Epoch: 1189 \tTraining Loss: 0.384713\n",
            "Epoch: 1190 \tTraining Loss: 0.383375\n",
            "Epoch: 1191 \tTraining Loss: 0.382217\n",
            "Epoch: 1192 \tTraining Loss: 0.380996\n",
            "Epoch: 1193 \tTraining Loss: 0.380697\n",
            "Epoch: 1194 \tTraining Loss: 0.381201\n",
            "Epoch: 1195 \tTraining Loss: 0.381498\n",
            "Epoch: 1196 \tTraining Loss: 0.381285\n",
            "Epoch: 1197 \tTraining Loss: 0.380348\n",
            "Epoch: 1198 \tTraining Loss: 0.380023\n",
            "Epoch: 1199 \tTraining Loss: 0.380346\n",
            "Epoch: 1200 \tTraining Loss: 0.380596\n",
            "Epoch: 1201 \tTraining Loss: 0.380732\n",
            "Epoch: 1202 \tTraining Loss: 0.379829\n",
            "Epoch: 1203 \tTraining Loss: 0.379364\n",
            "Epoch: 1204 \tTraining Loss: 0.379603\n",
            "Epoch: 1205 \tTraining Loss: 0.379720\n",
            "Epoch: 1206 \tTraining Loss: 0.379675\n",
            "Epoch: 1207 \tTraining Loss: 0.379606\n",
            "Epoch: 1208 \tTraining Loss: 0.379133\n",
            "Epoch: 1209 \tTraining Loss: 0.378785\n",
            "Epoch: 1210 \tTraining Loss: 0.378628\n",
            "Epoch: 1211 \tTraining Loss: 0.378561\n",
            "Epoch: 1212 \tTraining Loss: 0.378636\n",
            "Epoch: 1213 \tTraining Loss: 0.378747\n",
            "Epoch: 1214 \tTraining Loss: 0.378572\n",
            "Epoch: 1215 \tTraining Loss: 0.378431\n",
            "Epoch: 1216 \tTraining Loss: 0.378279\n",
            "Epoch: 1217 \tTraining Loss: 0.378155\n",
            "Epoch: 1218 \tTraining Loss: 0.377883\n",
            "Epoch: 1219 \tTraining Loss: 0.377775\n",
            "Epoch: 1220 \tTraining Loss: 0.377450\n",
            "Epoch: 1221 \tTraining Loss: 0.377245\n",
            "Epoch: 1222 \tTraining Loss: 0.377295\n",
            "Epoch: 1223 \tTraining Loss: 0.377200\n",
            "Epoch: 1224 \tTraining Loss: 0.377009\n",
            "Epoch: 1225 \tTraining Loss: 0.377036\n",
            "Epoch: 1226 \tTraining Loss: 0.376930\n",
            "Epoch: 1227 \tTraining Loss: 0.376871\n",
            "Epoch: 1228 \tTraining Loss: 0.377297\n",
            "Epoch: 1229 \tTraining Loss: 0.377576\n",
            "Epoch: 1230 \tTraining Loss: 0.378498\n",
            "Epoch: 1231 \tTraining Loss: 0.379930\n",
            "Epoch: 1232 \tTraining Loss: 0.383454\n",
            "Epoch: 1233 \tTraining Loss: 0.384352\n",
            "Epoch: 1234 \tTraining Loss: 0.391357\n",
            "Epoch: 1235 \tTraining Loss: 0.384072\n",
            "Epoch: 1236 \tTraining Loss: 0.382550\n",
            "Epoch: 1237 \tTraining Loss: 0.376868\n",
            "Epoch: 1238 \tTraining Loss: 0.379989\n",
            "Epoch: 1239 \tTraining Loss: 0.383700\n",
            "Epoch: 1240 \tTraining Loss: 0.378085\n",
            "Epoch: 1241 \tTraining Loss: 0.378451\n",
            "Epoch: 1242 \tTraining Loss: 0.381222\n",
            "Epoch: 1243 \tTraining Loss: 0.377113\n",
            "Epoch: 1244 \tTraining Loss: 0.378957\n",
            "Epoch: 1245 \tTraining Loss: 0.379368\n",
            "Epoch: 1246 \tTraining Loss: 0.376167\n",
            "Epoch: 1247 \tTraining Loss: 0.378697\n",
            "Epoch: 1248 \tTraining Loss: 0.377261\n",
            "Epoch: 1249 \tTraining Loss: 0.376065\n",
            "Epoch: 1250 \tTraining Loss: 0.377556\n",
            "Epoch: 1251 \tTraining Loss: 0.375792\n",
            "Epoch: 1252 \tTraining Loss: 0.375763\n",
            "Epoch: 1253 \tTraining Loss: 0.376534\n",
            "Epoch: 1254 \tTraining Loss: 0.375080\n",
            "Epoch: 1255 \tTraining Loss: 0.375057\n",
            "Epoch: 1256 \tTraining Loss: 0.375889\n",
            "Epoch: 1257 \tTraining Loss: 0.374183\n",
            "Epoch: 1258 \tTraining Loss: 0.374655\n",
            "Epoch: 1259 \tTraining Loss: 0.374400\n",
            "Epoch: 1260 \tTraining Loss: 0.373932\n",
            "Epoch: 1261 \tTraining Loss: 0.374190\n",
            "Epoch: 1262 \tTraining Loss: 0.374050\n",
            "Epoch: 1263 \tTraining Loss: 0.373321\n",
            "Epoch: 1264 \tTraining Loss: 0.373473\n",
            "Epoch: 1265 \tTraining Loss: 0.373387\n",
            "Epoch: 1266 \tTraining Loss: 0.372903\n",
            "Epoch: 1267 \tTraining Loss: 0.372842\n",
            "Epoch: 1268 \tTraining Loss: 0.372871\n",
            "Epoch: 1269 \tTraining Loss: 0.372645\n",
            "Epoch: 1270 \tTraining Loss: 0.372237\n",
            "Epoch: 1271 \tTraining Loss: 0.372447\n",
            "Epoch: 1272 \tTraining Loss: 0.372244\n",
            "Epoch: 1273 \tTraining Loss: 0.371965\n",
            "Epoch: 1274 \tTraining Loss: 0.371944\n",
            "Epoch: 1275 \tTraining Loss: 0.371882\n",
            "Epoch: 1276 \tTraining Loss: 0.371920\n",
            "Epoch: 1277 \tTraining Loss: 0.371609\n",
            "Epoch: 1278 \tTraining Loss: 0.371441\n",
            "Epoch: 1279 \tTraining Loss: 0.371397\n",
            "Epoch: 1280 \tTraining Loss: 0.371212\n",
            "Epoch: 1281 \tTraining Loss: 0.371004\n",
            "Epoch: 1282 \tTraining Loss: 0.370939\n",
            "Epoch: 1283 \tTraining Loss: 0.370819\n",
            "Epoch: 1284 \tTraining Loss: 0.370773\n",
            "Epoch: 1285 \tTraining Loss: 0.370643\n",
            "Epoch: 1286 \tTraining Loss: 0.370497\n",
            "Epoch: 1287 \tTraining Loss: 0.370341\n",
            "Epoch: 1288 \tTraining Loss: 0.370389\n",
            "Epoch: 1289 \tTraining Loss: 0.370323\n",
            "Epoch: 1290 \tTraining Loss: 0.370119\n",
            "Epoch: 1291 \tTraining Loss: 0.370084\n",
            "Epoch: 1292 \tTraining Loss: 0.369903\n",
            "Epoch: 1293 \tTraining Loss: 0.369715\n",
            "Epoch: 1294 \tTraining Loss: 0.369822\n",
            "Epoch: 1295 \tTraining Loss: 0.369652\n",
            "Epoch: 1296 \tTraining Loss: 0.369524\n",
            "Epoch: 1297 \tTraining Loss: 0.369500\n",
            "Epoch: 1298 \tTraining Loss: 0.369219\n",
            "Epoch: 1299 \tTraining Loss: 0.369120\n",
            "Epoch: 1300 \tTraining Loss: 0.369120\n",
            "Epoch: 1301 \tTraining Loss: 0.368988\n",
            "Epoch: 1302 \tTraining Loss: 0.368809\n",
            "Epoch: 1303 \tTraining Loss: 0.368855\n",
            "Epoch: 1304 \tTraining Loss: 0.368698\n",
            "Epoch: 1305 \tTraining Loss: 0.368512\n",
            "Epoch: 1306 \tTraining Loss: 0.368554\n",
            "Epoch: 1307 \tTraining Loss: 0.368345\n",
            "Epoch: 1308 \tTraining Loss: 0.368164\n",
            "Epoch: 1309 \tTraining Loss: 0.368269\n",
            "Epoch: 1310 \tTraining Loss: 0.368085\n",
            "Epoch: 1311 \tTraining Loss: 0.367915\n",
            "Epoch: 1312 \tTraining Loss: 0.367930\n",
            "Epoch: 1313 \tTraining Loss: 0.367699\n",
            "Epoch: 1314 \tTraining Loss: 0.367558\n",
            "Epoch: 1315 \tTraining Loss: 0.367647\n",
            "Epoch: 1316 \tTraining Loss: 0.367474\n",
            "Epoch: 1317 \tTraining Loss: 0.367324\n",
            "Epoch: 1318 \tTraining Loss: 0.367352\n",
            "Epoch: 1319 \tTraining Loss: 0.367156\n",
            "Epoch: 1320 \tTraining Loss: 0.367076\n",
            "Epoch: 1321 \tTraining Loss: 0.367248\n",
            "Epoch: 1322 \tTraining Loss: 0.367173\n",
            "Epoch: 1323 \tTraining Loss: 0.367270\n",
            "Epoch: 1324 \tTraining Loss: 0.367682\n",
            "Epoch: 1325 \tTraining Loss: 0.368344\n",
            "Epoch: 1326 \tTraining Loss: 0.368826\n",
            "Epoch: 1327 \tTraining Loss: 0.371305\n",
            "Epoch: 1328 \tTraining Loss: 0.370651\n",
            "Epoch: 1329 \tTraining Loss: 0.370410\n",
            "Epoch: 1330 \tTraining Loss: 0.367302\n",
            "Epoch: 1331 \tTraining Loss: 0.365845\n",
            "Epoch: 1332 \tTraining Loss: 0.366407\n",
            "Epoch: 1333 \tTraining Loss: 0.367396\n",
            "Epoch: 1334 \tTraining Loss: 0.367876\n",
            "Epoch: 1335 \tTraining Loss: 0.366449\n",
            "Epoch: 1336 \tTraining Loss: 0.365411\n",
            "Epoch: 1337 \tTraining Loss: 0.366503\n",
            "Epoch: 1338 \tTraining Loss: 0.367081\n",
            "Epoch: 1339 \tTraining Loss: 0.366453\n",
            "Epoch: 1340 \tTraining Loss: 0.365188\n",
            "Epoch: 1341 \tTraining Loss: 0.365467\n",
            "Epoch: 1342 \tTraining Loss: 0.366271\n",
            "Epoch: 1343 \tTraining Loss: 0.365573\n",
            "Epoch: 1344 \tTraining Loss: 0.364949\n",
            "Epoch: 1345 \tTraining Loss: 0.364766\n",
            "Epoch: 1346 \tTraining Loss: 0.365056\n",
            "Epoch: 1347 \tTraining Loss: 0.365236\n",
            "Epoch: 1348 \tTraining Loss: 0.364494\n",
            "Epoch: 1349 \tTraining Loss: 0.364501\n",
            "Epoch: 1350 \tTraining Loss: 0.364386\n",
            "Epoch: 1351 \tTraining Loss: 0.364437\n",
            "Epoch: 1352 \tTraining Loss: 0.364651\n",
            "Epoch: 1353 \tTraining Loss: 0.363755\n",
            "Epoch: 1354 \tTraining Loss: 0.364007\n",
            "Epoch: 1355 \tTraining Loss: 0.364008\n",
            "Epoch: 1356 \tTraining Loss: 0.363892\n",
            "Epoch: 1357 \tTraining Loss: 0.363748\n",
            "Epoch: 1358 \tTraining Loss: 0.363748\n",
            "Epoch: 1359 \tTraining Loss: 0.363721\n",
            "Epoch: 1360 \tTraining Loss: 0.363305\n",
            "Epoch: 1361 \tTraining Loss: 0.363235\n",
            "Epoch: 1362 \tTraining Loss: 0.363218\n",
            "Epoch: 1363 \tTraining Loss: 0.362946\n",
            "Epoch: 1364 \tTraining Loss: 0.363088\n",
            "Epoch: 1365 \tTraining Loss: 0.363042\n",
            "Epoch: 1366 \tTraining Loss: 0.363907\n",
            "Epoch: 1367 \tTraining Loss: 0.363528\n",
            "Epoch: 1368 \tTraining Loss: 0.364602\n",
            "Epoch: 1369 \tTraining Loss: 0.364559\n",
            "Epoch: 1370 \tTraining Loss: 0.363902\n",
            "Epoch: 1371 \tTraining Loss: 0.362557\n",
            "Epoch: 1372 \tTraining Loss: 0.363734\n",
            "Epoch: 1373 \tTraining Loss: 0.364165\n",
            "Epoch: 1374 \tTraining Loss: 0.363751\n",
            "Epoch: 1375 \tTraining Loss: 0.361879\n",
            "Epoch: 1376 \tTraining Loss: 0.364388\n",
            "Epoch: 1377 \tTraining Loss: 0.364947\n",
            "Epoch: 1378 \tTraining Loss: 0.363738\n",
            "Epoch: 1379 \tTraining Loss: 0.362892\n",
            "Epoch: 1380 \tTraining Loss: 0.363413\n",
            "Epoch: 1381 \tTraining Loss: 0.364528\n",
            "Epoch: 1382 \tTraining Loss: 0.365052\n",
            "Epoch: 1383 \tTraining Loss: 0.368851\n",
            "Epoch: 1384 \tTraining Loss: 0.369802\n",
            "Epoch: 1385 \tTraining Loss: 0.376792\n",
            "Epoch: 1386 \tTraining Loss: 0.373289\n",
            "Epoch: 1387 \tTraining Loss: 0.386969\n",
            "Epoch: 1388 \tTraining Loss: 0.368985\n",
            "Epoch: 1389 \tTraining Loss: 0.381747\n",
            "Epoch: 1390 \tTraining Loss: 0.412937\n",
            "Epoch: 1391 \tTraining Loss: 0.410744\n",
            "Epoch: 1392 \tTraining Loss: 0.457358\n",
            "Epoch: 1393 \tTraining Loss: 0.420563\n",
            "Epoch: 1394 \tTraining Loss: 0.427852\n",
            "Epoch: 1395 \tTraining Loss: 0.421738\n",
            "Epoch: 1396 \tTraining Loss: 0.406879\n",
            "Epoch: 1397 \tTraining Loss: 0.407357\n",
            "Epoch: 1398 \tTraining Loss: 0.406072\n",
            "Epoch: 1399 \tTraining Loss: 0.404879\n",
            "Epoch: 1400 \tTraining Loss: 0.403742\n",
            "Epoch: 1401 \tTraining Loss: 0.394605\n",
            "Epoch: 1402 \tTraining Loss: 0.384202\n",
            "Epoch: 1403 \tTraining Loss: 0.391139\n",
            "Epoch: 1404 \tTraining Loss: 0.379724\n",
            "Epoch: 1405 \tTraining Loss: 0.380390\n",
            "Epoch: 1406 \tTraining Loss: 0.382155\n",
            "Epoch: 1407 \tTraining Loss: 0.380053\n",
            "Epoch: 1408 \tTraining Loss: 0.372858\n",
            "Epoch: 1409 \tTraining Loss: 0.376193\n",
            "Epoch: 1410 \tTraining Loss: 0.375221\n",
            "Epoch: 1411 \tTraining Loss: 0.370505\n",
            "Epoch: 1412 \tTraining Loss: 0.374718\n",
            "Epoch: 1413 \tTraining Loss: 0.373573\n",
            "Epoch: 1414 \tTraining Loss: 0.368247\n",
            "Epoch: 1415 \tTraining Loss: 0.372196\n",
            "Epoch: 1416 \tTraining Loss: 0.367975\n",
            "Epoch: 1417 \tTraining Loss: 0.367317\n",
            "Epoch: 1418 \tTraining Loss: 0.371633\n",
            "Epoch: 1419 \tTraining Loss: 0.369573\n",
            "Epoch: 1420 \tTraining Loss: 0.366110\n",
            "Epoch: 1421 \tTraining Loss: 0.365713\n",
            "Epoch: 1422 \tTraining Loss: 0.366934\n",
            "Epoch: 1423 \tTraining Loss: 0.364574\n",
            "Epoch: 1424 \tTraining Loss: 0.365320\n",
            "Epoch: 1425 \tTraining Loss: 0.365767\n",
            "Epoch: 1426 \tTraining Loss: 0.366153\n",
            "Epoch: 1427 \tTraining Loss: 0.365431\n",
            "Epoch: 1428 \tTraining Loss: 0.364905\n",
            "Epoch: 1429 \tTraining Loss: 0.363196\n",
            "Epoch: 1430 \tTraining Loss: 0.362787\n",
            "Epoch: 1431 \tTraining Loss: 0.364539\n",
            "Epoch: 1432 \tTraining Loss: 0.364959\n",
            "Epoch: 1433 \tTraining Loss: 0.363387\n",
            "Epoch: 1434 \tTraining Loss: 0.361153\n",
            "Epoch: 1435 \tTraining Loss: 0.362059\n",
            "Epoch: 1436 \tTraining Loss: 0.361862\n",
            "Epoch: 1437 \tTraining Loss: 0.360681\n",
            "Epoch: 1438 \tTraining Loss: 0.359661\n",
            "Epoch: 1439 \tTraining Loss: 0.360453\n",
            "Epoch: 1440 \tTraining Loss: 0.360004\n",
            "Epoch: 1441 \tTraining Loss: 0.359383\n",
            "Epoch: 1442 \tTraining Loss: 0.359533\n",
            "Epoch: 1443 \tTraining Loss: 0.358793\n",
            "Epoch: 1444 \tTraining Loss: 0.358751\n",
            "Epoch: 1445 \tTraining Loss: 0.358676\n",
            "Epoch: 1446 \tTraining Loss: 0.358311\n",
            "Epoch: 1447 \tTraining Loss: 0.358464\n",
            "Epoch: 1448 \tTraining Loss: 0.358233\n",
            "Epoch: 1449 \tTraining Loss: 0.357847\n",
            "Epoch: 1450 \tTraining Loss: 0.357796\n",
            "Epoch: 1451 \tTraining Loss: 0.357654\n",
            "Epoch: 1452 \tTraining Loss: 0.357460\n",
            "Epoch: 1453 \tTraining Loss: 0.357436\n",
            "Epoch: 1454 \tTraining Loss: 0.357230\n",
            "Epoch: 1455 \tTraining Loss: 0.357142\n",
            "Epoch: 1456 \tTraining Loss: 0.357057\n",
            "Epoch: 1457 \tTraining Loss: 0.356882\n",
            "Epoch: 1458 \tTraining Loss: 0.356718\n",
            "Epoch: 1459 \tTraining Loss: 0.356692\n",
            "Epoch: 1460 \tTraining Loss: 0.356579\n",
            "Epoch: 1461 \tTraining Loss: 0.356486\n",
            "Epoch: 1462 \tTraining Loss: 0.356404\n",
            "Epoch: 1463 \tTraining Loss: 0.356274\n",
            "Epoch: 1464 \tTraining Loss: 0.356153\n",
            "Epoch: 1465 \tTraining Loss: 0.356072\n",
            "Epoch: 1466 \tTraining Loss: 0.356014\n",
            "Epoch: 1467 \tTraining Loss: 0.355933\n",
            "Epoch: 1468 \tTraining Loss: 0.355830\n",
            "Epoch: 1469 \tTraining Loss: 0.355741\n",
            "Epoch: 1470 \tTraining Loss: 0.355651\n",
            "Epoch: 1471 \tTraining Loss: 0.355578\n",
            "Epoch: 1472 \tTraining Loss: 0.355504\n",
            "Epoch: 1473 \tTraining Loss: 0.355428\n",
            "Epoch: 1474 \tTraining Loss: 0.355355\n",
            "Epoch: 1475 \tTraining Loss: 0.355263\n",
            "Epoch: 1476 \tTraining Loss: 0.355191\n",
            "Epoch: 1477 \tTraining Loss: 0.355118\n",
            "Epoch: 1478 \tTraining Loss: 0.355042\n",
            "Epoch: 1479 \tTraining Loss: 0.354966\n",
            "Epoch: 1480 \tTraining Loss: 0.354894\n",
            "Epoch: 1481 \tTraining Loss: 0.354819\n",
            "Epoch: 1482 \tTraining Loss: 0.354746\n",
            "Epoch: 1483 \tTraining Loss: 0.354676\n",
            "Epoch: 1484 \tTraining Loss: 0.354600\n",
            "Epoch: 1485 \tTraining Loss: 0.354528\n",
            "Epoch: 1486 \tTraining Loss: 0.354459\n",
            "Epoch: 1487 \tTraining Loss: 0.354387\n",
            "Epoch: 1488 \tTraining Loss: 0.354317\n",
            "Epoch: 1489 \tTraining Loss: 0.354248\n",
            "Epoch: 1490 \tTraining Loss: 0.354175\n",
            "Epoch: 1491 \tTraining Loss: 0.354106\n",
            "Epoch: 1492 \tTraining Loss: 0.354039\n",
            "Epoch: 1493 \tTraining Loss: 0.353968\n",
            "Epoch: 1494 \tTraining Loss: 0.353898\n",
            "Epoch: 1495 \tTraining Loss: 0.353831\n",
            "Epoch: 1496 \tTraining Loss: 0.353761\n",
            "Epoch: 1497 \tTraining Loss: 0.353692\n",
            "Epoch: 1498 \tTraining Loss: 0.353624\n",
            "Epoch: 1499 \tTraining Loss: 0.353555\n",
            "Epoch: 1500 \tTraining Loss: 0.353488\n",
            "Epoch: 1501 \tTraining Loss: 0.353420\n",
            "Epoch: 1502 \tTraining Loss: 0.353351\n",
            "Epoch: 1503 \tTraining Loss: 0.353284\n",
            "Epoch: 1504 \tTraining Loss: 0.353216\n",
            "Epoch: 1505 \tTraining Loss: 0.353148\n",
            "Epoch: 1506 \tTraining Loss: 0.353081\n",
            "Epoch: 1507 \tTraining Loss: 0.353014\n",
            "Epoch: 1508 \tTraining Loss: 0.352946\n",
            "Epoch: 1509 \tTraining Loss: 0.352879\n",
            "Epoch: 1510 \tTraining Loss: 0.352812\n",
            "Epoch: 1511 \tTraining Loss: 0.352744\n",
            "Epoch: 1512 \tTraining Loss: 0.352677\n",
            "Epoch: 1513 \tTraining Loss: 0.352611\n",
            "Epoch: 1514 \tTraining Loss: 0.352544\n",
            "Epoch: 1515 \tTraining Loss: 0.352477\n",
            "Epoch: 1516 \tTraining Loss: 0.352410\n",
            "Epoch: 1517 \tTraining Loss: 0.352343\n",
            "Epoch: 1518 \tTraining Loss: 0.352276\n",
            "Epoch: 1519 \tTraining Loss: 0.352209\n",
            "Epoch: 1520 \tTraining Loss: 0.352143\n",
            "Epoch: 1521 \tTraining Loss: 0.352076\n",
            "Epoch: 1522 \tTraining Loss: 0.352009\n",
            "Epoch: 1523 \tTraining Loss: 0.351942\n",
            "Epoch: 1524 \tTraining Loss: 0.351876\n",
            "Epoch: 1525 \tTraining Loss: 0.351809\n",
            "Epoch: 1526 \tTraining Loss: 0.351742\n",
            "Epoch: 1527 \tTraining Loss: 0.351676\n",
            "Epoch: 1528 \tTraining Loss: 0.351609\n",
            "Epoch: 1529 \tTraining Loss: 0.351542\n",
            "Epoch: 1530 \tTraining Loss: 0.351476\n",
            "Epoch: 1531 \tTraining Loss: 0.351409\n",
            "Epoch: 1532 \tTraining Loss: 0.351342\n",
            "Epoch: 1533 \tTraining Loss: 0.351276\n",
            "Epoch: 1534 \tTraining Loss: 0.351209\n",
            "Epoch: 1535 \tTraining Loss: 0.351142\n",
            "Epoch: 1536 \tTraining Loss: 0.351075\n",
            "Epoch: 1537 \tTraining Loss: 0.351009\n",
            "Epoch: 1538 \tTraining Loss: 0.350942\n",
            "Epoch: 1539 \tTraining Loss: 0.350875\n",
            "Epoch: 1540 \tTraining Loss: 0.350808\n",
            "Epoch: 1541 \tTraining Loss: 0.350741\n",
            "Epoch: 1542 \tTraining Loss: 0.350674\n",
            "Epoch: 1543 \tTraining Loss: 0.350607\n",
            "Epoch: 1544 \tTraining Loss: 0.350540\n",
            "Epoch: 1545 \tTraining Loss: 0.350472\n",
            "Epoch: 1546 \tTraining Loss: 0.350405\n",
            "Epoch: 1547 \tTraining Loss: 0.350337\n",
            "Epoch: 1548 \tTraining Loss: 0.350269\n",
            "Epoch: 1549 \tTraining Loss: 0.350201\n",
            "Epoch: 1550 \tTraining Loss: 0.350132\n",
            "Epoch: 1551 \tTraining Loss: 0.350062\n",
            "Epoch: 1552 \tTraining Loss: 0.349991\n",
            "Epoch: 1553 \tTraining Loss: 0.349919\n",
            "Epoch: 1554 \tTraining Loss: 0.349846\n",
            "Epoch: 1555 \tTraining Loss: 0.349769\n",
            "Epoch: 1556 \tTraining Loss: 0.349690\n",
            "Epoch: 1557 \tTraining Loss: 0.349606\n",
            "Epoch: 1558 \tTraining Loss: 0.349518\n",
            "Epoch: 1559 \tTraining Loss: 0.349424\n",
            "Epoch: 1560 \tTraining Loss: 0.349327\n",
            "Epoch: 1561 \tTraining Loss: 0.349230\n",
            "Epoch: 1562 \tTraining Loss: 0.349137\n",
            "Epoch: 1563 \tTraining Loss: 0.349052\n",
            "Epoch: 1564 \tTraining Loss: 0.348979\n",
            "Epoch: 1565 \tTraining Loss: 0.348934\n",
            "Epoch: 1566 \tTraining Loss: 0.348968\n",
            "Epoch: 1567 \tTraining Loss: 0.349155\n",
            "Epoch: 1568 \tTraining Loss: 0.348737\n",
            "Epoch: 1569 \tTraining Loss: 0.349166\n",
            "Epoch: 1570 \tTraining Loss: 0.348675\n",
            "Epoch: 1571 \tTraining Loss: 0.349098\n",
            "Epoch: 1572 \tTraining Loss: 0.348962\n",
            "Epoch: 1573 \tTraining Loss: 0.348749\n",
            "Epoch: 1574 \tTraining Loss: 0.348461\n",
            "Epoch: 1575 \tTraining Loss: 0.348445\n",
            "Epoch: 1576 \tTraining Loss: 0.348503\n",
            "Epoch: 1577 \tTraining Loss: 0.348288\n",
            "Epoch: 1578 \tTraining Loss: 0.348111\n",
            "Epoch: 1579 \tTraining Loss: 0.348168\n",
            "Epoch: 1580 \tTraining Loss: 0.348264\n",
            "Epoch: 1581 \tTraining Loss: 0.347988\n",
            "Epoch: 1582 \tTraining Loss: 0.348342\n",
            "Epoch: 1583 \tTraining Loss: 0.348433\n",
            "Epoch: 1584 \tTraining Loss: 0.348066\n",
            "Epoch: 1585 \tTraining Loss: 0.348160\n",
            "Epoch: 1586 \tTraining Loss: 0.348339\n",
            "Epoch: 1587 \tTraining Loss: 0.348242\n",
            "Epoch: 1588 \tTraining Loss: 0.348035\n",
            "Epoch: 1589 \tTraining Loss: 0.347417\n",
            "Epoch: 1590 \tTraining Loss: 0.348025\n",
            "Epoch: 1591 \tTraining Loss: 0.347860\n",
            "Epoch: 1592 \tTraining Loss: 0.347384\n",
            "Epoch: 1593 \tTraining Loss: 0.347728\n",
            "Epoch: 1594 \tTraining Loss: 0.347930\n",
            "Epoch: 1595 \tTraining Loss: 0.347780\n",
            "Epoch: 1596 \tTraining Loss: 0.347736\n",
            "Epoch: 1597 \tTraining Loss: 0.347636\n",
            "Epoch: 1598 \tTraining Loss: 0.347522\n",
            "Epoch: 1599 \tTraining Loss: 0.347397\n",
            "Epoch: 1600 \tTraining Loss: 0.346759\n",
            "Epoch: 1601 \tTraining Loss: 0.347381\n",
            "Epoch: 1602 \tTraining Loss: 0.347441\n",
            "Epoch: 1603 \tTraining Loss: 0.347082\n",
            "Epoch: 1604 \tTraining Loss: 0.346527\n",
            "Epoch: 1605 \tTraining Loss: 0.346689\n",
            "Epoch: 1606 \tTraining Loss: 0.346451\n",
            "Epoch: 1607 \tTraining Loss: 0.346296\n",
            "Epoch: 1608 \tTraining Loss: 0.346304\n",
            "Epoch: 1609 \tTraining Loss: 0.346200\n",
            "Epoch: 1610 \tTraining Loss: 0.346110\n",
            "Epoch: 1611 \tTraining Loss: 0.346196\n",
            "Epoch: 1612 \tTraining Loss: 0.346228\n",
            "Epoch: 1613 \tTraining Loss: 0.346452\n",
            "Epoch: 1614 \tTraining Loss: 0.347100\n",
            "Epoch: 1615 \tTraining Loss: 0.347855\n",
            "Epoch: 1616 \tTraining Loss: 0.350082\n",
            "Epoch: 1617 \tTraining Loss: 0.351184\n",
            "Epoch: 1618 \tTraining Loss: 0.354521\n",
            "Epoch: 1619 \tTraining Loss: 0.350386\n",
            "Epoch: 1620 \tTraining Loss: 0.347752\n",
            "Epoch: 1621 \tTraining Loss: 0.345774\n",
            "Epoch: 1622 \tTraining Loss: 0.347891\n",
            "Epoch: 1623 \tTraining Loss: 0.350544\n",
            "Epoch: 1624 \tTraining Loss: 0.346456\n",
            "Epoch: 1625 \tTraining Loss: 0.345766\n",
            "Epoch: 1626 \tTraining Loss: 0.347930\n",
            "Epoch: 1627 \tTraining Loss: 0.346533\n",
            "Epoch: 1628 \tTraining Loss: 0.345206\n",
            "Epoch: 1629 \tTraining Loss: 0.346014\n",
            "Epoch: 1630 \tTraining Loss: 0.345986\n",
            "Epoch: 1631 \tTraining Loss: 0.345068\n",
            "Epoch: 1632 \tTraining Loss: 0.345313\n",
            "Epoch: 1633 \tTraining Loss: 0.345582\n",
            "Epoch: 1634 \tTraining Loss: 0.345115\n",
            "Epoch: 1635 \tTraining Loss: 0.345042\n",
            "Epoch: 1636 \tTraining Loss: 0.344872\n",
            "Epoch: 1637 \tTraining Loss: 0.344913\n",
            "Epoch: 1638 \tTraining Loss: 0.344708\n",
            "Epoch: 1639 \tTraining Loss: 0.344520\n",
            "Epoch: 1640 \tTraining Loss: 0.344505\n",
            "Epoch: 1641 \tTraining Loss: 0.344726\n",
            "Epoch: 1642 \tTraining Loss: 0.344280\n",
            "Epoch: 1643 \tTraining Loss: 0.344172\n",
            "Epoch: 1644 \tTraining Loss: 0.344314\n",
            "Epoch: 1645 \tTraining Loss: 0.344257\n",
            "Epoch: 1646 \tTraining Loss: 0.344091\n",
            "Epoch: 1647 \tTraining Loss: 0.344207\n",
            "Epoch: 1648 \tTraining Loss: 0.343884\n",
            "Epoch: 1649 \tTraining Loss: 0.343927\n",
            "Epoch: 1650 \tTraining Loss: 0.344078\n",
            "Epoch: 1651 \tTraining Loss: 0.343747\n",
            "Epoch: 1652 \tTraining Loss: 0.343562\n",
            "Epoch: 1653 \tTraining Loss: 0.343763\n",
            "Epoch: 1654 \tTraining Loss: 0.343573\n",
            "Epoch: 1655 \tTraining Loss: 0.343476\n",
            "Epoch: 1656 \tTraining Loss: 0.343574\n",
            "Epoch: 1657 \tTraining Loss: 0.343296\n",
            "Epoch: 1658 \tTraining Loss: 0.343185\n",
            "Epoch: 1659 \tTraining Loss: 0.343310\n",
            "Epoch: 1660 \tTraining Loss: 0.343173\n",
            "Epoch: 1661 \tTraining Loss: 0.343092\n",
            "Epoch: 1662 \tTraining Loss: 0.343272\n",
            "Epoch: 1663 \tTraining Loss: 0.342933\n",
            "Epoch: 1664 \tTraining Loss: 0.342840\n",
            "Epoch: 1665 \tTraining Loss: 0.342888\n",
            "Epoch: 1666 \tTraining Loss: 0.342747\n",
            "Epoch: 1667 \tTraining Loss: 0.342607\n",
            "Epoch: 1668 \tTraining Loss: 0.342802\n",
            "Epoch: 1669 \tTraining Loss: 0.342600\n",
            "Epoch: 1670 \tTraining Loss: 0.342511\n",
            "Epoch: 1671 \tTraining Loss: 0.342654\n",
            "Epoch: 1672 \tTraining Loss: 0.342401\n",
            "Epoch: 1673 \tTraining Loss: 0.342291\n",
            "Epoch: 1674 \tTraining Loss: 0.342417\n",
            "Epoch: 1675 \tTraining Loss: 0.342240\n",
            "Epoch: 1676 \tTraining Loss: 0.342137\n",
            "Epoch: 1677 \tTraining Loss: 0.342306\n",
            "Epoch: 1678 \tTraining Loss: 0.342050\n",
            "Epoch: 1679 \tTraining Loss: 0.341962\n",
            "Epoch: 1680 \tTraining Loss: 0.342099\n",
            "Epoch: 1681 \tTraining Loss: 0.341984\n",
            "Epoch: 1682 \tTraining Loss: 0.341975\n",
            "Epoch: 1683 \tTraining Loss: 0.342349\n",
            "Epoch: 1684 \tTraining Loss: 0.342304\n",
            "Epoch: 1685 \tTraining Loss: 0.342727\n",
            "Epoch: 1686 \tTraining Loss: 0.343409\n",
            "Epoch: 1687 \tTraining Loss: 0.344784\n",
            "Epoch: 1688 \tTraining Loss: 0.345564\n",
            "Epoch: 1689 \tTraining Loss: 0.348411\n",
            "Epoch: 1690 \tTraining Loss: 0.346219\n",
            "Epoch: 1691 \tTraining Loss: 0.345217\n",
            "Epoch: 1692 \tTraining Loss: 0.341838\n",
            "Epoch: 1693 \tTraining Loss: 0.341571\n",
            "Epoch: 1694 \tTraining Loss: 0.343456\n",
            "Epoch: 1695 \tTraining Loss: 0.343272\n",
            "Epoch: 1696 \tTraining Loss: 0.341976\n",
            "Epoch: 1697 \tTraining Loss: 0.341032\n",
            "Epoch: 1698 \tTraining Loss: 0.342052\n",
            "Epoch: 1699 \tTraining Loss: 0.342582\n",
            "Epoch: 1700 \tTraining Loss: 0.341386\n",
            "Epoch: 1701 \tTraining Loss: 0.340844\n",
            "Epoch: 1702 \tTraining Loss: 0.341551\n",
            "Epoch: 1703 \tTraining Loss: 0.341583\n",
            "Epoch: 1704 \tTraining Loss: 0.341003\n",
            "Epoch: 1705 \tTraining Loss: 0.340570\n",
            "Epoch: 1706 \tTraining Loss: 0.341091\n",
            "Epoch: 1707 \tTraining Loss: 0.341529\n",
            "Epoch: 1708 \tTraining Loss: 0.340554\n",
            "Epoch: 1709 \tTraining Loss: 0.340277\n",
            "Epoch: 1710 \tTraining Loss: 0.340853\n",
            "Epoch: 1711 \tTraining Loss: 0.340658\n",
            "Epoch: 1712 \tTraining Loss: 0.340216\n",
            "Epoch: 1713 \tTraining Loss: 0.340130\n",
            "Epoch: 1714 \tTraining Loss: 0.340135\n",
            "Epoch: 1715 \tTraining Loss: 0.340170\n",
            "Epoch: 1716 \tTraining Loss: 0.340218\n",
            "Epoch: 1717 \tTraining Loss: 0.339818\n",
            "Epoch: 1718 \tTraining Loss: 0.339690\n",
            "Epoch: 1719 \tTraining Loss: 0.339856\n",
            "Epoch: 1720 \tTraining Loss: 0.339777\n",
            "Epoch: 1721 \tTraining Loss: 0.339702\n",
            "Epoch: 1722 \tTraining Loss: 0.339668\n",
            "Epoch: 1723 \tTraining Loss: 0.339387\n",
            "Epoch: 1724 \tTraining Loss: 0.339317\n",
            "Epoch: 1725 \tTraining Loss: 0.339543\n",
            "Epoch: 1726 \tTraining Loss: 0.339389\n",
            "Epoch: 1727 \tTraining Loss: 0.339224\n",
            "Epoch: 1728 \tTraining Loss: 0.339285\n",
            "Epoch: 1729 \tTraining Loss: 0.339032\n",
            "Epoch: 1730 \tTraining Loss: 0.338947\n",
            "Epoch: 1731 \tTraining Loss: 0.339049\n",
            "Epoch: 1732 \tTraining Loss: 0.338904\n",
            "Epoch: 1733 \tTraining Loss: 0.338862\n",
            "Epoch: 1734 \tTraining Loss: 0.338971\n",
            "Epoch: 1735 \tTraining Loss: 0.338762\n",
            "Epoch: 1736 \tTraining Loss: 0.338618\n",
            "Epoch: 1737 \tTraining Loss: 0.338695\n",
            "Epoch: 1738 \tTraining Loss: 0.338511\n",
            "Epoch: 1739 \tTraining Loss: 0.338412\n",
            "Epoch: 1740 \tTraining Loss: 0.338500\n",
            "Epoch: 1741 \tTraining Loss: 0.338328\n",
            "Epoch: 1742 \tTraining Loss: 0.338263\n",
            "Epoch: 1743 \tTraining Loss: 0.338358\n",
            "Epoch: 1744 \tTraining Loss: 0.338230\n",
            "Epoch: 1745 \tTraining Loss: 0.338154\n",
            "Epoch: 1746 \tTraining Loss: 0.338275\n",
            "Epoch: 1747 \tTraining Loss: 0.338267\n",
            "Epoch: 1748 \tTraining Loss: 0.338308\n",
            "Epoch: 1749 \tTraining Loss: 0.338727\n",
            "Epoch: 1750 \tTraining Loss: 0.338922\n",
            "Epoch: 1751 \tTraining Loss: 0.339781\n",
            "Epoch: 1752 \tTraining Loss: 0.340812\n",
            "Epoch: 1753 \tTraining Loss: 0.343888\n",
            "Epoch: 1754 \tTraining Loss: 0.344996\n",
            "Epoch: 1755 \tTraining Loss: 0.350496\n",
            "Epoch: 1756 \tTraining Loss: 0.345614\n",
            "Epoch: 1757 \tTraining Loss: 0.344777\n",
            "Epoch: 1758 \tTraining Loss: 0.338264\n",
            "Epoch: 1759 \tTraining Loss: 0.341478\n",
            "Epoch: 1760 \tTraining Loss: 0.346342\n",
            "Epoch: 1761 \tTraining Loss: 0.338333\n",
            "Epoch: 1762 \tTraining Loss: 0.340950\n",
            "Epoch: 1763 \tTraining Loss: 0.348273\n",
            "Epoch: 1764 \tTraining Loss: 0.338766\n",
            "Epoch: 1765 \tTraining Loss: 0.341103\n",
            "Epoch: 1766 \tTraining Loss: 0.346068\n",
            "Epoch: 1767 \tTraining Loss: 0.337835\n",
            "Epoch: 1768 \tTraining Loss: 0.345340\n",
            "Epoch: 1769 \tTraining Loss: 0.349596\n",
            "Epoch: 1770 \tTraining Loss: 0.337693\n",
            "Epoch: 1771 \tTraining Loss: 0.350413\n",
            "Epoch: 1772 \tTraining Loss: 0.386044\n",
            "Epoch: 1773 \tTraining Loss: 0.357483\n",
            "Epoch: 1774 \tTraining Loss: 0.391705\n",
            "Epoch: 1775 \tTraining Loss: 0.351811\n",
            "Epoch: 1776 \tTraining Loss: 0.369889\n",
            "Epoch: 1777 \tTraining Loss: 0.356365\n",
            "Epoch: 1778 \tTraining Loss: 0.365150\n",
            "Epoch: 1779 \tTraining Loss: 0.363485\n",
            "Epoch: 1780 \tTraining Loss: 0.354514\n",
            "Epoch: 1781 \tTraining Loss: 0.364019\n",
            "Epoch: 1782 \tTraining Loss: 0.345922\n",
            "Epoch: 1783 \tTraining Loss: 0.355249\n",
            "Epoch: 1784 \tTraining Loss: 0.356416\n",
            "Epoch: 1785 \tTraining Loss: 0.345245\n",
            "Epoch: 1786 \tTraining Loss: 0.348067\n",
            "Epoch: 1787 \tTraining Loss: 0.352087\n",
            "Epoch: 1788 \tTraining Loss: 0.342907\n",
            "Epoch: 1789 \tTraining Loss: 0.347825\n",
            "Epoch: 1790 \tTraining Loss: 0.346555\n",
            "Epoch: 1791 \tTraining Loss: 0.345718\n",
            "Epoch: 1792 \tTraining Loss: 0.341297\n",
            "Epoch: 1793 \tTraining Loss: 0.345964\n",
            "Epoch: 1794 \tTraining Loss: 0.341754\n",
            "Epoch: 1795 \tTraining Loss: 0.343196\n",
            "Epoch: 1796 \tTraining Loss: 0.342313\n",
            "Epoch: 1797 \tTraining Loss: 0.340629\n",
            "Epoch: 1798 \tTraining Loss: 0.340436\n",
            "Epoch: 1799 \tTraining Loss: 0.342129\n",
            "Epoch: 1800 \tTraining Loss: 0.340135\n",
            "Epoch: 1801 \tTraining Loss: 0.340835\n",
            "Epoch: 1802 \tTraining Loss: 0.339117\n",
            "Epoch: 1803 \tTraining Loss: 0.339751\n",
            "Epoch: 1804 \tTraining Loss: 0.338210\n",
            "Epoch: 1805 \tTraining Loss: 0.339652\n",
            "Epoch: 1806 \tTraining Loss: 0.338519\n",
            "Epoch: 1807 \tTraining Loss: 0.338170\n",
            "Epoch: 1808 \tTraining Loss: 0.337490\n",
            "Epoch: 1809 \tTraining Loss: 0.338319\n",
            "Epoch: 1810 \tTraining Loss: 0.338520\n",
            "Epoch: 1811 \tTraining Loss: 0.337118\n",
            "Epoch: 1812 \tTraining Loss: 0.338043\n",
            "Epoch: 1813 \tTraining Loss: 0.337310\n",
            "Epoch: 1814 \tTraining Loss: 0.337120\n",
            "Epoch: 1815 \tTraining Loss: 0.337049\n",
            "Epoch: 1816 \tTraining Loss: 0.336397\n",
            "Epoch: 1817 \tTraining Loss: 0.336492\n",
            "Epoch: 1818 \tTraining Loss: 0.335975\n",
            "Epoch: 1819 \tTraining Loss: 0.336498\n",
            "Epoch: 1820 \tTraining Loss: 0.336321\n",
            "Epoch: 1821 \tTraining Loss: 0.336286\n",
            "Epoch: 1822 \tTraining Loss: 0.335932\n",
            "Epoch: 1823 \tTraining Loss: 0.335567\n",
            "Epoch: 1824 \tTraining Loss: 0.335981\n",
            "Epoch: 1825 \tTraining Loss: 0.335575\n",
            "Epoch: 1826 \tTraining Loss: 0.335565\n",
            "Epoch: 1827 \tTraining Loss: 0.335348\n",
            "Epoch: 1828 \tTraining Loss: 0.335579\n",
            "Epoch: 1829 \tTraining Loss: 0.335139\n",
            "Epoch: 1830 \tTraining Loss: 0.335253\n",
            "Epoch: 1831 \tTraining Loss: 0.334919\n",
            "Epoch: 1832 \tTraining Loss: 0.335098\n",
            "Epoch: 1833 \tTraining Loss: 0.334735\n",
            "Epoch: 1834 \tTraining Loss: 0.334912\n",
            "Epoch: 1835 \tTraining Loss: 0.334538\n",
            "Epoch: 1836 \tTraining Loss: 0.334692\n",
            "Epoch: 1837 \tTraining Loss: 0.334467\n",
            "Epoch: 1838 \tTraining Loss: 0.334456\n",
            "Epoch: 1839 \tTraining Loss: 0.334335\n",
            "Epoch: 1840 \tTraining Loss: 0.334291\n",
            "Epoch: 1841 \tTraining Loss: 0.334225\n",
            "Epoch: 1842 \tTraining Loss: 0.334101\n",
            "Epoch: 1843 \tTraining Loss: 0.334099\n",
            "Epoch: 1844 \tTraining Loss: 0.333969\n",
            "Epoch: 1845 \tTraining Loss: 0.333963\n",
            "Epoch: 1846 \tTraining Loss: 0.333851\n",
            "Epoch: 1847 \tTraining Loss: 0.333811\n",
            "Epoch: 1848 \tTraining Loss: 0.333726\n",
            "Epoch: 1849 \tTraining Loss: 0.333689\n",
            "Epoch: 1850 \tTraining Loss: 0.333615\n",
            "Epoch: 1851 \tTraining Loss: 0.333545\n",
            "Epoch: 1852 \tTraining Loss: 0.333509\n",
            "Epoch: 1853 \tTraining Loss: 0.333414\n",
            "Epoch: 1854 \tTraining Loss: 0.333392\n",
            "Epoch: 1855 \tTraining Loss: 0.333299\n",
            "Epoch: 1856 \tTraining Loss: 0.333270\n",
            "Epoch: 1857 \tTraining Loss: 0.333186\n",
            "Epoch: 1858 \tTraining Loss: 0.333151\n",
            "Epoch: 1859 \tTraining Loss: 0.333076\n",
            "Epoch: 1860 \tTraining Loss: 0.333029\n",
            "Epoch: 1861 \tTraining Loss: 0.332969\n",
            "Epoch: 1862 \tTraining Loss: 0.332912\n",
            "Epoch: 1863 \tTraining Loss: 0.332862\n",
            "Epoch: 1864 \tTraining Loss: 0.332797\n",
            "Epoch: 1865 \tTraining Loss: 0.332749\n",
            "Epoch: 1866 \tTraining Loss: 0.332687\n",
            "Epoch: 1867 \tTraining Loss: 0.332639\n",
            "Epoch: 1868 \tTraining Loss: 0.332577\n",
            "Epoch: 1869 \tTraining Loss: 0.332528\n",
            "Epoch: 1870 \tTraining Loss: 0.332470\n",
            "Epoch: 1871 \tTraining Loss: 0.332417\n",
            "Epoch: 1872 \tTraining Loss: 0.332363\n",
            "Epoch: 1873 \tTraining Loss: 0.332307\n",
            "Epoch: 1874 \tTraining Loss: 0.332254\n",
            "Epoch: 1875 \tTraining Loss: 0.332200\n",
            "Epoch: 1876 \tTraining Loss: 0.332146\n",
            "Epoch: 1877 \tTraining Loss: 0.332092\n",
            "Epoch: 1878 \tTraining Loss: 0.332039\n",
            "Epoch: 1879 \tTraining Loss: 0.331985\n",
            "Epoch: 1880 \tTraining Loss: 0.331931\n",
            "Epoch: 1881 \tTraining Loss: 0.331879\n",
            "Epoch: 1882 \tTraining Loss: 0.331825\n",
            "Epoch: 1883 \tTraining Loss: 0.331772\n",
            "Epoch: 1884 \tTraining Loss: 0.331719\n",
            "Epoch: 1885 \tTraining Loss: 0.331666\n",
            "Epoch: 1886 \tTraining Loss: 0.331613\n",
            "Epoch: 1887 \tTraining Loss: 0.331560\n",
            "Epoch: 1888 \tTraining Loss: 0.331507\n",
            "Epoch: 1889 \tTraining Loss: 0.331454\n",
            "Epoch: 1890 \tTraining Loss: 0.331401\n",
            "Epoch: 1891 \tTraining Loss: 0.331348\n",
            "Epoch: 1892 \tTraining Loss: 0.331296\n",
            "Epoch: 1893 \tTraining Loss: 0.331243\n",
            "Epoch: 1894 \tTraining Loss: 0.331191\n",
            "Epoch: 1895 \tTraining Loss: 0.331138\n",
            "Epoch: 1896 \tTraining Loss: 0.331086\n",
            "Epoch: 1897 \tTraining Loss: 0.331033\n",
            "Epoch: 1898 \tTraining Loss: 0.330981\n",
            "Epoch: 1899 \tTraining Loss: 0.330929\n",
            "Epoch: 1900 \tTraining Loss: 0.330876\n",
            "Epoch: 1901 \tTraining Loss: 0.330824\n",
            "Epoch: 1902 \tTraining Loss: 0.330772\n",
            "Epoch: 1903 \tTraining Loss: 0.330720\n",
            "Epoch: 1904 \tTraining Loss: 0.330667\n",
            "Epoch: 1905 \tTraining Loss: 0.330615\n",
            "Epoch: 1906 \tTraining Loss: 0.330563\n",
            "Epoch: 1907 \tTraining Loss: 0.330511\n",
            "Epoch: 1908 \tTraining Loss: 0.330459\n",
            "Epoch: 1909 \tTraining Loss: 0.330407\n",
            "Epoch: 1910 \tTraining Loss: 0.330355\n",
            "Epoch: 1911 \tTraining Loss: 0.330304\n",
            "Epoch: 1912 \tTraining Loss: 0.330252\n",
            "Epoch: 1913 \tTraining Loss: 0.330200\n",
            "Epoch: 1914 \tTraining Loss: 0.330148\n",
            "Epoch: 1915 \tTraining Loss: 0.330096\n",
            "Epoch: 1916 \tTraining Loss: 0.330044\n",
            "Epoch: 1917 \tTraining Loss: 0.329993\n",
            "Epoch: 1918 \tTraining Loss: 0.329941\n",
            "Epoch: 1919 \tTraining Loss: 0.329889\n",
            "Epoch: 1920 \tTraining Loss: 0.329838\n",
            "Epoch: 1921 \tTraining Loss: 0.329786\n",
            "Epoch: 1922 \tTraining Loss: 0.329734\n",
            "Epoch: 1923 \tTraining Loss: 0.329683\n",
            "Epoch: 1924 \tTraining Loss: 0.329631\n",
            "Epoch: 1925 \tTraining Loss: 0.329580\n",
            "Epoch: 1926 \tTraining Loss: 0.329528\n",
            "Epoch: 1927 \tTraining Loss: 0.329477\n",
            "Epoch: 1928 \tTraining Loss: 0.329425\n",
            "Epoch: 1929 \tTraining Loss: 0.329374\n",
            "Epoch: 1930 \tTraining Loss: 0.329323\n",
            "Epoch: 1931 \tTraining Loss: 0.329271\n",
            "Epoch: 1932 \tTraining Loss: 0.329220\n",
            "Epoch: 1933 \tTraining Loss: 0.329168\n",
            "Epoch: 1934 \tTraining Loss: 0.329117\n",
            "Epoch: 1935 \tTraining Loss: 0.329066\n",
            "Epoch: 1936 \tTraining Loss: 0.329015\n",
            "Epoch: 1937 \tTraining Loss: 0.328963\n",
            "Epoch: 1938 \tTraining Loss: 0.328912\n",
            "Epoch: 1939 \tTraining Loss: 0.328861\n",
            "Epoch: 1940 \tTraining Loss: 0.328810\n",
            "Epoch: 1941 \tTraining Loss: 0.328758\n",
            "Epoch: 1942 \tTraining Loss: 0.328707\n",
            "Epoch: 1943 \tTraining Loss: 0.328656\n",
            "Epoch: 1944 \tTraining Loss: 0.328605\n",
            "Epoch: 1945 \tTraining Loss: 0.328554\n",
            "Epoch: 1946 \tTraining Loss: 0.328503\n",
            "Epoch: 1947 \tTraining Loss: 0.328452\n",
            "Epoch: 1948 \tTraining Loss: 0.328401\n",
            "Epoch: 1949 \tTraining Loss: 0.328350\n",
            "Epoch: 1950 \tTraining Loss: 0.328299\n",
            "Epoch: 1951 \tTraining Loss: 0.328248\n",
            "Epoch: 1952 \tTraining Loss: 0.328197\n",
            "Epoch: 1953 \tTraining Loss: 0.328146\n",
            "Epoch: 1954 \tTraining Loss: 0.328095\n",
            "Epoch: 1955 \tTraining Loss: 0.328044\n",
            "Epoch: 1956 \tTraining Loss: 0.327993\n",
            "Epoch: 1957 \tTraining Loss: 0.327942\n",
            "Epoch: 1958 \tTraining Loss: 0.327891\n",
            "Epoch: 1959 \tTraining Loss: 0.327840\n",
            "Epoch: 1960 \tTraining Loss: 0.327790\n",
            "Epoch: 1961 \tTraining Loss: 0.327739\n",
            "Epoch: 1962 \tTraining Loss: 0.327688\n",
            "Epoch: 1963 \tTraining Loss: 0.327637\n",
            "Epoch: 1964 \tTraining Loss: 0.327586\n",
            "Epoch: 1965 \tTraining Loss: 0.327536\n",
            "Epoch: 1966 \tTraining Loss: 0.327485\n",
            "Epoch: 1967 \tTraining Loss: 0.327434\n",
            "Epoch: 1968 \tTraining Loss: 0.327383\n",
            "Epoch: 1969 \tTraining Loss: 0.327333\n",
            "Epoch: 1970 \tTraining Loss: 0.327282\n",
            "Epoch: 1971 \tTraining Loss: 0.327231\n",
            "Epoch: 1972 \tTraining Loss: 0.327181\n",
            "Epoch: 1973 \tTraining Loss: 0.327130\n",
            "Epoch: 1974 \tTraining Loss: 0.327079\n",
            "Epoch: 1975 \tTraining Loss: 0.327029\n",
            "Epoch: 1976 \tTraining Loss: 0.326978\n",
            "Epoch: 1977 \tTraining Loss: 0.326928\n",
            "Epoch: 1978 \tTraining Loss: 0.326877\n",
            "Epoch: 1979 \tTraining Loss: 0.326826\n",
            "Epoch: 1980 \tTraining Loss: 0.326776\n",
            "Epoch: 1981 \tTraining Loss: 0.326725\n",
            "Epoch: 1982 \tTraining Loss: 0.326675\n",
            "Epoch: 1983 \tTraining Loss: 0.326624\n",
            "Epoch: 1984 \tTraining Loss: 0.326574\n",
            "Epoch: 1985 \tTraining Loss: 0.326523\n",
            "Epoch: 1986 \tTraining Loss: 0.326473\n",
            "Epoch: 1987 \tTraining Loss: 0.326422\n",
            "Epoch: 1988 \tTraining Loss: 0.326372\n",
            "Epoch: 1989 \tTraining Loss: 0.326321\n",
            "Epoch: 1990 \tTraining Loss: 0.326271\n",
            "Epoch: 1991 \tTraining Loss: 0.326221\n",
            "Epoch: 1992 \tTraining Loss: 0.326170\n",
            "Epoch: 1993 \tTraining Loss: 0.326120\n",
            "Epoch: 1994 \tTraining Loss: 0.326069\n",
            "Epoch: 1995 \tTraining Loss: 0.326019\n",
            "Epoch: 1996 \tTraining Loss: 0.325969\n",
            "Epoch: 1997 \tTraining Loss: 0.325918\n",
            "Epoch: 1998 \tTraining Loss: 0.325868\n",
            "Epoch: 1999 \tTraining Loss: 0.325818\n",
            "Epoch: 2000 \tTraining Loss: 0.325767\n",
            "Epoch: 2001 \tTraining Loss: 0.325717\n",
            "Epoch: 2002 \tTraining Loss: 0.325667\n",
            "Epoch: 2003 \tTraining Loss: 0.325616\n",
            "Epoch: 2004 \tTraining Loss: 0.325566\n",
            "Epoch: 2005 \tTraining Loss: 0.325516\n",
            "Epoch: 2006 \tTraining Loss: 0.325466\n",
            "Epoch: 2007 \tTraining Loss: 0.325415\n",
            "Epoch: 2008 \tTraining Loss: 0.325365\n",
            "Epoch: 2009 \tTraining Loss: 0.325315\n",
            "Epoch: 2010 \tTraining Loss: 0.325265\n",
            "Epoch: 2011 \tTraining Loss: 0.325215\n",
            "Epoch: 2012 \tTraining Loss: 0.325166\n",
            "Epoch: 2013 \tTraining Loss: 0.325118\n",
            "Epoch: 2014 \tTraining Loss: 0.325071\n",
            "Epoch: 2015 \tTraining Loss: 0.325027\n",
            "Epoch: 2016 \tTraining Loss: 0.324990\n",
            "Epoch: 2017 \tTraining Loss: 0.324970\n",
            "Epoch: 2018 \tTraining Loss: 0.324977\n",
            "Epoch: 2019 \tTraining Loss: 0.325069\n",
            "Epoch: 2020 \tTraining Loss: 0.325280\n",
            "Epoch: 2021 \tTraining Loss: 0.326008\n",
            "Epoch: 2022 \tTraining Loss: 0.327297\n",
            "Epoch: 2023 \tTraining Loss: 0.332700\n",
            "Epoch: 2024 \tTraining Loss: 0.341156\n",
            "Epoch: 2025 \tTraining Loss: 0.376192\n",
            "Epoch: 2026 \tTraining Loss: 0.331680\n",
            "Epoch: 2027 \tTraining Loss: 0.333425\n",
            "Epoch: 2028 \tTraining Loss: 0.359114\n",
            "Epoch: 2029 \tTraining Loss: 0.331535\n",
            "Epoch: 2030 \tTraining Loss: 0.334023\n",
            "Epoch: 2031 \tTraining Loss: 0.371923\n",
            "Epoch: 2032 \tTraining Loss: 0.329187\n",
            "Epoch: 2033 \tTraining Loss: 0.362734\n",
            "Epoch: 2034 \tTraining Loss: 0.373710\n",
            "Epoch: 2035 \tTraining Loss: 0.346107\n",
            "Epoch: 2036 \tTraining Loss: 0.398630\n",
            "Epoch: 2037 \tTraining Loss: 0.357806\n",
            "Epoch: 2038 \tTraining Loss: 0.372465\n",
            "Epoch: 2039 \tTraining Loss: 0.348228\n",
            "Epoch: 2040 \tTraining Loss: 0.350402\n",
            "Epoch: 2041 \tTraining Loss: 0.341927\n",
            "Epoch: 2042 \tTraining Loss: 0.344747\n",
            "Epoch: 2043 \tTraining Loss: 0.347026\n",
            "Epoch: 2044 \tTraining Loss: 0.336218\n",
            "Epoch: 2045 \tTraining Loss: 0.344952\n",
            "Epoch: 2046 \tTraining Loss: 0.335774\n",
            "Epoch: 2047 \tTraining Loss: 0.344353\n",
            "Epoch: 2048 \tTraining Loss: 0.334394\n",
            "Epoch: 2049 \tTraining Loss: 0.335062\n",
            "Epoch: 2050 \tTraining Loss: 0.338545\n",
            "Epoch: 2051 \tTraining Loss: 0.332990\n",
            "Epoch: 2052 \tTraining Loss: 0.339187\n",
            "Epoch: 2053 \tTraining Loss: 0.334538\n",
            "Epoch: 2054 \tTraining Loss: 0.332932\n",
            "Epoch: 2055 \tTraining Loss: 0.334804\n",
            "Epoch: 2056 \tTraining Loss: 0.329614\n",
            "Epoch: 2057 \tTraining Loss: 0.335285\n",
            "Epoch: 2058 \tTraining Loss: 0.332701\n",
            "Epoch: 2059 \tTraining Loss: 0.328635\n",
            "Epoch: 2060 \tTraining Loss: 0.330370\n",
            "Epoch: 2061 \tTraining Loss: 0.328167\n",
            "Epoch: 2062 \tTraining Loss: 0.328565\n",
            "Epoch: 2063 \tTraining Loss: 0.328248\n",
            "Epoch: 2064 \tTraining Loss: 0.327973\n",
            "Epoch: 2065 \tTraining Loss: 0.327128\n",
            "Epoch: 2066 \tTraining Loss: 0.326623\n",
            "Epoch: 2067 \tTraining Loss: 0.326954\n",
            "Epoch: 2068 \tTraining Loss: 0.327039\n",
            "Epoch: 2069 \tTraining Loss: 0.326193\n",
            "Epoch: 2070 \tTraining Loss: 0.325327\n",
            "Epoch: 2071 \tTraining Loss: 0.325621\n",
            "Epoch: 2072 \tTraining Loss: 0.325236\n",
            "Epoch: 2073 \tTraining Loss: 0.325694\n",
            "Epoch: 2074 \tTraining Loss: 0.324776\n",
            "Epoch: 2075 \tTraining Loss: 0.325325\n",
            "Epoch: 2076 \tTraining Loss: 0.325098\n",
            "Epoch: 2077 \tTraining Loss: 0.324572\n",
            "Epoch: 2078 \tTraining Loss: 0.324247\n",
            "Epoch: 2079 \tTraining Loss: 0.324694\n",
            "Epoch: 2080 \tTraining Loss: 0.324077\n",
            "Epoch: 2081 \tTraining Loss: 0.325209\n",
            "Epoch: 2082 \tTraining Loss: 0.323540\n",
            "Epoch: 2083 \tTraining Loss: 0.325045\n",
            "Epoch: 2084 \tTraining Loss: 0.325042\n",
            "Epoch: 2085 \tTraining Loss: 0.324556\n",
            "Epoch: 2086 \tTraining Loss: 0.323416\n",
            "Epoch: 2087 \tTraining Loss: 0.323583\n",
            "Epoch: 2088 \tTraining Loss: 0.323206\n",
            "Epoch: 2089 \tTraining Loss: 0.323170\n",
            "Epoch: 2090 \tTraining Loss: 0.323261\n",
            "Epoch: 2091 \tTraining Loss: 0.322957\n",
            "Epoch: 2092 \tTraining Loss: 0.323699\n",
            "Epoch: 2093 \tTraining Loss: 0.322702\n",
            "Epoch: 2094 \tTraining Loss: 0.323801\n",
            "Epoch: 2095 \tTraining Loss: 0.323973\n",
            "Epoch: 2096 \tTraining Loss: 0.323429\n",
            "Epoch: 2097 \tTraining Loss: 0.322594\n",
            "Epoch: 2098 \tTraining Loss: 0.323014\n",
            "Epoch: 2099 \tTraining Loss: 0.322269\n",
            "Epoch: 2100 \tTraining Loss: 0.323079\n",
            "Epoch: 2101 \tTraining Loss: 0.322159\n",
            "Epoch: 2102 \tTraining Loss: 0.322237\n",
            "Epoch: 2103 \tTraining Loss: 0.322387\n",
            "Epoch: 2104 \tTraining Loss: 0.321931\n",
            "Epoch: 2105 \tTraining Loss: 0.322335\n",
            "Epoch: 2106 \tTraining Loss: 0.321890\n",
            "Epoch: 2107 \tTraining Loss: 0.322308\n",
            "Epoch: 2108 \tTraining Loss: 0.321757\n",
            "Epoch: 2109 \tTraining Loss: 0.322374\n",
            "Epoch: 2110 \tTraining Loss: 0.321642\n",
            "Epoch: 2111 \tTraining Loss: 0.321671\n",
            "Epoch: 2112 \tTraining Loss: 0.321762\n",
            "Epoch: 2113 \tTraining Loss: 0.321500\n",
            "Epoch: 2114 \tTraining Loss: 0.321445\n",
            "Epoch: 2115 \tTraining Loss: 0.321590\n",
            "Epoch: 2116 \tTraining Loss: 0.321334\n",
            "Epoch: 2117 \tTraining Loss: 0.321626\n",
            "Epoch: 2118 \tTraining Loss: 0.321237\n",
            "Epoch: 2119 \tTraining Loss: 0.321213\n",
            "Epoch: 2120 \tTraining Loss: 0.321324\n",
            "Epoch: 2121 \tTraining Loss: 0.321103\n",
            "Epoch: 2122 \tTraining Loss: 0.321554\n",
            "Epoch: 2123 \tTraining Loss: 0.320941\n",
            "Epoch: 2124 \tTraining Loss: 0.321778\n",
            "Epoch: 2125 \tTraining Loss: 0.320935\n",
            "Epoch: 2126 \tTraining Loss: 0.320937\n",
            "Epoch: 2127 \tTraining Loss: 0.321490\n",
            "Epoch: 2128 \tTraining Loss: 0.320692\n",
            "Epoch: 2129 \tTraining Loss: 0.321813\n",
            "Epoch: 2130 \tTraining Loss: 0.321863\n",
            "Epoch: 2131 \tTraining Loss: 0.321510\n",
            "Epoch: 2132 \tTraining Loss: 0.320703\n",
            "Epoch: 2133 \tTraining Loss: 0.321729\n",
            "Epoch: 2134 \tTraining Loss: 0.320585\n",
            "Epoch: 2135 \tTraining Loss: 0.320689\n",
            "Epoch: 2136 \tTraining Loss: 0.320542\n",
            "Epoch: 2137 \tTraining Loss: 0.320387\n",
            "Epoch: 2138 \tTraining Loss: 0.320611\n",
            "Epoch: 2139 \tTraining Loss: 0.320325\n",
            "Epoch: 2140 \tTraining Loss: 0.320186\n",
            "Epoch: 2141 \tTraining Loss: 0.320402\n",
            "Epoch: 2142 \tTraining Loss: 0.320088\n",
            "Epoch: 2143 \tTraining Loss: 0.320171\n",
            "Epoch: 2144 \tTraining Loss: 0.320189\n",
            "Epoch: 2145 \tTraining Loss: 0.319988\n",
            "Epoch: 2146 \tTraining Loss: 0.319915\n",
            "Epoch: 2147 \tTraining Loss: 0.320017\n",
            "Epoch: 2148 \tTraining Loss: 0.319795\n",
            "Epoch: 2149 \tTraining Loss: 0.319810\n",
            "Epoch: 2150 \tTraining Loss: 0.319806\n",
            "Epoch: 2151 \tTraining Loss: 0.319698\n",
            "Epoch: 2152 \tTraining Loss: 0.319625\n",
            "Epoch: 2153 \tTraining Loss: 0.319625\n",
            "Epoch: 2154 \tTraining Loss: 0.319551\n",
            "Epoch: 2155 \tTraining Loss: 0.319474\n",
            "Epoch: 2156 \tTraining Loss: 0.319469\n",
            "Epoch: 2157 \tTraining Loss: 0.319409\n",
            "Epoch: 2158 \tTraining Loss: 0.319352\n",
            "Epoch: 2159 \tTraining Loss: 0.319315\n",
            "Epoch: 2160 \tTraining Loss: 0.319259\n",
            "Epoch: 2161 \tTraining Loss: 0.319211\n",
            "Epoch: 2162 \tTraining Loss: 0.319171\n",
            "Epoch: 2163 \tTraining Loss: 0.319116\n",
            "Epoch: 2164 \tTraining Loss: 0.319073\n",
            "Epoch: 2165 \tTraining Loss: 0.319037\n",
            "Epoch: 2166 \tTraining Loss: 0.318981\n",
            "Epoch: 2167 \tTraining Loss: 0.318933\n",
            "Epoch: 2168 \tTraining Loss: 0.318899\n",
            "Epoch: 2169 \tTraining Loss: 0.318849\n",
            "Epoch: 2170 \tTraining Loss: 0.318799\n",
            "Epoch: 2171 \tTraining Loss: 0.318764\n",
            "Epoch: 2172 \tTraining Loss: 0.318718\n",
            "Epoch: 2173 \tTraining Loss: 0.318666\n",
            "Epoch: 2174 \tTraining Loss: 0.318629\n",
            "Epoch: 2175 \tTraining Loss: 0.318587\n",
            "Epoch: 2176 \tTraining Loss: 0.318538\n",
            "Epoch: 2177 \tTraining Loss: 0.318497\n",
            "Epoch: 2178 \tTraining Loss: 0.318455\n",
            "Epoch: 2179 \tTraining Loss: 0.318409\n",
            "Epoch: 2180 \tTraining Loss: 0.318367\n",
            "Epoch: 2181 \tTraining Loss: 0.318324\n",
            "Epoch: 2182 \tTraining Loss: 0.318281\n",
            "Epoch: 2183 \tTraining Loss: 0.318238\n",
            "Epoch: 2184 \tTraining Loss: 0.318194\n",
            "Epoch: 2185 \tTraining Loss: 0.318152\n",
            "Epoch: 2186 \tTraining Loss: 0.318110\n",
            "Epoch: 2187 \tTraining Loss: 0.318065\n",
            "Epoch: 2188 \tTraining Loss: 0.318023\n",
            "Epoch: 2189 \tTraining Loss: 0.317982\n",
            "Epoch: 2190 \tTraining Loss: 0.317938\n",
            "Epoch: 2191 \tTraining Loss: 0.317895\n",
            "Epoch: 2192 \tTraining Loss: 0.317853\n",
            "Epoch: 2193 \tTraining Loss: 0.317810\n",
            "Epoch: 2194 \tTraining Loss: 0.317767\n",
            "Epoch: 2195 \tTraining Loss: 0.317726\n",
            "Epoch: 2196 \tTraining Loss: 0.317683\n",
            "Epoch: 2197 \tTraining Loss: 0.317640\n",
            "Epoch: 2198 \tTraining Loss: 0.317598\n",
            "Epoch: 2199 \tTraining Loss: 0.317556\n",
            "Epoch: 2200 \tTraining Loss: 0.317513\n",
            "Epoch: 2201 \tTraining Loss: 0.317471\n",
            "Epoch: 2202 \tTraining Loss: 0.317429\n",
            "Epoch: 2203 \tTraining Loss: 0.317386\n",
            "Epoch: 2204 \tTraining Loss: 0.317344\n",
            "Epoch: 2205 \tTraining Loss: 0.317302\n",
            "Epoch: 2206 \tTraining Loss: 0.317260\n",
            "Epoch: 2207 \tTraining Loss: 0.317218\n",
            "Epoch: 2208 \tTraining Loss: 0.317176\n",
            "Epoch: 2209 \tTraining Loss: 0.317133\n",
            "Epoch: 2210 \tTraining Loss: 0.317091\n",
            "Epoch: 2211 \tTraining Loss: 0.317049\n",
            "Epoch: 2212 \tTraining Loss: 0.317007\n",
            "Epoch: 2213 \tTraining Loss: 0.316965\n",
            "Epoch: 2214 \tTraining Loss: 0.316923\n",
            "Epoch: 2215 \tTraining Loss: 0.316881\n",
            "Epoch: 2216 \tTraining Loss: 0.316839\n",
            "Epoch: 2217 \tTraining Loss: 0.316797\n",
            "Epoch: 2218 \tTraining Loss: 0.316755\n",
            "Epoch: 2219 \tTraining Loss: 0.316714\n",
            "Epoch: 2220 \tTraining Loss: 0.316672\n",
            "Epoch: 2221 \tTraining Loss: 0.316630\n",
            "Epoch: 2222 \tTraining Loss: 0.316588\n",
            "Epoch: 2223 \tTraining Loss: 0.316546\n",
            "Epoch: 2224 \tTraining Loss: 0.316504\n",
            "Epoch: 2225 \tTraining Loss: 0.316462\n",
            "Epoch: 2226 \tTraining Loss: 0.316421\n",
            "Epoch: 2227 \tTraining Loss: 0.316379\n",
            "Epoch: 2228 \tTraining Loss: 0.316337\n",
            "Epoch: 2229 \tTraining Loss: 0.316295\n",
            "Epoch: 2230 \tTraining Loss: 0.316253\n",
            "Epoch: 2231 \tTraining Loss: 0.316212\n",
            "Epoch: 2232 \tTraining Loss: 0.316170\n",
            "Epoch: 2233 \tTraining Loss: 0.316128\n",
            "Epoch: 2234 \tTraining Loss: 0.316086\n",
            "Epoch: 2235 \tTraining Loss: 0.316045\n",
            "Epoch: 2236 \tTraining Loss: 0.316003\n",
            "Epoch: 2237 \tTraining Loss: 0.315961\n",
            "Epoch: 2238 \tTraining Loss: 0.315920\n",
            "Epoch: 2239 \tTraining Loss: 0.315878\n",
            "Epoch: 2240 \tTraining Loss: 0.315836\n",
            "Epoch: 2241 \tTraining Loss: 0.315795\n",
            "Epoch: 2242 \tTraining Loss: 0.315753\n",
            "Epoch: 2243 \tTraining Loss: 0.315711\n",
            "Epoch: 2244 \tTraining Loss: 0.315670\n",
            "Epoch: 2245 \tTraining Loss: 0.315628\n",
            "Epoch: 2246 \tTraining Loss: 0.315587\n",
            "Epoch: 2247 \tTraining Loss: 0.315545\n",
            "Epoch: 2248 \tTraining Loss: 0.315503\n",
            "Epoch: 2249 \tTraining Loss: 0.315462\n",
            "Epoch: 2250 \tTraining Loss: 0.315420\n",
            "Epoch: 2251 \tTraining Loss: 0.315378\n",
            "Epoch: 2252 \tTraining Loss: 0.315337\n",
            "Epoch: 2253 \tTraining Loss: 0.315295\n",
            "Epoch: 2254 \tTraining Loss: 0.315254\n",
            "Epoch: 2255 \tTraining Loss: 0.315212\n",
            "Epoch: 2256 \tTraining Loss: 0.315170\n",
            "Epoch: 2257 \tTraining Loss: 0.315129\n",
            "Epoch: 2258 \tTraining Loss: 0.315087\n",
            "Epoch: 2259 \tTraining Loss: 0.315046\n",
            "Epoch: 2260 \tTraining Loss: 0.315004\n",
            "Epoch: 2261 \tTraining Loss: 0.314963\n",
            "Epoch: 2262 \tTraining Loss: 0.314921\n",
            "Epoch: 2263 \tTraining Loss: 0.314880\n",
            "Epoch: 2264 \tTraining Loss: 0.314838\n",
            "Epoch: 2265 \tTraining Loss: 0.314796\n",
            "Epoch: 2266 \tTraining Loss: 0.314755\n",
            "Epoch: 2267 \tTraining Loss: 0.314713\n",
            "Epoch: 2268 \tTraining Loss: 0.314672\n",
            "Epoch: 2269 \tTraining Loss: 0.314630\n",
            "Epoch: 2270 \tTraining Loss: 0.314588\n",
            "Epoch: 2271 \tTraining Loss: 0.314547\n",
            "Epoch: 2272 \tTraining Loss: 0.314505\n",
            "Epoch: 2273 \tTraining Loss: 0.314463\n",
            "Epoch: 2274 \tTraining Loss: 0.314422\n",
            "Epoch: 2275 \tTraining Loss: 0.314380\n",
            "Epoch: 2276 \tTraining Loss: 0.314338\n",
            "Epoch: 2277 \tTraining Loss: 0.314296\n",
            "Epoch: 2278 \tTraining Loss: 0.314253\n",
            "Epoch: 2279 \tTraining Loss: 0.314211\n",
            "Epoch: 2280 \tTraining Loss: 0.314168\n",
            "Epoch: 2281 \tTraining Loss: 0.314126\n",
            "Epoch: 2282 \tTraining Loss: 0.314085\n",
            "Epoch: 2283 \tTraining Loss: 0.314043\n",
            "Epoch: 2284 \tTraining Loss: 0.314001\n",
            "Epoch: 2285 \tTraining Loss: 0.313958\n",
            "Epoch: 2286 \tTraining Loss: 0.313916\n",
            "Epoch: 2287 \tTraining Loss: 0.313874\n",
            "Epoch: 2288 \tTraining Loss: 0.313832\n",
            "Epoch: 2289 \tTraining Loss: 0.313790\n",
            "Epoch: 2290 \tTraining Loss: 0.313748\n",
            "Epoch: 2291 \tTraining Loss: 0.313705\n",
            "Epoch: 2292 \tTraining Loss: 0.313663\n",
            "Epoch: 2293 \tTraining Loss: 0.313621\n",
            "Epoch: 2294 \tTraining Loss: 0.313579\n",
            "Epoch: 2295 \tTraining Loss: 0.313536\n",
            "Epoch: 2296 \tTraining Loss: 0.313494\n",
            "Epoch: 2297 \tTraining Loss: 0.313451\n",
            "Epoch: 2298 \tTraining Loss: 0.313409\n",
            "Epoch: 2299 \tTraining Loss: 0.313367\n",
            "Epoch: 2300 \tTraining Loss: 0.313324\n",
            "Epoch: 2301 \tTraining Loss: 0.313282\n",
            "Epoch: 2302 \tTraining Loss: 0.313239\n",
            "Epoch: 2303 \tTraining Loss: 0.313197\n",
            "Epoch: 2304 \tTraining Loss: 0.313154\n",
            "Epoch: 2305 \tTraining Loss: 0.313111\n",
            "Epoch: 2306 \tTraining Loss: 0.313069\n",
            "Epoch: 2307 \tTraining Loss: 0.313026\n",
            "Epoch: 2308 \tTraining Loss: 0.312984\n",
            "Epoch: 2309 \tTraining Loss: 0.312941\n",
            "Epoch: 2310 \tTraining Loss: 0.312898\n",
            "Epoch: 2311 \tTraining Loss: 0.312856\n",
            "Epoch: 2312 \tTraining Loss: 0.312813\n",
            "Epoch: 2313 \tTraining Loss: 0.312770\n",
            "Epoch: 2314 \tTraining Loss: 0.312727\n",
            "Epoch: 2315 \tTraining Loss: 0.312685\n",
            "Epoch: 2316 \tTraining Loss: 0.312642\n",
            "Epoch: 2317 \tTraining Loss: 0.312599\n",
            "Epoch: 2318 \tTraining Loss: 0.312556\n",
            "Epoch: 2319 \tTraining Loss: 0.312513\n",
            "Epoch: 2320 \tTraining Loss: 0.312470\n",
            "Epoch: 2321 \tTraining Loss: 0.312428\n",
            "Epoch: 2322 \tTraining Loss: 0.312385\n",
            "Epoch: 2323 \tTraining Loss: 0.312342\n",
            "Epoch: 2324 \tTraining Loss: 0.312299\n",
            "Epoch: 2325 \tTraining Loss: 0.312256\n",
            "Epoch: 2326 \tTraining Loss: 0.312213\n",
            "Epoch: 2327 \tTraining Loss: 0.312170\n",
            "Epoch: 2328 \tTraining Loss: 0.312128\n",
            "Epoch: 2329 \tTraining Loss: 0.312085\n",
            "Epoch: 2330 \tTraining Loss: 0.312042\n",
            "Epoch: 2331 \tTraining Loss: 0.311999\n",
            "Epoch: 2332 \tTraining Loss: 0.311956\n",
            "Epoch: 2333 \tTraining Loss: 0.311914\n",
            "Epoch: 2334 \tTraining Loss: 0.311871\n",
            "Epoch: 2335 \tTraining Loss: 0.311828\n",
            "Epoch: 2336 \tTraining Loss: 0.311785\n",
            "Epoch: 2337 \tTraining Loss: 0.311743\n",
            "Epoch: 2338 \tTraining Loss: 0.311700\n",
            "Epoch: 2339 \tTraining Loss: 0.311657\n",
            "Epoch: 2340 \tTraining Loss: 0.311615\n",
            "Epoch: 2341 \tTraining Loss: 0.311572\n",
            "Epoch: 2342 \tTraining Loss: 0.311529\n",
            "Epoch: 2343 \tTraining Loss: 0.311487\n",
            "Epoch: 2344 \tTraining Loss: 0.311444\n",
            "Epoch: 2345 \tTraining Loss: 0.311402\n",
            "Epoch: 2346 \tTraining Loss: 0.311359\n",
            "Epoch: 2347 \tTraining Loss: 0.311317\n",
            "Epoch: 2348 \tTraining Loss: 0.311274\n",
            "Epoch: 2349 \tTraining Loss: 0.311232\n",
            "Epoch: 2350 \tTraining Loss: 0.311189\n",
            "Epoch: 2351 \tTraining Loss: 0.311147\n",
            "Epoch: 2352 \tTraining Loss: 0.311104\n",
            "Epoch: 2353 \tTraining Loss: 0.311062\n",
            "Epoch: 2354 \tTraining Loss: 0.311019\n",
            "Epoch: 2355 \tTraining Loss: 0.310977\n",
            "Epoch: 2356 \tTraining Loss: 0.310934\n",
            "Epoch: 2357 \tTraining Loss: 0.310892\n",
            "Epoch: 2358 \tTraining Loss: 0.310850\n",
            "Epoch: 2359 \tTraining Loss: 0.310807\n",
            "Epoch: 2360 \tTraining Loss: 0.310765\n",
            "Epoch: 2361 \tTraining Loss: 0.310723\n",
            "Epoch: 2362 \tTraining Loss: 0.310680\n",
            "Epoch: 2363 \tTraining Loss: 0.310638\n",
            "Epoch: 2364 \tTraining Loss: 0.310596\n",
            "Epoch: 2365 \tTraining Loss: 0.310553\n",
            "Epoch: 2366 \tTraining Loss: 0.310511\n",
            "Epoch: 2367 \tTraining Loss: 0.310469\n",
            "Epoch: 2368 \tTraining Loss: 0.310426\n",
            "Epoch: 2369 \tTraining Loss: 0.310384\n",
            "Epoch: 2370 \tTraining Loss: 0.310342\n",
            "Epoch: 2371 \tTraining Loss: 0.310300\n",
            "Epoch: 2372 \tTraining Loss: 0.310257\n",
            "Epoch: 2373 \tTraining Loss: 0.310215\n",
            "Epoch: 2374 \tTraining Loss: 0.310173\n",
            "Epoch: 2375 \tTraining Loss: 0.310131\n",
            "Epoch: 2376 \tTraining Loss: 0.310089\n",
            "Epoch: 2377 \tTraining Loss: 0.310046\n",
            "Epoch: 2378 \tTraining Loss: 0.310004\n",
            "Epoch: 2379 \tTraining Loss: 0.309962\n",
            "Epoch: 2380 \tTraining Loss: 0.309920\n",
            "Epoch: 2381 \tTraining Loss: 0.309878\n",
            "Epoch: 2382 \tTraining Loss: 0.309836\n",
            "Epoch: 2383 \tTraining Loss: 0.309793\n",
            "Epoch: 2384 \tTraining Loss: 0.309751\n",
            "Epoch: 2385 \tTraining Loss: 0.309709\n",
            "Epoch: 2386 \tTraining Loss: 0.309667\n",
            "Epoch: 2387 \tTraining Loss: 0.309625\n",
            "Epoch: 2388 \tTraining Loss: 0.309583\n",
            "Epoch: 2389 \tTraining Loss: 0.309541\n",
            "Epoch: 2390 \tTraining Loss: 0.309499\n",
            "Epoch: 2391 \tTraining Loss: 0.309457\n",
            "Epoch: 2392 \tTraining Loss: 0.309415\n",
            "Epoch: 2393 \tTraining Loss: 0.309373\n",
            "Epoch: 2394 \tTraining Loss: 0.309331\n",
            "Epoch: 2395 \tTraining Loss: 0.309289\n",
            "Epoch: 2396 \tTraining Loss: 0.309247\n",
            "Epoch: 2397 \tTraining Loss: 0.309205\n",
            "Epoch: 2398 \tTraining Loss: 0.309163\n",
            "Epoch: 2399 \tTraining Loss: 0.309121\n",
            "Epoch: 2400 \tTraining Loss: 0.309079\n",
            "Epoch: 2401 \tTraining Loss: 0.309037\n",
            "Epoch: 2402 \tTraining Loss: 0.308995\n",
            "Epoch: 2403 \tTraining Loss: 0.308953\n",
            "Epoch: 2404 \tTraining Loss: 0.308911\n",
            "Epoch: 2405 \tTraining Loss: 0.308869\n",
            "Epoch: 2406 \tTraining Loss: 0.308827\n",
            "Epoch: 2407 \tTraining Loss: 0.308785\n",
            "Epoch: 2408 \tTraining Loss: 0.308744\n",
            "Epoch: 2409 \tTraining Loss: 0.308702\n",
            "Epoch: 2410 \tTraining Loss: 0.308660\n",
            "Epoch: 2411 \tTraining Loss: 0.308618\n",
            "Epoch: 2412 \tTraining Loss: 0.308576\n",
            "Epoch: 2413 \tTraining Loss: 0.308535\n",
            "Epoch: 2414 \tTraining Loss: 0.308493\n",
            "Epoch: 2415 \tTraining Loss: 0.308451\n",
            "Epoch: 2416 \tTraining Loss: 0.308409\n",
            "Epoch: 2417 \tTraining Loss: 0.308368\n",
            "Epoch: 2418 \tTraining Loss: 0.308326\n",
            "Epoch: 2419 \tTraining Loss: 0.308284\n",
            "Epoch: 2420 \tTraining Loss: 0.308243\n",
            "Epoch: 2421 \tTraining Loss: 0.308201\n",
            "Epoch: 2422 \tTraining Loss: 0.308159\n",
            "Epoch: 2423 \tTraining Loss: 0.308118\n",
            "Epoch: 2424 \tTraining Loss: 0.308076\n",
            "Epoch: 2425 \tTraining Loss: 0.308035\n",
            "Epoch: 2426 \tTraining Loss: 0.307993\n",
            "Epoch: 2427 \tTraining Loss: 0.307951\n",
            "Epoch: 2428 \tTraining Loss: 0.307910\n",
            "Epoch: 2429 \tTraining Loss: 0.307868\n",
            "Epoch: 2430 \tTraining Loss: 0.307827\n",
            "Epoch: 2431 \tTraining Loss: 0.307785\n",
            "Epoch: 2432 \tTraining Loss: 0.307744\n",
            "Epoch: 2433 \tTraining Loss: 0.307702\n",
            "Epoch: 2434 \tTraining Loss: 0.307661\n",
            "Epoch: 2435 \tTraining Loss: 0.307619\n",
            "Epoch: 2436 \tTraining Loss: 0.307578\n",
            "Epoch: 2437 \tTraining Loss: 0.307537\n",
            "Epoch: 2438 \tTraining Loss: 0.307495\n",
            "Epoch: 2439 \tTraining Loss: 0.307454\n",
            "Epoch: 2440 \tTraining Loss: 0.307412\n",
            "Epoch: 2441 \tTraining Loss: 0.307371\n",
            "Epoch: 2442 \tTraining Loss: 0.307330\n",
            "Epoch: 2443 \tTraining Loss: 0.307288\n",
            "Epoch: 2444 \tTraining Loss: 0.307247\n",
            "Epoch: 2445 \tTraining Loss: 0.307206\n",
            "Epoch: 2446 \tTraining Loss: 0.307164\n",
            "Epoch: 2447 \tTraining Loss: 0.307123\n",
            "Epoch: 2448 \tTraining Loss: 0.307082\n",
            "Epoch: 2449 \tTraining Loss: 0.307041\n",
            "Epoch: 2450 \tTraining Loss: 0.307000\n",
            "Epoch: 2451 \tTraining Loss: 0.306960\n",
            "Epoch: 2452 \tTraining Loss: 0.306920\n",
            "Epoch: 2453 \tTraining Loss: 0.306882\n",
            "Epoch: 2454 \tTraining Loss: 0.306847\n",
            "Epoch: 2455 \tTraining Loss: 0.306818\n",
            "Epoch: 2456 \tTraining Loss: 0.306802\n",
            "Epoch: 2457 \tTraining Loss: 0.306819\n",
            "Epoch: 2458 \tTraining Loss: 0.306886\n",
            "Epoch: 2459 \tTraining Loss: 0.307154\n",
            "Epoch: 2460 \tTraining Loss: 0.307681\n",
            "Epoch: 2461 \tTraining Loss: 0.310115\n",
            "Epoch: 2462 \tTraining Loss: 0.317079\n",
            "Epoch: 2463 \tTraining Loss: 0.359643\n",
            "Epoch: 2464 \tTraining Loss: 0.339362\n",
            "Epoch: 2465 \tTraining Loss: 0.381987\n",
            "Epoch: 2466 \tTraining Loss: 0.341686\n",
            "Epoch: 2467 \tTraining Loss: 0.373496\n",
            "Epoch: 2468 \tTraining Loss: 0.369123\n",
            "Epoch: 2469 \tTraining Loss: 0.368908\n",
            "Epoch: 2470 \tTraining Loss: 0.380778\n",
            "Epoch: 2471 \tTraining Loss: 0.358436\n",
            "Epoch: 2472 \tTraining Loss: 0.357212\n",
            "Epoch: 2473 \tTraining Loss: 0.381355\n",
            "Epoch: 2474 \tTraining Loss: 0.366310\n",
            "Epoch: 2475 \tTraining Loss: 0.339526\n",
            "Epoch: 2476 \tTraining Loss: 0.377603\n",
            "Epoch: 2477 \tTraining Loss: 0.341497\n",
            "Epoch: 2478 \tTraining Loss: 0.350328\n",
            "Epoch: 2479 \tTraining Loss: 0.333097\n",
            "Epoch: 2480 \tTraining Loss: 0.332149\n",
            "Epoch: 2481 \tTraining Loss: 0.331538\n",
            "Epoch: 2482 \tTraining Loss: 0.339493\n",
            "Epoch: 2483 \tTraining Loss: 0.319038\n",
            "Epoch: 2484 \tTraining Loss: 0.337408\n",
            "Epoch: 2485 \tTraining Loss: 0.328214\n",
            "Epoch: 2486 \tTraining Loss: 0.329287\n",
            "Epoch: 2487 \tTraining Loss: 0.318776\n",
            "Epoch: 2488 \tTraining Loss: 0.326138\n",
            "Epoch: 2489 \tTraining Loss: 0.318469\n",
            "Epoch: 2490 \tTraining Loss: 0.320330\n",
            "Epoch: 2491 \tTraining Loss: 0.319531\n",
            "Epoch: 2492 \tTraining Loss: 0.316455\n",
            "Epoch: 2493 \tTraining Loss: 0.316685\n",
            "Epoch: 2494 \tTraining Loss: 0.314610\n",
            "Epoch: 2495 \tTraining Loss: 0.314453\n",
            "Epoch: 2496 \tTraining Loss: 0.312205\n",
            "Epoch: 2497 \tTraining Loss: 0.315703\n",
            "Epoch: 2498 \tTraining Loss: 0.312560\n",
            "Epoch: 2499 \tTraining Loss: 0.312153\n",
            "Epoch: 2500 \tTraining Loss: 0.311343\n",
            "Epoch: 2501 \tTraining Loss: 0.310862\n",
            "Epoch: 2502 \tTraining Loss: 0.310130\n",
            "Epoch: 2503 \tTraining Loss: 0.310334\n",
            "Epoch: 2504 \tTraining Loss: 0.310678\n",
            "Epoch: 2505 \tTraining Loss: 0.309323\n",
            "Epoch: 2506 \tTraining Loss: 0.309110\n",
            "Epoch: 2507 \tTraining Loss: 0.308716\n",
            "Epoch: 2508 \tTraining Loss: 0.308664\n",
            "Epoch: 2509 \tTraining Loss: 0.308250\n",
            "Epoch: 2510 \tTraining Loss: 0.308190\n",
            "Epoch: 2511 \tTraining Loss: 0.307972\n",
            "Epoch: 2512 \tTraining Loss: 0.307410\n",
            "Epoch: 2513 \tTraining Loss: 0.307574\n",
            "Epoch: 2514 \tTraining Loss: 0.307389\n",
            "Epoch: 2515 \tTraining Loss: 0.307157\n",
            "Epoch: 2516 \tTraining Loss: 0.306864\n",
            "Epoch: 2517 \tTraining Loss: 0.307039\n",
            "Epoch: 2518 \tTraining Loss: 0.306617\n",
            "Epoch: 2519 \tTraining Loss: 0.306551\n",
            "Epoch: 2520 \tTraining Loss: 0.306470\n",
            "Epoch: 2521 \tTraining Loss: 0.306218\n",
            "Epoch: 2522 \tTraining Loss: 0.306227\n",
            "Epoch: 2523 \tTraining Loss: 0.306129\n",
            "Epoch: 2524 \tTraining Loss: 0.306076\n",
            "Epoch: 2525 \tTraining Loss: 0.305896\n",
            "Epoch: 2526 \tTraining Loss: 0.305825\n",
            "Epoch: 2527 \tTraining Loss: 0.305689\n",
            "Epoch: 2528 \tTraining Loss: 0.305655\n",
            "Epoch: 2529 \tTraining Loss: 0.305554\n",
            "Epoch: 2530 \tTraining Loss: 0.305534\n",
            "Epoch: 2531 \tTraining Loss: 0.305448\n",
            "Epoch: 2532 \tTraining Loss: 0.305327\n",
            "Epoch: 2533 \tTraining Loss: 0.305301\n",
            "Epoch: 2534 \tTraining Loss: 0.305229\n",
            "Epoch: 2535 \tTraining Loss: 0.305128\n",
            "Epoch: 2536 \tTraining Loss: 0.305107\n",
            "Epoch: 2537 \tTraining Loss: 0.305030\n",
            "Epoch: 2538 \tTraining Loss: 0.304973\n",
            "Epoch: 2539 \tTraining Loss: 0.304925\n",
            "Epoch: 2540 \tTraining Loss: 0.304889\n",
            "Epoch: 2541 \tTraining Loss: 0.304796\n",
            "Epoch: 2542 \tTraining Loss: 0.304769\n",
            "Epoch: 2543 \tTraining Loss: 0.304703\n",
            "Epoch: 2544 \tTraining Loss: 0.304658\n",
            "Epoch: 2545 \tTraining Loss: 0.304611\n",
            "Epoch: 2546 \tTraining Loss: 0.304562\n",
            "Epoch: 2547 \tTraining Loss: 0.304511\n",
            "Epoch: 2548 \tTraining Loss: 0.304470\n",
            "Epoch: 2549 \tTraining Loss: 0.304425\n",
            "Epoch: 2550 \tTraining Loss: 0.304371\n",
            "Epoch: 2551 \tTraining Loss: 0.304332\n",
            "Epoch: 2552 \tTraining Loss: 0.304284\n",
            "Epoch: 2553 \tTraining Loss: 0.304244\n",
            "Epoch: 2554 \tTraining Loss: 0.304201\n",
            "Epoch: 2555 \tTraining Loss: 0.304159\n",
            "Epoch: 2556 \tTraining Loss: 0.304113\n",
            "Epoch: 2557 \tTraining Loss: 0.304073\n",
            "Epoch: 2558 \tTraining Loss: 0.304032\n",
            "Epoch: 2559 \tTraining Loss: 0.303991\n",
            "Epoch: 2560 \tTraining Loss: 0.303952\n",
            "Epoch: 2561 \tTraining Loss: 0.303910\n",
            "Epoch: 2562 \tTraining Loss: 0.303870\n",
            "Epoch: 2563 \tTraining Loss: 0.303831\n",
            "Epoch: 2564 \tTraining Loss: 0.303792\n",
            "Epoch: 2565 \tTraining Loss: 0.303753\n",
            "Epoch: 2566 \tTraining Loss: 0.303714\n",
            "Epoch: 2567 \tTraining Loss: 0.303676\n",
            "Epoch: 2568 \tTraining Loss: 0.303637\n",
            "Epoch: 2569 \tTraining Loss: 0.303599\n",
            "Epoch: 2570 \tTraining Loss: 0.303561\n",
            "Epoch: 2571 \tTraining Loss: 0.303523\n",
            "Epoch: 2572 \tTraining Loss: 0.303485\n",
            "Epoch: 2573 \tTraining Loss: 0.303449\n",
            "Epoch: 2574 \tTraining Loss: 0.303411\n",
            "Epoch: 2575 \tTraining Loss: 0.303374\n",
            "Epoch: 2576 \tTraining Loss: 0.303337\n",
            "Epoch: 2577 \tTraining Loss: 0.303300\n",
            "Epoch: 2578 \tTraining Loss: 0.303264\n",
            "Epoch: 2579 \tTraining Loss: 0.303227\n",
            "Epoch: 2580 \tTraining Loss: 0.303191\n",
            "Epoch: 2581 \tTraining Loss: 0.303154\n",
            "Epoch: 2582 \tTraining Loss: 0.303119\n",
            "Epoch: 2583 \tTraining Loss: 0.303082\n",
            "Epoch: 2584 \tTraining Loss: 0.303046\n",
            "Epoch: 2585 \tTraining Loss: 0.303011\n",
            "Epoch: 2586 \tTraining Loss: 0.302975\n",
            "Epoch: 2587 \tTraining Loss: 0.302939\n",
            "Epoch: 2588 \tTraining Loss: 0.302904\n",
            "Epoch: 2589 \tTraining Loss: 0.302868\n",
            "Epoch: 2590 \tTraining Loss: 0.302833\n",
            "Epoch: 2591 \tTraining Loss: 0.302797\n",
            "Epoch: 2592 \tTraining Loss: 0.302762\n",
            "Epoch: 2593 \tTraining Loss: 0.302727\n",
            "Epoch: 2594 \tTraining Loss: 0.302691\n",
            "Epoch: 2595 \tTraining Loss: 0.302656\n",
            "Epoch: 2596 \tTraining Loss: 0.302621\n",
            "Epoch: 2597 \tTraining Loss: 0.302586\n",
            "Epoch: 2598 \tTraining Loss: 0.302551\n",
            "Epoch: 2599 \tTraining Loss: 0.302516\n",
            "Epoch: 2600 \tTraining Loss: 0.302481\n",
            "Epoch: 2601 \tTraining Loss: 0.302446\n",
            "Epoch: 2602 \tTraining Loss: 0.302412\n",
            "Epoch: 2603 \tTraining Loss: 0.302377\n",
            "Epoch: 2604 \tTraining Loss: 0.302342\n",
            "Epoch: 2605 \tTraining Loss: 0.302307\n",
            "Epoch: 2606 \tTraining Loss: 0.302273\n",
            "Epoch: 2607 \tTraining Loss: 0.302238\n",
            "Epoch: 2608 \tTraining Loss: 0.302204\n",
            "Epoch: 2609 \tTraining Loss: 0.302169\n",
            "Epoch: 2610 \tTraining Loss: 0.302135\n",
            "Epoch: 2611 \tTraining Loss: 0.302100\n",
            "Epoch: 2612 \tTraining Loss: 0.302066\n",
            "Epoch: 2613 \tTraining Loss: 0.302032\n",
            "Epoch: 2614 \tTraining Loss: 0.301997\n",
            "Epoch: 2615 \tTraining Loss: 0.301963\n",
            "Epoch: 2616 \tTraining Loss: 0.301929\n",
            "Epoch: 2617 \tTraining Loss: 0.301895\n",
            "Epoch: 2618 \tTraining Loss: 0.301860\n",
            "Epoch: 2619 \tTraining Loss: 0.301826\n",
            "Epoch: 2620 \tTraining Loss: 0.301792\n",
            "Epoch: 2621 \tTraining Loss: 0.301758\n",
            "Epoch: 2622 \tTraining Loss: 0.301724\n",
            "Epoch: 2623 \tTraining Loss: 0.301690\n",
            "Epoch: 2624 \tTraining Loss: 0.301656\n",
            "Epoch: 2625 \tTraining Loss: 0.301622\n",
            "Epoch: 2626 \tTraining Loss: 0.301588\n",
            "Epoch: 2627 \tTraining Loss: 0.301554\n",
            "Epoch: 2628 \tTraining Loss: 0.301521\n",
            "Epoch: 2629 \tTraining Loss: 0.301487\n",
            "Epoch: 2630 \tTraining Loss: 0.301453\n",
            "Epoch: 2631 \tTraining Loss: 0.301419\n",
            "Epoch: 2632 \tTraining Loss: 0.301385\n",
            "Epoch: 2633 \tTraining Loss: 0.301352\n",
            "Epoch: 2634 \tTraining Loss: 0.301318\n",
            "Epoch: 2635 \tTraining Loss: 0.301284\n",
            "Epoch: 2636 \tTraining Loss: 0.301251\n",
            "Epoch: 2637 \tTraining Loss: 0.301217\n",
            "Epoch: 2638 \tTraining Loss: 0.301183\n",
            "Epoch: 2639 \tTraining Loss: 0.301150\n",
            "Epoch: 2640 \tTraining Loss: 0.301116\n",
            "Epoch: 2641 \tTraining Loss: 0.301083\n",
            "Epoch: 2642 \tTraining Loss: 0.301049\n",
            "Epoch: 2643 \tTraining Loss: 0.301016\n",
            "Epoch: 2644 \tTraining Loss: 0.300982\n",
            "Epoch: 2645 \tTraining Loss: 0.300949\n",
            "Epoch: 2646 \tTraining Loss: 0.300915\n",
            "Epoch: 2647 \tTraining Loss: 0.300882\n",
            "Epoch: 2648 \tTraining Loss: 0.300848\n",
            "Epoch: 2649 \tTraining Loss: 0.300815\n",
            "Epoch: 2650 \tTraining Loss: 0.300782\n",
            "Epoch: 2651 \tTraining Loss: 0.300748\n",
            "Epoch: 2652 \tTraining Loss: 0.300715\n",
            "Epoch: 2653 \tTraining Loss: 0.300682\n",
            "Epoch: 2654 \tTraining Loss: 0.300648\n",
            "Epoch: 2655 \tTraining Loss: 0.300615\n",
            "Epoch: 2656 \tTraining Loss: 0.300582\n",
            "Epoch: 2657 \tTraining Loss: 0.300548\n",
            "Epoch: 2658 \tTraining Loss: 0.300515\n",
            "Epoch: 2659 \tTraining Loss: 0.300482\n",
            "Epoch: 2660 \tTraining Loss: 0.300449\n",
            "Epoch: 2661 \tTraining Loss: 0.300416\n",
            "Epoch: 2662 \tTraining Loss: 0.300382\n",
            "Epoch: 2663 \tTraining Loss: 0.300349\n",
            "Epoch: 2664 \tTraining Loss: 0.300316\n",
            "Epoch: 2665 \tTraining Loss: 0.300283\n",
            "Epoch: 2666 \tTraining Loss: 0.300250\n",
            "Epoch: 2667 \tTraining Loss: 0.300217\n",
            "Epoch: 2668 \tTraining Loss: 0.300184\n",
            "Epoch: 2669 \tTraining Loss: 0.300151\n",
            "Epoch: 2670 \tTraining Loss: 0.300118\n",
            "Epoch: 2671 \tTraining Loss: 0.300085\n",
            "Epoch: 2672 \tTraining Loss: 0.300051\n",
            "Epoch: 2673 \tTraining Loss: 0.300018\n",
            "Epoch: 2674 \tTraining Loss: 0.299985\n",
            "Epoch: 2675 \tTraining Loss: 0.299952\n",
            "Epoch: 2676 \tTraining Loss: 0.299919\n",
            "Epoch: 2677 \tTraining Loss: 0.299886\n",
            "Epoch: 2678 \tTraining Loss: 0.299854\n",
            "Epoch: 2679 \tTraining Loss: 0.299821\n",
            "Epoch: 2680 \tTraining Loss: 0.299788\n",
            "Epoch: 2681 \tTraining Loss: 0.299755\n",
            "Epoch: 2682 \tTraining Loss: 0.299722\n",
            "Epoch: 2683 \tTraining Loss: 0.299689\n",
            "Epoch: 2684 \tTraining Loss: 0.299656\n",
            "Epoch: 2685 \tTraining Loss: 0.299623\n",
            "Epoch: 2686 \tTraining Loss: 0.299590\n",
            "Epoch: 2687 \tTraining Loss: 0.299557\n",
            "Epoch: 2688 \tTraining Loss: 0.299525\n",
            "Epoch: 2689 \tTraining Loss: 0.299492\n",
            "Epoch: 2690 \tTraining Loss: 0.299459\n",
            "Epoch: 2691 \tTraining Loss: 0.299426\n",
            "Epoch: 2692 \tTraining Loss: 0.299393\n",
            "Epoch: 2693 \tTraining Loss: 0.299361\n",
            "Epoch: 2694 \tTraining Loss: 0.299328\n",
            "Epoch: 2695 \tTraining Loss: 0.299295\n",
            "Epoch: 2696 \tTraining Loss: 0.299262\n",
            "Epoch: 2697 \tTraining Loss: 0.299229\n",
            "Epoch: 2698 \tTraining Loss: 0.299197\n",
            "Epoch: 2699 \tTraining Loss: 0.299164\n",
            "Epoch: 2700 \tTraining Loss: 0.299131\n",
            "Epoch: 2701 \tTraining Loss: 0.299099\n",
            "Epoch: 2702 \tTraining Loss: 0.299066\n",
            "Epoch: 2703 \tTraining Loss: 0.299033\n",
            "Epoch: 2704 \tTraining Loss: 0.299000\n",
            "Epoch: 2705 \tTraining Loss: 0.298968\n",
            "Epoch: 2706 \tTraining Loss: 0.298935\n",
            "Epoch: 2707 \tTraining Loss: 0.298902\n",
            "Epoch: 2708 \tTraining Loss: 0.298870\n",
            "Epoch: 2709 \tTraining Loss: 0.298837\n",
            "Epoch: 2710 \tTraining Loss: 0.298805\n",
            "Epoch: 2711 \tTraining Loss: 0.298772\n",
            "Epoch: 2712 \tTraining Loss: 0.298739\n",
            "Epoch: 2713 \tTraining Loss: 0.298707\n",
            "Epoch: 2714 \tTraining Loss: 0.298674\n",
            "Epoch: 2715 \tTraining Loss: 0.298642\n",
            "Epoch: 2716 \tTraining Loss: 0.298609\n",
            "Epoch: 2717 \tTraining Loss: 0.298576\n",
            "Epoch: 2718 \tTraining Loss: 0.298544\n",
            "Epoch: 2719 \tTraining Loss: 0.298511\n",
            "Epoch: 2720 \tTraining Loss: 0.298479\n",
            "Epoch: 2721 \tTraining Loss: 0.298446\n",
            "Epoch: 2722 \tTraining Loss: 0.298414\n",
            "Epoch: 2723 \tTraining Loss: 0.298381\n",
            "Epoch: 2724 \tTraining Loss: 0.298349\n",
            "Epoch: 2725 \tTraining Loss: 0.298316\n",
            "Epoch: 2726 \tTraining Loss: 0.298284\n",
            "Epoch: 2727 \tTraining Loss: 0.298251\n",
            "Epoch: 2728 \tTraining Loss: 0.298219\n",
            "Epoch: 2729 \tTraining Loss: 0.298186\n",
            "Epoch: 2730 \tTraining Loss: 0.298154\n",
            "Epoch: 2731 \tTraining Loss: 0.298121\n",
            "Epoch: 2732 \tTraining Loss: 0.298089\n",
            "Epoch: 2733 \tTraining Loss: 0.298056\n",
            "Epoch: 2734 \tTraining Loss: 0.298024\n",
            "Epoch: 2735 \tTraining Loss: 0.297991\n",
            "Epoch: 2736 \tTraining Loss: 0.297959\n",
            "Epoch: 2737 \tTraining Loss: 0.297927\n",
            "Epoch: 2738 \tTraining Loss: 0.297894\n",
            "Epoch: 2739 \tTraining Loss: 0.297862\n",
            "Epoch: 2740 \tTraining Loss: 0.297829\n",
            "Epoch: 2741 \tTraining Loss: 0.297797\n",
            "Epoch: 2742 \tTraining Loss: 0.297765\n",
            "Epoch: 2743 \tTraining Loss: 0.297732\n",
            "Epoch: 2744 \tTraining Loss: 0.297700\n",
            "Epoch: 2745 \tTraining Loss: 0.297667\n",
            "Epoch: 2746 \tTraining Loss: 0.297635\n",
            "Epoch: 2747 \tTraining Loss: 0.297603\n",
            "Epoch: 2748 \tTraining Loss: 0.297570\n",
            "Epoch: 2749 \tTraining Loss: 0.297538\n",
            "Epoch: 2750 \tTraining Loss: 0.297506\n",
            "Epoch: 2751 \tTraining Loss: 0.297473\n",
            "Epoch: 2752 \tTraining Loss: 0.297441\n",
            "Epoch: 2753 \tTraining Loss: 0.297409\n",
            "Epoch: 2754 \tTraining Loss: 0.297377\n",
            "Epoch: 2755 \tTraining Loss: 0.297344\n",
            "Epoch: 2756 \tTraining Loss: 0.297312\n",
            "Epoch: 2757 \tTraining Loss: 0.297280\n",
            "Epoch: 2758 \tTraining Loss: 0.297247\n",
            "Epoch: 2759 \tTraining Loss: 0.297215\n",
            "Epoch: 2760 \tTraining Loss: 0.297183\n",
            "Epoch: 2761 \tTraining Loss: 0.297151\n",
            "Epoch: 2762 \tTraining Loss: 0.297118\n",
            "Epoch: 2763 \tTraining Loss: 0.297086\n",
            "Epoch: 2764 \tTraining Loss: 0.297054\n",
            "Epoch: 2765 \tTraining Loss: 0.297022\n",
            "Epoch: 2766 \tTraining Loss: 0.296989\n",
            "Epoch: 2767 \tTraining Loss: 0.296957\n",
            "Epoch: 2768 \tTraining Loss: 0.296925\n",
            "Epoch: 2769 \tTraining Loss: 0.296893\n",
            "Epoch: 2770 \tTraining Loss: 0.296861\n",
            "Epoch: 2771 \tTraining Loss: 0.296828\n",
            "Epoch: 2772 \tTraining Loss: 0.296796\n",
            "Epoch: 2773 \tTraining Loss: 0.296764\n",
            "Epoch: 2774 \tTraining Loss: 0.296732\n",
            "Epoch: 2775 \tTraining Loss: 0.296700\n",
            "Epoch: 2776 \tTraining Loss: 0.296667\n",
            "Epoch: 2777 \tTraining Loss: 0.296635\n",
            "Epoch: 2778 \tTraining Loss: 0.296603\n",
            "Epoch: 2779 \tTraining Loss: 0.296571\n",
            "Epoch: 2780 \tTraining Loss: 0.296539\n",
            "Epoch: 2781 \tTraining Loss: 0.296507\n",
            "Epoch: 2782 \tTraining Loss: 0.296475\n",
            "Epoch: 2783 \tTraining Loss: 0.296442\n",
            "Epoch: 2784 \tTraining Loss: 0.296410\n",
            "Epoch: 2785 \tTraining Loss: 0.296378\n",
            "Epoch: 2786 \tTraining Loss: 0.296346\n",
            "Epoch: 2787 \tTraining Loss: 0.296314\n",
            "Epoch: 2788 \tTraining Loss: 0.296282\n",
            "Epoch: 2789 \tTraining Loss: 0.296250\n",
            "Epoch: 2790 \tTraining Loss: 0.296218\n",
            "Epoch: 2791 \tTraining Loss: 0.296186\n",
            "Epoch: 2792 \tTraining Loss: 0.296154\n",
            "Epoch: 2793 \tTraining Loss: 0.296121\n",
            "Epoch: 2794 \tTraining Loss: 0.296089\n",
            "Epoch: 2795 \tTraining Loss: 0.296057\n",
            "Epoch: 2796 \tTraining Loss: 0.296025\n",
            "Epoch: 2797 \tTraining Loss: 0.295993\n",
            "Epoch: 2798 \tTraining Loss: 0.295961\n",
            "Epoch: 2799 \tTraining Loss: 0.295929\n",
            "Epoch: 2800 \tTraining Loss: 0.295897\n",
            "Epoch: 2801 \tTraining Loss: 0.295865\n",
            "Epoch: 2802 \tTraining Loss: 0.295833\n",
            "Epoch: 2803 \tTraining Loss: 0.295801\n",
            "Epoch: 2804 \tTraining Loss: 0.295769\n",
            "Epoch: 2805 \tTraining Loss: 0.295737\n",
            "Epoch: 2806 \tTraining Loss: 0.295705\n",
            "Epoch: 2807 \tTraining Loss: 0.295673\n",
            "Epoch: 2808 \tTraining Loss: 0.295641\n",
            "Epoch: 2809 \tTraining Loss: 0.295609\n",
            "Epoch: 2810 \tTraining Loss: 0.295577\n",
            "Epoch: 2811 \tTraining Loss: 0.295545\n",
            "Epoch: 2812 \tTraining Loss: 0.295513\n",
            "Epoch: 2813 \tTraining Loss: 0.295481\n",
            "Epoch: 2814 \tTraining Loss: 0.295449\n",
            "Epoch: 2815 \tTraining Loss: 0.295417\n",
            "Epoch: 2816 \tTraining Loss: 0.295385\n",
            "Epoch: 2817 \tTraining Loss: 0.295353\n",
            "Epoch: 2818 \tTraining Loss: 0.295321\n",
            "Epoch: 2819 \tTraining Loss: 0.295289\n",
            "Epoch: 2820 \tTraining Loss: 0.295257\n",
            "Epoch: 2821 \tTraining Loss: 0.295226\n",
            "Epoch: 2822 \tTraining Loss: 0.295194\n",
            "Epoch: 2823 \tTraining Loss: 0.295162\n",
            "Epoch: 2824 \tTraining Loss: 0.295130\n",
            "Epoch: 2825 \tTraining Loss: 0.295098\n",
            "Epoch: 2826 \tTraining Loss: 0.295066\n",
            "Epoch: 2827 \tTraining Loss: 0.295034\n",
            "Epoch: 2828 \tTraining Loss: 0.295002\n",
            "Epoch: 2829 \tTraining Loss: 0.294970\n",
            "Epoch: 2830 \tTraining Loss: 0.294938\n",
            "Epoch: 2831 \tTraining Loss: 0.294907\n",
            "Epoch: 2832 \tTraining Loss: 0.294875\n",
            "Epoch: 2833 \tTraining Loss: 0.294843\n",
            "Epoch: 2834 \tTraining Loss: 0.294811\n",
            "Epoch: 2835 \tTraining Loss: 0.294779\n",
            "Epoch: 2836 \tTraining Loss: 0.294747\n",
            "Epoch: 2837 \tTraining Loss: 0.294715\n",
            "Epoch: 2838 \tTraining Loss: 0.294683\n",
            "Epoch: 2839 \tTraining Loss: 0.294652\n",
            "Epoch: 2840 \tTraining Loss: 0.294620\n",
            "Epoch: 2841 \tTraining Loss: 0.294588\n",
            "Epoch: 2842 \tTraining Loss: 0.294556\n",
            "Epoch: 2843 \tTraining Loss: 0.294524\n",
            "Epoch: 2844 \tTraining Loss: 0.294492\n",
            "Epoch: 2845 \tTraining Loss: 0.294461\n",
            "Epoch: 2846 \tTraining Loss: 0.294429\n",
            "Epoch: 2847 \tTraining Loss: 0.294397\n",
            "Epoch: 2848 \tTraining Loss: 0.294365\n",
            "Epoch: 2849 \tTraining Loss: 0.294333\n",
            "Epoch: 2850 \tTraining Loss: 0.294302\n",
            "Epoch: 2851 \tTraining Loss: 0.294270\n",
            "Epoch: 2852 \tTraining Loss: 0.294238\n",
            "Epoch: 2853 \tTraining Loss: 0.294206\n",
            "Epoch: 2854 \tTraining Loss: 0.294175\n",
            "Epoch: 2855 \tTraining Loss: 0.294143\n",
            "Epoch: 2856 \tTraining Loss: 0.294111\n",
            "Epoch: 2857 \tTraining Loss: 0.294079\n",
            "Epoch: 2858 \tTraining Loss: 0.294047\n",
            "Epoch: 2859 \tTraining Loss: 0.294016\n",
            "Epoch: 2860 \tTraining Loss: 0.293984\n",
            "Epoch: 2861 \tTraining Loss: 0.293952\n",
            "Epoch: 2862 \tTraining Loss: 0.293920\n",
            "Epoch: 2863 \tTraining Loss: 0.293889\n",
            "Epoch: 2864 \tTraining Loss: 0.293857\n",
            "Epoch: 2865 \tTraining Loss: 0.293825\n",
            "Epoch: 2866 \tTraining Loss: 0.293793\n",
            "Epoch: 2867 \tTraining Loss: 0.293762\n",
            "Epoch: 2868 \tTraining Loss: 0.293730\n",
            "Epoch: 2869 \tTraining Loss: 0.293698\n",
            "Epoch: 2870 \tTraining Loss: 0.293667\n",
            "Epoch: 2871 \tTraining Loss: 0.293635\n",
            "Epoch: 2872 \tTraining Loss: 0.293603\n",
            "Epoch: 2873 \tTraining Loss: 0.293571\n",
            "Epoch: 2874 \tTraining Loss: 0.293540\n",
            "Epoch: 2875 \tTraining Loss: 0.293508\n",
            "Epoch: 2876 \tTraining Loss: 0.293476\n",
            "Epoch: 2877 \tTraining Loss: 0.293445\n",
            "Epoch: 2878 \tTraining Loss: 0.293413\n",
            "Epoch: 2879 \tTraining Loss: 0.293381\n",
            "Epoch: 2880 \tTraining Loss: 0.293350\n",
            "Epoch: 2881 \tTraining Loss: 0.293318\n",
            "Epoch: 2882 \tTraining Loss: 0.293286\n",
            "Epoch: 2883 \tTraining Loss: 0.293255\n",
            "Epoch: 2884 \tTraining Loss: 0.293223\n",
            "Epoch: 2885 \tTraining Loss: 0.293191\n",
            "Epoch: 2886 \tTraining Loss: 0.293160\n",
            "Epoch: 2887 \tTraining Loss: 0.293128\n",
            "Epoch: 2888 \tTraining Loss: 0.293096\n",
            "Epoch: 2889 \tTraining Loss: 0.293065\n",
            "Epoch: 2890 \tTraining Loss: 0.293033\n",
            "Epoch: 2891 \tTraining Loss: 0.293001\n",
            "Epoch: 2892 \tTraining Loss: 0.292970\n",
            "Epoch: 2893 \tTraining Loss: 0.292938\n",
            "Epoch: 2894 \tTraining Loss: 0.292907\n",
            "Epoch: 2895 \tTraining Loss: 0.292875\n",
            "Epoch: 2896 \tTraining Loss: 0.292843\n",
            "Epoch: 2897 \tTraining Loss: 0.292812\n",
            "Epoch: 2898 \tTraining Loss: 0.292780\n",
            "Epoch: 2899 \tTraining Loss: 0.292749\n",
            "Epoch: 2900 \tTraining Loss: 0.292717\n",
            "Epoch: 2901 \tTraining Loss: 0.292686\n",
            "Epoch: 2902 \tTraining Loss: 0.292654\n",
            "Epoch: 2903 \tTraining Loss: 0.292622\n",
            "Epoch: 2904 \tTraining Loss: 0.292591\n",
            "Epoch: 2905 \tTraining Loss: 0.292559\n",
            "Epoch: 2906 \tTraining Loss: 0.292527\n",
            "Epoch: 2907 \tTraining Loss: 0.292496\n",
            "Epoch: 2908 \tTraining Loss: 0.292464\n",
            "Epoch: 2909 \tTraining Loss: 0.292433\n",
            "Epoch: 2910 \tTraining Loss: 0.292401\n",
            "Epoch: 2911 \tTraining Loss: 0.292370\n",
            "Epoch: 2912 \tTraining Loss: 0.292338\n",
            "Epoch: 2913 \tTraining Loss: 0.292306\n",
            "Epoch: 2914 \tTraining Loss: 0.292275\n",
            "Epoch: 2915 \tTraining Loss: 0.292243\n",
            "Epoch: 2916 \tTraining Loss: 0.292212\n",
            "Epoch: 2917 \tTraining Loss: 0.292180\n",
            "Epoch: 2918 \tTraining Loss: 0.292149\n",
            "Epoch: 2919 \tTraining Loss: 0.292117\n",
            "Epoch: 2920 \tTraining Loss: 0.292085\n",
            "Epoch: 2921 \tTraining Loss: 0.292054\n",
            "Epoch: 2922 \tTraining Loss: 0.292022\n",
            "Epoch: 2923 \tTraining Loss: 0.291991\n",
            "Epoch: 2924 \tTraining Loss: 0.291959\n",
            "Epoch: 2925 \tTraining Loss: 0.291928\n",
            "Epoch: 2926 \tTraining Loss: 0.291896\n",
            "Epoch: 2927 \tTraining Loss: 0.291865\n",
            "Epoch: 2928 \tTraining Loss: 0.291833\n",
            "Epoch: 2929 \tTraining Loss: 0.291802\n",
            "Epoch: 2930 \tTraining Loss: 0.291770\n",
            "Epoch: 2931 \tTraining Loss: 0.291739\n",
            "Epoch: 2932 \tTraining Loss: 0.291707\n",
            "Epoch: 2933 \tTraining Loss: 0.291676\n",
            "Epoch: 2934 \tTraining Loss: 0.291644\n",
            "Epoch: 2935 \tTraining Loss: 0.291613\n",
            "Epoch: 2936 \tTraining Loss: 0.291581\n",
            "Epoch: 2937 \tTraining Loss: 0.291550\n",
            "Epoch: 2938 \tTraining Loss: 0.291518\n",
            "Epoch: 2939 \tTraining Loss: 0.291487\n",
            "Epoch: 2940 \tTraining Loss: 0.291455\n",
            "Epoch: 2941 \tTraining Loss: 0.291424\n",
            "Epoch: 2942 \tTraining Loss: 0.291392\n",
            "Epoch: 2943 \tTraining Loss: 0.291361\n",
            "Epoch: 2944 \tTraining Loss: 0.291329\n",
            "Epoch: 2945 \tTraining Loss: 0.291298\n",
            "Epoch: 2946 \tTraining Loss: 0.291266\n",
            "Epoch: 2947 \tTraining Loss: 0.291235\n",
            "Epoch: 2948 \tTraining Loss: 0.291203\n",
            "Epoch: 2949 \tTraining Loss: 0.291172\n",
            "Epoch: 2950 \tTraining Loss: 0.291140\n",
            "Epoch: 2951 \tTraining Loss: 0.291109\n",
            "Epoch: 2952 \tTraining Loss: 0.291077\n",
            "Epoch: 2953 \tTraining Loss: 0.291046\n",
            "Epoch: 2954 \tTraining Loss: 0.291015\n",
            "Epoch: 2955 \tTraining Loss: 0.290983\n",
            "Epoch: 2956 \tTraining Loss: 0.290952\n",
            "Epoch: 2957 \tTraining Loss: 0.290921\n",
            "Epoch: 2958 \tTraining Loss: 0.290889\n",
            "Epoch: 2959 \tTraining Loss: 0.290858\n",
            "Epoch: 2960 \tTraining Loss: 0.290828\n",
            "Epoch: 2961 \tTraining Loss: 0.290797\n",
            "Epoch: 2962 \tTraining Loss: 0.290767\n",
            "Epoch: 2963 \tTraining Loss: 0.290737\n",
            "Epoch: 2964 \tTraining Loss: 0.290708\n",
            "Epoch: 2965 \tTraining Loss: 0.290679\n",
            "Epoch: 2966 \tTraining Loss: 0.290651\n",
            "Epoch: 2967 \tTraining Loss: 0.290623\n",
            "Epoch: 2968 \tTraining Loss: 0.290596\n",
            "Epoch: 2969 \tTraining Loss: 0.290570\n",
            "Epoch: 2970 \tTraining Loss: 0.290546\n",
            "Epoch: 2971 \tTraining Loss: 0.290521\n",
            "Epoch: 2972 \tTraining Loss: 0.290502\n",
            "Epoch: 2973 \tTraining Loss: 0.290480\n",
            "Epoch: 2974 \tTraining Loss: 0.290468\n",
            "Epoch: 2975 \tTraining Loss: 0.290451\n",
            "Epoch: 2976 \tTraining Loss: 0.290453\n",
            "Epoch: 2977 \tTraining Loss: 0.290445\n",
            "Epoch: 2978 \tTraining Loss: 0.290477\n",
            "Epoch: 2979 \tTraining Loss: 0.290485\n",
            "Epoch: 2980 \tTraining Loss: 0.290588\n",
            "Epoch: 2981 \tTraining Loss: 0.290643\n",
            "Epoch: 2982 \tTraining Loss: 0.290982\n",
            "Epoch: 2983 \tTraining Loss: 0.291290\n",
            "Epoch: 2984 \tTraining Loss: 0.293236\n",
            "Epoch: 2985 \tTraining Loss: 0.298953\n",
            "Epoch: 2986 \tTraining Loss: 0.348865\n",
            "Epoch: 2987 \tTraining Loss: 0.346275\n",
            "Epoch: 2988 \tTraining Loss: 0.457368\n",
            "Epoch: 2989 \tTraining Loss: 0.407444\n",
            "Epoch: 2990 \tTraining Loss: 0.411588\n",
            "Epoch: 2991 \tTraining Loss: 0.407915\n",
            "Epoch: 2992 \tTraining Loss: 0.353970\n",
            "Epoch: 2993 \tTraining Loss: 0.442932\n",
            "Epoch: 2994 \tTraining Loss: 0.371680\n",
            "Epoch: 2995 \tTraining Loss: 0.389803\n",
            "Epoch: 2996 \tTraining Loss: 0.376227\n",
            "Epoch: 2997 \tTraining Loss: 0.362636\n",
            "Epoch: 2998 \tTraining Loss: 0.373468\n",
            "Epoch: 2999 \tTraining Loss: 0.355817\n",
            "Epoch: 3000 \tTraining Loss: 0.338529\n",
            "Epoch: 3001 \tTraining Loss: 0.344435\n",
            "Epoch: 3002 \tTraining Loss: 0.352246\n",
            "Epoch: 3003 \tTraining Loss: 0.338478\n",
            "Epoch: 3004 \tTraining Loss: 0.333764\n",
            "Epoch: 3005 \tTraining Loss: 0.332619\n",
            "Epoch: 3006 \tTraining Loss: 0.331343\n",
            "Epoch: 3007 \tTraining Loss: 0.327908\n",
            "Epoch: 3008 \tTraining Loss: 0.317085\n",
            "Epoch: 3009 \tTraining Loss: 0.317480\n",
            "Epoch: 3010 \tTraining Loss: 0.319805\n",
            "Epoch: 3011 \tTraining Loss: 0.313935\n",
            "Epoch: 3012 \tTraining Loss: 0.308993\n",
            "Epoch: 3013 \tTraining Loss: 0.309700\n",
            "Epoch: 3014 \tTraining Loss: 0.308587\n",
            "Epoch: 3015 \tTraining Loss: 0.305349\n",
            "Epoch: 3016 \tTraining Loss: 0.303829\n",
            "Epoch: 3017 \tTraining Loss: 0.301702\n",
            "Epoch: 3018 \tTraining Loss: 0.303280\n",
            "Epoch: 3019 \tTraining Loss: 0.301899\n",
            "Epoch: 3020 \tTraining Loss: 0.297998\n",
            "Epoch: 3021 \tTraining Loss: 0.300382\n",
            "Epoch: 3022 \tTraining Loss: 0.300408\n",
            "Epoch: 3023 \tTraining Loss: 0.300466\n",
            "Epoch: 3024 \tTraining Loss: 0.297991\n",
            "Epoch: 3025 \tTraining Loss: 0.298588\n",
            "Epoch: 3026 \tTraining Loss: 0.296220\n",
            "Epoch: 3027 \tTraining Loss: 0.298195\n",
            "Epoch: 3028 \tTraining Loss: 0.299978\n",
            "Epoch: 3029 \tTraining Loss: 0.299581\n",
            "Epoch: 3030 \tTraining Loss: 0.296960\n",
            "Epoch: 3031 \tTraining Loss: 0.294914\n",
            "Epoch: 3032 \tTraining Loss: 0.300791\n",
            "Epoch: 3033 \tTraining Loss: 0.301993\n",
            "Epoch: 3034 \tTraining Loss: 0.301628\n",
            "Epoch: 3035 \tTraining Loss: 0.300817\n",
            "Epoch: 3036 \tTraining Loss: 0.301301\n",
            "Epoch: 3037 \tTraining Loss: 0.301097\n",
            "Epoch: 3038 \tTraining Loss: 0.299204\n",
            "Epoch: 3039 \tTraining Loss: 0.298630\n",
            "Epoch: 3040 \tTraining Loss: 0.297749\n",
            "Epoch: 3041 \tTraining Loss: 0.296065\n",
            "Epoch: 3042 \tTraining Loss: 0.293407\n",
            "Epoch: 3043 \tTraining Loss: 0.294841\n",
            "Epoch: 3044 \tTraining Loss: 0.296724\n",
            "Epoch: 3045 \tTraining Loss: 0.296993\n",
            "Epoch: 3046 \tTraining Loss: 0.295959\n",
            "Epoch: 3047 \tTraining Loss: 0.293412\n",
            "Epoch: 3048 \tTraining Loss: 0.295265\n",
            "Epoch: 3049 \tTraining Loss: 0.296281\n",
            "Epoch: 3050 \tTraining Loss: 0.297336\n",
            "Epoch: 3051 \tTraining Loss: 0.297672\n",
            "Epoch: 3052 \tTraining Loss: 0.297454\n",
            "Epoch: 3053 \tTraining Loss: 0.297056\n",
            "Epoch: 3054 \tTraining Loss: 0.296134\n",
            "Epoch: 3055 \tTraining Loss: 0.294993\n",
            "Epoch: 3056 \tTraining Loss: 0.294254\n",
            "Epoch: 3057 \tTraining Loss: 0.294029\n",
            "Epoch: 3058 \tTraining Loss: 0.293841\n",
            "Epoch: 3059 \tTraining Loss: 0.292167\n",
            "Epoch: 3060 \tTraining Loss: 0.294749\n",
            "Epoch: 3061 \tTraining Loss: 0.296535\n",
            "Epoch: 3062 \tTraining Loss: 0.296660\n",
            "Epoch: 3063 \tTraining Loss: 0.296195\n",
            "Epoch: 3064 \tTraining Loss: 0.294476\n",
            "Epoch: 3065 \tTraining Loss: 0.291039\n",
            "Epoch: 3066 \tTraining Loss: 0.293696\n",
            "Epoch: 3067 \tTraining Loss: 0.294351\n",
            "Epoch: 3068 \tTraining Loss: 0.294324\n",
            "Epoch: 3069 \tTraining Loss: 0.293994\n",
            "Epoch: 3070 \tTraining Loss: 0.293894\n",
            "Epoch: 3071 \tTraining Loss: 0.293800\n",
            "Epoch: 3072 \tTraining Loss: 0.293566\n",
            "Epoch: 3073 \tTraining Loss: 0.293245\n",
            "Epoch: 3074 \tTraining Loss: 0.293034\n",
            "Epoch: 3075 \tTraining Loss: 0.292998\n",
            "Epoch: 3076 \tTraining Loss: 0.292972\n",
            "Epoch: 3077 \tTraining Loss: 0.292749\n",
            "Epoch: 3078 \tTraining Loss: 0.291467\n",
            "Epoch: 3079 \tTraining Loss: 0.292130\n",
            "Epoch: 3080 \tTraining Loss: 0.293162\n",
            "Epoch: 3081 \tTraining Loss: 0.292746\n",
            "Epoch: 3082 \tTraining Loss: 0.291600\n",
            "Epoch: 3083 \tTraining Loss: 0.291778\n",
            "Epoch: 3084 \tTraining Loss: 0.292322\n",
            "Epoch: 3085 \tTraining Loss: 0.291812\n",
            "Epoch: 3086 \tTraining Loss: 0.289494\n",
            "Epoch: 3087 \tTraining Loss: 0.291194\n",
            "Epoch: 3088 \tTraining Loss: 0.291461\n",
            "Epoch: 3089 \tTraining Loss: 0.291156\n",
            "Epoch: 3090 \tTraining Loss: 0.290400\n",
            "Epoch: 3091 \tTraining Loss: 0.289235\n",
            "Epoch: 3092 \tTraining Loss: 0.289025\n",
            "Epoch: 3093 \tTraining Loss: 0.288858\n",
            "Epoch: 3094 \tTraining Loss: 0.289276\n",
            "Epoch: 3095 \tTraining Loss: 0.288961\n",
            "Epoch: 3096 \tTraining Loss: 0.289789\n",
            "Epoch: 3097 \tTraining Loss: 0.288740\n",
            "Epoch: 3098 \tTraining Loss: 0.289301\n",
            "Epoch: 3099 \tTraining Loss: 0.288695\n",
            "Epoch: 3100 \tTraining Loss: 0.289879\n",
            "Epoch: 3101 \tTraining Loss: 0.289353\n",
            "Epoch: 3102 \tTraining Loss: 0.289703\n",
            "Epoch: 3103 \tTraining Loss: 0.290332\n",
            "Epoch: 3104 \tTraining Loss: 0.290529\n",
            "Epoch: 3105 \tTraining Loss: 0.289944\n",
            "Epoch: 3106 \tTraining Loss: 0.288990\n",
            "Epoch: 3107 \tTraining Loss: 0.291635\n",
            "Epoch: 3108 \tTraining Loss: 0.292587\n",
            "Epoch: 3109 \tTraining Loss: 0.292326\n",
            "Epoch: 3110 \tTraining Loss: 0.290370\n",
            "Epoch: 3111 \tTraining Loss: 0.289975\n",
            "Epoch: 3112 \tTraining Loss: 0.292814\n",
            "Epoch: 3113 \tTraining Loss: 0.294063\n",
            "Epoch: 3114 \tTraining Loss: 0.293726\n",
            "Epoch: 3115 \tTraining Loss: 0.292814\n",
            "Epoch: 3116 \tTraining Loss: 0.291494\n",
            "Epoch: 3117 \tTraining Loss: 0.290263\n",
            "Epoch: 3118 \tTraining Loss: 0.289441\n",
            "Epoch: 3119 \tTraining Loss: 0.291124\n",
            "Epoch: 3120 \tTraining Loss: 0.288580\n",
            "Epoch: 3121 \tTraining Loss: 0.288443\n",
            "Epoch: 3122 \tTraining Loss: 0.288800\n",
            "Epoch: 3123 \tTraining Loss: 0.288772\n",
            "Epoch: 3124 \tTraining Loss: 0.290103\n",
            "Epoch: 3125 \tTraining Loss: 0.288465\n",
            "Epoch: 3126 \tTraining Loss: 0.288153\n",
            "Epoch: 3127 \tTraining Loss: 0.289300\n",
            "Epoch: 3128 \tTraining Loss: 0.289245\n",
            "Epoch: 3129 \tTraining Loss: 0.289040\n",
            "Epoch: 3130 \tTraining Loss: 0.288437\n",
            "Epoch: 3131 \tTraining Loss: 0.290289\n",
            "Epoch: 3132 \tTraining Loss: 0.287917\n",
            "Epoch: 3133 \tTraining Loss: 0.288864\n",
            "Epoch: 3134 \tTraining Loss: 0.288500\n",
            "Epoch: 3135 \tTraining Loss: 0.288594\n",
            "Epoch: 3136 \tTraining Loss: 0.289098\n",
            "Epoch: 3137 \tTraining Loss: 0.288014\n",
            "Epoch: 3138 \tTraining Loss: 0.287833\n",
            "Epoch: 3139 \tTraining Loss: 0.288911\n",
            "Epoch: 3140 \tTraining Loss: 0.288534\n",
            "Epoch: 3141 \tTraining Loss: 0.287622\n",
            "Epoch: 3142 \tTraining Loss: 0.289917\n",
            "Epoch: 3143 \tTraining Loss: 0.287805\n",
            "Epoch: 3144 \tTraining Loss: 0.288721\n",
            "Epoch: 3145 \tTraining Loss: 0.289373\n",
            "Epoch: 3146 \tTraining Loss: 0.289867\n",
            "Epoch: 3147 \tTraining Loss: 0.289091\n",
            "Epoch: 3148 \tTraining Loss: 0.288489\n",
            "Epoch: 3149 \tTraining Loss: 0.287363\n",
            "Epoch: 3150 \tTraining Loss: 0.289315\n",
            "Epoch: 3151 \tTraining Loss: 0.287426\n",
            "Epoch: 3152 \tTraining Loss: 0.287326\n",
            "Epoch: 3153 \tTraining Loss: 0.288477\n",
            "Epoch: 3154 \tTraining Loss: 0.288496\n",
            "Epoch: 3155 \tTraining Loss: 0.288418\n",
            "Epoch: 3156 \tTraining Loss: 0.287208\n",
            "Epoch: 3157 \tTraining Loss: 0.287168\n",
            "Epoch: 3158 \tTraining Loss: 0.288453\n",
            "Epoch: 3159 \tTraining Loss: 0.288302\n",
            "Epoch: 3160 \tTraining Loss: 0.287074\n",
            "Epoch: 3161 \tTraining Loss: 0.287099\n",
            "Epoch: 3162 \tTraining Loss: 0.287238\n",
            "Epoch: 3163 \tTraining Loss: 0.287036\n",
            "Epoch: 3164 \tTraining Loss: 0.288140\n",
            "Epoch: 3165 \tTraining Loss: 0.288021\n",
            "Epoch: 3166 \tTraining Loss: 0.286924\n",
            "Epoch: 3167 \tTraining Loss: 0.286847\n",
            "Epoch: 3168 \tTraining Loss: 0.287616\n",
            "Epoch: 3169 \tTraining Loss: 0.286708\n",
            "Epoch: 3170 \tTraining Loss: 0.287753\n",
            "Epoch: 3171 \tTraining Loss: 0.287165\n",
            "Epoch: 3172 \tTraining Loss: 0.286940\n",
            "Epoch: 3173 \tTraining Loss: 0.287876\n",
            "Epoch: 3174 \tTraining Loss: 0.286649\n",
            "Epoch: 3175 \tTraining Loss: 0.286581\n",
            "Epoch: 3176 \tTraining Loss: 0.287121\n",
            "Epoch: 3177 \tTraining Loss: 0.286492\n",
            "Epoch: 3178 \tTraining Loss: 0.287501\n",
            "Epoch: 3179 \tTraining Loss: 0.286440\n",
            "Epoch: 3180 \tTraining Loss: 0.286424\n",
            "Epoch: 3181 \tTraining Loss: 0.286635\n",
            "Epoch: 3182 \tTraining Loss: 0.286400\n",
            "Epoch: 3183 \tTraining Loss: 0.286312\n",
            "Epoch: 3184 \tTraining Loss: 0.286333\n",
            "Epoch: 3185 \tTraining Loss: 0.286418\n",
            "Epoch: 3186 \tTraining Loss: 0.286239\n",
            "Epoch: 3187 \tTraining Loss: 0.286215\n",
            "Epoch: 3188 \tTraining Loss: 0.286265\n",
            "Epoch: 3189 \tTraining Loss: 0.286260\n",
            "Epoch: 3190 \tTraining Loss: 0.286192\n",
            "Epoch: 3191 \tTraining Loss: 0.286129\n",
            "Epoch: 3192 \tTraining Loss: 0.286070\n",
            "Epoch: 3193 \tTraining Loss: 0.286071\n",
            "Epoch: 3194 \tTraining Loss: 0.286091\n",
            "Epoch: 3195 \tTraining Loss: 0.286018\n",
            "Epoch: 3196 \tTraining Loss: 0.285966\n",
            "Epoch: 3197 \tTraining Loss: 0.285959\n",
            "Epoch: 3198 \tTraining Loss: 0.285948\n",
            "Epoch: 3199 \tTraining Loss: 0.285927\n",
            "Epoch: 3200 \tTraining Loss: 0.285894\n",
            "Epoch: 3201 \tTraining Loss: 0.285852\n",
            "Epoch: 3202 \tTraining Loss: 0.285824\n",
            "Epoch: 3203 \tTraining Loss: 0.285808\n",
            "Epoch: 3204 \tTraining Loss: 0.285791\n",
            "Epoch: 3205 \tTraining Loss: 0.285764\n",
            "Epoch: 3206 \tTraining Loss: 0.285727\n",
            "Epoch: 3207 \tTraining Loss: 0.285702\n",
            "Epoch: 3208 \tTraining Loss: 0.285686\n",
            "Epoch: 3209 \tTraining Loss: 0.285662\n",
            "Epoch: 3210 \tTraining Loss: 0.285638\n",
            "Epoch: 3211 \tTraining Loss: 0.285612\n",
            "Epoch: 3212 \tTraining Loss: 0.285582\n",
            "Epoch: 3213 \tTraining Loss: 0.285560\n",
            "Epoch: 3214 \tTraining Loss: 0.285540\n",
            "Epoch: 3215 \tTraining Loss: 0.285516\n",
            "Epoch: 3216 \tTraining Loss: 0.285491\n",
            "Epoch: 3217 \tTraining Loss: 0.285465\n",
            "Epoch: 3218 \tTraining Loss: 0.285441\n",
            "Epoch: 3219 \tTraining Loss: 0.285420\n",
            "Epoch: 3220 \tTraining Loss: 0.285396\n",
            "Epoch: 3221 \tTraining Loss: 0.285372\n",
            "Epoch: 3222 \tTraining Loss: 0.285348\n",
            "Epoch: 3223 \tTraining Loss: 0.285324\n",
            "Epoch: 3224 \tTraining Loss: 0.285302\n",
            "Epoch: 3225 \tTraining Loss: 0.285278\n",
            "Epoch: 3226 \tTraining Loss: 0.285255\n",
            "Epoch: 3227 \tTraining Loss: 0.285232\n",
            "Epoch: 3228 \tTraining Loss: 0.285208\n",
            "Epoch: 3229 \tTraining Loss: 0.285184\n",
            "Epoch: 3230 \tTraining Loss: 0.285162\n",
            "Epoch: 3231 \tTraining Loss: 0.285138\n",
            "Epoch: 3232 \tTraining Loss: 0.285115\n",
            "Epoch: 3233 \tTraining Loss: 0.285092\n",
            "Epoch: 3234 \tTraining Loss: 0.285068\n",
            "Epoch: 3235 \tTraining Loss: 0.285045\n",
            "Epoch: 3236 \tTraining Loss: 0.285022\n",
            "Epoch: 3237 \tTraining Loss: 0.284999\n",
            "Epoch: 3238 \tTraining Loss: 0.284976\n",
            "Epoch: 3239 \tTraining Loss: 0.284953\n",
            "Epoch: 3240 \tTraining Loss: 0.284930\n",
            "Epoch: 3241 \tTraining Loss: 0.284907\n",
            "Epoch: 3242 \tTraining Loss: 0.284884\n",
            "Epoch: 3243 \tTraining Loss: 0.284861\n",
            "Epoch: 3244 \tTraining Loss: 0.284837\n",
            "Epoch: 3245 \tTraining Loss: 0.284814\n",
            "Epoch: 3246 \tTraining Loss: 0.284791\n",
            "Epoch: 3247 \tTraining Loss: 0.284768\n",
            "Epoch: 3248 \tTraining Loss: 0.284745\n",
            "Epoch: 3249 \tTraining Loss: 0.284722\n",
            "Epoch: 3250 \tTraining Loss: 0.284699\n",
            "Epoch: 3251 \tTraining Loss: 0.284676\n",
            "Epoch: 3252 \tTraining Loss: 0.284653\n",
            "Epoch: 3253 \tTraining Loss: 0.284630\n",
            "Epoch: 3254 \tTraining Loss: 0.284607\n",
            "Epoch: 3255 \tTraining Loss: 0.284584\n",
            "Epoch: 3256 \tTraining Loss: 0.284561\n",
            "Epoch: 3257 \tTraining Loss: 0.284538\n",
            "Epoch: 3258 \tTraining Loss: 0.284515\n",
            "Epoch: 3259 \tTraining Loss: 0.284492\n",
            "Epoch: 3260 \tTraining Loss: 0.284469\n",
            "Epoch: 3261 \tTraining Loss: 0.284446\n",
            "Epoch: 3262 \tTraining Loss: 0.284424\n",
            "Epoch: 3263 \tTraining Loss: 0.284401\n",
            "Epoch: 3264 \tTraining Loss: 0.284378\n",
            "Epoch: 3265 \tTraining Loss: 0.284355\n",
            "Epoch: 3266 \tTraining Loss: 0.284332\n",
            "Epoch: 3267 \tTraining Loss: 0.284309\n",
            "Epoch: 3268 \tTraining Loss: 0.284286\n",
            "Epoch: 3269 \tTraining Loss: 0.284263\n",
            "Epoch: 3270 \tTraining Loss: 0.284240\n",
            "Epoch: 3271 \tTraining Loss: 0.284217\n",
            "Epoch: 3272 \tTraining Loss: 0.284194\n",
            "Epoch: 3273 \tTraining Loss: 0.284171\n",
            "Epoch: 3274 \tTraining Loss: 0.284148\n",
            "Epoch: 3275 \tTraining Loss: 0.284125\n",
            "Epoch: 3276 \tTraining Loss: 0.284103\n",
            "Epoch: 3277 \tTraining Loss: 0.284080\n",
            "Epoch: 3278 \tTraining Loss: 0.284057\n",
            "Epoch: 3279 \tTraining Loss: 0.284034\n",
            "Epoch: 3280 \tTraining Loss: 0.284011\n",
            "Epoch: 3281 \tTraining Loss: 0.283988\n",
            "Epoch: 3282 \tTraining Loss: 0.283965\n",
            "Epoch: 3283 \tTraining Loss: 0.283942\n",
            "Epoch: 3284 \tTraining Loss: 0.283919\n",
            "Epoch: 3285 \tTraining Loss: 0.283896\n",
            "Epoch: 3286 \tTraining Loss: 0.283874\n",
            "Epoch: 3287 \tTraining Loss: 0.283851\n",
            "Epoch: 3288 \tTraining Loss: 0.283828\n",
            "Epoch: 3289 \tTraining Loss: 0.283805\n",
            "Epoch: 3290 \tTraining Loss: 0.283782\n",
            "Epoch: 3291 \tTraining Loss: 0.283759\n",
            "Epoch: 3292 \tTraining Loss: 0.283736\n",
            "Epoch: 3293 \tTraining Loss: 0.283713\n",
            "Epoch: 3294 \tTraining Loss: 0.283691\n",
            "Epoch: 3295 \tTraining Loss: 0.283668\n",
            "Epoch: 3296 \tTraining Loss: 0.283645\n",
            "Epoch: 3297 \tTraining Loss: 0.283622\n",
            "Epoch: 3298 \tTraining Loss: 0.283599\n",
            "Epoch: 3299 \tTraining Loss: 0.283576\n",
            "Epoch: 3300 \tTraining Loss: 0.283553\n",
            "Epoch: 3301 \tTraining Loss: 0.283531\n",
            "Epoch: 3302 \tTraining Loss: 0.283508\n",
            "Epoch: 3303 \tTraining Loss: 0.283485\n",
            "Epoch: 3304 \tTraining Loss: 0.283462\n",
            "Epoch: 3305 \tTraining Loss: 0.283439\n",
            "Epoch: 3306 \tTraining Loss: 0.283416\n",
            "Epoch: 3307 \tTraining Loss: 0.283393\n",
            "Epoch: 3308 \tTraining Loss: 0.283371\n",
            "Epoch: 3309 \tTraining Loss: 0.283348\n",
            "Epoch: 3310 \tTraining Loss: 0.283325\n",
            "Epoch: 3311 \tTraining Loss: 0.283302\n",
            "Epoch: 3312 \tTraining Loss: 0.283279\n",
            "Epoch: 3313 \tTraining Loss: 0.283256\n",
            "Epoch: 3314 \tTraining Loss: 0.283233\n",
            "Epoch: 3315 \tTraining Loss: 0.283211\n",
            "Epoch: 3316 \tTraining Loss: 0.283188\n",
            "Epoch: 3317 \tTraining Loss: 0.283165\n",
            "Epoch: 3318 \tTraining Loss: 0.283142\n",
            "Epoch: 3319 \tTraining Loss: 0.283119\n",
            "Epoch: 3320 \tTraining Loss: 0.283096\n",
            "Epoch: 3321 \tTraining Loss: 0.283073\n",
            "Epoch: 3322 \tTraining Loss: 0.283051\n",
            "Epoch: 3323 \tTraining Loss: 0.283028\n",
            "Epoch: 3324 \tTraining Loss: 0.283005\n",
            "Epoch: 3325 \tTraining Loss: 0.282982\n",
            "Epoch: 3326 \tTraining Loss: 0.282959\n",
            "Epoch: 3327 \tTraining Loss: 0.282936\n",
            "Epoch: 3328 \tTraining Loss: 0.282913\n",
            "Epoch: 3329 \tTraining Loss: 0.282891\n",
            "Epoch: 3330 \tTraining Loss: 0.282868\n",
            "Epoch: 3331 \tTraining Loss: 0.282845\n",
            "Epoch: 3332 \tTraining Loss: 0.282822\n",
            "Epoch: 3333 \tTraining Loss: 0.282799\n",
            "Epoch: 3334 \tTraining Loss: 0.282776\n",
            "Epoch: 3335 \tTraining Loss: 0.282753\n",
            "Epoch: 3336 \tTraining Loss: 0.282730\n",
            "Epoch: 3337 \tTraining Loss: 0.282708\n",
            "Epoch: 3338 \tTraining Loss: 0.282685\n",
            "Epoch: 3339 \tTraining Loss: 0.282662\n",
            "Epoch: 3340 \tTraining Loss: 0.282639\n",
            "Epoch: 3341 \tTraining Loss: 0.282616\n",
            "Epoch: 3342 \tTraining Loss: 0.282593\n",
            "Epoch: 3343 \tTraining Loss: 0.282570\n",
            "Epoch: 3344 \tTraining Loss: 0.282547\n",
            "Epoch: 3345 \tTraining Loss: 0.282525\n",
            "Epoch: 3346 \tTraining Loss: 0.282502\n",
            "Epoch: 3347 \tTraining Loss: 0.282479\n",
            "Epoch: 3348 \tTraining Loss: 0.282456\n",
            "Epoch: 3349 \tTraining Loss: 0.282433\n",
            "Epoch: 3350 \tTraining Loss: 0.282410\n",
            "Epoch: 3351 \tTraining Loss: 0.282387\n",
            "Epoch: 3352 \tTraining Loss: 0.282364\n",
            "Epoch: 3353 \tTraining Loss: 0.282341\n",
            "Epoch: 3354 \tTraining Loss: 0.282318\n",
            "Epoch: 3355 \tTraining Loss: 0.282295\n",
            "Epoch: 3356 \tTraining Loss: 0.282273\n",
            "Epoch: 3357 \tTraining Loss: 0.282250\n",
            "Epoch: 3358 \tTraining Loss: 0.282227\n",
            "Epoch: 3359 \tTraining Loss: 0.282204\n",
            "Epoch: 3360 \tTraining Loss: 0.282181\n",
            "Epoch: 3361 \tTraining Loss: 0.282158\n",
            "Epoch: 3362 \tTraining Loss: 0.282135\n",
            "Epoch: 3363 \tTraining Loss: 0.282112\n",
            "Epoch: 3364 \tTraining Loss: 0.282089\n",
            "Epoch: 3365 \tTraining Loss: 0.282066\n",
            "Epoch: 3366 \tTraining Loss: 0.282043\n",
            "Epoch: 3367 \tTraining Loss: 0.282020\n",
            "Epoch: 3368 \tTraining Loss: 0.281997\n",
            "Epoch: 3369 \tTraining Loss: 0.281974\n",
            "Epoch: 3370 \tTraining Loss: 0.281951\n",
            "Epoch: 3371 \tTraining Loss: 0.281928\n",
            "Epoch: 3372 \tTraining Loss: 0.281905\n",
            "Epoch: 3373 \tTraining Loss: 0.281882\n",
            "Epoch: 3374 \tTraining Loss: 0.281859\n",
            "Epoch: 3375 \tTraining Loss: 0.281836\n",
            "Epoch: 3376 \tTraining Loss: 0.281813\n",
            "Epoch: 3377 \tTraining Loss: 0.281789\n",
            "Epoch: 3378 \tTraining Loss: 0.281766\n",
            "Epoch: 3379 \tTraining Loss: 0.281743\n",
            "Epoch: 3380 \tTraining Loss: 0.281720\n",
            "Epoch: 3381 \tTraining Loss: 0.281697\n",
            "Epoch: 3382 \tTraining Loss: 0.281674\n",
            "Epoch: 3383 \tTraining Loss: 0.281651\n",
            "Epoch: 3384 \tTraining Loss: 0.281628\n",
            "Epoch: 3385 \tTraining Loss: 0.281605\n",
            "Epoch: 3386 \tTraining Loss: 0.281582\n",
            "Epoch: 3387 \tTraining Loss: 0.281559\n",
            "Epoch: 3388 \tTraining Loss: 0.281536\n",
            "Epoch: 3389 \tTraining Loss: 0.281513\n",
            "Epoch: 3390 \tTraining Loss: 0.281490\n",
            "Epoch: 3391 \tTraining Loss: 0.281467\n",
            "Epoch: 3392 \tTraining Loss: 0.281444\n",
            "Epoch: 3393 \tTraining Loss: 0.281421\n",
            "Epoch: 3394 \tTraining Loss: 0.281398\n",
            "Epoch: 3395 \tTraining Loss: 0.281375\n",
            "Epoch: 3396 \tTraining Loss: 0.281352\n",
            "Epoch: 3397 \tTraining Loss: 0.281329\n",
            "Epoch: 3398 \tTraining Loss: 0.281307\n",
            "Epoch: 3399 \tTraining Loss: 0.281284\n",
            "Epoch: 3400 \tTraining Loss: 0.281261\n",
            "Epoch: 3401 \tTraining Loss: 0.281238\n",
            "Epoch: 3402 \tTraining Loss: 0.281215\n",
            "Epoch: 3403 \tTraining Loss: 0.281192\n",
            "Epoch: 3404 \tTraining Loss: 0.281169\n",
            "Epoch: 3405 \tTraining Loss: 0.281146\n",
            "Epoch: 3406 \tTraining Loss: 0.281123\n",
            "Epoch: 3407 \tTraining Loss: 0.281100\n",
            "Epoch: 3408 \tTraining Loss: 0.281077\n",
            "Epoch: 3409 \tTraining Loss: 0.281054\n",
            "Epoch: 3410 \tTraining Loss: 0.281031\n",
            "Epoch: 3411 \tTraining Loss: 0.281008\n",
            "Epoch: 3412 \tTraining Loss: 0.280985\n",
            "Epoch: 3413 \tTraining Loss: 0.280962\n",
            "Epoch: 3414 \tTraining Loss: 0.280939\n",
            "Epoch: 3415 \tTraining Loss: 0.280916\n",
            "Epoch: 3416 \tTraining Loss: 0.280893\n",
            "Epoch: 3417 \tTraining Loss: 0.280870\n",
            "Epoch: 3418 \tTraining Loss: 0.280847\n",
            "Epoch: 3419 \tTraining Loss: 0.280824\n",
            "Epoch: 3420 \tTraining Loss: 0.280801\n",
            "Epoch: 3421 \tTraining Loss: 0.280778\n",
            "Epoch: 3422 \tTraining Loss: 0.280755\n",
            "Epoch: 3423 \tTraining Loss: 0.280732\n",
            "Epoch: 3424 \tTraining Loss: 0.280709\n",
            "Epoch: 3425 \tTraining Loss: 0.280686\n",
            "Epoch: 3426 \tTraining Loss: 0.280663\n",
            "Epoch: 3427 \tTraining Loss: 0.280640\n",
            "Epoch: 3428 \tTraining Loss: 0.280617\n",
            "Epoch: 3429 \tTraining Loss: 0.280594\n",
            "Epoch: 3430 \tTraining Loss: 0.280571\n",
            "Epoch: 3431 \tTraining Loss: 0.280548\n",
            "Epoch: 3432 \tTraining Loss: 0.280525\n",
            "Epoch: 3433 \tTraining Loss: 0.280502\n",
            "Epoch: 3434 \tTraining Loss: 0.280479\n",
            "Epoch: 3435 \tTraining Loss: 0.280456\n",
            "Epoch: 3436 \tTraining Loss: 0.280433\n",
            "Epoch: 3437 \tTraining Loss: 0.280410\n",
            "Epoch: 3438 \tTraining Loss: 0.280387\n",
            "Epoch: 3439 \tTraining Loss: 0.280364\n",
            "Epoch: 3440 \tTraining Loss: 0.280341\n",
            "Epoch: 3441 \tTraining Loss: 0.280318\n",
            "Epoch: 3442 \tTraining Loss: 0.280295\n",
            "Epoch: 3443 \tTraining Loss: 0.280272\n",
            "Epoch: 3444 \tTraining Loss: 0.280249\n",
            "Epoch: 3445 \tTraining Loss: 0.280226\n",
            "Epoch: 3446 \tTraining Loss: 0.280203\n",
            "Epoch: 3447 \tTraining Loss: 0.280180\n",
            "Epoch: 3448 \tTraining Loss: 0.280157\n",
            "Epoch: 3449 \tTraining Loss: 0.280134\n",
            "Epoch: 3450 \tTraining Loss: 0.280111\n",
            "Epoch: 3451 \tTraining Loss: 0.280088\n",
            "Epoch: 3452 \tTraining Loss: 0.280065\n",
            "Epoch: 3453 \tTraining Loss: 0.280042\n",
            "Epoch: 3454 \tTraining Loss: 0.280019\n",
            "Epoch: 3455 \tTraining Loss: 0.279996\n",
            "Epoch: 3456 \tTraining Loss: 0.279973\n",
            "Epoch: 3457 \tTraining Loss: 0.279949\n",
            "Epoch: 3458 \tTraining Loss: 0.279926\n",
            "Epoch: 3459 \tTraining Loss: 0.279903\n",
            "Epoch: 3460 \tTraining Loss: 0.279880\n",
            "Epoch: 3461 \tTraining Loss: 0.279857\n",
            "Epoch: 3462 \tTraining Loss: 0.279834\n",
            "Epoch: 3463 \tTraining Loss: 0.279811\n",
            "Epoch: 3464 \tTraining Loss: 0.279788\n",
            "Epoch: 3465 \tTraining Loss: 0.279765\n",
            "Epoch: 3466 \tTraining Loss: 0.279742\n",
            "Epoch: 3467 \tTraining Loss: 0.279719\n",
            "Epoch: 3468 \tTraining Loss: 0.279695\n",
            "Epoch: 3469 \tTraining Loss: 0.279672\n",
            "Epoch: 3470 \tTraining Loss: 0.279649\n",
            "Epoch: 3471 \tTraining Loss: 0.279626\n",
            "Epoch: 3472 \tTraining Loss: 0.279603\n",
            "Epoch: 3473 \tTraining Loss: 0.279580\n",
            "Epoch: 3474 \tTraining Loss: 0.279557\n",
            "Epoch: 3475 \tTraining Loss: 0.279534\n",
            "Epoch: 3476 \tTraining Loss: 0.279511\n",
            "Epoch: 3477 \tTraining Loss: 0.279487\n",
            "Epoch: 3478 \tTraining Loss: 0.279464\n",
            "Epoch: 3479 \tTraining Loss: 0.279441\n",
            "Epoch: 3480 \tTraining Loss: 0.279418\n",
            "Epoch: 3481 \tTraining Loss: 0.279395\n",
            "Epoch: 3482 \tTraining Loss: 0.279372\n",
            "Epoch: 3483 \tTraining Loss: 0.279349\n",
            "Epoch: 3484 \tTraining Loss: 0.279325\n",
            "Epoch: 3485 \tTraining Loss: 0.279302\n",
            "Epoch: 3486 \tTraining Loss: 0.279279\n",
            "Epoch: 3487 \tTraining Loss: 0.279256\n",
            "Epoch: 3488 \tTraining Loss: 0.279233\n",
            "Epoch: 3489 \tTraining Loss: 0.279210\n",
            "Epoch: 3490 \tTraining Loss: 0.279186\n",
            "Epoch: 3491 \tTraining Loss: 0.279163\n",
            "Epoch: 3492 \tTraining Loss: 0.279140\n",
            "Epoch: 3493 \tTraining Loss: 0.279117\n",
            "Epoch: 3494 \tTraining Loss: 0.279094\n",
            "Epoch: 3495 \tTraining Loss: 0.279071\n",
            "Epoch: 3496 \tTraining Loss: 0.279047\n",
            "Epoch: 3497 \tTraining Loss: 0.279024\n",
            "Epoch: 3498 \tTraining Loss: 0.279001\n",
            "Epoch: 3499 \tTraining Loss: 0.278978\n",
            "Epoch: 3500 \tTraining Loss: 0.278955\n",
            "Epoch: 3501 \tTraining Loss: 0.278931\n",
            "Epoch: 3502 \tTraining Loss: 0.278908\n",
            "Epoch: 3503 \tTraining Loss: 0.278885\n",
            "Epoch: 3504 \tTraining Loss: 0.278862\n",
            "Epoch: 3505 \tTraining Loss: 0.278839\n",
            "Epoch: 3506 \tTraining Loss: 0.278815\n",
            "Epoch: 3507 \tTraining Loss: 0.278792\n",
            "Epoch: 3508 \tTraining Loss: 0.278769\n",
            "Epoch: 3509 \tTraining Loss: 0.278746\n",
            "Epoch: 3510 \tTraining Loss: 0.278722\n",
            "Epoch: 3511 \tTraining Loss: 0.278699\n",
            "Epoch: 3512 \tTraining Loss: 0.278676\n",
            "Epoch: 3513 \tTraining Loss: 0.278653\n",
            "Epoch: 3514 \tTraining Loss: 0.278629\n",
            "Epoch: 3515 \tTraining Loss: 0.278606\n",
            "Epoch: 3516 \tTraining Loss: 0.278583\n",
            "Epoch: 3517 \tTraining Loss: 0.278560\n",
            "Epoch: 3518 \tTraining Loss: 0.278536\n",
            "Epoch: 3519 \tTraining Loss: 0.278513\n",
            "Epoch: 3520 \tTraining Loss: 0.278490\n",
            "Epoch: 3521 \tTraining Loss: 0.278467\n",
            "Epoch: 3522 \tTraining Loss: 0.278443\n",
            "Epoch: 3523 \tTraining Loss: 0.278420\n",
            "Epoch: 3524 \tTraining Loss: 0.278397\n",
            "Epoch: 3525 \tTraining Loss: 0.278374\n",
            "Epoch: 3526 \tTraining Loss: 0.278350\n",
            "Epoch: 3527 \tTraining Loss: 0.278327\n",
            "Epoch: 3528 \tTraining Loss: 0.278304\n",
            "Epoch: 3529 \tTraining Loss: 0.278280\n",
            "Epoch: 3530 \tTraining Loss: 0.278257\n",
            "Epoch: 3531 \tTraining Loss: 0.278234\n",
            "Epoch: 3532 \tTraining Loss: 0.278211\n",
            "Epoch: 3533 \tTraining Loss: 0.278187\n",
            "Epoch: 3534 \tTraining Loss: 0.278164\n",
            "Epoch: 3535 \tTraining Loss: 0.278141\n",
            "Epoch: 3536 \tTraining Loss: 0.278117\n",
            "Epoch: 3537 \tTraining Loss: 0.278094\n",
            "Epoch: 3538 \tTraining Loss: 0.278071\n",
            "Epoch: 3539 \tTraining Loss: 0.278047\n",
            "Epoch: 3540 \tTraining Loss: 0.278024\n",
            "Epoch: 3541 \tTraining Loss: 0.278001\n",
            "Epoch: 3542 \tTraining Loss: 0.277977\n",
            "Epoch: 3543 \tTraining Loss: 0.277954\n",
            "Epoch: 3544 \tTraining Loss: 0.277931\n",
            "Epoch: 3545 \tTraining Loss: 0.277907\n",
            "Epoch: 3546 \tTraining Loss: 0.277884\n",
            "Epoch: 3547 \tTraining Loss: 0.277861\n",
            "Epoch: 3548 \tTraining Loss: 0.277837\n",
            "Epoch: 3549 \tTraining Loss: 0.277814\n",
            "Epoch: 3550 \tTraining Loss: 0.277791\n",
            "Epoch: 3551 \tTraining Loss: 0.277767\n",
            "Epoch: 3552 \tTraining Loss: 0.277744\n",
            "Epoch: 3553 \tTraining Loss: 0.277721\n",
            "Epoch: 3554 \tTraining Loss: 0.277697\n",
            "Epoch: 3555 \tTraining Loss: 0.277674\n",
            "Epoch: 3556 \tTraining Loss: 0.277651\n",
            "Epoch: 3557 \tTraining Loss: 0.277627\n",
            "Epoch: 3558 \tTraining Loss: 0.277604\n",
            "Epoch: 3559 \tTraining Loss: 0.277580\n",
            "Epoch: 3560 \tTraining Loss: 0.277557\n",
            "Epoch: 3561 \tTraining Loss: 0.277534\n",
            "Epoch: 3562 \tTraining Loss: 0.277510\n",
            "Epoch: 3563 \tTraining Loss: 0.277487\n",
            "Epoch: 3564 \tTraining Loss: 0.277463\n",
            "Epoch: 3565 \tTraining Loss: 0.277440\n",
            "Epoch: 3566 \tTraining Loss: 0.277417\n",
            "Epoch: 3567 \tTraining Loss: 0.277393\n",
            "Epoch: 3568 \tTraining Loss: 0.277370\n",
            "Epoch: 3569 \tTraining Loss: 0.277346\n",
            "Epoch: 3570 \tTraining Loss: 0.277323\n",
            "Epoch: 3571 \tTraining Loss: 0.277300\n",
            "Epoch: 3572 \tTraining Loss: 0.277276\n",
            "Epoch: 3573 \tTraining Loss: 0.277253\n",
            "Epoch: 3574 \tTraining Loss: 0.277229\n",
            "Epoch: 3575 \tTraining Loss: 0.277206\n",
            "Epoch: 3576 \tTraining Loss: 0.277182\n",
            "Epoch: 3577 \tTraining Loss: 0.277159\n",
            "Epoch: 3578 \tTraining Loss: 0.277136\n",
            "Epoch: 3579 \tTraining Loss: 0.277112\n",
            "Epoch: 3580 \tTraining Loss: 0.277089\n",
            "Epoch: 3581 \tTraining Loss: 0.277065\n",
            "Epoch: 3582 \tTraining Loss: 0.277042\n",
            "Epoch: 3583 \tTraining Loss: 0.277018\n",
            "Epoch: 3584 \tTraining Loss: 0.276995\n",
            "Epoch: 3585 \tTraining Loss: 0.276971\n",
            "Epoch: 3586 \tTraining Loss: 0.276948\n",
            "Epoch: 3587 \tTraining Loss: 0.276925\n",
            "Epoch: 3588 \tTraining Loss: 0.276901\n",
            "Epoch: 3589 \tTraining Loss: 0.276878\n",
            "Epoch: 3590 \tTraining Loss: 0.276854\n",
            "Epoch: 3591 \tTraining Loss: 0.276831\n",
            "Epoch: 3592 \tTraining Loss: 0.276807\n",
            "Epoch: 3593 \tTraining Loss: 0.276784\n",
            "Epoch: 3594 \tTraining Loss: 0.276760\n",
            "Epoch: 3595 \tTraining Loss: 0.276737\n",
            "Epoch: 3596 \tTraining Loss: 0.276713\n",
            "Epoch: 3597 \tTraining Loss: 0.276690\n",
            "Epoch: 3598 \tTraining Loss: 0.276666\n",
            "Epoch: 3599 \tTraining Loss: 0.276643\n",
            "Epoch: 3600 \tTraining Loss: 0.276619\n",
            "Epoch: 3601 \tTraining Loss: 0.276596\n",
            "Epoch: 3602 \tTraining Loss: 0.276573\n",
            "Epoch: 3603 \tTraining Loss: 0.276549\n",
            "Epoch: 3604 \tTraining Loss: 0.276526\n",
            "Epoch: 3605 \tTraining Loss: 0.276503\n",
            "Epoch: 3606 \tTraining Loss: 0.276480\n",
            "Epoch: 3607 \tTraining Loss: 0.276457\n",
            "Epoch: 3608 \tTraining Loss: 0.276436\n",
            "Epoch: 3609 \tTraining Loss: 0.276415\n",
            "Epoch: 3610 \tTraining Loss: 0.276397\n",
            "Epoch: 3611 \tTraining Loss: 0.276382\n",
            "Epoch: 3612 \tTraining Loss: 0.276374\n",
            "Epoch: 3613 \tTraining Loss: 0.276376\n",
            "Epoch: 3614 \tTraining Loss: 0.276402\n",
            "Epoch: 3615 \tTraining Loss: 0.276450\n",
            "Epoch: 3616 \tTraining Loss: 0.276597\n",
            "Epoch: 3617 \tTraining Loss: 0.276792\n",
            "Epoch: 3618 \tTraining Loss: 0.277697\n",
            "Epoch: 3619 \tTraining Loss: 0.280180\n",
            "Epoch: 3620 \tTraining Loss: 0.314785\n",
            "Epoch: 3621 \tTraining Loss: 0.443886\n",
            "Epoch: 3622 \tTraining Loss: 0.607106\n",
            "Epoch: 3623 \tTraining Loss: 0.606771\n",
            "Epoch: 3624 \tTraining Loss: 0.559703\n",
            "Epoch: 3625 \tTraining Loss: 0.459559\n",
            "Epoch: 3626 \tTraining Loss: 0.547882\n",
            "Epoch: 3627 \tTraining Loss: 0.471665\n",
            "Epoch: 3628 \tTraining Loss: 0.487243\n",
            "Epoch: 3629 \tTraining Loss: 0.479978\n",
            "Epoch: 3630 \tTraining Loss: 0.473150\n",
            "Epoch: 3631 \tTraining Loss: 0.427163\n",
            "Epoch: 3632 \tTraining Loss: 0.416156\n",
            "Epoch: 3633 \tTraining Loss: 0.408021\n",
            "Epoch: 3634 \tTraining Loss: 0.397459\n",
            "Epoch: 3635 \tTraining Loss: 0.393223\n",
            "Epoch: 3636 \tTraining Loss: 0.381969\n",
            "Epoch: 3637 \tTraining Loss: 0.375299\n",
            "Epoch: 3638 \tTraining Loss: 0.346752\n",
            "Epoch: 3639 \tTraining Loss: 0.349303\n",
            "Epoch: 3640 \tTraining Loss: 0.363227\n",
            "Epoch: 3641 \tTraining Loss: 0.336987\n",
            "Epoch: 3642 \tTraining Loss: 0.318475\n",
            "Epoch: 3643 \tTraining Loss: 0.328555\n",
            "Epoch: 3644 \tTraining Loss: 0.333909\n",
            "Epoch: 3645 \tTraining Loss: 0.324837\n",
            "Epoch: 3646 \tTraining Loss: 0.316918\n",
            "Epoch: 3647 \tTraining Loss: 0.320264\n",
            "Epoch: 3648 \tTraining Loss: 0.313308\n",
            "Epoch: 3649 \tTraining Loss: 0.311381\n",
            "Epoch: 3650 \tTraining Loss: 0.304489\n",
            "Epoch: 3651 \tTraining Loss: 0.301247\n",
            "Epoch: 3652 \tTraining Loss: 0.299273\n",
            "Epoch: 3653 \tTraining Loss: 0.302868\n",
            "Epoch: 3654 \tTraining Loss: 0.299184\n",
            "Epoch: 3655 \tTraining Loss: 0.298323\n",
            "Epoch: 3656 \tTraining Loss: 0.296262\n",
            "Epoch: 3657 \tTraining Loss: 0.295688\n",
            "Epoch: 3658 \tTraining Loss: 0.294082\n",
            "Epoch: 3659 \tTraining Loss: 0.293110\n",
            "Epoch: 3660 \tTraining Loss: 0.291531\n",
            "Epoch: 3661 \tTraining Loss: 0.292780\n",
            "Epoch: 3662 \tTraining Loss: 0.292289\n",
            "Epoch: 3663 \tTraining Loss: 0.290700\n",
            "Epoch: 3664 \tTraining Loss: 0.289869\n",
            "Epoch: 3665 \tTraining Loss: 0.289181\n",
            "Epoch: 3666 \tTraining Loss: 0.288354\n",
            "Epoch: 3667 \tTraining Loss: 0.287382\n",
            "Epoch: 3668 \tTraining Loss: 0.287238\n",
            "Epoch: 3669 \tTraining Loss: 0.286402\n",
            "Epoch: 3670 \tTraining Loss: 0.286150\n",
            "Epoch: 3671 \tTraining Loss: 0.285627\n",
            "Epoch: 3672 \tTraining Loss: 0.285197\n",
            "Epoch: 3673 \tTraining Loss: 0.284894\n",
            "Epoch: 3674 \tTraining Loss: 0.284222\n",
            "Epoch: 3675 \tTraining Loss: 0.284218\n",
            "Epoch: 3676 \tTraining Loss: 0.283762\n",
            "Epoch: 3677 \tTraining Loss: 0.283789\n",
            "Epoch: 3678 \tTraining Loss: 0.283602\n",
            "Epoch: 3679 \tTraining Loss: 0.283140\n",
            "Epoch: 3680 \tTraining Loss: 0.282915\n",
            "Epoch: 3681 \tTraining Loss: 0.282801\n",
            "Epoch: 3682 \tTraining Loss: 0.282691\n",
            "Epoch: 3683 \tTraining Loss: 0.282359\n",
            "Epoch: 3684 \tTraining Loss: 0.282600\n",
            "Epoch: 3685 \tTraining Loss: 0.282177\n",
            "Epoch: 3686 \tTraining Loss: 0.282276\n",
            "Epoch: 3687 \tTraining Loss: 0.282549\n",
            "Epoch: 3688 \tTraining Loss: 0.281723\n",
            "Epoch: 3689 \tTraining Loss: 0.282374\n",
            "Epoch: 3690 \tTraining Loss: 0.281484\n",
            "Epoch: 3691 \tTraining Loss: 0.281742\n",
            "Epoch: 3692 \tTraining Loss: 0.281209\n",
            "Epoch: 3693 \tTraining Loss: 0.281324\n",
            "Epoch: 3694 \tTraining Loss: 0.281181\n",
            "Epoch: 3695 \tTraining Loss: 0.281040\n",
            "Epoch: 3696 \tTraining Loss: 0.280807\n",
            "Epoch: 3697 \tTraining Loss: 0.280861\n",
            "Epoch: 3698 \tTraining Loss: 0.280635\n",
            "Epoch: 3699 \tTraining Loss: 0.280711\n",
            "Epoch: 3700 \tTraining Loss: 0.280362\n",
            "Epoch: 3701 \tTraining Loss: 0.280486\n",
            "Epoch: 3702 \tTraining Loss: 0.280277\n",
            "Epoch: 3703 \tTraining Loss: 0.280239\n",
            "Epoch: 3704 \tTraining Loss: 0.280118\n",
            "Epoch: 3705 \tTraining Loss: 0.280112\n",
            "Epoch: 3706 \tTraining Loss: 0.280071\n",
            "Epoch: 3707 \tTraining Loss: 0.279988\n",
            "Epoch: 3708 \tTraining Loss: 0.279910\n",
            "Epoch: 3709 \tTraining Loss: 0.279719\n",
            "Epoch: 3710 \tTraining Loss: 0.279765\n",
            "Epoch: 3711 \tTraining Loss: 0.279631\n",
            "Epoch: 3712 \tTraining Loss: 0.279608\n",
            "Epoch: 3713 \tTraining Loss: 0.279535\n",
            "Epoch: 3714 \tTraining Loss: 0.279469\n",
            "Epoch: 3715 \tTraining Loss: 0.279420\n",
            "Epoch: 3716 \tTraining Loss: 0.279357\n",
            "Epoch: 3717 \tTraining Loss: 0.279288\n",
            "Epoch: 3718 \tTraining Loss: 0.279273\n",
            "Epoch: 3719 \tTraining Loss: 0.279190\n",
            "Epoch: 3720 \tTraining Loss: 0.279166\n",
            "Epoch: 3721 \tTraining Loss: 0.279074\n",
            "Epoch: 3722 \tTraining Loss: 0.279064\n",
            "Epoch: 3723 \tTraining Loss: 0.278984\n",
            "Epoch: 3724 \tTraining Loss: 0.278966\n",
            "Epoch: 3725 \tTraining Loss: 0.278901\n",
            "Epoch: 3726 \tTraining Loss: 0.278863\n",
            "Epoch: 3727 \tTraining Loss: 0.278815\n",
            "Epoch: 3728 \tTraining Loss: 0.278779\n",
            "Epoch: 3729 \tTraining Loss: 0.278720\n",
            "Epoch: 3730 \tTraining Loss: 0.278698\n",
            "Epoch: 3731 \tTraining Loss: 0.278638\n",
            "Epoch: 3732 \tTraining Loss: 0.278611\n",
            "Epoch: 3733 \tTraining Loss: 0.278559\n",
            "Epoch: 3734 \tTraining Loss: 0.278527\n",
            "Epoch: 3735 \tTraining Loss: 0.278482\n",
            "Epoch: 3736 \tTraining Loss: 0.278448\n",
            "Epoch: 3737 \tTraining Loss: 0.278405\n",
            "Epoch: 3738 \tTraining Loss: 0.278368\n",
            "Epoch: 3739 \tTraining Loss: 0.278330\n",
            "Epoch: 3740 \tTraining Loss: 0.278291\n",
            "Epoch: 3741 \tTraining Loss: 0.278255\n",
            "Epoch: 3742 \tTraining Loss: 0.278218\n",
            "Epoch: 3743 \tTraining Loss: 0.278181\n",
            "Epoch: 3744 \tTraining Loss: 0.278146\n",
            "Epoch: 3745 \tTraining Loss: 0.278109\n",
            "Epoch: 3746 \tTraining Loss: 0.278074\n",
            "Epoch: 3747 \tTraining Loss: 0.278039\n",
            "Epoch: 3748 \tTraining Loss: 0.278003\n",
            "Epoch: 3749 \tTraining Loss: 0.277968\n",
            "Epoch: 3750 \tTraining Loss: 0.277934\n",
            "Epoch: 3751 \tTraining Loss: 0.277898\n",
            "Epoch: 3752 \tTraining Loss: 0.277865\n",
            "Epoch: 3753 \tTraining Loss: 0.277830\n",
            "Epoch: 3754 \tTraining Loss: 0.277797\n",
            "Epoch: 3755 \tTraining Loss: 0.277763\n",
            "Epoch: 3756 \tTraining Loss: 0.277730\n",
            "Epoch: 3757 \tTraining Loss: 0.277696\n",
            "Epoch: 3758 \tTraining Loss: 0.277664\n",
            "Epoch: 3759 \tTraining Loss: 0.277631\n",
            "Epoch: 3760 \tTraining Loss: 0.277598\n",
            "Epoch: 3761 \tTraining Loss: 0.277566\n",
            "Epoch: 3762 \tTraining Loss: 0.277533\n",
            "Epoch: 3763 \tTraining Loss: 0.277501\n",
            "Epoch: 3764 \tTraining Loss: 0.277469\n",
            "Epoch: 3765 \tTraining Loss: 0.277437\n",
            "Epoch: 3766 \tTraining Loss: 0.277405\n",
            "Epoch: 3767 \tTraining Loss: 0.277374\n",
            "Epoch: 3768 \tTraining Loss: 0.277342\n",
            "Epoch: 3769 \tTraining Loss: 0.277311\n",
            "Epoch: 3770 \tTraining Loss: 0.277280\n",
            "Epoch: 3771 \tTraining Loss: 0.277249\n",
            "Epoch: 3772 \tTraining Loss: 0.277218\n",
            "Epoch: 3773 \tTraining Loss: 0.277187\n",
            "Epoch: 3774 \tTraining Loss: 0.277157\n",
            "Epoch: 3775 \tTraining Loss: 0.277126\n",
            "Epoch: 3776 \tTraining Loss: 0.277096\n",
            "Epoch: 3777 \tTraining Loss: 0.277066\n",
            "Epoch: 3778 \tTraining Loss: 0.277036\n",
            "Epoch: 3779 \tTraining Loss: 0.277006\n",
            "Epoch: 3780 \tTraining Loss: 0.276976\n",
            "Epoch: 3781 \tTraining Loss: 0.276946\n",
            "Epoch: 3782 \tTraining Loss: 0.276917\n",
            "Epoch: 3783 \tTraining Loss: 0.276887\n",
            "Epoch: 3784 \tTraining Loss: 0.276858\n",
            "Epoch: 3785 \tTraining Loss: 0.276829\n",
            "Epoch: 3786 \tTraining Loss: 0.276800\n",
            "Epoch: 3787 \tTraining Loss: 0.276771\n",
            "Epoch: 3788 \tTraining Loss: 0.276742\n",
            "Epoch: 3789 \tTraining Loss: 0.276713\n",
            "Epoch: 3790 \tTraining Loss: 0.276685\n",
            "Epoch: 3791 \tTraining Loss: 0.276656\n",
            "Epoch: 3792 \tTraining Loss: 0.276628\n",
            "Epoch: 3793 \tTraining Loss: 0.276600\n",
            "Epoch: 3794 \tTraining Loss: 0.276571\n",
            "Epoch: 3795 \tTraining Loss: 0.276543\n",
            "Epoch: 3796 \tTraining Loss: 0.276515\n",
            "Epoch: 3797 \tTraining Loss: 0.276488\n",
            "Epoch: 3798 \tTraining Loss: 0.276460\n",
            "Epoch: 3799 \tTraining Loss: 0.276432\n",
            "Epoch: 3800 \tTraining Loss: 0.276405\n",
            "Epoch: 3801 \tTraining Loss: 0.276377\n",
            "Epoch: 3802 \tTraining Loss: 0.276350\n",
            "Epoch: 3803 \tTraining Loss: 0.276322\n",
            "Epoch: 3804 \tTraining Loss: 0.276295\n",
            "Epoch: 3805 \tTraining Loss: 0.276268\n",
            "Epoch: 3806 \tTraining Loss: 0.276241\n",
            "Epoch: 3807 \tTraining Loss: 0.276214\n",
            "Epoch: 3808 \tTraining Loss: 0.276188\n",
            "Epoch: 3809 \tTraining Loss: 0.276161\n",
            "Epoch: 3810 \tTraining Loss: 0.276134\n",
            "Epoch: 3811 \tTraining Loss: 0.276108\n",
            "Epoch: 3812 \tTraining Loss: 0.276081\n",
            "Epoch: 3813 \tTraining Loss: 0.276055\n",
            "Epoch: 3814 \tTraining Loss: 0.276029\n",
            "Epoch: 3815 \tTraining Loss: 0.276003\n",
            "Epoch: 3816 \tTraining Loss: 0.275977\n",
            "Epoch: 3817 \tTraining Loss: 0.275951\n",
            "Epoch: 3818 \tTraining Loss: 0.275925\n",
            "Epoch: 3819 \tTraining Loss: 0.275899\n",
            "Epoch: 3820 \tTraining Loss: 0.275873\n",
            "Epoch: 3821 \tTraining Loss: 0.275848\n",
            "Epoch: 3822 \tTraining Loss: 0.275822\n",
            "Epoch: 3823 \tTraining Loss: 0.275797\n",
            "Epoch: 3824 \tTraining Loss: 0.275771\n",
            "Epoch: 3825 \tTraining Loss: 0.275746\n",
            "Epoch: 3826 \tTraining Loss: 0.275721\n",
            "Epoch: 3827 \tTraining Loss: 0.275695\n",
            "Epoch: 3828 \tTraining Loss: 0.275670\n",
            "Epoch: 3829 \tTraining Loss: 0.275645\n",
            "Epoch: 3830 \tTraining Loss: 0.275620\n",
            "Epoch: 3831 \tTraining Loss: 0.275595\n",
            "Epoch: 3832 \tTraining Loss: 0.275570\n",
            "Epoch: 3833 \tTraining Loss: 0.275546\n",
            "Epoch: 3834 \tTraining Loss: 0.275521\n",
            "Epoch: 3835 \tTraining Loss: 0.275496\n",
            "Epoch: 3836 \tTraining Loss: 0.275472\n",
            "Epoch: 3837 \tTraining Loss: 0.275447\n",
            "Epoch: 3838 \tTraining Loss: 0.275423\n",
            "Epoch: 3839 \tTraining Loss: 0.275398\n",
            "Epoch: 3840 \tTraining Loss: 0.275374\n",
            "Epoch: 3841 \tTraining Loss: 0.275350\n",
            "Epoch: 3842 \tTraining Loss: 0.275325\n",
            "Epoch: 3843 \tTraining Loss: 0.275301\n",
            "Epoch: 3844 \tTraining Loss: 0.275277\n",
            "Epoch: 3845 \tTraining Loss: 0.275253\n",
            "Epoch: 3846 \tTraining Loss: 0.275229\n",
            "Epoch: 3847 \tTraining Loss: 0.275205\n",
            "Epoch: 3848 \tTraining Loss: 0.275181\n",
            "Epoch: 3849 \tTraining Loss: 0.275157\n",
            "Epoch: 3850 \tTraining Loss: 0.275134\n",
            "Epoch: 3851 \tTraining Loss: 0.275110\n",
            "Epoch: 3852 \tTraining Loss: 0.275086\n",
            "Epoch: 3853 \tTraining Loss: 0.275062\n",
            "Epoch: 3854 \tTraining Loss: 0.275039\n",
            "Epoch: 3855 \tTraining Loss: 0.275015\n",
            "Epoch: 3856 \tTraining Loss: 0.274992\n",
            "Epoch: 3857 \tTraining Loss: 0.274968\n",
            "Epoch: 3858 \tTraining Loss: 0.274945\n",
            "Epoch: 3859 \tTraining Loss: 0.274922\n",
            "Epoch: 3860 \tTraining Loss: 0.274898\n",
            "Epoch: 3861 \tTraining Loss: 0.274875\n",
            "Epoch: 3862 \tTraining Loss: 0.274852\n",
            "Epoch: 3863 \tTraining Loss: 0.274829\n",
            "Epoch: 3864 \tTraining Loss: 0.274805\n",
            "Epoch: 3865 \tTraining Loss: 0.274782\n",
            "Epoch: 3866 \tTraining Loss: 0.274759\n",
            "Epoch: 3867 \tTraining Loss: 0.274736\n",
            "Epoch: 3868 \tTraining Loss: 0.274713\n",
            "Epoch: 3869 \tTraining Loss: 0.274690\n",
            "Epoch: 3870 \tTraining Loss: 0.274667\n",
            "Epoch: 3871 \tTraining Loss: 0.274644\n",
            "Epoch: 3872 \tTraining Loss: 0.274622\n",
            "Epoch: 3873 \tTraining Loss: 0.274599\n",
            "Epoch: 3874 \tTraining Loss: 0.274576\n",
            "Epoch: 3875 \tTraining Loss: 0.274553\n",
            "Epoch: 3876 \tTraining Loss: 0.274530\n",
            "Epoch: 3877 \tTraining Loss: 0.274508\n",
            "Epoch: 3878 \tTraining Loss: 0.274485\n",
            "Epoch: 3879 \tTraining Loss: 0.274462\n",
            "Epoch: 3880 \tTraining Loss: 0.274440\n",
            "Epoch: 3881 \tTraining Loss: 0.274417\n",
            "Epoch: 3882 \tTraining Loss: 0.274395\n",
            "Epoch: 3883 \tTraining Loss: 0.274372\n",
            "Epoch: 3884 \tTraining Loss: 0.274350\n",
            "Epoch: 3885 \tTraining Loss: 0.274327\n",
            "Epoch: 3886 \tTraining Loss: 0.274305\n",
            "Epoch: 3887 \tTraining Loss: 0.274282\n",
            "Epoch: 3888 \tTraining Loss: 0.274260\n",
            "Epoch: 3889 \tTraining Loss: 0.274238\n",
            "Epoch: 3890 \tTraining Loss: 0.274215\n",
            "Epoch: 3891 \tTraining Loss: 0.274193\n",
            "Epoch: 3892 \tTraining Loss: 0.274171\n",
            "Epoch: 3893 \tTraining Loss: 0.274148\n",
            "Epoch: 3894 \tTraining Loss: 0.274126\n",
            "Epoch: 3895 \tTraining Loss: 0.274104\n",
            "Epoch: 3896 \tTraining Loss: 0.274082\n",
            "Epoch: 3897 \tTraining Loss: 0.274060\n",
            "Epoch: 3898 \tTraining Loss: 0.274038\n",
            "Epoch: 3899 \tTraining Loss: 0.274015\n",
            "Epoch: 3900 \tTraining Loss: 0.273993\n",
            "Epoch: 3901 \tTraining Loss: 0.273971\n",
            "Epoch: 3902 \tTraining Loss: 0.273949\n",
            "Epoch: 3903 \tTraining Loss: 0.273927\n",
            "Epoch: 3904 \tTraining Loss: 0.273905\n",
            "Epoch: 3905 \tTraining Loss: 0.273883\n",
            "Epoch: 3906 \tTraining Loss: 0.273861\n",
            "Epoch: 3907 \tTraining Loss: 0.273839\n",
            "Epoch: 3908 \tTraining Loss: 0.273817\n",
            "Epoch: 3909 \tTraining Loss: 0.273795\n",
            "Epoch: 3910 \tTraining Loss: 0.273774\n",
            "Epoch: 3911 \tTraining Loss: 0.273752\n",
            "Epoch: 3912 \tTraining Loss: 0.273730\n",
            "Epoch: 3913 \tTraining Loss: 0.273708\n",
            "Epoch: 3914 \tTraining Loss: 0.273686\n",
            "Epoch: 3915 \tTraining Loss: 0.273664\n",
            "Epoch: 3916 \tTraining Loss: 0.273643\n",
            "Epoch: 3917 \tTraining Loss: 0.273621\n",
            "Epoch: 3918 \tTraining Loss: 0.273599\n",
            "Epoch: 3919 \tTraining Loss: 0.273577\n",
            "Epoch: 3920 \tTraining Loss: 0.273556\n",
            "Epoch: 3921 \tTraining Loss: 0.273534\n",
            "Epoch: 3922 \tTraining Loss: 0.273512\n",
            "Epoch: 3923 \tTraining Loss: 0.273491\n",
            "Epoch: 3924 \tTraining Loss: 0.273469\n",
            "Epoch: 3925 \tTraining Loss: 0.273447\n",
            "Epoch: 3926 \tTraining Loss: 0.273426\n",
            "Epoch: 3927 \tTraining Loss: 0.273404\n",
            "Epoch: 3928 \tTraining Loss: 0.273383\n",
            "Epoch: 3929 \tTraining Loss: 0.273361\n",
            "Epoch: 3930 \tTraining Loss: 0.273339\n",
            "Epoch: 3931 \tTraining Loss: 0.273318\n",
            "Epoch: 3932 \tTraining Loss: 0.273296\n",
            "Epoch: 3933 \tTraining Loss: 0.273275\n",
            "Epoch: 3934 \tTraining Loss: 0.273253\n",
            "Epoch: 3935 \tTraining Loss: 0.273232\n",
            "Epoch: 3936 \tTraining Loss: 0.273210\n",
            "Epoch: 3937 \tTraining Loss: 0.273189\n",
            "Epoch: 3938 \tTraining Loss: 0.273168\n",
            "Epoch: 3939 \tTraining Loss: 0.273146\n",
            "Epoch: 3940 \tTraining Loss: 0.273125\n",
            "Epoch: 3941 \tTraining Loss: 0.273103\n",
            "Epoch: 3942 \tTraining Loss: 0.273082\n",
            "Epoch: 3943 \tTraining Loss: 0.273061\n",
            "Epoch: 3944 \tTraining Loss: 0.273039\n",
            "Epoch: 3945 \tTraining Loss: 0.273018\n",
            "Epoch: 3946 \tTraining Loss: 0.272997\n",
            "Epoch: 3947 \tTraining Loss: 0.272975\n",
            "Epoch: 3948 \tTraining Loss: 0.272954\n",
            "Epoch: 3949 \tTraining Loss: 0.272933\n",
            "Epoch: 3950 \tTraining Loss: 0.272912\n",
            "Epoch: 3951 \tTraining Loss: 0.272890\n",
            "Epoch: 3952 \tTraining Loss: 0.272869\n",
            "Epoch: 3953 \tTraining Loss: 0.272848\n",
            "Epoch: 3954 \tTraining Loss: 0.272827\n",
            "Epoch: 3955 \tTraining Loss: 0.272805\n",
            "Epoch: 3956 \tTraining Loss: 0.272784\n",
            "Epoch: 3957 \tTraining Loss: 0.272763\n",
            "Epoch: 3958 \tTraining Loss: 0.272742\n",
            "Epoch: 3959 \tTraining Loss: 0.272721\n",
            "Epoch: 3960 \tTraining Loss: 0.272699\n",
            "Epoch: 3961 \tTraining Loss: 0.272678\n",
            "Epoch: 3962 \tTraining Loss: 0.272657\n",
            "Epoch: 3963 \tTraining Loss: 0.272636\n",
            "Epoch: 3964 \tTraining Loss: 0.272615\n",
            "Epoch: 3965 \tTraining Loss: 0.272594\n",
            "Epoch: 3966 \tTraining Loss: 0.272573\n",
            "Epoch: 3967 \tTraining Loss: 0.272552\n",
            "Epoch: 3968 \tTraining Loss: 0.272531\n",
            "Epoch: 3969 \tTraining Loss: 0.272510\n",
            "Epoch: 3970 \tTraining Loss: 0.272489\n",
            "Epoch: 3971 \tTraining Loss: 0.272467\n",
            "Epoch: 3972 \tTraining Loss: 0.272446\n",
            "Epoch: 3973 \tTraining Loss: 0.272425\n",
            "Epoch: 3974 \tTraining Loss: 0.272404\n",
            "Epoch: 3975 \tTraining Loss: 0.272383\n",
            "Epoch: 3976 \tTraining Loss: 0.272362\n",
            "Epoch: 3977 \tTraining Loss: 0.272341\n",
            "Epoch: 3978 \tTraining Loss: 0.272320\n",
            "Epoch: 3979 \tTraining Loss: 0.272299\n",
            "Epoch: 3980 \tTraining Loss: 0.272278\n",
            "Epoch: 3981 \tTraining Loss: 0.272258\n",
            "Epoch: 3982 \tTraining Loss: 0.272237\n",
            "Epoch: 3983 \tTraining Loss: 0.272216\n",
            "Epoch: 3984 \tTraining Loss: 0.272195\n",
            "Epoch: 3985 \tTraining Loss: 0.272174\n",
            "Epoch: 3986 \tTraining Loss: 0.272153\n",
            "Epoch: 3987 \tTraining Loss: 0.272132\n",
            "Epoch: 3988 \tTraining Loss: 0.272111\n",
            "Epoch: 3989 \tTraining Loss: 0.272090\n",
            "Epoch: 3990 \tTraining Loss: 0.272069\n",
            "Epoch: 3991 \tTraining Loss: 0.272048\n",
            "Epoch: 3992 \tTraining Loss: 0.272027\n",
            "Epoch: 3993 \tTraining Loss: 0.272007\n",
            "Epoch: 3994 \tTraining Loss: 0.271986\n",
            "Epoch: 3995 \tTraining Loss: 0.271965\n",
            "Epoch: 3996 \tTraining Loss: 0.271944\n",
            "Epoch: 3997 \tTraining Loss: 0.271923\n",
            "Epoch: 3998 \tTraining Loss: 0.271902\n",
            "Epoch: 3999 \tTraining Loss: 0.271882\n",
            "Epoch: 4000 \tTraining Loss: 0.271861\n",
            "Epoch: 4001 \tTraining Loss: 0.271840\n",
            "Epoch: 4002 \tTraining Loss: 0.271819\n",
            "Epoch: 4003 \tTraining Loss: 0.271798\n",
            "Epoch: 4004 \tTraining Loss: 0.271778\n",
            "Epoch: 4005 \tTraining Loss: 0.271757\n",
            "Epoch: 4006 \tTraining Loss: 0.271736\n",
            "Epoch: 4007 \tTraining Loss: 0.271715\n",
            "Epoch: 4008 \tTraining Loss: 0.271694\n",
            "Epoch: 4009 \tTraining Loss: 0.271674\n",
            "Epoch: 4010 \tTraining Loss: 0.271653\n",
            "Epoch: 4011 \tTraining Loss: 0.271632\n",
            "Epoch: 4012 \tTraining Loss: 0.271611\n",
            "Epoch: 4013 \tTraining Loss: 0.271591\n",
            "Epoch: 4014 \tTraining Loss: 0.271570\n",
            "Epoch: 4015 \tTraining Loss: 0.271549\n",
            "Epoch: 4016 \tTraining Loss: 0.271528\n",
            "Epoch: 4017 \tTraining Loss: 0.271508\n",
            "Epoch: 4018 \tTraining Loss: 0.271487\n",
            "Epoch: 4019 \tTraining Loss: 0.271466\n",
            "Epoch: 4020 \tTraining Loss: 0.271446\n",
            "Epoch: 4021 \tTraining Loss: 0.271425\n",
            "Epoch: 4022 \tTraining Loss: 0.271404\n",
            "Epoch: 4023 \tTraining Loss: 0.271384\n",
            "Epoch: 4024 \tTraining Loss: 0.271363\n",
            "Epoch: 4025 \tTraining Loss: 0.271342\n",
            "Epoch: 4026 \tTraining Loss: 0.271322\n",
            "Epoch: 4027 \tTraining Loss: 0.271301\n",
            "Epoch: 4028 \tTraining Loss: 0.271280\n",
            "Epoch: 4029 \tTraining Loss: 0.271260\n",
            "Epoch: 4030 \tTraining Loss: 0.271239\n",
            "Epoch: 4031 \tTraining Loss: 0.271218\n",
            "Epoch: 4032 \tTraining Loss: 0.271198\n",
            "Epoch: 4033 \tTraining Loss: 0.271177\n",
            "Epoch: 4034 \tTraining Loss: 0.271156\n",
            "Epoch: 4035 \tTraining Loss: 0.271136\n",
            "Epoch: 4036 \tTraining Loss: 0.271115\n",
            "Epoch: 4037 \tTraining Loss: 0.271094\n",
            "Epoch: 4038 \tTraining Loss: 0.271074\n",
            "Epoch: 4039 \tTraining Loss: 0.271053\n",
            "Epoch: 4040 \tTraining Loss: 0.271033\n",
            "Epoch: 4041 \tTraining Loss: 0.271012\n",
            "Epoch: 4042 \tTraining Loss: 0.270991\n",
            "Epoch: 4043 \tTraining Loss: 0.270971\n",
            "Epoch: 4044 \tTraining Loss: 0.270950\n",
            "Epoch: 4045 \tTraining Loss: 0.270930\n",
            "Epoch: 4046 \tTraining Loss: 0.270909\n",
            "Epoch: 4047 \tTraining Loss: 0.270888\n",
            "Epoch: 4048 \tTraining Loss: 0.270868\n",
            "Epoch: 4049 \tTraining Loss: 0.270847\n",
            "Epoch: 4050 \tTraining Loss: 0.270827\n",
            "Epoch: 4051 \tTraining Loss: 0.270806\n",
            "Epoch: 4052 \tTraining Loss: 0.270785\n",
            "Epoch: 4053 \tTraining Loss: 0.270765\n",
            "Epoch: 4054 \tTraining Loss: 0.270744\n",
            "Epoch: 4055 \tTraining Loss: 0.270724\n",
            "Epoch: 4056 \tTraining Loss: 0.270703\n",
            "Epoch: 4057 \tTraining Loss: 0.270683\n",
            "Epoch: 4058 \tTraining Loss: 0.270662\n",
            "Epoch: 4059 \tTraining Loss: 0.270642\n",
            "Epoch: 4060 \tTraining Loss: 0.270621\n",
            "Epoch: 4061 \tTraining Loss: 0.270600\n",
            "Epoch: 4062 \tTraining Loss: 0.270580\n",
            "Epoch: 4063 \tTraining Loss: 0.270559\n",
            "Epoch: 4064 \tTraining Loss: 0.270539\n",
            "Epoch: 4065 \tTraining Loss: 0.270518\n",
            "Epoch: 4066 \tTraining Loss: 0.270498\n",
            "Epoch: 4067 \tTraining Loss: 0.270477\n",
            "Epoch: 4068 \tTraining Loss: 0.270457\n",
            "Epoch: 4069 \tTraining Loss: 0.270436\n",
            "Epoch: 4070 \tTraining Loss: 0.270416\n",
            "Epoch: 4071 \tTraining Loss: 0.270395\n",
            "Epoch: 4072 \tTraining Loss: 0.270375\n",
            "Epoch: 4073 \tTraining Loss: 0.270354\n",
            "Epoch: 4074 \tTraining Loss: 0.270334\n",
            "Epoch: 4075 \tTraining Loss: 0.270313\n",
            "Epoch: 4076 \tTraining Loss: 0.270293\n",
            "Epoch: 4077 \tTraining Loss: 0.270272\n",
            "Epoch: 4078 \tTraining Loss: 0.270252\n",
            "Epoch: 4079 \tTraining Loss: 0.270231\n",
            "Epoch: 4080 \tTraining Loss: 0.270211\n",
            "Epoch: 4081 \tTraining Loss: 0.270190\n",
            "Epoch: 4082 \tTraining Loss: 0.270170\n",
            "Epoch: 4083 \tTraining Loss: 0.270149\n",
            "Epoch: 4084 \tTraining Loss: 0.270129\n",
            "Epoch: 4085 \tTraining Loss: 0.270108\n",
            "Epoch: 4086 \tTraining Loss: 0.270088\n",
            "Epoch: 4087 \tTraining Loss: 0.270067\n",
            "Epoch: 4088 \tTraining Loss: 0.270047\n",
            "Epoch: 4089 \tTraining Loss: 0.270026\n",
            "Epoch: 4090 \tTraining Loss: 0.270006\n",
            "Epoch: 4091 \tTraining Loss: 0.269986\n",
            "Epoch: 4092 \tTraining Loss: 0.269965\n",
            "Epoch: 4093 \tTraining Loss: 0.269945\n",
            "Epoch: 4094 \tTraining Loss: 0.269924\n",
            "Epoch: 4095 \tTraining Loss: 0.269904\n",
            "Epoch: 4096 \tTraining Loss: 0.269883\n",
            "Epoch: 4097 \tTraining Loss: 0.269863\n",
            "Epoch: 4098 \tTraining Loss: 0.269842\n",
            "Epoch: 4099 \tTraining Loss: 0.269822\n",
            "Epoch: 4100 \tTraining Loss: 0.269801\n",
            "Epoch: 4101 \tTraining Loss: 0.269781\n",
            "Epoch: 4102 \tTraining Loss: 0.269761\n",
            "Epoch: 4103 \tTraining Loss: 0.269740\n",
            "Epoch: 4104 \tTraining Loss: 0.269720\n",
            "Epoch: 4105 \tTraining Loss: 0.269699\n",
            "Epoch: 4106 \tTraining Loss: 0.269679\n",
            "Epoch: 4107 \tTraining Loss: 0.269658\n",
            "Epoch: 4108 \tTraining Loss: 0.269638\n",
            "Epoch: 4109 \tTraining Loss: 0.269618\n",
            "Epoch: 4110 \tTraining Loss: 0.269597\n",
            "Epoch: 4111 \tTraining Loss: 0.269577\n",
            "Epoch: 4112 \tTraining Loss: 0.269556\n",
            "Epoch: 4113 \tTraining Loss: 0.269536\n",
            "Epoch: 4114 \tTraining Loss: 0.269515\n",
            "Epoch: 4115 \tTraining Loss: 0.269495\n",
            "Epoch: 4116 \tTraining Loss: 0.269475\n",
            "Epoch: 4117 \tTraining Loss: 0.269454\n",
            "Epoch: 4118 \tTraining Loss: 0.269434\n",
            "Epoch: 4119 \tTraining Loss: 0.269413\n",
            "Epoch: 4120 \tTraining Loss: 0.269393\n",
            "Epoch: 4121 \tTraining Loss: 0.269373\n",
            "Epoch: 4122 \tTraining Loss: 0.269352\n",
            "Epoch: 4123 \tTraining Loss: 0.269332\n",
            "Epoch: 4124 \tTraining Loss: 0.269311\n",
            "Epoch: 4125 \tTraining Loss: 0.269291\n",
            "Epoch: 4126 \tTraining Loss: 0.269271\n",
            "Epoch: 4127 \tTraining Loss: 0.269250\n",
            "Epoch: 4128 \tTraining Loss: 0.269230\n",
            "Epoch: 4129 \tTraining Loss: 0.269209\n",
            "Epoch: 4130 \tTraining Loss: 0.269189\n",
            "Epoch: 4131 \tTraining Loss: 0.269169\n",
            "Epoch: 4132 \tTraining Loss: 0.269148\n",
            "Epoch: 4133 \tTraining Loss: 0.269128\n",
            "Epoch: 4134 \tTraining Loss: 0.269107\n",
            "Epoch: 4135 \tTraining Loss: 0.269087\n",
            "Epoch: 4136 \tTraining Loss: 0.269067\n",
            "Epoch: 4137 \tTraining Loss: 0.269047\n",
            "Epoch: 4138 \tTraining Loss: 0.269028\n",
            "Epoch: 4139 \tTraining Loss: 0.269011\n",
            "Epoch: 4140 \tTraining Loss: 0.268997\n",
            "Epoch: 4141 \tTraining Loss: 0.268989\n",
            "Epoch: 4142 \tTraining Loss: 0.268997\n",
            "Epoch: 4143 \tTraining Loss: 0.269002\n",
            "Epoch: 4144 \tTraining Loss: 0.269014\n",
            "Epoch: 4145 \tTraining Loss: 0.268960\n",
            "Epoch: 4146 \tTraining Loss: 0.268911\n",
            "Epoch: 4147 \tTraining Loss: 0.268855\n",
            "Epoch: 4148 \tTraining Loss: 0.268830\n",
            "Epoch: 4149 \tTraining Loss: 0.268826\n",
            "Epoch: 4150 \tTraining Loss: 0.268838\n",
            "Epoch: 4151 \tTraining Loss: 0.268864\n",
            "Epoch: 4152 \tTraining Loss: 0.268822\n",
            "Epoch: 4153 \tTraining Loss: 0.268798\n",
            "Epoch: 4154 \tTraining Loss: 0.268735\n",
            "Epoch: 4155 \tTraining Loss: 0.268699\n",
            "Epoch: 4156 \tTraining Loss: 0.268679\n",
            "Epoch: 4157 \tTraining Loss: 0.268656\n",
            "Epoch: 4158 \tTraining Loss: 0.268642\n",
            "Epoch: 4159 \tTraining Loss: 0.268616\n",
            "Epoch: 4160 \tTraining Loss: 0.268616\n",
            "Epoch: 4161 \tTraining Loss: 0.268642\n",
            "Epoch: 4162 \tTraining Loss: 0.268656\n",
            "Epoch: 4163 \tTraining Loss: 0.268604\n",
            "Epoch: 4164 \tTraining Loss: 0.268520\n",
            "Epoch: 4165 \tTraining Loss: 0.268484\n",
            "Epoch: 4166 \tTraining Loss: 0.268496\n",
            "Epoch: 4167 \tTraining Loss: 0.268523\n",
            "Epoch: 4168 \tTraining Loss: 0.268534\n",
            "Epoch: 4169 \tTraining Loss: 0.268456\n",
            "Epoch: 4170 \tTraining Loss: 0.268395\n",
            "Epoch: 4171 \tTraining Loss: 0.268450\n",
            "Epoch: 4172 \tTraining Loss: 0.268514\n",
            "Epoch: 4173 \tTraining Loss: 0.268531\n",
            "Epoch: 4174 \tTraining Loss: 0.268445\n",
            "Epoch: 4175 \tTraining Loss: 0.268465\n",
            "Epoch: 4176 \tTraining Loss: 0.268682\n",
            "Epoch: 4177 \tTraining Loss: 0.268661\n",
            "Epoch: 4178 \tTraining Loss: 0.268660\n",
            "Epoch: 4179 \tTraining Loss: 0.268469\n",
            "Epoch: 4180 \tTraining Loss: 0.268408\n",
            "Epoch: 4181 \tTraining Loss: 0.268436\n",
            "Epoch: 4182 \tTraining Loss: 0.268779\n",
            "Epoch: 4183 \tTraining Loss: 0.268623\n",
            "Epoch: 4184 \tTraining Loss: 0.268759\n",
            "Epoch: 4185 \tTraining Loss: 0.268594\n",
            "Epoch: 4186 \tTraining Loss: 0.268463\n",
            "Epoch: 4187 \tTraining Loss: 0.268406\n",
            "Epoch: 4188 \tTraining Loss: 0.268228\n",
            "Epoch: 4189 \tTraining Loss: 0.268171\n",
            "Epoch: 4190 \tTraining Loss: 0.268262\n",
            "Epoch: 4191 \tTraining Loss: 0.268104\n",
            "Epoch: 4192 \tTraining Loss: 0.268183\n",
            "Epoch: 4193 \tTraining Loss: 0.268114\n",
            "Epoch: 4194 \tTraining Loss: 0.268021\n",
            "Epoch: 4195 \tTraining Loss: 0.268026\n",
            "Epoch: 4196 \tTraining Loss: 0.268125\n",
            "Epoch: 4197 \tTraining Loss: 0.268359\n",
            "Epoch: 4198 \tTraining Loss: 0.268109\n",
            "Epoch: 4199 \tTraining Loss: 0.268215\n",
            "Epoch: 4200 \tTraining Loss: 0.268100\n",
            "Epoch: 4201 \tTraining Loss: 0.267986\n",
            "Epoch: 4202 \tTraining Loss: 0.267958\n",
            "Epoch: 4203 \tTraining Loss: 0.268226\n",
            "Epoch: 4204 \tTraining Loss: 0.268427\n",
            "Epoch: 4205 \tTraining Loss: 0.268407\n",
            "Epoch: 4206 \tTraining Loss: 0.268171\n",
            "Epoch: 4207 \tTraining Loss: 0.268431\n",
            "Epoch: 4208 \tTraining Loss: 0.268252\n",
            "Epoch: 4209 \tTraining Loss: 0.268181\n",
            "Epoch: 4210 \tTraining Loss: 0.269030\n",
            "Epoch: 4211 \tTraining Loss: 0.268120\n",
            "Epoch: 4212 \tTraining Loss: 0.268405\n",
            "Epoch: 4213 \tTraining Loss: 0.269246\n",
            "Epoch: 4214 \tTraining Loss: 0.267978\n",
            "Epoch: 4215 \tTraining Loss: 0.268515\n",
            "Epoch: 4216 \tTraining Loss: 0.268654\n",
            "Epoch: 4217 \tTraining Loss: 0.268023\n",
            "Epoch: 4218 \tTraining Loss: 0.269234\n",
            "Epoch: 4219 \tTraining Loss: 0.269549\n",
            "Epoch: 4220 \tTraining Loss: 0.269295\n",
            "Epoch: 4221 \tTraining Loss: 0.268677\n",
            "Epoch: 4222 \tTraining Loss: 0.269483\n",
            "Epoch: 4223 \tTraining Loss: 0.269045\n",
            "Epoch: 4224 \tTraining Loss: 0.268544\n",
            "Epoch: 4225 \tTraining Loss: 0.268171\n",
            "Epoch: 4226 \tTraining Loss: 0.268434\n",
            "Epoch: 4227 \tTraining Loss: 0.268877\n",
            "Epoch: 4228 \tTraining Loss: 0.267973\n",
            "Epoch: 4229 \tTraining Loss: 0.268032\n",
            "Epoch: 4230 \tTraining Loss: 0.268053\n",
            "Epoch: 4231 \tTraining Loss: 0.268194\n",
            "Epoch: 4232 \tTraining Loss: 0.267945\n",
            "Epoch: 4233 \tTraining Loss: 0.267632\n",
            "Epoch: 4234 \tTraining Loss: 0.267658\n",
            "Epoch: 4235 \tTraining Loss: 0.267620\n",
            "Epoch: 4236 \tTraining Loss: 0.267541\n",
            "Epoch: 4237 \tTraining Loss: 0.267359\n",
            "Epoch: 4238 \tTraining Loss: 0.267393\n",
            "Epoch: 4239 \tTraining Loss: 0.267326\n",
            "Epoch: 4240 \tTraining Loss: 0.267298\n",
            "Epoch: 4241 \tTraining Loss: 0.267276\n",
            "Epoch: 4242 \tTraining Loss: 0.267195\n",
            "Epoch: 4243 \tTraining Loss: 0.267083\n",
            "Epoch: 4244 \tTraining Loss: 0.267062\n",
            "Epoch: 4245 \tTraining Loss: 0.267005\n",
            "Epoch: 4246 \tTraining Loss: 0.267024\n",
            "Epoch: 4247 \tTraining Loss: 0.266950\n",
            "Epoch: 4248 \tTraining Loss: 0.266845\n",
            "Epoch: 4249 \tTraining Loss: 0.266822\n",
            "Epoch: 4250 \tTraining Loss: 0.266860\n",
            "Epoch: 4251 \tTraining Loss: 0.266776\n",
            "Epoch: 4252 \tTraining Loss: 0.266752\n",
            "Epoch: 4253 \tTraining Loss: 0.266704\n",
            "Epoch: 4254 \tTraining Loss: 0.266698\n",
            "Epoch: 4255 \tTraining Loss: 0.266635\n",
            "Epoch: 4256 \tTraining Loss: 0.266647\n",
            "Epoch: 4257 \tTraining Loss: 0.266589\n",
            "Epoch: 4258 \tTraining Loss: 0.266560\n",
            "Epoch: 4259 \tTraining Loss: 0.266529\n",
            "Epoch: 4260 \tTraining Loss: 0.266523\n",
            "Epoch: 4261 \tTraining Loss: 0.266472\n",
            "Epoch: 4262 \tTraining Loss: 0.266443\n",
            "Epoch: 4263 \tTraining Loss: 0.266424\n",
            "Epoch: 4264 \tTraining Loss: 0.266409\n",
            "Epoch: 4265 \tTraining Loss: 0.266369\n",
            "Epoch: 4266 \tTraining Loss: 0.266338\n",
            "Epoch: 4267 \tTraining Loss: 0.266327\n",
            "Epoch: 4268 \tTraining Loss: 0.266304\n",
            "Epoch: 4269 \tTraining Loss: 0.266277\n",
            "Epoch: 4270 \tTraining Loss: 0.266244\n",
            "Epoch: 4271 \tTraining Loss: 0.266231\n",
            "Epoch: 4272 \tTraining Loss: 0.266212\n",
            "Epoch: 4273 \tTraining Loss: 0.266188\n",
            "Epoch: 4274 \tTraining Loss: 0.266157\n",
            "Epoch: 4275 \tTraining Loss: 0.266140\n",
            "Epoch: 4276 \tTraining Loss: 0.266123\n",
            "Epoch: 4277 \tTraining Loss: 0.266101\n",
            "Epoch: 4278 \tTraining Loss: 0.266074\n",
            "Epoch: 4279 \tTraining Loss: 0.266052\n",
            "Epoch: 4280 \tTraining Loss: 0.266033\n",
            "Epoch: 4281 \tTraining Loss: 0.266012\n",
            "Epoch: 4282 \tTraining Loss: 0.265990\n",
            "Epoch: 4283 \tTraining Loss: 0.265966\n",
            "Epoch: 4284 \tTraining Loss: 0.265945\n",
            "Epoch: 4285 \tTraining Loss: 0.265925\n",
            "Epoch: 4286 \tTraining Loss: 0.265905\n",
            "Epoch: 4287 \tTraining Loss: 0.265882\n",
            "Epoch: 4288 \tTraining Loss: 0.265860\n",
            "Epoch: 4289 \tTraining Loss: 0.265839\n",
            "Epoch: 4290 \tTraining Loss: 0.265820\n",
            "Epoch: 4291 \tTraining Loss: 0.265799\n",
            "Epoch: 4292 \tTraining Loss: 0.265777\n",
            "Epoch: 4293 \tTraining Loss: 0.265756\n",
            "Epoch: 4294 \tTraining Loss: 0.265736\n",
            "Epoch: 4295 \tTraining Loss: 0.265715\n",
            "Epoch: 4296 \tTraining Loss: 0.265694\n",
            "Epoch: 4297 \tTraining Loss: 0.265673\n",
            "Epoch: 4298 \tTraining Loss: 0.265652\n",
            "Epoch: 4299 \tTraining Loss: 0.265632\n",
            "Epoch: 4300 \tTraining Loss: 0.265611\n",
            "Epoch: 4301 \tTraining Loss: 0.265591\n",
            "Epoch: 4302 \tTraining Loss: 0.265570\n",
            "Epoch: 4303 \tTraining Loss: 0.265549\n",
            "Epoch: 4304 \tTraining Loss: 0.265529\n",
            "Epoch: 4305 \tTraining Loss: 0.265508\n",
            "Epoch: 4306 \tTraining Loss: 0.265487\n",
            "Epoch: 4307 \tTraining Loss: 0.265467\n",
            "Epoch: 4308 \tTraining Loss: 0.265446\n",
            "Epoch: 4309 \tTraining Loss: 0.265426\n",
            "Epoch: 4310 \tTraining Loss: 0.265405\n",
            "Epoch: 4311 \tTraining Loss: 0.265385\n",
            "Epoch: 4312 \tTraining Loss: 0.265364\n",
            "Epoch: 4313 \tTraining Loss: 0.265344\n",
            "Epoch: 4314 \tTraining Loss: 0.265323\n",
            "Epoch: 4315 \tTraining Loss: 0.265303\n",
            "Epoch: 4316 \tTraining Loss: 0.265282\n",
            "Epoch: 4317 \tTraining Loss: 0.265262\n",
            "Epoch: 4318 \tTraining Loss: 0.265241\n",
            "Epoch: 4319 \tTraining Loss: 0.265221\n",
            "Epoch: 4320 \tTraining Loss: 0.265200\n",
            "Epoch: 4321 \tTraining Loss: 0.265180\n",
            "Epoch: 4322 \tTraining Loss: 0.265159\n",
            "Epoch: 4323 \tTraining Loss: 0.265139\n",
            "Epoch: 4324 \tTraining Loss: 0.265119\n",
            "Epoch: 4325 \tTraining Loss: 0.265098\n",
            "Epoch: 4326 \tTraining Loss: 0.265078\n",
            "Epoch: 4327 \tTraining Loss: 0.265057\n",
            "Epoch: 4328 \tTraining Loss: 0.265037\n",
            "Epoch: 4329 \tTraining Loss: 0.265016\n",
            "Epoch: 4330 \tTraining Loss: 0.264996\n",
            "Epoch: 4331 \tTraining Loss: 0.264976\n",
            "Epoch: 4332 \tTraining Loss: 0.264955\n",
            "Epoch: 4333 \tTraining Loss: 0.264935\n",
            "Epoch: 4334 \tTraining Loss: 0.264915\n",
            "Epoch: 4335 \tTraining Loss: 0.264894\n",
            "Epoch: 4336 \tTraining Loss: 0.264874\n",
            "Epoch: 4337 \tTraining Loss: 0.264853\n",
            "Epoch: 4338 \tTraining Loss: 0.264833\n",
            "Epoch: 4339 \tTraining Loss: 0.264813\n",
            "Epoch: 4340 \tTraining Loss: 0.264792\n",
            "Epoch: 4341 \tTraining Loss: 0.264772\n",
            "Epoch: 4342 \tTraining Loss: 0.264752\n",
            "Epoch: 4343 \tTraining Loss: 0.264731\n",
            "Epoch: 4344 \tTraining Loss: 0.264711\n",
            "Epoch: 4345 \tTraining Loss: 0.264690\n",
            "Epoch: 4346 \tTraining Loss: 0.264670\n",
            "Epoch: 4347 \tTraining Loss: 0.264650\n",
            "Epoch: 4348 \tTraining Loss: 0.264629\n",
            "Epoch: 4349 \tTraining Loss: 0.264609\n",
            "Epoch: 4350 \tTraining Loss: 0.264589\n",
            "Epoch: 4351 \tTraining Loss: 0.264568\n",
            "Epoch: 4352 \tTraining Loss: 0.264548\n",
            "Epoch: 4353 \tTraining Loss: 0.264528\n",
            "Epoch: 4354 \tTraining Loss: 0.264507\n",
            "Epoch: 4355 \tTraining Loss: 0.264487\n",
            "Epoch: 4356 \tTraining Loss: 0.264467\n",
            "Epoch: 4357 \tTraining Loss: 0.264446\n",
            "Epoch: 4358 \tTraining Loss: 0.264426\n",
            "Epoch: 4359 \tTraining Loss: 0.264406\n",
            "Epoch: 4360 \tTraining Loss: 0.264385\n",
            "Epoch: 4361 \tTraining Loss: 0.264365\n",
            "Epoch: 4362 \tTraining Loss: 0.264345\n",
            "Epoch: 4363 \tTraining Loss: 0.264324\n",
            "Epoch: 4364 \tTraining Loss: 0.264304\n",
            "Epoch: 4365 \tTraining Loss: 0.264284\n",
            "Epoch: 4366 \tTraining Loss: 0.264263\n",
            "Epoch: 4367 \tTraining Loss: 0.264243\n",
            "Epoch: 4368 \tTraining Loss: 0.264223\n",
            "Epoch: 4369 \tTraining Loss: 0.264202\n",
            "Epoch: 4370 \tTraining Loss: 0.264182\n",
            "Epoch: 4371 \tTraining Loss: 0.264162\n",
            "Epoch: 4372 \tTraining Loss: 0.264141\n",
            "Epoch: 4373 \tTraining Loss: 0.264121\n",
            "Epoch: 4374 \tTraining Loss: 0.264101\n",
            "Epoch: 4375 \tTraining Loss: 0.264080\n",
            "Epoch: 4376 \tTraining Loss: 0.264060\n",
            "Epoch: 4377 \tTraining Loss: 0.264040\n",
            "Epoch: 4378 \tTraining Loss: 0.264019\n",
            "Epoch: 4379 \tTraining Loss: 0.263999\n",
            "Epoch: 4380 \tTraining Loss: 0.263979\n",
            "Epoch: 4381 \tTraining Loss: 0.263959\n",
            "Epoch: 4382 \tTraining Loss: 0.263938\n",
            "Epoch: 4383 \tTraining Loss: 0.263918\n",
            "Epoch: 4384 \tTraining Loss: 0.263898\n",
            "Epoch: 4385 \tTraining Loss: 0.263877\n",
            "Epoch: 4386 \tTraining Loss: 0.263857\n",
            "Epoch: 4387 \tTraining Loss: 0.263837\n",
            "Epoch: 4388 \tTraining Loss: 0.263816\n",
            "Epoch: 4389 \tTraining Loss: 0.263796\n",
            "Epoch: 4390 \tTraining Loss: 0.263776\n",
            "Epoch: 4391 \tTraining Loss: 0.263755\n",
            "Epoch: 4392 \tTraining Loss: 0.263735\n",
            "Epoch: 4393 \tTraining Loss: 0.263715\n",
            "Epoch: 4394 \tTraining Loss: 0.263694\n",
            "Epoch: 4395 \tTraining Loss: 0.263674\n",
            "Epoch: 4396 \tTraining Loss: 0.263654\n",
            "Epoch: 4397 \tTraining Loss: 0.263633\n",
            "Epoch: 4398 \tTraining Loss: 0.263613\n",
            "Epoch: 4399 \tTraining Loss: 0.263593\n",
            "Epoch: 4400 \tTraining Loss: 0.263572\n",
            "Epoch: 4401 \tTraining Loss: 0.263552\n",
            "Epoch: 4402 \tTraining Loss: 0.263532\n",
            "Epoch: 4403 \tTraining Loss: 0.263511\n",
            "Epoch: 4404 \tTraining Loss: 0.263491\n",
            "Epoch: 4405 \tTraining Loss: 0.263471\n",
            "Epoch: 4406 \tTraining Loss: 0.263450\n",
            "Epoch: 4407 \tTraining Loss: 0.263430\n",
            "Epoch: 4408 \tTraining Loss: 0.263410\n",
            "Epoch: 4409 \tTraining Loss: 0.263389\n",
            "Epoch: 4410 \tTraining Loss: 0.263369\n",
            "Epoch: 4411 \tTraining Loss: 0.263349\n",
            "Epoch: 4412 \tTraining Loss: 0.263329\n",
            "Epoch: 4413 \tTraining Loss: 0.263308\n",
            "Epoch: 4414 \tTraining Loss: 0.263288\n",
            "Epoch: 4415 \tTraining Loss: 0.263269\n",
            "Epoch: 4416 \tTraining Loss: 0.263249\n",
            "Epoch: 4417 \tTraining Loss: 0.263230\n",
            "Epoch: 4418 \tTraining Loss: 0.263212\n",
            "Epoch: 4419 \tTraining Loss: 0.263196\n",
            "Epoch: 4420 \tTraining Loss: 0.263182\n",
            "Epoch: 4421 \tTraining Loss: 0.263173\n",
            "Epoch: 4422 \tTraining Loss: 0.263174\n",
            "Epoch: 4423 \tTraining Loss: 0.263187\n",
            "Epoch: 4424 \tTraining Loss: 0.263236\n",
            "Epoch: 4425 \tTraining Loss: 0.263315\n",
            "Epoch: 4426 \tTraining Loss: 0.263543\n",
            "Epoch: 4427 \tTraining Loss: 0.263872\n",
            "Epoch: 4428 \tTraining Loss: 0.265584\n",
            "Epoch: 4429 \tTraining Loss: 0.277105\n",
            "Epoch: 4430 \tTraining Loss: 0.437657\n",
            "Epoch: 4431 \tTraining Loss: 0.367947\n",
            "Epoch: 4432 \tTraining Loss: 0.370689\n",
            "Epoch: 4433 \tTraining Loss: 0.639335\n",
            "Epoch: 4434 \tTraining Loss: 0.592517\n",
            "Epoch: 4435 \tTraining Loss: 0.581665\n",
            "Epoch: 4436 \tTraining Loss: 0.534302\n",
            "Epoch: 4437 \tTraining Loss: 0.553222\n",
            "Epoch: 4438 \tTraining Loss: 0.508274\n",
            "Epoch: 4439 \tTraining Loss: 0.510244\n",
            "Epoch: 4440 \tTraining Loss: 0.484371\n",
            "Epoch: 4441 \tTraining Loss: 0.460065\n",
            "Epoch: 4442 \tTraining Loss: 0.456202\n",
            "Epoch: 4443 \tTraining Loss: 0.453944\n",
            "Epoch: 4444 \tTraining Loss: 0.406330\n",
            "Epoch: 4445 \tTraining Loss: 0.383583\n",
            "Epoch: 4446 \tTraining Loss: 0.404775\n",
            "Epoch: 4447 \tTraining Loss: 0.383674\n",
            "Epoch: 4448 \tTraining Loss: 0.366015\n",
            "Epoch: 4449 \tTraining Loss: 0.357554\n",
            "Epoch: 4450 \tTraining Loss: 0.355787\n",
            "Epoch: 4451 \tTraining Loss: 0.353307\n",
            "Epoch: 4452 \tTraining Loss: 0.343483\n",
            "Epoch: 4453 \tTraining Loss: 0.329419\n",
            "Epoch: 4454 \tTraining Loss: 0.336136\n",
            "Epoch: 4455 \tTraining Loss: 0.323856\n",
            "Epoch: 4456 \tTraining Loss: 0.316053\n",
            "Epoch: 4457 \tTraining Loss: 0.318101\n",
            "Epoch: 4458 \tTraining Loss: 0.313179\n",
            "Epoch: 4459 \tTraining Loss: 0.301902\n",
            "Epoch: 4460 \tTraining Loss: 0.303071\n",
            "Epoch: 4461 \tTraining Loss: 0.301627\n",
            "Epoch: 4462 \tTraining Loss: 0.295425\n",
            "Epoch: 4463 \tTraining Loss: 0.294506\n",
            "Epoch: 4464 \tTraining Loss: 0.294436\n",
            "Epoch: 4465 \tTraining Loss: 0.297107\n",
            "Epoch: 4466 \tTraining Loss: 0.290470\n",
            "Epoch: 4467 \tTraining Loss: 0.288574\n",
            "Epoch: 4468 \tTraining Loss: 0.285998\n",
            "Epoch: 4469 \tTraining Loss: 0.284267\n",
            "Epoch: 4470 \tTraining Loss: 0.283546\n",
            "Epoch: 4471 \tTraining Loss: 0.284079\n",
            "Epoch: 4472 \tTraining Loss: 0.282682\n",
            "Epoch: 4473 \tTraining Loss: 0.280298\n",
            "Epoch: 4474 \tTraining Loss: 0.276523\n",
            "Epoch: 4475 \tTraining Loss: 0.279056\n",
            "Epoch: 4476 \tTraining Loss: 0.276528\n",
            "Epoch: 4477 \tTraining Loss: 0.277207\n",
            "Epoch: 4478 \tTraining Loss: 0.276976\n",
            "Epoch: 4479 \tTraining Loss: 0.276653\n",
            "Epoch: 4480 \tTraining Loss: 0.276053\n",
            "Epoch: 4481 \tTraining Loss: 0.272632\n",
            "Epoch: 4482 \tTraining Loss: 0.273100\n",
            "Epoch: 4483 \tTraining Loss: 0.273700\n",
            "Epoch: 4484 \tTraining Loss: 0.273783\n",
            "Epoch: 4485 \tTraining Loss: 0.274927\n",
            "Epoch: 4486 \tTraining Loss: 0.275070\n",
            "Epoch: 4487 \tTraining Loss: 0.274169\n",
            "Epoch: 4488 \tTraining Loss: 0.272840\n",
            "Epoch: 4489 \tTraining Loss: 0.271453\n",
            "Epoch: 4490 \tTraining Loss: 0.272880\n",
            "Epoch: 4491 \tTraining Loss: 0.272262\n",
            "Epoch: 4492 \tTraining Loss: 0.270364\n",
            "Epoch: 4493 \tTraining Loss: 0.270905\n",
            "Epoch: 4494 \tTraining Loss: 0.269926\n",
            "Epoch: 4495 \tTraining Loss: 0.271701\n",
            "Epoch: 4496 \tTraining Loss: 0.271178\n",
            "Epoch: 4497 \tTraining Loss: 0.269400\n",
            "Epoch: 4498 \tTraining Loss: 0.269820\n",
            "Epoch: 4499 \tTraining Loss: 0.269090\n",
            "Epoch: 4500 \tTraining Loss: 0.270751\n",
            "Epoch: 4501 \tTraining Loss: 0.270171\n",
            "Epoch: 4502 \tTraining Loss: 0.268737\n",
            "Epoch: 4503 \tTraining Loss: 0.270297\n",
            "Epoch: 4504 \tTraining Loss: 0.268606\n",
            "Epoch: 4505 \tTraining Loss: 0.268561\n",
            "Epoch: 4506 \tTraining Loss: 0.269820\n",
            "Epoch: 4507 \tTraining Loss: 0.269617\n",
            "Epoch: 4508 \tTraining Loss: 0.268281\n",
            "Epoch: 4509 \tTraining Loss: 0.268355\n",
            "Epoch: 4510 \tTraining Loss: 0.269445\n",
            "Epoch: 4511 \tTraining Loss: 0.269163\n",
            "Epoch: 4512 \tTraining Loss: 0.271327\n",
            "Epoch: 4513 \tTraining Loss: 0.270900\n",
            "Epoch: 4514 \tTraining Loss: 0.267728\n",
            "Epoch: 4515 \tTraining Loss: 0.270189\n",
            "Epoch: 4516 \tTraining Loss: 0.271341\n",
            "Epoch: 4517 \tTraining Loss: 0.270665\n",
            "Epoch: 4518 \tTraining Loss: 0.269533\n",
            "Epoch: 4519 \tTraining Loss: 0.267671\n",
            "Epoch: 4520 \tTraining Loss: 0.269168\n",
            "Epoch: 4521 \tTraining Loss: 0.269235\n",
            "Epoch: 4522 \tTraining Loss: 0.267761\n",
            "Epoch: 4523 \tTraining Loss: 0.269338\n",
            "Epoch: 4524 \tTraining Loss: 0.270954\n",
            "Epoch: 4525 \tTraining Loss: 0.271002\n",
            "Epoch: 4526 \tTraining Loss: 0.269462\n",
            "Epoch: 4527 \tTraining Loss: 0.267917\n",
            "Epoch: 4528 \tTraining Loss: 0.271664\n",
            "Epoch: 4529 \tTraining Loss: 0.273568\n",
            "Epoch: 4530 \tTraining Loss: 0.274471\n",
            "Epoch: 4531 \tTraining Loss: 0.274423\n",
            "Epoch: 4532 \tTraining Loss: 0.273815\n",
            "Epoch: 4533 \tTraining Loss: 0.273139\n",
            "Epoch: 4534 \tTraining Loss: 0.272414\n",
            "Epoch: 4535 \tTraining Loss: 0.271241\n",
            "Epoch: 4536 \tTraining Loss: 0.267736\n",
            "Epoch: 4537 \tTraining Loss: 0.269367\n",
            "Epoch: 4538 \tTraining Loss: 0.268997\n",
            "Epoch: 4539 \tTraining Loss: 0.267163\n",
            "Epoch: 4540 \tTraining Loss: 0.269730\n",
            "Epoch: 4541 \tTraining Loss: 0.267856\n",
            "Epoch: 4542 \tTraining Loss: 0.268607\n",
            "Epoch: 4543 \tTraining Loss: 0.269036\n",
            "Epoch: 4544 \tTraining Loss: 0.268607\n",
            "Epoch: 4545 \tTraining Loss: 0.266531\n",
            "Epoch: 4546 \tTraining Loss: 0.267314\n",
            "Epoch: 4547 \tTraining Loss: 0.266429\n",
            "Epoch: 4548 \tTraining Loss: 0.266468\n",
            "Epoch: 4549 \tTraining Loss: 0.266793\n",
            "Epoch: 4550 \tTraining Loss: 0.266213\n",
            "Epoch: 4551 \tTraining Loss: 0.267239\n",
            "Epoch: 4552 \tTraining Loss: 0.267112\n",
            "Epoch: 4553 \tTraining Loss: 0.266048\n",
            "Epoch: 4554 \tTraining Loss: 0.267644\n",
            "Epoch: 4555 \tTraining Loss: 0.266650\n",
            "Epoch: 4556 \tTraining Loss: 0.267626\n",
            "Epoch: 4557 \tTraining Loss: 0.270681\n",
            "Epoch: 4558 \tTraining Loss: 0.271199\n",
            "Epoch: 4559 \tTraining Loss: 0.270912\n",
            "Epoch: 4560 \tTraining Loss: 0.270125\n",
            "Epoch: 4561 \tTraining Loss: 0.267767\n",
            "Epoch: 4562 \tTraining Loss: 0.266478\n",
            "Epoch: 4563 \tTraining Loss: 0.268884\n",
            "Epoch: 4564 \tTraining Loss: 0.269654\n",
            "Epoch: 4565 \tTraining Loss: 0.268756\n",
            "Epoch: 4566 \tTraining Loss: 0.267837\n",
            "Epoch: 4567 \tTraining Loss: 0.265926\n",
            "Epoch: 4568 \tTraining Loss: 0.266786\n",
            "Epoch: 4569 \tTraining Loss: 0.266047\n",
            "Epoch: 4570 \tTraining Loss: 0.267343\n",
            "Epoch: 4571 \tTraining Loss: 0.265793\n",
            "Epoch: 4572 \tTraining Loss: 0.265570\n",
            "Epoch: 4573 \tTraining Loss: 0.265950\n",
            "Epoch: 4574 \tTraining Loss: 0.265583\n",
            "Epoch: 4575 \tTraining Loss: 0.265853\n",
            "Epoch: 4576 \tTraining Loss: 0.265612\n",
            "Epoch: 4577 \tTraining Loss: 0.265461\n",
            "Epoch: 4578 \tTraining Loss: 0.265413\n",
            "Epoch: 4579 \tTraining Loss: 0.265440\n",
            "Epoch: 4580 \tTraining Loss: 0.265422\n",
            "Epoch: 4581 \tTraining Loss: 0.265295\n",
            "Epoch: 4582 \tTraining Loss: 0.265193\n",
            "Epoch: 4583 \tTraining Loss: 0.265174\n",
            "Epoch: 4584 \tTraining Loss: 0.265301\n",
            "Epoch: 4585 \tTraining Loss: 0.265093\n",
            "Epoch: 4586 \tTraining Loss: 0.265091\n",
            "Epoch: 4587 \tTraining Loss: 0.265105\n",
            "Epoch: 4588 \tTraining Loss: 0.265091\n",
            "Epoch: 4589 \tTraining Loss: 0.265033\n",
            "Epoch: 4590 \tTraining Loss: 0.264984\n",
            "Epoch: 4591 \tTraining Loss: 0.264944\n",
            "Epoch: 4592 \tTraining Loss: 0.264926\n",
            "Epoch: 4593 \tTraining Loss: 0.264908\n",
            "Epoch: 4594 \tTraining Loss: 0.264870\n",
            "Epoch: 4595 \tTraining Loss: 0.264828\n",
            "Epoch: 4596 \tTraining Loss: 0.264801\n",
            "Epoch: 4597 \tTraining Loss: 0.264790\n",
            "Epoch: 4598 \tTraining Loss: 0.264779\n",
            "Epoch: 4599 \tTraining Loss: 0.264747\n",
            "Epoch: 4600 \tTraining Loss: 0.264703\n",
            "Epoch: 4601 \tTraining Loss: 0.264666\n",
            "Epoch: 4602 \tTraining Loss: 0.264648\n",
            "Epoch: 4603 \tTraining Loss: 0.264636\n",
            "Epoch: 4604 \tTraining Loss: 0.264614\n",
            "Epoch: 4605 \tTraining Loss: 0.264581\n",
            "Epoch: 4606 \tTraining Loss: 0.264546\n",
            "Epoch: 4607 \tTraining Loss: 0.264514\n",
            "Epoch: 4608 \tTraining Loss: 0.264486\n",
            "Epoch: 4609 \tTraining Loss: 0.264456\n",
            "Epoch: 4610 \tTraining Loss: 0.264416\n",
            "Epoch: 4611 \tTraining Loss: 0.264352\n",
            "Epoch: 4612 \tTraining Loss: 0.264238\n",
            "Epoch: 4613 \tTraining Loss: 0.263996\n",
            "Epoch: 4614 \tTraining Loss: 0.263601\n",
            "Epoch: 4615 \tTraining Loss: 0.263510\n",
            "Epoch: 4616 \tTraining Loss: 0.263506\n",
            "Epoch: 4617 \tTraining Loss: 0.263493\n",
            "Epoch: 4618 \tTraining Loss: 0.263334\n",
            "Epoch: 4619 \tTraining Loss: 0.263318\n",
            "Epoch: 4620 \tTraining Loss: 0.263284\n",
            "Epoch: 4621 \tTraining Loss: 0.263204\n",
            "Epoch: 4622 \tTraining Loss: 0.263139\n",
            "Epoch: 4623 \tTraining Loss: 0.263127\n",
            "Epoch: 4624 \tTraining Loss: 0.263076\n",
            "Epoch: 4625 \tTraining Loss: 0.262927\n",
            "Epoch: 4626 \tTraining Loss: 0.262814\n",
            "Epoch: 4627 \tTraining Loss: 0.262545\n",
            "Epoch: 4628 \tTraining Loss: 0.262572\n",
            "Epoch: 4629 \tTraining Loss: 0.262422\n",
            "Epoch: 4630 \tTraining Loss: 0.262400\n",
            "Epoch: 4631 \tTraining Loss: 0.262348\n",
            "Epoch: 4632 \tTraining Loss: 0.262294\n",
            "Epoch: 4633 \tTraining Loss: 0.262174\n",
            "Epoch: 4634 \tTraining Loss: 0.262113\n",
            "Epoch: 4635 \tTraining Loss: 0.262085\n",
            "Epoch: 4636 \tTraining Loss: 0.262063\n",
            "Epoch: 4637 \tTraining Loss: 0.261931\n",
            "Epoch: 4638 \tTraining Loss: 0.261914\n",
            "Epoch: 4639 \tTraining Loss: 0.261875\n",
            "Epoch: 4640 \tTraining Loss: 0.261847\n",
            "Epoch: 4641 \tTraining Loss: 0.261798\n",
            "Epoch: 4642 \tTraining Loss: 0.261807\n",
            "Epoch: 4643 \tTraining Loss: 0.261711\n",
            "Epoch: 4644 \tTraining Loss: 0.261681\n",
            "Epoch: 4645 \tTraining Loss: 0.261679\n",
            "Epoch: 4646 \tTraining Loss: 0.261628\n",
            "Epoch: 4647 \tTraining Loss: 0.261593\n",
            "Epoch: 4648 \tTraining Loss: 0.261565\n",
            "Epoch: 4649 \tTraining Loss: 0.261548\n",
            "Epoch: 4650 \tTraining Loss: 0.261517\n",
            "Epoch: 4651 \tTraining Loss: 0.261501\n",
            "Epoch: 4652 \tTraining Loss: 0.261461\n",
            "Epoch: 4653 \tTraining Loss: 0.261447\n",
            "Epoch: 4654 \tTraining Loss: 0.261421\n",
            "Epoch: 4655 \tTraining Loss: 0.261393\n",
            "Epoch: 4656 \tTraining Loss: 0.261374\n",
            "Epoch: 4657 \tTraining Loss: 0.261355\n",
            "Epoch: 4658 \tTraining Loss: 0.261327\n",
            "Epoch: 4659 \tTraining Loss: 0.261309\n",
            "Epoch: 4660 \tTraining Loss: 0.261288\n",
            "Epoch: 4661 \tTraining Loss: 0.261264\n",
            "Epoch: 4662 \tTraining Loss: 0.261250\n",
            "Epoch: 4663 \tTraining Loss: 0.261223\n",
            "Epoch: 4664 \tTraining Loss: 0.261204\n",
            "Epoch: 4665 \tTraining Loss: 0.261185\n",
            "Epoch: 4666 \tTraining Loss: 0.261166\n",
            "Epoch: 4667 \tTraining Loss: 0.261144\n",
            "Epoch: 4668 \tTraining Loss: 0.261127\n",
            "Epoch: 4669 \tTraining Loss: 0.261107\n",
            "Epoch: 4670 \tTraining Loss: 0.261087\n",
            "Epoch: 4671 \tTraining Loss: 0.261070\n",
            "Epoch: 4672 \tTraining Loss: 0.261051\n",
            "Epoch: 4673 \tTraining Loss: 0.261033\n",
            "Epoch: 4674 \tTraining Loss: 0.261015\n",
            "Epoch: 4675 \tTraining Loss: 0.260996\n",
            "Epoch: 4676 \tTraining Loss: 0.260979\n",
            "Epoch: 4677 \tTraining Loss: 0.260962\n",
            "Epoch: 4678 \tTraining Loss: 0.260943\n",
            "Epoch: 4679 \tTraining Loss: 0.260925\n",
            "Epoch: 4680 \tTraining Loss: 0.260908\n",
            "Epoch: 4681 \tTraining Loss: 0.260890\n",
            "Epoch: 4682 \tTraining Loss: 0.260873\n",
            "Epoch: 4683 \tTraining Loss: 0.260856\n",
            "Epoch: 4684 \tTraining Loss: 0.260838\n",
            "Epoch: 4685 \tTraining Loss: 0.260821\n",
            "Epoch: 4686 \tTraining Loss: 0.260804\n",
            "Epoch: 4687 \tTraining Loss: 0.260786\n",
            "Epoch: 4688 \tTraining Loss: 0.260769\n",
            "Epoch: 4689 \tTraining Loss: 0.260752\n",
            "Epoch: 4690 \tTraining Loss: 0.260735\n",
            "Epoch: 4691 \tTraining Loss: 0.260718\n",
            "Epoch: 4692 \tTraining Loss: 0.260701\n",
            "Epoch: 4693 \tTraining Loss: 0.260684\n",
            "Epoch: 4694 \tTraining Loss: 0.260668\n",
            "Epoch: 4695 \tTraining Loss: 0.260651\n",
            "Epoch: 4696 \tTraining Loss: 0.260634\n",
            "Epoch: 4697 \tTraining Loss: 0.260617\n",
            "Epoch: 4698 \tTraining Loss: 0.260600\n",
            "Epoch: 4699 \tTraining Loss: 0.260584\n",
            "Epoch: 4700 \tTraining Loss: 0.260567\n",
            "Epoch: 4701 \tTraining Loss: 0.260551\n",
            "Epoch: 4702 \tTraining Loss: 0.260534\n",
            "Epoch: 4703 \tTraining Loss: 0.260517\n",
            "Epoch: 4704 \tTraining Loss: 0.260501\n",
            "Epoch: 4705 \tTraining Loss: 0.260484\n",
            "Epoch: 4706 \tTraining Loss: 0.260468\n",
            "Epoch: 4707 \tTraining Loss: 0.260451\n",
            "Epoch: 4708 \tTraining Loss: 0.260435\n",
            "Epoch: 4709 \tTraining Loss: 0.260418\n",
            "Epoch: 4710 \tTraining Loss: 0.260402\n",
            "Epoch: 4711 \tTraining Loss: 0.260385\n",
            "Epoch: 4712 \tTraining Loss: 0.260369\n",
            "Epoch: 4713 \tTraining Loss: 0.260353\n",
            "Epoch: 4714 \tTraining Loss: 0.260336\n",
            "Epoch: 4715 \tTraining Loss: 0.260320\n",
            "Epoch: 4716 \tTraining Loss: 0.260304\n",
            "Epoch: 4717 \tTraining Loss: 0.260287\n",
            "Epoch: 4718 \tTraining Loss: 0.260271\n",
            "Epoch: 4719 \tTraining Loss: 0.260255\n",
            "Epoch: 4720 \tTraining Loss: 0.260239\n",
            "Epoch: 4721 \tTraining Loss: 0.260222\n",
            "Epoch: 4722 \tTraining Loss: 0.260206\n",
            "Epoch: 4723 \tTraining Loss: 0.260190\n",
            "Epoch: 4724 \tTraining Loss: 0.260174\n",
            "Epoch: 4725 \tTraining Loss: 0.260157\n",
            "Epoch: 4726 \tTraining Loss: 0.260141\n",
            "Epoch: 4727 \tTraining Loss: 0.260125\n",
            "Epoch: 4728 \tTraining Loss: 0.260109\n",
            "Epoch: 4729 \tTraining Loss: 0.260093\n",
            "Epoch: 4730 \tTraining Loss: 0.260077\n",
            "Epoch: 4731 \tTraining Loss: 0.260061\n",
            "Epoch: 4732 \tTraining Loss: 0.260045\n",
            "Epoch: 4733 \tTraining Loss: 0.260029\n",
            "Epoch: 4734 \tTraining Loss: 0.260013\n",
            "Epoch: 4735 \tTraining Loss: 0.259996\n",
            "Epoch: 4736 \tTraining Loss: 0.259980\n",
            "Epoch: 4737 \tTraining Loss: 0.259964\n",
            "Epoch: 4738 \tTraining Loss: 0.259948\n",
            "Epoch: 4739 \tTraining Loss: 0.259932\n",
            "Epoch: 4740 \tTraining Loss: 0.259916\n",
            "Epoch: 4741 \tTraining Loss: 0.259900\n",
            "Epoch: 4742 \tTraining Loss: 0.259884\n",
            "Epoch: 4743 \tTraining Loss: 0.259868\n",
            "Epoch: 4744 \tTraining Loss: 0.259852\n",
            "Epoch: 4745 \tTraining Loss: 0.259836\n",
            "Epoch: 4746 \tTraining Loss: 0.259820\n",
            "Epoch: 4747 \tTraining Loss: 0.259804\n",
            "Epoch: 4748 \tTraining Loss: 0.259788\n",
            "Epoch: 4749 \tTraining Loss: 0.259772\n",
            "Epoch: 4750 \tTraining Loss: 0.259755\n",
            "Epoch: 4751 \tTraining Loss: 0.259739\n",
            "Epoch: 4752 \tTraining Loss: 0.259723\n",
            "Epoch: 4753 \tTraining Loss: 0.259706\n",
            "Epoch: 4754 \tTraining Loss: 0.259690\n",
            "Epoch: 4755 \tTraining Loss: 0.259673\n",
            "Epoch: 4756 \tTraining Loss: 0.259657\n",
            "Epoch: 4757 \tTraining Loss: 0.259640\n",
            "Epoch: 4758 \tTraining Loss: 0.259623\n",
            "Epoch: 4759 \tTraining Loss: 0.259605\n",
            "Epoch: 4760 \tTraining Loss: 0.259588\n",
            "Epoch: 4761 \tTraining Loss: 0.259571\n",
            "Epoch: 4762 \tTraining Loss: 0.259555\n",
            "Epoch: 4763 \tTraining Loss: 0.259539\n",
            "Epoch: 4764 \tTraining Loss: 0.259523\n",
            "Epoch: 4765 \tTraining Loss: 0.259508\n",
            "Epoch: 4766 \tTraining Loss: 0.259493\n",
            "Epoch: 4767 \tTraining Loss: 0.259477\n",
            "Epoch: 4768 \tTraining Loss: 0.259461\n",
            "Epoch: 4769 \tTraining Loss: 0.259445\n",
            "Epoch: 4770 \tTraining Loss: 0.259429\n",
            "Epoch: 4771 \tTraining Loss: 0.259413\n",
            "Epoch: 4772 \tTraining Loss: 0.259397\n",
            "Epoch: 4773 \tTraining Loss: 0.259381\n",
            "Epoch: 4774 \tTraining Loss: 0.259365\n",
            "Epoch: 4775 \tTraining Loss: 0.259349\n",
            "Epoch: 4776 \tTraining Loss: 0.259333\n",
            "Epoch: 4777 \tTraining Loss: 0.259318\n",
            "Epoch: 4778 \tTraining Loss: 0.259302\n",
            "Epoch: 4779 \tTraining Loss: 0.259287\n",
            "Epoch: 4780 \tTraining Loss: 0.259271\n",
            "Epoch: 4781 \tTraining Loss: 0.259255\n",
            "Epoch: 4782 \tTraining Loss: 0.259239\n",
            "Epoch: 4783 \tTraining Loss: 0.259223\n",
            "Epoch: 4784 \tTraining Loss: 0.259207\n",
            "Epoch: 4785 \tTraining Loss: 0.259192\n",
            "Epoch: 4786 \tTraining Loss: 0.259176\n",
            "Epoch: 4787 \tTraining Loss: 0.259160\n",
            "Epoch: 4788 \tTraining Loss: 0.259145\n",
            "Epoch: 4789 \tTraining Loss: 0.259129\n",
            "Epoch: 4790 \tTraining Loss: 0.259113\n",
            "Epoch: 4791 \tTraining Loss: 0.259098\n",
            "Epoch: 4792 \tTraining Loss: 0.259082\n",
            "Epoch: 4793 \tTraining Loss: 0.259066\n",
            "Epoch: 4794 \tTraining Loss: 0.259050\n",
            "Epoch: 4795 \tTraining Loss: 0.259035\n",
            "Epoch: 4796 \tTraining Loss: 0.259019\n",
            "Epoch: 4797 \tTraining Loss: 0.259003\n",
            "Epoch: 4798 \tTraining Loss: 0.258987\n",
            "Epoch: 4799 \tTraining Loss: 0.258972\n",
            "Epoch: 4800 \tTraining Loss: 0.258956\n",
            "Epoch: 4801 \tTraining Loss: 0.258940\n",
            "Epoch: 4802 \tTraining Loss: 0.258925\n",
            "Epoch: 4803 \tTraining Loss: 0.258909\n",
            "Epoch: 4804 \tTraining Loss: 0.258893\n",
            "Epoch: 4805 \tTraining Loss: 0.258877\n",
            "Epoch: 4806 \tTraining Loss: 0.258862\n",
            "Epoch: 4807 \tTraining Loss: 0.258846\n",
            "Epoch: 4808 \tTraining Loss: 0.258830\n",
            "Epoch: 4809 \tTraining Loss: 0.258814\n",
            "Epoch: 4810 \tTraining Loss: 0.258798\n",
            "Epoch: 4811 \tTraining Loss: 0.258783\n",
            "Epoch: 4812 \tTraining Loss: 0.258767\n",
            "Epoch: 4813 \tTraining Loss: 0.258751\n",
            "Epoch: 4814 \tTraining Loss: 0.258735\n",
            "Epoch: 4815 \tTraining Loss: 0.258719\n",
            "Epoch: 4816 \tTraining Loss: 0.258703\n",
            "Epoch: 4817 \tTraining Loss: 0.258687\n",
            "Epoch: 4818 \tTraining Loss: 0.258671\n",
            "Epoch: 4819 \tTraining Loss: 0.258655\n",
            "Epoch: 4820 \tTraining Loss: 0.258639\n",
            "Epoch: 4821 \tTraining Loss: 0.258623\n",
            "Epoch: 4822 \tTraining Loss: 0.258606\n",
            "Epoch: 4823 \tTraining Loss: 0.258590\n",
            "Epoch: 4824 \tTraining Loss: 0.258574\n",
            "Epoch: 4825 \tTraining Loss: 0.258557\n",
            "Epoch: 4826 \tTraining Loss: 0.258541\n",
            "Epoch: 4827 \tTraining Loss: 0.258524\n",
            "Epoch: 4828 \tTraining Loss: 0.258507\n",
            "Epoch: 4829 \tTraining Loss: 0.258490\n",
            "Epoch: 4830 \tTraining Loss: 0.258472\n",
            "Epoch: 4831 \tTraining Loss: 0.258454\n",
            "Epoch: 4832 \tTraining Loss: 0.258436\n",
            "Epoch: 4833 \tTraining Loss: 0.258418\n",
            "Epoch: 4834 \tTraining Loss: 0.258399\n",
            "Epoch: 4835 \tTraining Loss: 0.258380\n",
            "Epoch: 4836 \tTraining Loss: 0.258361\n",
            "Epoch: 4837 \tTraining Loss: 0.258340\n",
            "Epoch: 4838 \tTraining Loss: 0.258319\n",
            "Epoch: 4839 \tTraining Loss: 0.258297\n",
            "Epoch: 4840 \tTraining Loss: 0.258274\n",
            "Epoch: 4841 \tTraining Loss: 0.258249\n",
            "Epoch: 4842 \tTraining Loss: 0.258220\n",
            "Epoch: 4843 \tTraining Loss: 0.258193\n",
            "Epoch: 4844 \tTraining Loss: 0.258170\n",
            "Epoch: 4845 \tTraining Loss: 0.258135\n",
            "Epoch: 4846 \tTraining Loss: 0.258103\n",
            "Epoch: 4847 \tTraining Loss: 0.258071\n",
            "Epoch: 4848 \tTraining Loss: 0.258031\n",
            "Epoch: 4849 \tTraining Loss: 0.257990\n",
            "Epoch: 4850 \tTraining Loss: 0.257953\n",
            "Epoch: 4851 \tTraining Loss: 0.257907\n",
            "Epoch: 4852 \tTraining Loss: 0.257868\n",
            "Epoch: 4853 \tTraining Loss: 0.257827\n",
            "Epoch: 4854 \tTraining Loss: 0.257788\n",
            "Epoch: 4855 \tTraining Loss: 0.257755\n",
            "Epoch: 4856 \tTraining Loss: 0.257724\n",
            "Epoch: 4857 \tTraining Loss: 0.257697\n",
            "Epoch: 4858 \tTraining Loss: 0.257675\n",
            "Epoch: 4859 \tTraining Loss: 0.257654\n",
            "Epoch: 4860 \tTraining Loss: 0.257636\n",
            "Epoch: 4861 \tTraining Loss: 0.257621\n",
            "Epoch: 4862 \tTraining Loss: 0.257605\n",
            "Epoch: 4863 \tTraining Loss: 0.257593\n",
            "Epoch: 4864 \tTraining Loss: 0.257600\n",
            "Epoch: 4865 \tTraining Loss: 0.257728\n",
            "Epoch: 4866 \tTraining Loss: 0.257987\n",
            "Epoch: 4867 \tTraining Loss: 0.259083\n",
            "Epoch: 4868 \tTraining Loss: 0.259344\n",
            "Epoch: 4869 \tTraining Loss: 0.258371\n",
            "Epoch: 4870 \tTraining Loss: 0.260522\n",
            "Epoch: 4871 \tTraining Loss: 0.257583\n",
            "Epoch: 4872 \tTraining Loss: 0.259269\n",
            "Epoch: 4873 \tTraining Loss: 0.259859\n",
            "Epoch: 4874 \tTraining Loss: 0.259874\n",
            "Epoch: 4875 \tTraining Loss: 0.259467\n",
            "Epoch: 4876 \tTraining Loss: 0.258436\n",
            "Epoch: 4877 \tTraining Loss: 0.259470\n",
            "Epoch: 4878 \tTraining Loss: 0.257671\n",
            "Epoch: 4879 \tTraining Loss: 0.258290\n",
            "Epoch: 4880 \tTraining Loss: 0.257736\n",
            "Epoch: 4881 \tTraining Loss: 0.258029\n",
            "Epoch: 4882 \tTraining Loss: 0.257435\n",
            "Epoch: 4883 \tTraining Loss: 0.257682\n",
            "Epoch: 4884 \tTraining Loss: 0.257404\n",
            "Epoch: 4885 \tTraining Loss: 0.257592\n",
            "Epoch: 4886 \tTraining Loss: 0.257280\n",
            "Epoch: 4887 \tTraining Loss: 0.257417\n",
            "Epoch: 4888 \tTraining Loss: 0.257260\n",
            "Epoch: 4889 \tTraining Loss: 0.257329\n",
            "Epoch: 4890 \tTraining Loss: 0.257177\n",
            "Epoch: 4891 \tTraining Loss: 0.257231\n",
            "Epoch: 4892 \tTraining Loss: 0.257135\n",
            "Epoch: 4893 \tTraining Loss: 0.257183\n",
            "Epoch: 4894 \tTraining Loss: 0.257076\n",
            "Epoch: 4895 \tTraining Loss: 0.257103\n",
            "Epoch: 4896 \tTraining Loss: 0.257027\n",
            "Epoch: 4897 \tTraining Loss: 0.257062\n",
            "Epoch: 4898 \tTraining Loss: 0.256998\n",
            "Epoch: 4899 \tTraining Loss: 0.256993\n",
            "Epoch: 4900 \tTraining Loss: 0.256960\n",
            "Epoch: 4901 \tTraining Loss: 0.256919\n",
            "Epoch: 4902 \tTraining Loss: 0.256925\n",
            "Epoch: 4903 \tTraining Loss: 0.256873\n",
            "Epoch: 4904 \tTraining Loss: 0.256888\n",
            "Epoch: 4905 \tTraining Loss: 0.256847\n",
            "Epoch: 4906 \tTraining Loss: 0.256829\n",
            "Epoch: 4907 \tTraining Loss: 0.256820\n",
            "Epoch: 4908 \tTraining Loss: 0.256779\n",
            "Epoch: 4909 \tTraining Loss: 0.256776\n",
            "Epoch: 4910 \tTraining Loss: 0.256747\n",
            "Epoch: 4911 \tTraining Loss: 0.256731\n",
            "Epoch: 4912 \tTraining Loss: 0.256720\n",
            "Epoch: 4913 \tTraining Loss: 0.256691\n",
            "Epoch: 4914 \tTraining Loss: 0.256685\n",
            "Epoch: 4915 \tTraining Loss: 0.256661\n",
            "Epoch: 4916 \tTraining Loss: 0.256641\n",
            "Epoch: 4917 \tTraining Loss: 0.256631\n",
            "Epoch: 4918 \tTraining Loss: 0.256603\n",
            "Epoch: 4919 \tTraining Loss: 0.256590\n",
            "Epoch: 4920 \tTraining Loss: 0.256572\n",
            "Epoch: 4921 \tTraining Loss: 0.256549\n",
            "Epoch: 4922 \tTraining Loss: 0.256536\n",
            "Epoch: 4923 \tTraining Loss: 0.256515\n",
            "Epoch: 4924 \tTraining Loss: 0.256498\n",
            "Epoch: 4925 \tTraining Loss: 0.256482\n",
            "Epoch: 4926 \tTraining Loss: 0.256461\n",
            "Epoch: 4927 \tTraining Loss: 0.256447\n",
            "Epoch: 4928 \tTraining Loss: 0.256428\n",
            "Epoch: 4929 \tTraining Loss: 0.256410\n",
            "Epoch: 4930 \tTraining Loss: 0.256395\n",
            "Epoch: 4931 \tTraining Loss: 0.256375\n",
            "Epoch: 4932 \tTraining Loss: 0.256359\n",
            "Epoch: 4933 \tTraining Loss: 0.256342\n",
            "Epoch: 4934 \tTraining Loss: 0.256324\n",
            "Epoch: 4935 \tTraining Loss: 0.256308\n",
            "Epoch: 4936 \tTraining Loss: 0.256290\n",
            "Epoch: 4937 \tTraining Loss: 0.256273\n",
            "Epoch: 4938 \tTraining Loss: 0.256256\n",
            "Epoch: 4939 \tTraining Loss: 0.256238\n",
            "Epoch: 4940 \tTraining Loss: 0.256222\n",
            "Epoch: 4941 \tTraining Loss: 0.256204\n",
            "Epoch: 4942 \tTraining Loss: 0.256187\n",
            "Epoch: 4943 \tTraining Loss: 0.256171\n",
            "Epoch: 4944 \tTraining Loss: 0.256153\n",
            "Epoch: 4945 \tTraining Loss: 0.256136\n",
            "Epoch: 4946 \tTraining Loss: 0.256119\n",
            "Epoch: 4947 \tTraining Loss: 0.256102\n",
            "Epoch: 4948 \tTraining Loss: 0.256085\n",
            "Epoch: 4949 \tTraining Loss: 0.256068\n",
            "Epoch: 4950 \tTraining Loss: 0.256051\n",
            "Epoch: 4951 \tTraining Loss: 0.256034\n",
            "Epoch: 4952 \tTraining Loss: 0.256017\n",
            "Epoch: 4953 \tTraining Loss: 0.256000\n",
            "Epoch: 4954 \tTraining Loss: 0.255984\n",
            "Epoch: 4955 \tTraining Loss: 0.255967\n",
            "Epoch: 4956 \tTraining Loss: 0.255950\n",
            "Epoch: 4957 \tTraining Loss: 0.255933\n",
            "Epoch: 4958 \tTraining Loss: 0.255916\n",
            "Epoch: 4959 \tTraining Loss: 0.255899\n",
            "Epoch: 4960 \tTraining Loss: 0.255882\n",
            "Epoch: 4961 \tTraining Loss: 0.255866\n",
            "Epoch: 4962 \tTraining Loss: 0.255849\n",
            "Epoch: 4963 \tTraining Loss: 0.255832\n",
            "Epoch: 4964 \tTraining Loss: 0.255815\n",
            "Epoch: 4965 \tTraining Loss: 0.255798\n",
            "Epoch: 4966 \tTraining Loss: 0.255781\n",
            "Epoch: 4967 \tTraining Loss: 0.255765\n",
            "Epoch: 4968 \tTraining Loss: 0.255748\n",
            "Epoch: 4969 \tTraining Loss: 0.255731\n",
            "Epoch: 4970 \tTraining Loss: 0.255714\n",
            "Epoch: 4971 \tTraining Loss: 0.255698\n",
            "Epoch: 4972 \tTraining Loss: 0.255681\n",
            "Epoch: 4973 \tTraining Loss: 0.255664\n",
            "Epoch: 4974 \tTraining Loss: 0.255647\n",
            "Epoch: 4975 \tTraining Loss: 0.255631\n",
            "Epoch: 4976 \tTraining Loss: 0.255614\n",
            "Epoch: 4977 \tTraining Loss: 0.255597\n",
            "Epoch: 4978 \tTraining Loss: 0.255580\n",
            "Epoch: 4979 \tTraining Loss: 0.255564\n",
            "Epoch: 4980 \tTraining Loss: 0.255547\n",
            "Epoch: 4981 \tTraining Loss: 0.255530\n",
            "Epoch: 4982 \tTraining Loss: 0.255514\n",
            "Epoch: 4983 \tTraining Loss: 0.255497\n",
            "Epoch: 4984 \tTraining Loss: 0.255480\n",
            "Epoch: 4985 \tTraining Loss: 0.255463\n",
            "Epoch: 4986 \tTraining Loss: 0.255447\n",
            "Epoch: 4987 \tTraining Loss: 0.255430\n",
            "Epoch: 4988 \tTraining Loss: 0.255413\n",
            "Epoch: 4989 \tTraining Loss: 0.255397\n",
            "Epoch: 4990 \tTraining Loss: 0.255380\n",
            "Epoch: 4991 \tTraining Loss: 0.255363\n",
            "Epoch: 4992 \tTraining Loss: 0.255347\n",
            "Epoch: 4993 \tTraining Loss: 0.255330\n",
            "Epoch: 4994 \tTraining Loss: 0.255314\n",
            "Epoch: 4995 \tTraining Loss: 0.255297\n",
            "Epoch: 4996 \tTraining Loss: 0.255280\n",
            "Epoch: 4997 \tTraining Loss: 0.255264\n",
            "Epoch: 4998 \tTraining Loss: 0.255247\n",
            "Epoch: 4999 \tTraining Loss: 0.255230\n",
            "Epoch: 5000 \tTraining Loss: 0.255214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Test"
      ],
      "metadata": {
        "id": "2ZwjCNHE1I41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test.float())\n",
        "    predicted_labels = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "predicted_labels = np.where(predicted_labels == 0 ,-1,1)\n",
        "y_test = np.where(y_test == 0 ,-1,1)\n",
        "\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006fa432-fe69-43c0-f197-65d737c96dbd",
        "id": "kwMMpqKC1I41"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  1  1  1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1 -1  1  1 -1 -1  1\n",
            "  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1  1 -1  1  1 -1 -1  1 -1 -1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
            " -1 -1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1  1  1  1 -1  1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1  1\n",
            "  1  1  1  1  1 -1 -1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1  1  1  1 -1 -1 -1  1  1  1  1  1  1  1  1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# a = np.array(*predicted_labels)\n",
        "test_accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f'Test Accuracy: {100 * test_accuracy:.2f}%')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
        "import seaborn as sns\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5b34538a-8d9d-4d5f-b999-48088e7e7e91",
        "id": "tFcg2EfV1I41"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 49.38%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## with use شاخص کل & شاخص کل(هم وزن)"
      ],
      "metadata": {
        "id": "VD1sojrw13_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Add indicators to Features"
      ],
      "metadata": {
        "id": "nT2H-gGK18in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = np.vstack((adjClose[1:],aroon[1:],macd[1:],rsi[1:],close_all[1:],aroon_all[1:],macd_all[1:],rsi_all[1:],close_HamVazn[1:],aroon_HamVazn[1:],macd_HamVazn[1:],rsi_HamVazn[1:]))\n",
        "\n",
        "labels = np.zeros(2006)\n",
        "labels = np.where(merged_Foolad[1:] >= merged_Foolad[:-1], 1, 0)"
      ],
      "metadata": {
        "id": "YzN6WUmv18in"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775db030-e7e9-4f61-ecdd-143dc63ea4e1",
        "id": "dcQ39AQk18in"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 2006)"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Split data and adjust window size"
      ],
      "metadata": {
        "id": "yRQreFUh18in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 5\n",
        "X, y = create_windows(np.hstack((features.T, labels.reshape(-1, 1))), window_size)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_size = int(len(X) * 0.80)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]"
      ],
      "metadata": {
        "id": "jaBZmNup18io"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define RNN Model"
      ],
      "metadata": {
        "id": "6nGC0N3L18io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "_oUZdvyB18io"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 12  # 12 features\n",
        "hidden_size = 50\n",
        "output_size = 2  # Two classes\n",
        "model = RNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)"
      ],
      "metadata": {
        "id": "qRb_AMHx18io"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Training"
      ],
      "metadata": {
        "id": "wtHLIjix18io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5000\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    outputs = model(X_train.float())\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        loss.item()\n",
        "        ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f39f99-ebd1-4f05-9ede-8cfa6474b363",
        "id": "nS5hfwii18io"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.668316\n",
            "Epoch: 2 \tTraining Loss: 0.665750\n",
            "Epoch: 3 \tTraining Loss: 0.663335\n",
            "Epoch: 4 \tTraining Loss: 0.661066\n",
            "Epoch: 5 \tTraining Loss: 0.658849\n",
            "Epoch: 6 \tTraining Loss: 0.656836\n",
            "Epoch: 7 \tTraining Loss: 0.654714\n",
            "Epoch: 8 \tTraining Loss: 0.653120\n",
            "Epoch: 9 \tTraining Loss: 0.651656\n",
            "Epoch: 10 \tTraining Loss: 0.650353\n",
            "Epoch: 11 \tTraining Loss: 0.649106\n",
            "Epoch: 12 \tTraining Loss: 0.648121\n",
            "Epoch: 13 \tTraining Loss: 0.647348\n",
            "Epoch: 14 \tTraining Loss: 0.646981\n",
            "Epoch: 15 \tTraining Loss: 0.646761\n",
            "Epoch: 16 \tTraining Loss: 0.646519\n",
            "Epoch: 17 \tTraining Loss: 0.646299\n",
            "Epoch: 18 \tTraining Loss: 0.646108\n",
            "Epoch: 19 \tTraining Loss: 0.645882\n",
            "Epoch: 20 \tTraining Loss: 0.645585\n",
            "Epoch: 21 \tTraining Loss: 0.645229\n",
            "Epoch: 22 \tTraining Loss: 0.644819\n",
            "Epoch: 23 \tTraining Loss: 0.644379\n",
            "Epoch: 24 \tTraining Loss: 0.643912\n",
            "Epoch: 25 \tTraining Loss: 0.643519\n",
            "Epoch: 26 \tTraining Loss: 0.643190\n",
            "Epoch: 27 \tTraining Loss: 0.642848\n",
            "Epoch: 28 \tTraining Loss: 0.642505\n",
            "Epoch: 29 \tTraining Loss: 0.642186\n",
            "Epoch: 30 \tTraining Loss: 0.641867\n",
            "Epoch: 31 \tTraining Loss: 0.641584\n",
            "Epoch: 32 \tTraining Loss: 0.641255\n",
            "Epoch: 33 \tTraining Loss: 0.641067\n",
            "Epoch: 34 \tTraining Loss: 0.640656\n",
            "Epoch: 35 \tTraining Loss: 0.640301\n",
            "Epoch: 36 \tTraining Loss: 0.640184\n",
            "Epoch: 37 \tTraining Loss: 0.639994\n",
            "Epoch: 38 \tTraining Loss: 0.639808\n",
            "Epoch: 39 \tTraining Loss: 0.639640\n",
            "Epoch: 40 \tTraining Loss: 0.639516\n",
            "Epoch: 41 \tTraining Loss: 0.639393\n",
            "Epoch: 42 \tTraining Loss: 0.639250\n",
            "Epoch: 43 \tTraining Loss: 0.639105\n",
            "Epoch: 44 \tTraining Loss: 0.638967\n",
            "Epoch: 45 \tTraining Loss: 0.638812\n",
            "Epoch: 46 \tTraining Loss: 0.638640\n",
            "Epoch: 47 \tTraining Loss: 0.638477\n",
            "Epoch: 48 \tTraining Loss: 0.638334\n",
            "Epoch: 49 \tTraining Loss: 0.638195\n",
            "Epoch: 50 \tTraining Loss: 0.638050\n",
            "Epoch: 51 \tTraining Loss: 0.637910\n",
            "Epoch: 52 \tTraining Loss: 0.637762\n",
            "Epoch: 53 \tTraining Loss: 0.637601\n",
            "Epoch: 54 \tTraining Loss: 0.637437\n",
            "Epoch: 55 \tTraining Loss: 0.637278\n",
            "Epoch: 56 \tTraining Loss: 0.637127\n",
            "Epoch: 57 \tTraining Loss: 0.636977\n",
            "Epoch: 58 \tTraining Loss: 0.636835\n",
            "Epoch: 59 \tTraining Loss: 0.636703\n",
            "Epoch: 60 \tTraining Loss: 0.636574\n",
            "Epoch: 61 \tTraining Loss: 0.636447\n",
            "Epoch: 62 \tTraining Loss: 0.636327\n",
            "Epoch: 63 \tTraining Loss: 0.636210\n",
            "Epoch: 64 \tTraining Loss: 0.636089\n",
            "Epoch: 65 \tTraining Loss: 0.635948\n",
            "Epoch: 66 \tTraining Loss: 0.635777\n",
            "Epoch: 67 \tTraining Loss: 0.635630\n",
            "Epoch: 68 \tTraining Loss: 0.635504\n",
            "Epoch: 69 \tTraining Loss: 0.635376\n",
            "Epoch: 70 \tTraining Loss: 0.635275\n",
            "Epoch: 71 \tTraining Loss: 0.635174\n",
            "Epoch: 72 \tTraining Loss: 0.635046\n",
            "Epoch: 73 \tTraining Loss: 0.634919\n",
            "Epoch: 74 \tTraining Loss: 0.634811\n",
            "Epoch: 75 \tTraining Loss: 0.634701\n",
            "Epoch: 76 \tTraining Loss: 0.634578\n",
            "Epoch: 77 \tTraining Loss: 0.634456\n",
            "Epoch: 78 \tTraining Loss: 0.634337\n",
            "Epoch: 79 \tTraining Loss: 0.634220\n",
            "Epoch: 80 \tTraining Loss: 0.634104\n",
            "Epoch: 81 \tTraining Loss: 0.633983\n",
            "Epoch: 82 \tTraining Loss: 0.633867\n",
            "Epoch: 83 \tTraining Loss: 0.633753\n",
            "Epoch: 84 \tTraining Loss: 0.633637\n",
            "Epoch: 85 \tTraining Loss: 0.633515\n",
            "Epoch: 86 \tTraining Loss: 0.633396\n",
            "Epoch: 87 \tTraining Loss: 0.633283\n",
            "Epoch: 88 \tTraining Loss: 0.633170\n",
            "Epoch: 89 \tTraining Loss: 0.633056\n",
            "Epoch: 90 \tTraining Loss: 0.632939\n",
            "Epoch: 91 \tTraining Loss: 0.632817\n",
            "Epoch: 92 \tTraining Loss: 0.632687\n",
            "Epoch: 93 \tTraining Loss: 0.632551\n",
            "Epoch: 94 \tTraining Loss: 0.632424\n",
            "Epoch: 95 \tTraining Loss: 0.632312\n",
            "Epoch: 96 \tTraining Loss: 0.632191\n",
            "Epoch: 97 \tTraining Loss: 0.632055\n",
            "Epoch: 98 \tTraining Loss: 0.631914\n",
            "Epoch: 99 \tTraining Loss: 0.631791\n",
            "Epoch: 100 \tTraining Loss: 0.631653\n",
            "Epoch: 101 \tTraining Loss: 0.631500\n",
            "Epoch: 102 \tTraining Loss: 0.631362\n",
            "Epoch: 103 \tTraining Loss: 0.631215\n",
            "Epoch: 104 \tTraining Loss: 0.631056\n",
            "Epoch: 105 \tTraining Loss: 0.630923\n",
            "Epoch: 106 \tTraining Loss: 0.630772\n",
            "Epoch: 107 \tTraining Loss: 0.630606\n",
            "Epoch: 108 \tTraining Loss: 0.630459\n",
            "Epoch: 109 \tTraining Loss: 0.630302\n",
            "Epoch: 110 \tTraining Loss: 0.630142\n",
            "Epoch: 111 \tTraining Loss: 0.629991\n",
            "Epoch: 112 \tTraining Loss: 0.629831\n",
            "Epoch: 113 \tTraining Loss: 0.629677\n",
            "Epoch: 114 \tTraining Loss: 0.629523\n",
            "Epoch: 115 \tTraining Loss: 0.629354\n",
            "Epoch: 116 \tTraining Loss: 0.629193\n",
            "Epoch: 117 \tTraining Loss: 0.629025\n",
            "Epoch: 118 \tTraining Loss: 0.628853\n",
            "Epoch: 119 \tTraining Loss: 0.628680\n",
            "Epoch: 120 \tTraining Loss: 0.628504\n",
            "Epoch: 121 \tTraining Loss: 0.628331\n",
            "Epoch: 122 \tTraining Loss: 0.628163\n",
            "Epoch: 123 \tTraining Loss: 0.628009\n",
            "Epoch: 124 \tTraining Loss: 0.627856\n",
            "Epoch: 125 \tTraining Loss: 0.627666\n",
            "Epoch: 126 \tTraining Loss: 0.627507\n",
            "Epoch: 127 \tTraining Loss: 0.627373\n",
            "Epoch: 128 \tTraining Loss: 0.627217\n",
            "Epoch: 129 \tTraining Loss: 0.627082\n",
            "Epoch: 130 \tTraining Loss: 0.626936\n",
            "Epoch: 131 \tTraining Loss: 0.626785\n",
            "Epoch: 132 \tTraining Loss: 0.626649\n",
            "Epoch: 133 \tTraining Loss: 0.626495\n",
            "Epoch: 134 \tTraining Loss: 0.626338\n",
            "Epoch: 135 \tTraining Loss: 0.626203\n",
            "Epoch: 136 \tTraining Loss: 0.626056\n",
            "Epoch: 137 \tTraining Loss: 0.625903\n",
            "Epoch: 138 \tTraining Loss: 0.625756\n",
            "Epoch: 139 \tTraining Loss: 0.625606\n",
            "Epoch: 140 \tTraining Loss: 0.625458\n",
            "Epoch: 141 \tTraining Loss: 0.625311\n",
            "Epoch: 142 \tTraining Loss: 0.625155\n",
            "Epoch: 143 \tTraining Loss: 0.624997\n",
            "Epoch: 144 \tTraining Loss: 0.624842\n",
            "Epoch: 145 \tTraining Loss: 0.624679\n",
            "Epoch: 146 \tTraining Loss: 0.624517\n",
            "Epoch: 147 \tTraining Loss: 0.624332\n",
            "Epoch: 148 \tTraining Loss: 0.624167\n",
            "Epoch: 149 \tTraining Loss: 0.623962\n",
            "Epoch: 150 \tTraining Loss: 0.623758\n",
            "Epoch: 151 \tTraining Loss: 0.623494\n",
            "Epoch: 152 \tTraining Loss: 0.623316\n",
            "Epoch: 153 \tTraining Loss: 0.623085\n",
            "Epoch: 154 \tTraining Loss: 0.622934\n",
            "Epoch: 155 \tTraining Loss: 0.622774\n",
            "Epoch: 156 \tTraining Loss: 0.622581\n",
            "Epoch: 157 \tTraining Loss: 0.622388\n",
            "Epoch: 158 \tTraining Loss: 0.622239\n",
            "Epoch: 159 \tTraining Loss: 0.622010\n",
            "Epoch: 160 \tTraining Loss: 0.621899\n",
            "Epoch: 161 \tTraining Loss: 0.621670\n",
            "Epoch: 162 \tTraining Loss: 0.621482\n",
            "Epoch: 163 \tTraining Loss: 0.621324\n",
            "Epoch: 164 \tTraining Loss: 0.621129\n",
            "Epoch: 165 \tTraining Loss: 0.620978\n",
            "Epoch: 166 \tTraining Loss: 0.620795\n",
            "Epoch: 167 \tTraining Loss: 0.620614\n",
            "Epoch: 168 \tTraining Loss: 0.620433\n",
            "Epoch: 169 \tTraining Loss: 0.620239\n",
            "Epoch: 170 \tTraining Loss: 0.620077\n",
            "Epoch: 171 \tTraining Loss: 0.619897\n",
            "Epoch: 172 \tTraining Loss: 0.619734\n",
            "Epoch: 173 \tTraining Loss: 0.619561\n",
            "Epoch: 174 \tTraining Loss: 0.619385\n",
            "Epoch: 175 \tTraining Loss: 0.619209\n",
            "Epoch: 176 \tTraining Loss: 0.619019\n",
            "Epoch: 177 \tTraining Loss: 0.618842\n",
            "Epoch: 178 \tTraining Loss: 0.618660\n",
            "Epoch: 179 \tTraining Loss: 0.618488\n",
            "Epoch: 180 \tTraining Loss: 0.618315\n",
            "Epoch: 181 \tTraining Loss: 0.618143\n",
            "Epoch: 182 \tTraining Loss: 0.617963\n",
            "Epoch: 183 \tTraining Loss: 0.617763\n",
            "Epoch: 184 \tTraining Loss: 0.617551\n",
            "Epoch: 185 \tTraining Loss: 0.617345\n",
            "Epoch: 186 \tTraining Loss: 0.617178\n",
            "Epoch: 187 \tTraining Loss: 0.616992\n",
            "Epoch: 188 \tTraining Loss: 0.616787\n",
            "Epoch: 189 \tTraining Loss: 0.616573\n",
            "Epoch: 190 \tTraining Loss: 0.616373\n",
            "Epoch: 191 \tTraining Loss: 0.616178\n",
            "Epoch: 192 \tTraining Loss: 0.615978\n",
            "Epoch: 193 \tTraining Loss: 0.615777\n",
            "Epoch: 194 \tTraining Loss: 0.615565\n",
            "Epoch: 195 \tTraining Loss: 0.615341\n",
            "Epoch: 196 \tTraining Loss: 0.615116\n",
            "Epoch: 197 \tTraining Loss: 0.614904\n",
            "Epoch: 198 \tTraining Loss: 0.614699\n",
            "Epoch: 199 \tTraining Loss: 0.614495\n",
            "Epoch: 200 \tTraining Loss: 0.614287\n",
            "Epoch: 201 \tTraining Loss: 0.614081\n",
            "Epoch: 202 \tTraining Loss: 0.613866\n",
            "Epoch: 203 \tTraining Loss: 0.613618\n",
            "Epoch: 204 \tTraining Loss: 0.613358\n",
            "Epoch: 205 \tTraining Loss: 0.613102\n",
            "Epoch: 206 \tTraining Loss: 0.612640\n",
            "Epoch: 207 \tTraining Loss: 0.612467\n",
            "Epoch: 208 \tTraining Loss: 0.612164\n",
            "Epoch: 209 \tTraining Loss: 0.611778\n",
            "Epoch: 210 \tTraining Loss: 0.611581\n",
            "Epoch: 211 \tTraining Loss: 0.611247\n",
            "Epoch: 212 \tTraining Loss: 0.611006\n",
            "Epoch: 213 \tTraining Loss: 0.610865\n",
            "Epoch: 214 \tTraining Loss: 0.610631\n",
            "Epoch: 215 \tTraining Loss: 0.610309\n",
            "Epoch: 216 \tTraining Loss: 0.610021\n",
            "Epoch: 217 \tTraining Loss: 0.609739\n",
            "Epoch: 218 \tTraining Loss: 0.609500\n",
            "Epoch: 219 \tTraining Loss: 0.609281\n",
            "Epoch: 220 \tTraining Loss: 0.608977\n",
            "Epoch: 221 \tTraining Loss: 0.608638\n",
            "Epoch: 222 \tTraining Loss: 0.608370\n",
            "Epoch: 223 \tTraining Loss: 0.608135\n",
            "Epoch: 224 \tTraining Loss: 0.607831\n",
            "Epoch: 225 \tTraining Loss: 0.607450\n",
            "Epoch: 226 \tTraining Loss: 0.607126\n",
            "Epoch: 227 \tTraining Loss: 0.606876\n",
            "Epoch: 228 \tTraining Loss: 0.606605\n",
            "Epoch: 229 \tTraining Loss: 0.606250\n",
            "Epoch: 230 \tTraining Loss: 0.605862\n",
            "Epoch: 231 \tTraining Loss: 0.605519\n",
            "Epoch: 232 \tTraining Loss: 0.605225\n",
            "Epoch: 233 \tTraining Loss: 0.604968\n",
            "Epoch: 234 \tTraining Loss: 0.604711\n",
            "Epoch: 235 \tTraining Loss: 0.604402\n",
            "Epoch: 236 \tTraining Loss: 0.604026\n",
            "Epoch: 237 \tTraining Loss: 0.603639\n",
            "Epoch: 238 \tTraining Loss: 0.603293\n",
            "Epoch: 239 \tTraining Loss: 0.602998\n",
            "Epoch: 240 \tTraining Loss: 0.602757\n",
            "Epoch: 241 \tTraining Loss: 0.602598\n",
            "Epoch: 242 \tTraining Loss: 0.602454\n",
            "Epoch: 243 \tTraining Loss: 0.602144\n",
            "Epoch: 244 \tTraining Loss: 0.601515\n",
            "Epoch: 245 \tTraining Loss: 0.601212\n",
            "Epoch: 246 \tTraining Loss: 0.601122\n",
            "Epoch: 247 \tTraining Loss: 0.600533\n",
            "Epoch: 248 \tTraining Loss: 0.600975\n",
            "Epoch: 249 \tTraining Loss: 0.600684\n",
            "Epoch: 250 \tTraining Loss: 0.600597\n",
            "Epoch: 251 \tTraining Loss: 0.600186\n",
            "Epoch: 252 \tTraining Loss: 0.600191\n",
            "Epoch: 253 \tTraining Loss: 0.600316\n",
            "Epoch: 254 \tTraining Loss: 0.600070\n",
            "Epoch: 255 \tTraining Loss: 0.599691\n",
            "Epoch: 256 \tTraining Loss: 0.599387\n",
            "Epoch: 257 \tTraining Loss: 0.599056\n",
            "Epoch: 258 \tTraining Loss: 0.598617\n",
            "Epoch: 259 \tTraining Loss: 0.598122\n",
            "Epoch: 260 \tTraining Loss: 0.597685\n",
            "Epoch: 261 \tTraining Loss: 0.597405\n",
            "Epoch: 262 \tTraining Loss: 0.597173\n",
            "Epoch: 263 \tTraining Loss: 0.596882\n",
            "Epoch: 264 \tTraining Loss: 0.596600\n",
            "Epoch: 265 \tTraining Loss: 0.596375\n",
            "Epoch: 266 \tTraining Loss: 0.596107\n",
            "Epoch: 267 \tTraining Loss: 0.595786\n",
            "Epoch: 268 \tTraining Loss: 0.595452\n",
            "Epoch: 269 \tTraining Loss: 0.595113\n",
            "Epoch: 270 \tTraining Loss: 0.594844\n",
            "Epoch: 271 \tTraining Loss: 0.594562\n",
            "Epoch: 272 \tTraining Loss: 0.594254\n",
            "Epoch: 273 \tTraining Loss: 0.593952\n",
            "Epoch: 274 \tTraining Loss: 0.593675\n",
            "Epoch: 275 \tTraining Loss: 0.593361\n",
            "Epoch: 276 \tTraining Loss: 0.593027\n",
            "Epoch: 277 \tTraining Loss: 0.592721\n",
            "Epoch: 278 \tTraining Loss: 0.592423\n",
            "Epoch: 279 \tTraining Loss: 0.592137\n",
            "Epoch: 280 \tTraining Loss: 0.591840\n",
            "Epoch: 281 \tTraining Loss: 0.591594\n",
            "Epoch: 282 \tTraining Loss: 0.591346\n",
            "Epoch: 283 \tTraining Loss: 0.591093\n",
            "Epoch: 284 \tTraining Loss: 0.590792\n",
            "Epoch: 285 \tTraining Loss: 0.590502\n",
            "Epoch: 286 \tTraining Loss: 0.590200\n",
            "Epoch: 287 \tTraining Loss: 0.589890\n",
            "Epoch: 288 \tTraining Loss: 0.589584\n",
            "Epoch: 289 \tTraining Loss: 0.589278\n",
            "Epoch: 290 \tTraining Loss: 0.588993\n",
            "Epoch: 291 \tTraining Loss: 0.588718\n",
            "Epoch: 292 \tTraining Loss: 0.588433\n",
            "Epoch: 293 \tTraining Loss: 0.588171\n",
            "Epoch: 294 \tTraining Loss: 0.587969\n",
            "Epoch: 295 \tTraining Loss: 0.587942\n",
            "Epoch: 296 \tTraining Loss: 0.588053\n",
            "Epoch: 297 \tTraining Loss: 0.588114\n",
            "Epoch: 298 \tTraining Loss: 0.586976\n",
            "Epoch: 299 \tTraining Loss: 0.586557\n",
            "Epoch: 300 \tTraining Loss: 0.586841\n",
            "Epoch: 301 \tTraining Loss: 0.586119\n",
            "Epoch: 302 \tTraining Loss: 0.585665\n",
            "Epoch: 303 \tTraining Loss: 0.585764\n",
            "Epoch: 304 \tTraining Loss: 0.585162\n",
            "Epoch: 305 \tTraining Loss: 0.584836\n",
            "Epoch: 306 \tTraining Loss: 0.584797\n",
            "Epoch: 307 \tTraining Loss: 0.584229\n",
            "Epoch: 308 \tTraining Loss: 0.584035\n",
            "Epoch: 309 \tTraining Loss: 0.583885\n",
            "Epoch: 310 \tTraining Loss: 0.583382\n",
            "Epoch: 311 \tTraining Loss: 0.583192\n",
            "Epoch: 312 \tTraining Loss: 0.583039\n",
            "Epoch: 313 \tTraining Loss: 0.582597\n",
            "Epoch: 314 \tTraining Loss: 0.582394\n",
            "Epoch: 315 \tTraining Loss: 0.582154\n",
            "Epoch: 316 \tTraining Loss: 0.581678\n",
            "Epoch: 317 \tTraining Loss: 0.581429\n",
            "Epoch: 318 \tTraining Loss: 0.581182\n",
            "Epoch: 319 \tTraining Loss: 0.580884\n",
            "Epoch: 320 \tTraining Loss: 0.580625\n",
            "Epoch: 321 \tTraining Loss: 0.580542\n",
            "Epoch: 322 \tTraining Loss: 0.580204\n",
            "Epoch: 323 \tTraining Loss: 0.579901\n",
            "Epoch: 324 \tTraining Loss: 0.579649\n",
            "Epoch: 325 \tTraining Loss: 0.579444\n",
            "Epoch: 326 \tTraining Loss: 0.579047\n",
            "Epoch: 327 \tTraining Loss: 0.578905\n",
            "Epoch: 328 \tTraining Loss: 0.578654\n",
            "Epoch: 329 \tTraining Loss: 0.578352\n",
            "Epoch: 330 \tTraining Loss: 0.578065\n",
            "Epoch: 331 \tTraining Loss: 0.577908\n",
            "Epoch: 332 \tTraining Loss: 0.577572\n",
            "Epoch: 333 \tTraining Loss: 0.577418\n",
            "Epoch: 334 \tTraining Loss: 0.577083\n",
            "Epoch: 335 \tTraining Loss: 0.577214\n",
            "Epoch: 336 \tTraining Loss: 0.577315\n",
            "Epoch: 337 \tTraining Loss: 0.577375\n",
            "Epoch: 338 \tTraining Loss: 0.577262\n",
            "Epoch: 339 \tTraining Loss: 0.577136\n",
            "Epoch: 340 \tTraining Loss: 0.576865\n",
            "Epoch: 341 \tTraining Loss: 0.576591\n",
            "Epoch: 342 \tTraining Loss: 0.576312\n",
            "Epoch: 343 \tTraining Loss: 0.576014\n",
            "Epoch: 344 \tTraining Loss: 0.575750\n",
            "Epoch: 345 \tTraining Loss: 0.575508\n",
            "Epoch: 346 \tTraining Loss: 0.575234\n",
            "Epoch: 347 \tTraining Loss: 0.574976\n",
            "Epoch: 348 \tTraining Loss: 0.574721\n",
            "Epoch: 349 \tTraining Loss: 0.574455\n",
            "Epoch: 350 \tTraining Loss: 0.574185\n",
            "Epoch: 351 \tTraining Loss: 0.573898\n",
            "Epoch: 352 \tTraining Loss: 0.573635\n",
            "Epoch: 353 \tTraining Loss: 0.573378\n",
            "Epoch: 354 \tTraining Loss: 0.573117\n",
            "Epoch: 355 \tTraining Loss: 0.572856\n",
            "Epoch: 356 \tTraining Loss: 0.572589\n",
            "Epoch: 357 \tTraining Loss: 0.572320\n",
            "Epoch: 358 \tTraining Loss: 0.572064\n",
            "Epoch: 359 \tTraining Loss: 0.571845\n",
            "Epoch: 360 \tTraining Loss: 0.571657\n",
            "Epoch: 361 \tTraining Loss: 0.571662\n",
            "Epoch: 362 \tTraining Loss: 0.572020\n",
            "Epoch: 363 \tTraining Loss: 0.573193\n",
            "Epoch: 364 \tTraining Loss: 0.571791\n",
            "Epoch: 365 \tTraining Loss: 0.570391\n",
            "Epoch: 366 \tTraining Loss: 0.570666\n",
            "Epoch: 367 \tTraining Loss: 0.570757\n",
            "Epoch: 368 \tTraining Loss: 0.569891\n",
            "Epoch: 369 \tTraining Loss: 0.569570\n",
            "Epoch: 370 \tTraining Loss: 0.569858\n",
            "Epoch: 371 \tTraining Loss: 0.569286\n",
            "Epoch: 372 \tTraining Loss: 0.568755\n",
            "Epoch: 373 \tTraining Loss: 0.569012\n",
            "Epoch: 374 \tTraining Loss: 0.568518\n",
            "Epoch: 375 \tTraining Loss: 0.568002\n",
            "Epoch: 376 \tTraining Loss: 0.568182\n",
            "Epoch: 377 \tTraining Loss: 0.567739\n",
            "Epoch: 378 \tTraining Loss: 0.567237\n",
            "Epoch: 379 \tTraining Loss: 0.567378\n",
            "Epoch: 380 \tTraining Loss: 0.566925\n",
            "Epoch: 381 \tTraining Loss: 0.566441\n",
            "Epoch: 382 \tTraining Loss: 0.566482\n",
            "Epoch: 383 \tTraining Loss: 0.566043\n",
            "Epoch: 384 \tTraining Loss: 0.565522\n",
            "Epoch: 385 \tTraining Loss: 0.565576\n",
            "Epoch: 386 \tTraining Loss: 0.565219\n",
            "Epoch: 387 \tTraining Loss: 0.564742\n",
            "Epoch: 388 \tTraining Loss: 0.564755\n",
            "Epoch: 389 \tTraining Loss: 0.564504\n",
            "Epoch: 390 \tTraining Loss: 0.564046\n",
            "Epoch: 391 \tTraining Loss: 0.564032\n",
            "Epoch: 392 \tTraining Loss: 0.563787\n",
            "Epoch: 393 \tTraining Loss: 0.563333\n",
            "Epoch: 394 \tTraining Loss: 0.563287\n",
            "Epoch: 395 \tTraining Loss: 0.563102\n",
            "Epoch: 396 \tTraining Loss: 0.562629\n",
            "Epoch: 397 \tTraining Loss: 0.562496\n",
            "Epoch: 398 \tTraining Loss: 0.562349\n",
            "Epoch: 399 \tTraining Loss: 0.561922\n",
            "Epoch: 400 \tTraining Loss: 0.561716\n",
            "Epoch: 401 \tTraining Loss: 0.561601\n",
            "Epoch: 402 \tTraining Loss: 0.561235\n",
            "Epoch: 403 \tTraining Loss: 0.560953\n",
            "Epoch: 404 \tTraining Loss: 0.560823\n",
            "Epoch: 405 \tTraining Loss: 0.560565\n",
            "Epoch: 406 \tTraining Loss: 0.560253\n",
            "Epoch: 407 \tTraining Loss: 0.560048\n",
            "Epoch: 408 \tTraining Loss: 0.559861\n",
            "Epoch: 409 \tTraining Loss: 0.559586\n",
            "Epoch: 410 \tTraining Loss: 0.559300\n",
            "Epoch: 411 \tTraining Loss: 0.559093\n",
            "Epoch: 412 \tTraining Loss: 0.558889\n",
            "Epoch: 413 \tTraining Loss: 0.558613\n",
            "Epoch: 414 \tTraining Loss: 0.558338\n",
            "Epoch: 415 \tTraining Loss: 0.558112\n",
            "Epoch: 416 \tTraining Loss: 0.557902\n",
            "Epoch: 417 \tTraining Loss: 0.557665\n",
            "Epoch: 418 \tTraining Loss: 0.557399\n",
            "Epoch: 419 \tTraining Loss: 0.557156\n",
            "Epoch: 420 \tTraining Loss: 0.556934\n",
            "Epoch: 421 \tTraining Loss: 0.556705\n",
            "Epoch: 422 \tTraining Loss: 0.556466\n",
            "Epoch: 423 \tTraining Loss: 0.556214\n",
            "Epoch: 424 \tTraining Loss: 0.555965\n",
            "Epoch: 425 \tTraining Loss: 0.555731\n",
            "Epoch: 426 \tTraining Loss: 0.555505\n",
            "Epoch: 427 \tTraining Loss: 0.555281\n",
            "Epoch: 428 \tTraining Loss: 0.555043\n",
            "Epoch: 429 \tTraining Loss: 0.554800\n",
            "Epoch: 430 \tTraining Loss: 0.554560\n",
            "Epoch: 431 \tTraining Loss: 0.554317\n",
            "Epoch: 432 \tTraining Loss: 0.554078\n",
            "Epoch: 433 \tTraining Loss: 0.553843\n",
            "Epoch: 434 \tTraining Loss: 0.553611\n",
            "Epoch: 435 \tTraining Loss: 0.553380\n",
            "Epoch: 436 \tTraining Loss: 0.553149\n",
            "Epoch: 437 \tTraining Loss: 0.552925\n",
            "Epoch: 438 \tTraining Loss: 0.552708\n",
            "Epoch: 439 \tTraining Loss: 0.552503\n",
            "Epoch: 440 \tTraining Loss: 0.552334\n",
            "Epoch: 441 \tTraining Loss: 0.552208\n",
            "Epoch: 442 \tTraining Loss: 0.552213\n",
            "Epoch: 443 \tTraining Loss: 0.552242\n",
            "Epoch: 444 \tTraining Loss: 0.552441\n",
            "Epoch: 445 \tTraining Loss: 0.551979\n",
            "Epoch: 446 \tTraining Loss: 0.551324\n",
            "Epoch: 447 \tTraining Loss: 0.550629\n",
            "Epoch: 448 \tTraining Loss: 0.550506\n",
            "Epoch: 449 \tTraining Loss: 0.550666\n",
            "Epoch: 450 \tTraining Loss: 0.550428\n",
            "Epoch: 451 \tTraining Loss: 0.549913\n",
            "Epoch: 452 \tTraining Loss: 0.549442\n",
            "Epoch: 453 \tTraining Loss: 0.549349\n",
            "Epoch: 454 \tTraining Loss: 0.549352\n",
            "Epoch: 455 \tTraining Loss: 0.549014\n",
            "Epoch: 456 \tTraining Loss: 0.548559\n",
            "Epoch: 457 \tTraining Loss: 0.548302\n",
            "Epoch: 458 \tTraining Loss: 0.548245\n",
            "Epoch: 459 \tTraining Loss: 0.548094\n",
            "Epoch: 460 \tTraining Loss: 0.547711\n",
            "Epoch: 461 \tTraining Loss: 0.547378\n",
            "Epoch: 462 \tTraining Loss: 0.547214\n",
            "Epoch: 463 \tTraining Loss: 0.547073\n",
            "Epoch: 464 \tTraining Loss: 0.546838\n",
            "Epoch: 465 \tTraining Loss: 0.546513\n",
            "Epoch: 466 \tTraining Loss: 0.546236\n",
            "Epoch: 467 \tTraining Loss: 0.546053\n",
            "Epoch: 468 \tTraining Loss: 0.545885\n",
            "Epoch: 469 \tTraining Loss: 0.545653\n",
            "Epoch: 470 \tTraining Loss: 0.545364\n",
            "Epoch: 471 \tTraining Loss: 0.545105\n",
            "Epoch: 472 \tTraining Loss: 0.544894\n",
            "Epoch: 473 \tTraining Loss: 0.544696\n",
            "Epoch: 474 \tTraining Loss: 0.544483\n",
            "Epoch: 475 \tTraining Loss: 0.544249\n",
            "Epoch: 476 \tTraining Loss: 0.544005\n",
            "Epoch: 477 \tTraining Loss: 0.543758\n",
            "Epoch: 478 \tTraining Loss: 0.543524\n",
            "Epoch: 479 \tTraining Loss: 0.543306\n",
            "Epoch: 480 \tTraining Loss: 0.543098\n",
            "Epoch: 481 \tTraining Loss: 0.542888\n",
            "Epoch: 482 \tTraining Loss: 0.542656\n",
            "Epoch: 483 \tTraining Loss: 0.542418\n",
            "Epoch: 484 \tTraining Loss: 0.542184\n",
            "Epoch: 485 \tTraining Loss: 0.541949\n",
            "Epoch: 486 \tTraining Loss: 0.541713\n",
            "Epoch: 487 \tTraining Loss: 0.541480\n",
            "Epoch: 488 \tTraining Loss: 0.541254\n",
            "Epoch: 489 \tTraining Loss: 0.541027\n",
            "Epoch: 490 \tTraining Loss: 0.540799\n",
            "Epoch: 491 \tTraining Loss: 0.540574\n",
            "Epoch: 492 \tTraining Loss: 0.540358\n",
            "Epoch: 493 \tTraining Loss: 0.540149\n",
            "Epoch: 494 \tTraining Loss: 0.539951\n",
            "Epoch: 495 \tTraining Loss: 0.539764\n",
            "Epoch: 496 \tTraining Loss: 0.539613\n",
            "Epoch: 497 \tTraining Loss: 0.539501\n",
            "Epoch: 498 \tTraining Loss: 0.539503\n",
            "Epoch: 499 \tTraining Loss: 0.539528\n",
            "Epoch: 500 \tTraining Loss: 0.539672\n",
            "Epoch: 501 \tTraining Loss: 0.539368\n",
            "Epoch: 502 \tTraining Loss: 0.538831\n",
            "Epoch: 503 \tTraining Loss: 0.538007\n",
            "Epoch: 504 \tTraining Loss: 0.537642\n",
            "Epoch: 505 \tTraining Loss: 0.537758\n",
            "Epoch: 506 \tTraining Loss: 0.537800\n",
            "Epoch: 507 \tTraining Loss: 0.537494\n",
            "Epoch: 508 \tTraining Loss: 0.536879\n",
            "Epoch: 509 \tTraining Loss: 0.536544\n",
            "Epoch: 510 \tTraining Loss: 0.536553\n",
            "Epoch: 511 \tTraining Loss: 0.536481\n",
            "Epoch: 512 \tTraining Loss: 0.536146\n",
            "Epoch: 513 \tTraining Loss: 0.535700\n",
            "Epoch: 514 \tTraining Loss: 0.535470\n",
            "Epoch: 515 \tTraining Loss: 0.535416\n",
            "Epoch: 516 \tTraining Loss: 0.535261\n",
            "Epoch: 517 \tTraining Loss: 0.534942\n",
            "Epoch: 518 \tTraining Loss: 0.534582\n",
            "Epoch: 519 \tTraining Loss: 0.534306\n",
            "Epoch: 520 \tTraining Loss: 0.534290\n",
            "Epoch: 521 \tTraining Loss: 0.534027\n",
            "Epoch: 522 \tTraining Loss: 0.533787\n",
            "Epoch: 523 \tTraining Loss: 0.533502\n",
            "Epoch: 524 \tTraining Loss: 0.533258\n",
            "Epoch: 525 \tTraining Loss: 0.533078\n",
            "Epoch: 526 \tTraining Loss: 0.532895\n",
            "Epoch: 527 \tTraining Loss: 0.532640\n",
            "Epoch: 528 \tTraining Loss: 0.532366\n",
            "Epoch: 529 \tTraining Loss: 0.532119\n",
            "Epoch: 530 \tTraining Loss: 0.531877\n",
            "Epoch: 531 \tTraining Loss: 0.531652\n",
            "Epoch: 532 \tTraining Loss: 0.531461\n",
            "Epoch: 533 \tTraining Loss: 0.531268\n",
            "Epoch: 534 \tTraining Loss: 0.531051\n",
            "Epoch: 535 \tTraining Loss: 0.530798\n",
            "Epoch: 536 \tTraining Loss: 0.530534\n",
            "Epoch: 537 \tTraining Loss: 0.530278\n",
            "Epoch: 538 \tTraining Loss: 0.530040\n",
            "Epoch: 539 \tTraining Loss: 0.529814\n",
            "Epoch: 540 \tTraining Loss: 0.529591\n",
            "Epoch: 541 \tTraining Loss: 0.529372\n",
            "Epoch: 542 \tTraining Loss: 0.529165\n",
            "Epoch: 543 \tTraining Loss: 0.528979\n",
            "Epoch: 544 \tTraining Loss: 0.528821\n",
            "Epoch: 545 \tTraining Loss: 0.528712\n",
            "Epoch: 546 \tTraining Loss: 0.528663\n",
            "Epoch: 547 \tTraining Loss: 0.528739\n",
            "Epoch: 548 \tTraining Loss: 0.528826\n",
            "Epoch: 549 \tTraining Loss: 0.528947\n",
            "Epoch: 550 \tTraining Loss: 0.528496\n",
            "Epoch: 551 \tTraining Loss: 0.527740\n",
            "Epoch: 552 \tTraining Loss: 0.526951\n",
            "Epoch: 553 \tTraining Loss: 0.526763\n",
            "Epoch: 554 \tTraining Loss: 0.526974\n",
            "Epoch: 555 \tTraining Loss: 0.526928\n",
            "Epoch: 556 \tTraining Loss: 0.526460\n",
            "Epoch: 557 \tTraining Loss: 0.525812\n",
            "Epoch: 558 \tTraining Loss: 0.525545\n",
            "Epoch: 559 \tTraining Loss: 0.525588\n",
            "Epoch: 560 \tTraining Loss: 0.525480\n",
            "Epoch: 561 \tTraining Loss: 0.525113\n",
            "Epoch: 562 \tTraining Loss: 0.524659\n",
            "Epoch: 563 \tTraining Loss: 0.524408\n",
            "Epoch: 564 \tTraining Loss: 0.524323\n",
            "Epoch: 565 \tTraining Loss: 0.524168\n",
            "Epoch: 566 \tTraining Loss: 0.523875\n",
            "Epoch: 567 \tTraining Loss: 0.523523\n",
            "Epoch: 568 \tTraining Loss: 0.523268\n",
            "Epoch: 569 \tTraining Loss: 0.523116\n",
            "Epoch: 570 \tTraining Loss: 0.522967\n",
            "Epoch: 571 \tTraining Loss: 0.522749\n",
            "Epoch: 572 \tTraining Loss: 0.522450\n",
            "Epoch: 573 \tTraining Loss: 0.522146\n",
            "Epoch: 574 \tTraining Loss: 0.521919\n",
            "Epoch: 575 \tTraining Loss: 0.521753\n",
            "Epoch: 576 \tTraining Loss: 0.521579\n",
            "Epoch: 577 \tTraining Loss: 0.521360\n",
            "Epoch: 578 \tTraining Loss: 0.521113\n",
            "Epoch: 579 \tTraining Loss: 0.520846\n",
            "Epoch: 580 \tTraining Loss: 0.520585\n",
            "Epoch: 581 \tTraining Loss: 0.520360\n",
            "Epoch: 582 \tTraining Loss: 0.520161\n",
            "Epoch: 583 \tTraining Loss: 0.519961\n",
            "Epoch: 584 \tTraining Loss: 0.519751\n",
            "Epoch: 585 \tTraining Loss: 0.519540\n",
            "Epoch: 586 \tTraining Loss: 0.519312\n",
            "Epoch: 587 \tTraining Loss: 0.519073\n",
            "Epoch: 588 \tTraining Loss: 0.518820\n",
            "Epoch: 589 \tTraining Loss: 0.518568\n",
            "Epoch: 590 \tTraining Loss: 0.518312\n",
            "Epoch: 591 \tTraining Loss: 0.518048\n",
            "Epoch: 592 \tTraining Loss: 0.517938\n",
            "Epoch: 593 \tTraining Loss: 0.518193\n",
            "Epoch: 594 \tTraining Loss: 0.517690\n",
            "Epoch: 595 \tTraining Loss: 0.517319\n",
            "Epoch: 596 \tTraining Loss: 0.517308\n",
            "Epoch: 597 \tTraining Loss: 0.517033\n",
            "Epoch: 598 \tTraining Loss: 0.516997\n",
            "Epoch: 599 \tTraining Loss: 0.516891\n",
            "Epoch: 600 \tTraining Loss: 0.517052\n",
            "Epoch: 601 \tTraining Loss: 0.517386\n",
            "Epoch: 602 \tTraining Loss: 0.517784\n",
            "Epoch: 603 \tTraining Loss: 0.518184\n",
            "Epoch: 604 \tTraining Loss: 0.517135\n",
            "Epoch: 605 \tTraining Loss: 0.515594\n",
            "Epoch: 606 \tTraining Loss: 0.515052\n",
            "Epoch: 607 \tTraining Loss: 0.515635\n",
            "Epoch: 608 \tTraining Loss: 0.515859\n",
            "Epoch: 609 \tTraining Loss: 0.514797\n",
            "Epoch: 610 \tTraining Loss: 0.514028\n",
            "Epoch: 611 \tTraining Loss: 0.514284\n",
            "Epoch: 612 \tTraining Loss: 0.514423\n",
            "Epoch: 613 \tTraining Loss: 0.513772\n",
            "Epoch: 614 \tTraining Loss: 0.513179\n",
            "Epoch: 615 \tTraining Loss: 0.513351\n",
            "Epoch: 616 \tTraining Loss: 0.513341\n",
            "Epoch: 617 \tTraining Loss: 0.512754\n",
            "Epoch: 618 \tTraining Loss: 0.512355\n",
            "Epoch: 619 \tTraining Loss: 0.512317\n",
            "Epoch: 620 \tTraining Loss: 0.512270\n",
            "Epoch: 621 \tTraining Loss: 0.511836\n",
            "Epoch: 622 \tTraining Loss: 0.511466\n",
            "Epoch: 623 \tTraining Loss: 0.511389\n",
            "Epoch: 624 \tTraining Loss: 0.511279\n",
            "Epoch: 625 \tTraining Loss: 0.510979\n",
            "Epoch: 626 \tTraining Loss: 0.510640\n",
            "Epoch: 627 \tTraining Loss: 0.510468\n",
            "Epoch: 628 \tTraining Loss: 0.510374\n",
            "Epoch: 629 \tTraining Loss: 0.510162\n",
            "Epoch: 630 \tTraining Loss: 0.509818\n",
            "Epoch: 631 \tTraining Loss: 0.509582\n",
            "Epoch: 632 \tTraining Loss: 0.509459\n",
            "Epoch: 633 \tTraining Loss: 0.509283\n",
            "Epoch: 634 \tTraining Loss: 0.509021\n",
            "Epoch: 635 \tTraining Loss: 0.508762\n",
            "Epoch: 636 \tTraining Loss: 0.508552\n",
            "Epoch: 637 \tTraining Loss: 0.508390\n",
            "Epoch: 638 \tTraining Loss: 0.508206\n",
            "Epoch: 639 \tTraining Loss: 0.507960\n",
            "Epoch: 640 \tTraining Loss: 0.507707\n",
            "Epoch: 641 \tTraining Loss: 0.507492\n",
            "Epoch: 642 \tTraining Loss: 0.507306\n",
            "Epoch: 643 \tTraining Loss: 0.507115\n",
            "Epoch: 644 \tTraining Loss: 0.506909\n",
            "Epoch: 645 \tTraining Loss: 0.506678\n",
            "Epoch: 646 \tTraining Loss: 0.506449\n",
            "Epoch: 647 \tTraining Loss: 0.506230\n",
            "Epoch: 648 \tTraining Loss: 0.506029\n",
            "Epoch: 649 \tTraining Loss: 0.505831\n",
            "Epoch: 650 \tTraining Loss: 0.505631\n",
            "Epoch: 651 \tTraining Loss: 0.505422\n",
            "Epoch: 652 \tTraining Loss: 0.505208\n",
            "Epoch: 653 \tTraining Loss: 0.504988\n",
            "Epoch: 654 \tTraining Loss: 0.504765\n",
            "Epoch: 655 \tTraining Loss: 0.504544\n",
            "Epoch: 656 \tTraining Loss: 0.504329\n",
            "Epoch: 657 \tTraining Loss: 0.504114\n",
            "Epoch: 658 \tTraining Loss: 0.503899\n",
            "Epoch: 659 \tTraining Loss: 0.503687\n",
            "Epoch: 660 \tTraining Loss: 0.503476\n",
            "Epoch: 661 \tTraining Loss: 0.503265\n",
            "Epoch: 662 \tTraining Loss: 0.503058\n",
            "Epoch: 663 \tTraining Loss: 0.502857\n",
            "Epoch: 664 \tTraining Loss: 0.502669\n",
            "Epoch: 665 \tTraining Loss: 0.502508\n",
            "Epoch: 666 \tTraining Loss: 0.502412\n",
            "Epoch: 667 \tTraining Loss: 0.502435\n",
            "Epoch: 668 \tTraining Loss: 0.502792\n",
            "Epoch: 669 \tTraining Loss: 0.503540\n",
            "Epoch: 670 \tTraining Loss: 0.505332\n",
            "Epoch: 671 \tTraining Loss: 0.505762\n",
            "Epoch: 672 \tTraining Loss: 0.504855\n",
            "Epoch: 673 \tTraining Loss: 0.501198\n",
            "Epoch: 674 \tTraining Loss: 0.501720\n",
            "Epoch: 675 \tTraining Loss: 0.503631\n",
            "Epoch: 676 \tTraining Loss: 0.501451\n",
            "Epoch: 677 \tTraining Loss: 0.500470\n",
            "Epoch: 678 \tTraining Loss: 0.501649\n",
            "Epoch: 679 \tTraining Loss: 0.500722\n",
            "Epoch: 680 \tTraining Loss: 0.499683\n",
            "Epoch: 681 \tTraining Loss: 0.500337\n",
            "Epoch: 682 \tTraining Loss: 0.499900\n",
            "Epoch: 683 \tTraining Loss: 0.498956\n",
            "Epoch: 684 \tTraining Loss: 0.499368\n",
            "Epoch: 685 \tTraining Loss: 0.499108\n",
            "Epoch: 686 \tTraining Loss: 0.498397\n",
            "Epoch: 687 \tTraining Loss: 0.498364\n",
            "Epoch: 688 \tTraining Loss: 0.498359\n",
            "Epoch: 689 \tTraining Loss: 0.497844\n",
            "Epoch: 690 \tTraining Loss: 0.497564\n",
            "Epoch: 691 \tTraining Loss: 0.497651\n",
            "Epoch: 692 \tTraining Loss: 0.497292\n",
            "Epoch: 693 \tTraining Loss: 0.496915\n",
            "Epoch: 694 \tTraining Loss: 0.496941\n",
            "Epoch: 695 \tTraining Loss: 0.496711\n",
            "Epoch: 696 \tTraining Loss: 0.496362\n",
            "Epoch: 697 \tTraining Loss: 0.496213\n",
            "Epoch: 698 \tTraining Loss: 0.496128\n",
            "Epoch: 699 \tTraining Loss: 0.495820\n",
            "Epoch: 700 \tTraining Loss: 0.495565\n",
            "Epoch: 701 \tTraining Loss: 0.495458\n",
            "Epoch: 702 \tTraining Loss: 0.495265\n",
            "Epoch: 703 \tTraining Loss: 0.495006\n",
            "Epoch: 704 \tTraining Loss: 0.494804\n",
            "Epoch: 705 \tTraining Loss: 0.494677\n",
            "Epoch: 706 \tTraining Loss: 0.494464\n",
            "Epoch: 707 \tTraining Loss: 0.494233\n",
            "Epoch: 708 \tTraining Loss: 0.494036\n",
            "Epoch: 709 \tTraining Loss: 0.493906\n",
            "Epoch: 710 \tTraining Loss: 0.493682\n",
            "Epoch: 711 \tTraining Loss: 0.493467\n",
            "Epoch: 712 \tTraining Loss: 0.493285\n",
            "Epoch: 713 \tTraining Loss: 0.493136\n",
            "Epoch: 714 \tTraining Loss: 0.492930\n",
            "Epoch: 715 \tTraining Loss: 0.492716\n",
            "Epoch: 716 \tTraining Loss: 0.492536\n",
            "Epoch: 717 \tTraining Loss: 0.492372\n",
            "Epoch: 718 \tTraining Loss: 0.492187\n",
            "Epoch: 719 \tTraining Loss: 0.491968\n",
            "Epoch: 720 \tTraining Loss: 0.491789\n",
            "Epoch: 721 \tTraining Loss: 0.491615\n",
            "Epoch: 722 \tTraining Loss: 0.491440\n",
            "Epoch: 723 \tTraining Loss: 0.491236\n",
            "Epoch: 724 \tTraining Loss: 0.491045\n",
            "Epoch: 725 \tTraining Loss: 0.490864\n",
            "Epoch: 726 \tTraining Loss: 0.490689\n",
            "Epoch: 727 \tTraining Loss: 0.490505\n",
            "Epoch: 728 \tTraining Loss: 0.490313\n",
            "Epoch: 729 \tTraining Loss: 0.490126\n",
            "Epoch: 730 \tTraining Loss: 0.489941\n",
            "Epoch: 731 \tTraining Loss: 0.489761\n",
            "Epoch: 732 \tTraining Loss: 0.489580\n",
            "Epoch: 733 \tTraining Loss: 0.489396\n",
            "Epoch: 734 \tTraining Loss: 0.489209\n",
            "Epoch: 735 \tTraining Loss: 0.489022\n",
            "Epoch: 736 \tTraining Loss: 0.488838\n",
            "Epoch: 737 \tTraining Loss: 0.488656\n",
            "Epoch: 738 \tTraining Loss: 0.488476\n",
            "Epoch: 739 \tTraining Loss: 0.488294\n",
            "Epoch: 740 \tTraining Loss: 0.488110\n",
            "Epoch: 741 \tTraining Loss: 0.487924\n",
            "Epoch: 742 \tTraining Loss: 0.487740\n",
            "Epoch: 743 \tTraining Loss: 0.487556\n",
            "Epoch: 744 \tTraining Loss: 0.487374\n",
            "Epoch: 745 \tTraining Loss: 0.487192\n",
            "Epoch: 746 \tTraining Loss: 0.487010\n",
            "Epoch: 747 \tTraining Loss: 0.486828\n",
            "Epoch: 748 \tTraining Loss: 0.486646\n",
            "Epoch: 749 \tTraining Loss: 0.486464\n",
            "Epoch: 750 \tTraining Loss: 0.486282\n",
            "Epoch: 751 \tTraining Loss: 0.486100\n",
            "Epoch: 752 \tTraining Loss: 0.485919\n",
            "Epoch: 753 \tTraining Loss: 0.485738\n",
            "Epoch: 754 \tTraining Loss: 0.485558\n",
            "Epoch: 755 \tTraining Loss: 0.485380\n",
            "Epoch: 756 \tTraining Loss: 0.485206\n",
            "Epoch: 757 \tTraining Loss: 0.485042\n",
            "Epoch: 758 \tTraining Loss: 0.484895\n",
            "Epoch: 759 \tTraining Loss: 0.484778\n",
            "Epoch: 760 \tTraining Loss: 0.484742\n",
            "Epoch: 761 \tTraining Loss: 0.484832\n",
            "Epoch: 762 \tTraining Loss: 0.485315\n",
            "Epoch: 763 \tTraining Loss: 0.486210\n",
            "Epoch: 764 \tTraining Loss: 0.488662\n",
            "Epoch: 765 \tTraining Loss: 0.489394\n",
            "Epoch: 766 \tTraining Loss: 0.489441\n",
            "Epoch: 767 \tTraining Loss: 0.484204\n",
            "Epoch: 768 \tTraining Loss: 0.484807\n",
            "Epoch: 769 \tTraining Loss: 0.487743\n",
            "Epoch: 770 \tTraining Loss: 0.484431\n",
            "Epoch: 771 \tTraining Loss: 0.483759\n",
            "Epoch: 772 \tTraining Loss: 0.485641\n",
            "Epoch: 773 \tTraining Loss: 0.483205\n",
            "Epoch: 774 \tTraining Loss: 0.483279\n",
            "Epoch: 775 \tTraining Loss: 0.484278\n",
            "Epoch: 776 \tTraining Loss: 0.482242\n",
            "Epoch: 777 \tTraining Loss: 0.482798\n",
            "Epoch: 778 \tTraining Loss: 0.483155\n",
            "Epoch: 779 \tTraining Loss: 0.481696\n",
            "Epoch: 780 \tTraining Loss: 0.481897\n",
            "Epoch: 781 \tTraining Loss: 0.482244\n",
            "Epoch: 782 \tTraining Loss: 0.480995\n",
            "Epoch: 783 \tTraining Loss: 0.481403\n",
            "Epoch: 784 \tTraining Loss: 0.481329\n",
            "Epoch: 785 \tTraining Loss: 0.480363\n",
            "Epoch: 786 \tTraining Loss: 0.480562\n",
            "Epoch: 787 \tTraining Loss: 0.480202\n",
            "Epoch: 788 \tTraining Loss: 0.479569\n",
            "Epoch: 789 \tTraining Loss: 0.479592\n",
            "Epoch: 790 \tTraining Loss: 0.479396\n",
            "Epoch: 791 \tTraining Loss: 0.478965\n",
            "Epoch: 792 \tTraining Loss: 0.478893\n",
            "Epoch: 793 \tTraining Loss: 0.478694\n",
            "Epoch: 794 \tTraining Loss: 0.478396\n",
            "Epoch: 795 \tTraining Loss: 0.478020\n",
            "Epoch: 796 \tTraining Loss: 0.478141\n",
            "Epoch: 797 \tTraining Loss: 0.477660\n",
            "Epoch: 798 \tTraining Loss: 0.477448\n",
            "Epoch: 799 \tTraining Loss: 0.477463\n",
            "Epoch: 800 \tTraining Loss: 0.477121\n",
            "Epoch: 801 \tTraining Loss: 0.476845\n",
            "Epoch: 802 \tTraining Loss: 0.476771\n",
            "Epoch: 803 \tTraining Loss: 0.476567\n",
            "Epoch: 804 \tTraining Loss: 0.476191\n",
            "Epoch: 805 \tTraining Loss: 0.476207\n",
            "Epoch: 806 \tTraining Loss: 0.476051\n",
            "Epoch: 807 \tTraining Loss: 0.475699\n",
            "Epoch: 808 \tTraining Loss: 0.475628\n",
            "Epoch: 809 \tTraining Loss: 0.475442\n",
            "Epoch: 810 \tTraining Loss: 0.475199\n",
            "Epoch: 811 \tTraining Loss: 0.475047\n",
            "Epoch: 812 \tTraining Loss: 0.474823\n",
            "Epoch: 813 \tTraining Loss: 0.474720\n",
            "Epoch: 814 \tTraining Loss: 0.474477\n",
            "Epoch: 815 \tTraining Loss: 0.474269\n",
            "Epoch: 816 \tTraining Loss: 0.474170\n",
            "Epoch: 817 \tTraining Loss: 0.473947\n",
            "Epoch: 818 \tTraining Loss: 0.473748\n",
            "Epoch: 819 \tTraining Loss: 0.473589\n",
            "Epoch: 820 \tTraining Loss: 0.473416\n",
            "Epoch: 821 \tTraining Loss: 0.473245\n",
            "Epoch: 822 \tTraining Loss: 0.473038\n",
            "Epoch: 823 \tTraining Loss: 0.472878\n",
            "Epoch: 824 \tTraining Loss: 0.472719\n",
            "Epoch: 825 \tTraining Loss: 0.472530\n",
            "Epoch: 826 \tTraining Loss: 0.472355\n",
            "Epoch: 827 \tTraining Loss: 0.472178\n",
            "Epoch: 828 \tTraining Loss: 0.472025\n",
            "Epoch: 829 \tTraining Loss: 0.471851\n",
            "Epoch: 830 \tTraining Loss: 0.471661\n",
            "Epoch: 831 \tTraining Loss: 0.471502\n",
            "Epoch: 832 \tTraining Loss: 0.471337\n",
            "Epoch: 833 \tTraining Loss: 0.471161\n",
            "Epoch: 834 \tTraining Loss: 0.470993\n",
            "Epoch: 835 \tTraining Loss: 0.470818\n",
            "Epoch: 836 \tTraining Loss: 0.470647\n",
            "Epoch: 837 \tTraining Loss: 0.470491\n",
            "Epoch: 838 \tTraining Loss: 0.470315\n",
            "Epoch: 839 \tTraining Loss: 0.470141\n",
            "Epoch: 840 \tTraining Loss: 0.469978\n",
            "Epoch: 841 \tTraining Loss: 0.469809\n",
            "Epoch: 842 \tTraining Loss: 0.469644\n",
            "Epoch: 843 \tTraining Loss: 0.469477\n",
            "Epoch: 844 \tTraining Loss: 0.469306\n",
            "Epoch: 845 \tTraining Loss: 0.469140\n",
            "Epoch: 846 \tTraining Loss: 0.468975\n",
            "Epoch: 847 \tTraining Loss: 0.468809\n",
            "Epoch: 848 \tTraining Loss: 0.468645\n",
            "Epoch: 849 \tTraining Loss: 0.468478\n",
            "Epoch: 850 \tTraining Loss: 0.468310\n",
            "Epoch: 851 \tTraining Loss: 0.468146\n",
            "Epoch: 852 \tTraining Loss: 0.467983\n",
            "Epoch: 853 \tTraining Loss: 0.467817\n",
            "Epoch: 854 \tTraining Loss: 0.467654\n",
            "Epoch: 855 \tTraining Loss: 0.467490\n",
            "Epoch: 856 \tTraining Loss: 0.467324\n",
            "Epoch: 857 \tTraining Loss: 0.467158\n",
            "Epoch: 858 \tTraining Loss: 0.466981\n",
            "Epoch: 859 \tTraining Loss: 0.466873\n",
            "Epoch: 860 \tTraining Loss: 0.466749\n",
            "Epoch: 861 \tTraining Loss: 0.466651\n",
            "Epoch: 862 \tTraining Loss: 0.466465\n",
            "Epoch: 863 \tTraining Loss: 0.466281\n",
            "Epoch: 864 \tTraining Loss: 0.466137\n",
            "Epoch: 865 \tTraining Loss: 0.465973\n",
            "Epoch: 866 \tTraining Loss: 0.465786\n",
            "Epoch: 867 \tTraining Loss: 0.465602\n",
            "Epoch: 868 \tTraining Loss: 0.465446\n",
            "Epoch: 869 \tTraining Loss: 0.465292\n",
            "Epoch: 870 \tTraining Loss: 0.465121\n",
            "Epoch: 871 \tTraining Loss: 0.464950\n",
            "Epoch: 872 \tTraining Loss: 0.464802\n",
            "Epoch: 873 \tTraining Loss: 0.464652\n",
            "Epoch: 874 \tTraining Loss: 0.464488\n",
            "Epoch: 875 \tTraining Loss: 0.464324\n",
            "Epoch: 876 \tTraining Loss: 0.464178\n",
            "Epoch: 877 \tTraining Loss: 0.464035\n",
            "Epoch: 878 \tTraining Loss: 0.463885\n",
            "Epoch: 879 \tTraining Loss: 0.463734\n",
            "Epoch: 880 \tTraining Loss: 0.463598\n",
            "Epoch: 881 \tTraining Loss: 0.463477\n",
            "Epoch: 882 \tTraining Loss: 0.463384\n",
            "Epoch: 883 \tTraining Loss: 0.463312\n",
            "Epoch: 884 \tTraining Loss: 0.463325\n",
            "Epoch: 885 \tTraining Loss: 0.463393\n",
            "Epoch: 886 \tTraining Loss: 0.463757\n",
            "Epoch: 887 \tTraining Loss: 0.464113\n",
            "Epoch: 888 \tTraining Loss: 0.465238\n",
            "Epoch: 889 \tTraining Loss: 0.465364\n",
            "Epoch: 890 \tTraining Loss: 0.465486\n",
            "Epoch: 891 \tTraining Loss: 0.463254\n",
            "Epoch: 892 \tTraining Loss: 0.461855\n",
            "Epoch: 893 \tTraining Loss: 0.462628\n",
            "Epoch: 894 \tTraining Loss: 0.463468\n",
            "Epoch: 895 \tTraining Loss: 0.463057\n",
            "Epoch: 896 \tTraining Loss: 0.461538\n",
            "Epoch: 897 \tTraining Loss: 0.461436\n",
            "Epoch: 898 \tTraining Loss: 0.462364\n",
            "Epoch: 899 \tTraining Loss: 0.462097\n",
            "Epoch: 900 \tTraining Loss: 0.461092\n",
            "Epoch: 901 \tTraining Loss: 0.460536\n",
            "Epoch: 902 \tTraining Loss: 0.461105\n",
            "Epoch: 903 \tTraining Loss: 0.461391\n",
            "Epoch: 904 \tTraining Loss: 0.460494\n",
            "Epoch: 905 \tTraining Loss: 0.459932\n",
            "Epoch: 906 \tTraining Loss: 0.460302\n",
            "Epoch: 907 \tTraining Loss: 0.460356\n",
            "Epoch: 908 \tTraining Loss: 0.459795\n",
            "Epoch: 909 \tTraining Loss: 0.459352\n",
            "Epoch: 910 \tTraining Loss: 0.459447\n",
            "Epoch: 911 \tTraining Loss: 0.459619\n",
            "Epoch: 912 \tTraining Loss: 0.459237\n",
            "Epoch: 913 \tTraining Loss: 0.458790\n",
            "Epoch: 914 \tTraining Loss: 0.458687\n",
            "Epoch: 915 \tTraining Loss: 0.458737\n",
            "Epoch: 916 \tTraining Loss: 0.458663\n",
            "Epoch: 917 \tTraining Loss: 0.458304\n",
            "Epoch: 918 \tTraining Loss: 0.458047\n",
            "Epoch: 919 \tTraining Loss: 0.457999\n",
            "Epoch: 920 \tTraining Loss: 0.457965\n",
            "Epoch: 921 \tTraining Loss: 0.457788\n",
            "Epoch: 922 \tTraining Loss: 0.457506\n",
            "Epoch: 923 \tTraining Loss: 0.457344\n",
            "Epoch: 924 \tTraining Loss: 0.457294\n",
            "Epoch: 925 \tTraining Loss: 0.457191\n",
            "Epoch: 926 \tTraining Loss: 0.456999\n",
            "Epoch: 927 \tTraining Loss: 0.456790\n",
            "Epoch: 928 \tTraining Loss: 0.456642\n",
            "Epoch: 929 \tTraining Loss: 0.456553\n",
            "Epoch: 930 \tTraining Loss: 0.456439\n",
            "Epoch: 931 \tTraining Loss: 0.456277\n",
            "Epoch: 932 \tTraining Loss: 0.456094\n",
            "Epoch: 933 \tTraining Loss: 0.455936\n",
            "Epoch: 934 \tTraining Loss: 0.455823\n",
            "Epoch: 935 \tTraining Loss: 0.455709\n",
            "Epoch: 936 \tTraining Loss: 0.455573\n",
            "Epoch: 937 \tTraining Loss: 0.455408\n",
            "Epoch: 938 \tTraining Loss: 0.455243\n",
            "Epoch: 939 \tTraining Loss: 0.455103\n",
            "Epoch: 940 \tTraining Loss: 0.454980\n",
            "Epoch: 941 \tTraining Loss: 0.454857\n",
            "Epoch: 942 \tTraining Loss: 0.454717\n",
            "Epoch: 943 \tTraining Loss: 0.454564\n",
            "Epoch: 944 \tTraining Loss: 0.454409\n",
            "Epoch: 945 \tTraining Loss: 0.454261\n",
            "Epoch: 946 \tTraining Loss: 0.454124\n",
            "Epoch: 947 \tTraining Loss: 0.453994\n",
            "Epoch: 948 \tTraining Loss: 0.453863\n",
            "Epoch: 949 \tTraining Loss: 0.453725\n",
            "Epoch: 950 \tTraining Loss: 0.453583\n",
            "Epoch: 951 \tTraining Loss: 0.453435\n",
            "Epoch: 952 \tTraining Loss: 0.453289\n",
            "Epoch: 953 \tTraining Loss: 0.453146\n",
            "Epoch: 954 \tTraining Loss: 0.453006\n",
            "Epoch: 955 \tTraining Loss: 0.452870\n",
            "Epoch: 956 \tTraining Loss: 0.452734\n",
            "Epoch: 957 \tTraining Loss: 0.452598\n",
            "Epoch: 958 \tTraining Loss: 0.452462\n",
            "Epoch: 959 \tTraining Loss: 0.452325\n",
            "Epoch: 960 \tTraining Loss: 0.452187\n",
            "Epoch: 961 \tTraining Loss: 0.452049\n",
            "Epoch: 962 \tTraining Loss: 0.451911\n",
            "Epoch: 963 \tTraining Loss: 0.451772\n",
            "Epoch: 964 \tTraining Loss: 0.451635\n",
            "Epoch: 965 \tTraining Loss: 0.451498\n",
            "Epoch: 966 \tTraining Loss: 0.451363\n",
            "Epoch: 967 \tTraining Loss: 0.451229\n",
            "Epoch: 968 \tTraining Loss: 0.451099\n",
            "Epoch: 969 \tTraining Loss: 0.450973\n",
            "Epoch: 970 \tTraining Loss: 0.450856\n",
            "Epoch: 971 \tTraining Loss: 0.450753\n",
            "Epoch: 972 \tTraining Loss: 0.450673\n",
            "Epoch: 973 \tTraining Loss: 0.450640\n",
            "Epoch: 974 \tTraining Loss: 0.450666\n",
            "Epoch: 975 \tTraining Loss: 0.450841\n",
            "Epoch: 976 \tTraining Loss: 0.451130\n",
            "Epoch: 977 \tTraining Loss: 0.451857\n",
            "Epoch: 978 \tTraining Loss: 0.452545\n",
            "Epoch: 979 \tTraining Loss: 0.454364\n",
            "Epoch: 980 \tTraining Loss: 0.454425\n",
            "Epoch: 981 \tTraining Loss: 0.454484\n",
            "Epoch: 982 \tTraining Loss: 0.450859\n",
            "Epoch: 983 \tTraining Loss: 0.449739\n",
            "Epoch: 984 \tTraining Loss: 0.452002\n",
            "Epoch: 985 \tTraining Loss: 0.452065\n",
            "Epoch: 986 \tTraining Loss: 0.450088\n",
            "Epoch: 987 \tTraining Loss: 0.449590\n",
            "Epoch: 988 \tTraining Loss: 0.450560\n",
            "Epoch: 989 \tTraining Loss: 0.450711\n",
            "Epoch: 990 \tTraining Loss: 0.449088\n",
            "Epoch: 991 \tTraining Loss: 0.448944\n",
            "Epoch: 992 \tTraining Loss: 0.449952\n",
            "Epoch: 993 \tTraining Loss: 0.448669\n",
            "Epoch: 994 \tTraining Loss: 0.448345\n",
            "Epoch: 995 \tTraining Loss: 0.448764\n",
            "Epoch: 996 \tTraining Loss: 0.448309\n",
            "Epoch: 997 \tTraining Loss: 0.447633\n",
            "Epoch: 998 \tTraining Loss: 0.448043\n",
            "Epoch: 999 \tTraining Loss: 0.447902\n",
            "Epoch: 1000 \tTraining Loss: 0.447223\n",
            "Epoch: 1001 \tTraining Loss: 0.447403\n",
            "Epoch: 1002 \tTraining Loss: 0.447249\n",
            "Epoch: 1003 \tTraining Loss: 0.447194\n",
            "Epoch: 1004 \tTraining Loss: 0.446751\n",
            "Epoch: 1005 \tTraining Loss: 0.446693\n",
            "Epoch: 1006 \tTraining Loss: 0.446776\n",
            "Epoch: 1007 \tTraining Loss: 0.446413\n",
            "Epoch: 1008 \tTraining Loss: 0.446219\n",
            "Epoch: 1009 \tTraining Loss: 0.446178\n",
            "Epoch: 1010 \tTraining Loss: 0.446096\n",
            "Epoch: 1011 \tTraining Loss: 0.445792\n",
            "Epoch: 1012 \tTraining Loss: 0.445719\n",
            "Epoch: 1013 \tTraining Loss: 0.445583\n",
            "Epoch: 1014 \tTraining Loss: 0.445502\n",
            "Epoch: 1015 \tTraining Loss: 0.445309\n",
            "Epoch: 1016 \tTraining Loss: 0.445097\n",
            "Epoch: 1017 \tTraining Loss: 0.445135\n",
            "Epoch: 1018 \tTraining Loss: 0.444950\n",
            "Epoch: 1019 \tTraining Loss: 0.444742\n",
            "Epoch: 1020 \tTraining Loss: 0.444649\n",
            "Epoch: 1021 \tTraining Loss: 0.444583\n",
            "Epoch: 1022 \tTraining Loss: 0.444415\n",
            "Epoch: 1023 \tTraining Loss: 0.444248\n",
            "Epoch: 1024 \tTraining Loss: 0.444159\n",
            "Epoch: 1025 \tTraining Loss: 0.444050\n",
            "Epoch: 1026 \tTraining Loss: 0.443919\n",
            "Epoch: 1027 \tTraining Loss: 0.443760\n",
            "Epoch: 1028 \tTraining Loss: 0.443658\n",
            "Epoch: 1029 \tTraining Loss: 0.443550\n",
            "Epoch: 1030 \tTraining Loss: 0.443417\n",
            "Epoch: 1031 \tTraining Loss: 0.443298\n",
            "Epoch: 1032 \tTraining Loss: 0.443164\n",
            "Epoch: 1033 \tTraining Loss: 0.443050\n",
            "Epoch: 1034 \tTraining Loss: 0.442941\n",
            "Epoch: 1035 \tTraining Loss: 0.442823\n",
            "Epoch: 1036 \tTraining Loss: 0.442685\n",
            "Epoch: 1037 \tTraining Loss: 0.442565\n",
            "Epoch: 1038 \tTraining Loss: 0.442467\n",
            "Epoch: 1039 \tTraining Loss: 0.442343\n",
            "Epoch: 1040 \tTraining Loss: 0.442219\n",
            "Epoch: 1041 \tTraining Loss: 0.442100\n",
            "Epoch: 1042 \tTraining Loss: 0.441981\n",
            "Epoch: 1043 \tTraining Loss: 0.441870\n",
            "Epoch: 1044 \tTraining Loss: 0.441754\n",
            "Epoch: 1045 \tTraining Loss: 0.441637\n",
            "Epoch: 1046 \tTraining Loss: 0.441515\n",
            "Epoch: 1047 \tTraining Loss: 0.441397\n",
            "Epoch: 1048 \tTraining Loss: 0.441284\n",
            "Epoch: 1049 \tTraining Loss: 0.441169\n",
            "Epoch: 1050 \tTraining Loss: 0.441057\n",
            "Epoch: 1051 \tTraining Loss: 0.440937\n",
            "Epoch: 1052 \tTraining Loss: 0.440819\n",
            "Epoch: 1053 \tTraining Loss: 0.440705\n",
            "Epoch: 1054 \tTraining Loss: 0.440590\n",
            "Epoch: 1055 \tTraining Loss: 0.440476\n",
            "Epoch: 1056 \tTraining Loss: 0.440362\n",
            "Epoch: 1057 \tTraining Loss: 0.440248\n",
            "Epoch: 1058 \tTraining Loss: 0.440132\n",
            "Epoch: 1059 \tTraining Loss: 0.440015\n",
            "Epoch: 1060 \tTraining Loss: 0.439902\n",
            "Epoch: 1061 \tTraining Loss: 0.439787\n",
            "Epoch: 1062 \tTraining Loss: 0.439674\n",
            "Epoch: 1063 \tTraining Loss: 0.439560\n",
            "Epoch: 1064 \tTraining Loss: 0.439447\n",
            "Epoch: 1065 \tTraining Loss: 0.439334\n",
            "Epoch: 1066 \tTraining Loss: 0.439220\n",
            "Epoch: 1067 \tTraining Loss: 0.439106\n",
            "Epoch: 1068 \tTraining Loss: 0.438993\n",
            "Epoch: 1069 \tTraining Loss: 0.438879\n",
            "Epoch: 1070 \tTraining Loss: 0.438767\n",
            "Epoch: 1071 \tTraining Loss: 0.438654\n",
            "Epoch: 1072 \tTraining Loss: 0.438542\n",
            "Epoch: 1073 \tTraining Loss: 0.438430\n",
            "Epoch: 1074 \tTraining Loss: 0.438318\n",
            "Epoch: 1075 \tTraining Loss: 0.438206\n",
            "Epoch: 1076 \tTraining Loss: 0.438094\n",
            "Epoch: 1077 \tTraining Loss: 0.437983\n",
            "Epoch: 1078 \tTraining Loss: 0.437872\n",
            "Epoch: 1079 \tTraining Loss: 0.437761\n",
            "Epoch: 1080 \tTraining Loss: 0.437650\n",
            "Epoch: 1081 \tTraining Loss: 0.437539\n",
            "Epoch: 1082 \tTraining Loss: 0.437429\n",
            "Epoch: 1083 \tTraining Loss: 0.437320\n",
            "Epoch: 1084 \tTraining Loss: 0.437211\n",
            "Epoch: 1085 \tTraining Loss: 0.437103\n",
            "Epoch: 1086 \tTraining Loss: 0.436998\n",
            "Epoch: 1087 \tTraining Loss: 0.436896\n",
            "Epoch: 1088 \tTraining Loss: 0.436800\n",
            "Epoch: 1089 \tTraining Loss: 0.436714\n",
            "Epoch: 1090 \tTraining Loss: 0.436644\n",
            "Epoch: 1091 \tTraining Loss: 0.436608\n",
            "Epoch: 1092 \tTraining Loss: 0.436610\n",
            "Epoch: 1093 \tTraining Loss: 0.436704\n",
            "Epoch: 1094 \tTraining Loss: 0.436829\n",
            "Epoch: 1095 \tTraining Loss: 0.437211\n",
            "Epoch: 1096 \tTraining Loss: 0.437516\n",
            "Epoch: 1097 \tTraining Loss: 0.438268\n",
            "Epoch: 1098 \tTraining Loss: 0.438159\n",
            "Epoch: 1099 \tTraining Loss: 0.437804\n",
            "Epoch: 1100 \tTraining Loss: 0.436302\n",
            "Epoch: 1101 \tTraining Loss: 0.435410\n",
            "Epoch: 1102 \tTraining Loss: 0.435807\n",
            "Epoch: 1103 \tTraining Loss: 0.436581\n",
            "Epoch: 1104 \tTraining Loss: 0.436989\n",
            "Epoch: 1105 \tTraining Loss: 0.436140\n",
            "Epoch: 1106 \tTraining Loss: 0.435067\n",
            "Epoch: 1107 \tTraining Loss: 0.435107\n",
            "Epoch: 1108 \tTraining Loss: 0.435620\n",
            "Epoch: 1109 \tTraining Loss: 0.435782\n",
            "Epoch: 1110 \tTraining Loss: 0.435166\n",
            "Epoch: 1111 \tTraining Loss: 0.434453\n",
            "Epoch: 1112 \tTraining Loss: 0.434538\n",
            "Epoch: 1113 \tTraining Loss: 0.434838\n",
            "Epoch: 1114 \tTraining Loss: 0.434758\n",
            "Epoch: 1115 \tTraining Loss: 0.434216\n",
            "Epoch: 1116 \tTraining Loss: 0.433833\n",
            "Epoch: 1117 \tTraining Loss: 0.434027\n",
            "Epoch: 1118 \tTraining Loss: 0.434166\n",
            "Epoch: 1119 \tTraining Loss: 0.433900\n",
            "Epoch: 1120 \tTraining Loss: 0.433462\n",
            "Epoch: 1121 \tTraining Loss: 0.433343\n",
            "Epoch: 1122 \tTraining Loss: 0.433525\n",
            "Epoch: 1123 \tTraining Loss: 0.433512\n",
            "Epoch: 1124 \tTraining Loss: 0.433214\n",
            "Epoch: 1125 \tTraining Loss: 0.432936\n",
            "Epoch: 1126 \tTraining Loss: 0.432839\n",
            "Epoch: 1127 \tTraining Loss: 0.432896\n",
            "Epoch: 1128 \tTraining Loss: 0.432859\n",
            "Epoch: 1129 \tTraining Loss: 0.432666\n",
            "Epoch: 1130 \tTraining Loss: 0.432438\n",
            "Epoch: 1131 \tTraining Loss: 0.432277\n",
            "Epoch: 1132 \tTraining Loss: 0.432264\n",
            "Epoch: 1133 \tTraining Loss: 0.432256\n",
            "Epoch: 1134 \tTraining Loss: 0.432132\n",
            "Epoch: 1135 \tTraining Loss: 0.431930\n",
            "Epoch: 1136 \tTraining Loss: 0.431756\n",
            "Epoch: 1137 \tTraining Loss: 0.431686\n",
            "Epoch: 1138 \tTraining Loss: 0.431659\n",
            "Epoch: 1139 \tTraining Loss: 0.431571\n",
            "Epoch: 1140 \tTraining Loss: 0.431428\n",
            "Epoch: 1141 \tTraining Loss: 0.431275\n",
            "Epoch: 1142 \tTraining Loss: 0.431160\n",
            "Epoch: 1143 \tTraining Loss: 0.431088\n",
            "Epoch: 1144 \tTraining Loss: 0.431014\n",
            "Epoch: 1145 \tTraining Loss: 0.430921\n",
            "Epoch: 1146 \tTraining Loss: 0.430799\n",
            "Epoch: 1147 \tTraining Loss: 0.430663\n",
            "Epoch: 1148 \tTraining Loss: 0.430552\n",
            "Epoch: 1149 \tTraining Loss: 0.430466\n",
            "Epoch: 1150 \tTraining Loss: 0.430389\n",
            "Epoch: 1151 \tTraining Loss: 0.430298\n",
            "Epoch: 1152 \tTraining Loss: 0.430182\n",
            "Epoch: 1153 \tTraining Loss: 0.430060\n",
            "Epoch: 1154 \tTraining Loss: 0.429948\n",
            "Epoch: 1155 \tTraining Loss: 0.429852\n",
            "Epoch: 1156 \tTraining Loss: 0.429764\n",
            "Epoch: 1157 \tTraining Loss: 0.429671\n",
            "Epoch: 1158 \tTraining Loss: 0.429571\n",
            "Epoch: 1159 \tTraining Loss: 0.429464\n",
            "Epoch: 1160 \tTraining Loss: 0.429354\n",
            "Epoch: 1161 \tTraining Loss: 0.429245\n",
            "Epoch: 1162 \tTraining Loss: 0.429140\n",
            "Epoch: 1163 \tTraining Loss: 0.429039\n",
            "Epoch: 1164 \tTraining Loss: 0.428940\n",
            "Epoch: 1165 \tTraining Loss: 0.428834\n",
            "Epoch: 1166 \tTraining Loss: 0.428718\n",
            "Epoch: 1167 \tTraining Loss: 0.428590\n",
            "Epoch: 1168 \tTraining Loss: 0.428455\n",
            "Epoch: 1169 \tTraining Loss: 0.428327\n",
            "Epoch: 1170 \tTraining Loss: 0.428209\n",
            "Epoch: 1171 \tTraining Loss: 0.428094\n",
            "Epoch: 1172 \tTraining Loss: 0.427986\n",
            "Epoch: 1173 \tTraining Loss: 0.427894\n",
            "Epoch: 1174 \tTraining Loss: 0.427809\n",
            "Epoch: 1175 \tTraining Loss: 0.427715\n",
            "Epoch: 1176 \tTraining Loss: 0.427610\n",
            "Epoch: 1177 \tTraining Loss: 0.427502\n",
            "Epoch: 1178 \tTraining Loss: 0.427400\n",
            "Epoch: 1179 \tTraining Loss: 0.427304\n",
            "Epoch: 1180 \tTraining Loss: 0.427210\n",
            "Epoch: 1181 \tTraining Loss: 0.427115\n",
            "Epoch: 1182 \tTraining Loss: 0.427020\n",
            "Epoch: 1183 \tTraining Loss: 0.426926\n",
            "Epoch: 1184 \tTraining Loss: 0.426834\n",
            "Epoch: 1185 \tTraining Loss: 0.426748\n",
            "Epoch: 1186 \tTraining Loss: 0.426670\n",
            "Epoch: 1187 \tTraining Loss: 0.426606\n",
            "Epoch: 1188 \tTraining Loss: 0.426557\n",
            "Epoch: 1189 \tTraining Loss: 0.426543\n",
            "Epoch: 1190 \tTraining Loss: 0.426554\n",
            "Epoch: 1191 \tTraining Loss: 0.426639\n",
            "Epoch: 1192 \tTraining Loss: 0.426760\n",
            "Epoch: 1193 \tTraining Loss: 0.426988\n",
            "Epoch: 1194 \tTraining Loss: 0.427328\n",
            "Epoch: 1195 \tTraining Loss: 0.427761\n",
            "Epoch: 1196 \tTraining Loss: 0.428789\n",
            "Epoch: 1197 \tTraining Loss: 0.429774\n",
            "Epoch: 1198 \tTraining Loss: 0.432902\n",
            "Epoch: 1199 \tTraining Loss: 0.432996\n",
            "Epoch: 1200 \tTraining Loss: 0.432658\n",
            "Epoch: 1201 \tTraining Loss: 0.428979\n",
            "Epoch: 1202 \tTraining Loss: 0.426504\n",
            "Epoch: 1203 \tTraining Loss: 0.429480\n",
            "Epoch: 1204 \tTraining Loss: 0.429834\n",
            "Epoch: 1205 \tTraining Loss: 0.427029\n",
            "Epoch: 1206 \tTraining Loss: 0.428015\n",
            "Epoch: 1207 \tTraining Loss: 0.427380\n",
            "Epoch: 1208 \tTraining Loss: 0.429120\n",
            "Epoch: 1209 \tTraining Loss: 0.427428\n",
            "Epoch: 1210 \tTraining Loss: 0.428208\n",
            "Epoch: 1211 \tTraining Loss: 0.426863\n",
            "Epoch: 1212 \tTraining Loss: 0.427069\n",
            "Epoch: 1213 \tTraining Loss: 0.426777\n",
            "Epoch: 1214 \tTraining Loss: 0.427228\n",
            "Epoch: 1215 \tTraining Loss: 0.425757\n",
            "Epoch: 1216 \tTraining Loss: 0.426421\n",
            "Epoch: 1217 \tTraining Loss: 0.425618\n",
            "Epoch: 1218 \tTraining Loss: 0.425326\n",
            "Epoch: 1219 \tTraining Loss: 0.425009\n",
            "Epoch: 1220 \tTraining Loss: 0.425106\n",
            "Epoch: 1221 \tTraining Loss: 0.424505\n",
            "Epoch: 1222 \tTraining Loss: 0.424435\n",
            "Epoch: 1223 \tTraining Loss: 0.424417\n",
            "Epoch: 1224 \tTraining Loss: 0.424011\n",
            "Epoch: 1225 \tTraining Loss: 0.423865\n",
            "Epoch: 1226 \tTraining Loss: 0.423871\n",
            "Epoch: 1227 \tTraining Loss: 0.423743\n",
            "Epoch: 1228 \tTraining Loss: 0.423369\n",
            "Epoch: 1229 \tTraining Loss: 0.423552\n",
            "Epoch: 1230 \tTraining Loss: 0.423165\n",
            "Epoch: 1231 \tTraining Loss: 0.423170\n",
            "Epoch: 1232 \tTraining Loss: 0.423013\n",
            "Epoch: 1233 \tTraining Loss: 0.422893\n",
            "Epoch: 1234 \tTraining Loss: 0.422722\n",
            "Epoch: 1235 \tTraining Loss: 0.422714\n",
            "Epoch: 1236 \tTraining Loss: 0.422565\n",
            "Epoch: 1237 \tTraining Loss: 0.422436\n",
            "Epoch: 1238 \tTraining Loss: 0.422358\n",
            "Epoch: 1239 \tTraining Loss: 0.422290\n",
            "Epoch: 1240 \tTraining Loss: 0.422142\n",
            "Epoch: 1241 \tTraining Loss: 0.422061\n",
            "Epoch: 1242 \tTraining Loss: 0.422016\n",
            "Epoch: 1243 \tTraining Loss: 0.421853\n",
            "Epoch: 1244 \tTraining Loss: 0.421790\n",
            "Epoch: 1245 \tTraining Loss: 0.421717\n",
            "Epoch: 1246 \tTraining Loss: 0.421601\n",
            "Epoch: 1247 \tTraining Loss: 0.421522\n",
            "Epoch: 1248 \tTraining Loss: 0.421447\n",
            "Epoch: 1249 \tTraining Loss: 0.421338\n",
            "Epoch: 1250 \tTraining Loss: 0.421266\n",
            "Epoch: 1251 \tTraining Loss: 0.421177\n",
            "Epoch: 1252 \tTraining Loss: 0.421079\n",
            "Epoch: 1253 \tTraining Loss: 0.421003\n",
            "Epoch: 1254 \tTraining Loss: 0.420909\n",
            "Epoch: 1255 \tTraining Loss: 0.420822\n",
            "Epoch: 1256 \tTraining Loss: 0.420736\n",
            "Epoch: 1257 \tTraining Loss: 0.420662\n",
            "Epoch: 1258 \tTraining Loss: 0.420567\n",
            "Epoch: 1259 \tTraining Loss: 0.420490\n",
            "Epoch: 1260 \tTraining Loss: 0.420404\n",
            "Epoch: 1261 \tTraining Loss: 0.420333\n",
            "Epoch: 1262 \tTraining Loss: 0.420241\n",
            "Epoch: 1263 \tTraining Loss: 0.420164\n",
            "Epoch: 1264 \tTraining Loss: 0.420080\n",
            "Epoch: 1265 \tTraining Loss: 0.420004\n",
            "Epoch: 1266 \tTraining Loss: 0.419915\n",
            "Epoch: 1267 \tTraining Loss: 0.419838\n",
            "Epoch: 1268 \tTraining Loss: 0.419758\n",
            "Epoch: 1269 \tTraining Loss: 0.419678\n",
            "Epoch: 1270 \tTraining Loss: 0.419593\n",
            "Epoch: 1271 \tTraining Loss: 0.419518\n",
            "Epoch: 1272 \tTraining Loss: 0.419437\n",
            "Epoch: 1273 \tTraining Loss: 0.419355\n",
            "Epoch: 1274 \tTraining Loss: 0.419276\n",
            "Epoch: 1275 \tTraining Loss: 0.419197\n",
            "Epoch: 1276 \tTraining Loss: 0.419117\n",
            "Epoch: 1277 \tTraining Loss: 0.419037\n",
            "Epoch: 1278 \tTraining Loss: 0.418958\n",
            "Epoch: 1279 \tTraining Loss: 0.418878\n",
            "Epoch: 1280 \tTraining Loss: 0.418799\n",
            "Epoch: 1281 \tTraining Loss: 0.418720\n",
            "Epoch: 1282 \tTraining Loss: 0.418641\n",
            "Epoch: 1283 \tTraining Loss: 0.418562\n",
            "Epoch: 1284 \tTraining Loss: 0.418483\n",
            "Epoch: 1285 \tTraining Loss: 0.418404\n",
            "Epoch: 1286 \tTraining Loss: 0.418325\n",
            "Epoch: 1287 \tTraining Loss: 0.418246\n",
            "Epoch: 1288 \tTraining Loss: 0.418168\n",
            "Epoch: 1289 \tTraining Loss: 0.418089\n",
            "Epoch: 1290 \tTraining Loss: 0.418011\n",
            "Epoch: 1291 \tTraining Loss: 0.417932\n",
            "Epoch: 1292 \tTraining Loss: 0.417854\n",
            "Epoch: 1293 \tTraining Loss: 0.417776\n",
            "Epoch: 1294 \tTraining Loss: 0.417697\n",
            "Epoch: 1295 \tTraining Loss: 0.417619\n",
            "Epoch: 1296 \tTraining Loss: 0.417541\n",
            "Epoch: 1297 \tTraining Loss: 0.417463\n",
            "Epoch: 1298 \tTraining Loss: 0.417385\n",
            "Epoch: 1299 \tTraining Loss: 0.417307\n",
            "Epoch: 1300 \tTraining Loss: 0.417229\n",
            "Epoch: 1301 \tTraining Loss: 0.417151\n",
            "Epoch: 1302 \tTraining Loss: 0.417073\n",
            "Epoch: 1303 \tTraining Loss: 0.416995\n",
            "Epoch: 1304 \tTraining Loss: 0.416918\n",
            "Epoch: 1305 \tTraining Loss: 0.416840\n",
            "Epoch: 1306 \tTraining Loss: 0.416762\n",
            "Epoch: 1307 \tTraining Loss: 0.416684\n",
            "Epoch: 1308 \tTraining Loss: 0.416606\n",
            "Epoch: 1309 \tTraining Loss: 0.416528\n",
            "Epoch: 1310 \tTraining Loss: 0.416450\n",
            "Epoch: 1311 \tTraining Loss: 0.416370\n",
            "Epoch: 1312 \tTraining Loss: 0.416289\n",
            "Epoch: 1313 \tTraining Loss: 0.416202\n",
            "Epoch: 1314 \tTraining Loss: 0.416099\n",
            "Epoch: 1315 \tTraining Loss: 0.415995\n",
            "Epoch: 1316 \tTraining Loss: 0.415959\n",
            "Epoch: 1317 \tTraining Loss: 0.415851\n",
            "Epoch: 1318 \tTraining Loss: 0.415771\n",
            "Epoch: 1319 \tTraining Loss: 0.415709\n",
            "Epoch: 1320 \tTraining Loss: 0.415624\n",
            "Epoch: 1321 \tTraining Loss: 0.415535\n",
            "Epoch: 1322 \tTraining Loss: 0.415468\n",
            "Epoch: 1323 \tTraining Loss: 0.415387\n",
            "Epoch: 1324 \tTraining Loss: 0.415302\n",
            "Epoch: 1325 \tTraining Loss: 0.415233\n",
            "Epoch: 1326 \tTraining Loss: 0.415154\n",
            "Epoch: 1327 \tTraining Loss: 0.415071\n",
            "Epoch: 1328 \tTraining Loss: 0.414999\n",
            "Epoch: 1329 \tTraining Loss: 0.414923\n",
            "Epoch: 1330 \tTraining Loss: 0.414840\n",
            "Epoch: 1331 \tTraining Loss: 0.414767\n",
            "Epoch: 1332 \tTraining Loss: 0.414693\n",
            "Epoch: 1333 \tTraining Loss: 0.414611\n",
            "Epoch: 1334 \tTraining Loss: 0.414537\n",
            "Epoch: 1335 \tTraining Loss: 0.414463\n",
            "Epoch: 1336 \tTraining Loss: 0.414383\n",
            "Epoch: 1337 \tTraining Loss: 0.414307\n",
            "Epoch: 1338 \tTraining Loss: 0.414233\n",
            "Epoch: 1339 \tTraining Loss: 0.414155\n",
            "Epoch: 1340 \tTraining Loss: 0.414079\n",
            "Epoch: 1341 \tTraining Loss: 0.414005\n",
            "Epoch: 1342 \tTraining Loss: 0.413928\n",
            "Epoch: 1343 \tTraining Loss: 0.413852\n",
            "Epoch: 1344 \tTraining Loss: 0.413777\n",
            "Epoch: 1345 \tTraining Loss: 0.413701\n",
            "Epoch: 1346 \tTraining Loss: 0.413625\n",
            "Epoch: 1347 \tTraining Loss: 0.413550\n",
            "Epoch: 1348 \tTraining Loss: 0.413474\n",
            "Epoch: 1349 \tTraining Loss: 0.413399\n",
            "Epoch: 1350 \tTraining Loss: 0.413324\n",
            "Epoch: 1351 \tTraining Loss: 0.413248\n",
            "Epoch: 1352 \tTraining Loss: 0.413173\n",
            "Epoch: 1353 \tTraining Loss: 0.413098\n",
            "Epoch: 1354 \tTraining Loss: 0.413023\n",
            "Epoch: 1355 \tTraining Loss: 0.412948\n",
            "Epoch: 1356 \tTraining Loss: 0.412873\n",
            "Epoch: 1357 \tTraining Loss: 0.412798\n",
            "Epoch: 1358 \tTraining Loss: 0.412723\n",
            "Epoch: 1359 \tTraining Loss: 0.412648\n",
            "Epoch: 1360 \tTraining Loss: 0.412573\n",
            "Epoch: 1361 \tTraining Loss: 0.412498\n",
            "Epoch: 1362 \tTraining Loss: 0.412424\n",
            "Epoch: 1363 \tTraining Loss: 0.412349\n",
            "Epoch: 1364 \tTraining Loss: 0.412274\n",
            "Epoch: 1365 \tTraining Loss: 0.412199\n",
            "Epoch: 1366 \tTraining Loss: 0.412125\n",
            "Epoch: 1367 \tTraining Loss: 0.412050\n",
            "Epoch: 1368 \tTraining Loss: 0.411976\n",
            "Epoch: 1369 \tTraining Loss: 0.411901\n",
            "Epoch: 1370 \tTraining Loss: 0.411827\n",
            "Epoch: 1371 \tTraining Loss: 0.411753\n",
            "Epoch: 1372 \tTraining Loss: 0.411679\n",
            "Epoch: 1373 \tTraining Loss: 0.411605\n",
            "Epoch: 1374 \tTraining Loss: 0.411532\n",
            "Epoch: 1375 \tTraining Loss: 0.411460\n",
            "Epoch: 1376 \tTraining Loss: 0.411390\n",
            "Epoch: 1377 \tTraining Loss: 0.411325\n",
            "Epoch: 1378 \tTraining Loss: 0.411271\n",
            "Epoch: 1379 \tTraining Loss: 0.411238\n",
            "Epoch: 1380 \tTraining Loss: 0.411260\n",
            "Epoch: 1381 \tTraining Loss: 0.411392\n",
            "Epoch: 1382 \tTraining Loss: 0.411826\n",
            "Epoch: 1383 \tTraining Loss: 0.412900\n",
            "Epoch: 1384 \tTraining Loss: 0.416493\n",
            "Epoch: 1385 \tTraining Loss: 0.430970\n",
            "Epoch: 1386 \tTraining Loss: 0.447574\n",
            "Epoch: 1387 \tTraining Loss: 0.470114\n",
            "Epoch: 1388 \tTraining Loss: 0.422903\n",
            "Epoch: 1389 \tTraining Loss: 0.456645\n",
            "Epoch: 1390 \tTraining Loss: 0.448363\n",
            "Epoch: 1391 \tTraining Loss: 0.448814\n",
            "Epoch: 1392 \tTraining Loss: 0.434507\n",
            "Epoch: 1393 \tTraining Loss: 0.432917\n",
            "Epoch: 1394 \tTraining Loss: 0.433776\n",
            "Epoch: 1395 \tTraining Loss: 0.432563\n",
            "Epoch: 1396 \tTraining Loss: 0.429942\n",
            "Epoch: 1397 \tTraining Loss: 0.426942\n",
            "Epoch: 1398 \tTraining Loss: 0.428988\n",
            "Epoch: 1399 \tTraining Loss: 0.420446\n",
            "Epoch: 1400 \tTraining Loss: 0.421871\n",
            "Epoch: 1401 \tTraining Loss: 0.424287\n",
            "Epoch: 1402 \tTraining Loss: 0.416974\n",
            "Epoch: 1403 \tTraining Loss: 0.421582\n",
            "Epoch: 1404 \tTraining Loss: 0.418379\n",
            "Epoch: 1405 \tTraining Loss: 0.415926\n",
            "Epoch: 1406 \tTraining Loss: 0.416899\n",
            "Epoch: 1407 \tTraining Loss: 0.415011\n",
            "Epoch: 1408 \tTraining Loss: 0.415427\n",
            "Epoch: 1409 \tTraining Loss: 0.415023\n",
            "Epoch: 1410 \tTraining Loss: 0.412998\n",
            "Epoch: 1411 \tTraining Loss: 0.414210\n",
            "Epoch: 1412 \tTraining Loss: 0.413862\n",
            "Epoch: 1413 \tTraining Loss: 0.412161\n",
            "Epoch: 1414 \tTraining Loss: 0.412869\n",
            "Epoch: 1415 \tTraining Loss: 0.412029\n",
            "Epoch: 1416 \tTraining Loss: 0.411906\n",
            "Epoch: 1417 \tTraining Loss: 0.411678\n",
            "Epoch: 1418 \tTraining Loss: 0.411442\n",
            "Epoch: 1419 \tTraining Loss: 0.411349\n",
            "Epoch: 1420 \tTraining Loss: 0.411107\n",
            "Epoch: 1421 \tTraining Loss: 0.410650\n",
            "Epoch: 1422 \tTraining Loss: 0.410472\n",
            "Epoch: 1423 \tTraining Loss: 0.410365\n",
            "Epoch: 1424 \tTraining Loss: 0.410378\n",
            "Epoch: 1425 \tTraining Loss: 0.410133\n",
            "Epoch: 1426 \tTraining Loss: 0.409874\n",
            "Epoch: 1427 \tTraining Loss: 0.409748\n",
            "Epoch: 1428 \tTraining Loss: 0.409618\n",
            "Epoch: 1429 \tTraining Loss: 0.409684\n",
            "Epoch: 1430 \tTraining Loss: 0.409332\n",
            "Epoch: 1431 \tTraining Loss: 0.409256\n",
            "Epoch: 1432 \tTraining Loss: 0.409253\n",
            "Epoch: 1433 \tTraining Loss: 0.409033\n",
            "Epoch: 1434 \tTraining Loss: 0.408923\n",
            "Epoch: 1435 \tTraining Loss: 0.408857\n",
            "Epoch: 1436 \tTraining Loss: 0.408794\n",
            "Epoch: 1437 \tTraining Loss: 0.408764\n",
            "Epoch: 1438 \tTraining Loss: 0.408554\n",
            "Epoch: 1439 \tTraining Loss: 0.408520\n",
            "Epoch: 1440 \tTraining Loss: 0.408512\n",
            "Epoch: 1441 \tTraining Loss: 0.408371\n",
            "Epoch: 1442 \tTraining Loss: 0.408325\n",
            "Epoch: 1443 \tTraining Loss: 0.408258\n",
            "Epoch: 1444 \tTraining Loss: 0.408155\n",
            "Epoch: 1445 \tTraining Loss: 0.408107\n",
            "Epoch: 1446 \tTraining Loss: 0.408054\n",
            "Epoch: 1447 \tTraining Loss: 0.407979\n",
            "Epoch: 1448 \tTraining Loss: 0.407908\n",
            "Epoch: 1449 \tTraining Loss: 0.407851\n",
            "Epoch: 1450 \tTraining Loss: 0.407786\n",
            "Epoch: 1451 \tTraining Loss: 0.407727\n",
            "Epoch: 1452 \tTraining Loss: 0.407659\n",
            "Epoch: 1453 \tTraining Loss: 0.407596\n",
            "Epoch: 1454 \tTraining Loss: 0.407556\n",
            "Epoch: 1455 \tTraining Loss: 0.407483\n",
            "Epoch: 1456 \tTraining Loss: 0.407418\n",
            "Epoch: 1457 \tTraining Loss: 0.407372\n",
            "Epoch: 1458 \tTraining Loss: 0.407308\n",
            "Epoch: 1459 \tTraining Loss: 0.407258\n",
            "Epoch: 1460 \tTraining Loss: 0.407200\n",
            "Epoch: 1461 \tTraining Loss: 0.407138\n",
            "Epoch: 1462 \tTraining Loss: 0.407088\n",
            "Epoch: 1463 \tTraining Loss: 0.407031\n",
            "Epoch: 1464 \tTraining Loss: 0.406977\n",
            "Epoch: 1465 \tTraining Loss: 0.406924\n",
            "Epoch: 1466 \tTraining Loss: 0.406868\n",
            "Epoch: 1467 \tTraining Loss: 0.406815\n",
            "Epoch: 1468 \tTraining Loss: 0.406761\n",
            "Epoch: 1469 \tTraining Loss: 0.406707\n",
            "Epoch: 1470 \tTraining Loss: 0.406656\n",
            "Epoch: 1471 \tTraining Loss: 0.406603\n",
            "Epoch: 1472 \tTraining Loss: 0.406548\n",
            "Epoch: 1473 \tTraining Loss: 0.406497\n",
            "Epoch: 1474 \tTraining Loss: 0.406445\n",
            "Epoch: 1475 \tTraining Loss: 0.406391\n",
            "Epoch: 1476 \tTraining Loss: 0.406340\n",
            "Epoch: 1477 \tTraining Loss: 0.406288\n",
            "Epoch: 1478 \tTraining Loss: 0.406236\n",
            "Epoch: 1479 \tTraining Loss: 0.406184\n",
            "Epoch: 1480 \tTraining Loss: 0.406132\n",
            "Epoch: 1481 \tTraining Loss: 0.406082\n",
            "Epoch: 1482 \tTraining Loss: 0.406030\n",
            "Epoch: 1483 \tTraining Loss: 0.405978\n",
            "Epoch: 1484 \tTraining Loss: 0.405927\n",
            "Epoch: 1485 \tTraining Loss: 0.405876\n",
            "Epoch: 1486 \tTraining Loss: 0.405825\n",
            "Epoch: 1487 \tTraining Loss: 0.405774\n",
            "Epoch: 1488 \tTraining Loss: 0.405722\n",
            "Epoch: 1489 \tTraining Loss: 0.405672\n",
            "Epoch: 1490 \tTraining Loss: 0.405621\n",
            "Epoch: 1491 \tTraining Loss: 0.405570\n",
            "Epoch: 1492 \tTraining Loss: 0.405519\n",
            "Epoch: 1493 \tTraining Loss: 0.405468\n",
            "Epoch: 1494 \tTraining Loss: 0.405417\n",
            "Epoch: 1495 \tTraining Loss: 0.405367\n",
            "Epoch: 1496 \tTraining Loss: 0.405316\n",
            "Epoch: 1497 \tTraining Loss: 0.405265\n",
            "Epoch: 1498 \tTraining Loss: 0.405215\n",
            "Epoch: 1499 \tTraining Loss: 0.405164\n",
            "Epoch: 1500 \tTraining Loss: 0.405114\n",
            "Epoch: 1501 \tTraining Loss: 0.405063\n",
            "Epoch: 1502 \tTraining Loss: 0.405013\n",
            "Epoch: 1503 \tTraining Loss: 0.404962\n",
            "Epoch: 1504 \tTraining Loss: 0.404912\n",
            "Epoch: 1505 \tTraining Loss: 0.404861\n",
            "Epoch: 1506 \tTraining Loss: 0.404811\n",
            "Epoch: 1507 \tTraining Loss: 0.404761\n",
            "Epoch: 1508 \tTraining Loss: 0.404710\n",
            "Epoch: 1509 \tTraining Loss: 0.404660\n",
            "Epoch: 1510 \tTraining Loss: 0.404610\n",
            "Epoch: 1511 \tTraining Loss: 0.404559\n",
            "Epoch: 1512 \tTraining Loss: 0.404509\n",
            "Epoch: 1513 \tTraining Loss: 0.404459\n",
            "Epoch: 1514 \tTraining Loss: 0.404408\n",
            "Epoch: 1515 \tTraining Loss: 0.404358\n",
            "Epoch: 1516 \tTraining Loss: 0.404308\n",
            "Epoch: 1517 \tTraining Loss: 0.404258\n",
            "Epoch: 1518 \tTraining Loss: 0.404208\n",
            "Epoch: 1519 \tTraining Loss: 0.404157\n",
            "Epoch: 1520 \tTraining Loss: 0.404107\n",
            "Epoch: 1521 \tTraining Loss: 0.404057\n",
            "Epoch: 1522 \tTraining Loss: 0.404007\n",
            "Epoch: 1523 \tTraining Loss: 0.403957\n",
            "Epoch: 1524 \tTraining Loss: 0.403907\n",
            "Epoch: 1525 \tTraining Loss: 0.403856\n",
            "Epoch: 1526 \tTraining Loss: 0.403806\n",
            "Epoch: 1527 \tTraining Loss: 0.403756\n",
            "Epoch: 1528 \tTraining Loss: 0.403706\n",
            "Epoch: 1529 \tTraining Loss: 0.403656\n",
            "Epoch: 1530 \tTraining Loss: 0.403606\n",
            "Epoch: 1531 \tTraining Loss: 0.403556\n",
            "Epoch: 1532 \tTraining Loss: 0.403506\n",
            "Epoch: 1533 \tTraining Loss: 0.403456\n",
            "Epoch: 1534 \tTraining Loss: 0.403406\n",
            "Epoch: 1535 \tTraining Loss: 0.403355\n",
            "Epoch: 1536 \tTraining Loss: 0.403305\n",
            "Epoch: 1537 \tTraining Loss: 0.403255\n",
            "Epoch: 1538 \tTraining Loss: 0.403205\n",
            "Epoch: 1539 \tTraining Loss: 0.403155\n",
            "Epoch: 1540 \tTraining Loss: 0.403105\n",
            "Epoch: 1541 \tTraining Loss: 0.403055\n",
            "Epoch: 1542 \tTraining Loss: 0.403005\n",
            "Epoch: 1543 \tTraining Loss: 0.402955\n",
            "Epoch: 1544 \tTraining Loss: 0.402905\n",
            "Epoch: 1545 \tTraining Loss: 0.402855\n",
            "Epoch: 1546 \tTraining Loss: 0.402805\n",
            "Epoch: 1547 \tTraining Loss: 0.402755\n",
            "Epoch: 1548 \tTraining Loss: 0.402705\n",
            "Epoch: 1549 \tTraining Loss: 0.402655\n",
            "Epoch: 1550 \tTraining Loss: 0.402605\n",
            "Epoch: 1551 \tTraining Loss: 0.402555\n",
            "Epoch: 1552 \tTraining Loss: 0.402505\n",
            "Epoch: 1553 \tTraining Loss: 0.402455\n",
            "Epoch: 1554 \tTraining Loss: 0.402405\n",
            "Epoch: 1555 \tTraining Loss: 0.402355\n",
            "Epoch: 1556 \tTraining Loss: 0.402305\n",
            "Epoch: 1557 \tTraining Loss: 0.402255\n",
            "Epoch: 1558 \tTraining Loss: 0.402205\n",
            "Epoch: 1559 \tTraining Loss: 0.402155\n",
            "Epoch: 1560 \tTraining Loss: 0.402105\n",
            "Epoch: 1561 \tTraining Loss: 0.402055\n",
            "Epoch: 1562 \tTraining Loss: 0.402005\n",
            "Epoch: 1563 \tTraining Loss: 0.401955\n",
            "Epoch: 1564 \tTraining Loss: 0.401904\n",
            "Epoch: 1565 \tTraining Loss: 0.401854\n",
            "Epoch: 1566 \tTraining Loss: 0.401804\n",
            "Epoch: 1567 \tTraining Loss: 0.401754\n",
            "Epoch: 1568 \tTraining Loss: 0.401704\n",
            "Epoch: 1569 \tTraining Loss: 0.401654\n",
            "Epoch: 1570 \tTraining Loss: 0.401604\n",
            "Epoch: 1571 \tTraining Loss: 0.401554\n",
            "Epoch: 1572 \tTraining Loss: 0.401504\n",
            "Epoch: 1573 \tTraining Loss: 0.401454\n",
            "Epoch: 1574 \tTraining Loss: 0.401404\n",
            "Epoch: 1575 \tTraining Loss: 0.401353\n",
            "Epoch: 1576 \tTraining Loss: 0.401303\n",
            "Epoch: 1577 \tTraining Loss: 0.401253\n",
            "Epoch: 1578 \tTraining Loss: 0.401202\n",
            "Epoch: 1579 \tTraining Loss: 0.401152\n",
            "Epoch: 1580 \tTraining Loss: 0.401101\n",
            "Epoch: 1581 \tTraining Loss: 0.401051\n",
            "Epoch: 1582 \tTraining Loss: 0.401000\n",
            "Epoch: 1583 \tTraining Loss: 0.400949\n",
            "Epoch: 1584 \tTraining Loss: 0.400897\n",
            "Epoch: 1585 \tTraining Loss: 0.400844\n",
            "Epoch: 1586 \tTraining Loss: 0.400790\n",
            "Epoch: 1587 \tTraining Loss: 0.400732\n",
            "Epoch: 1588 \tTraining Loss: 0.400663\n",
            "Epoch: 1589 \tTraining Loss: 0.400564\n",
            "Epoch: 1590 \tTraining Loss: 0.400409\n",
            "Epoch: 1591 \tTraining Loss: 0.400267\n",
            "Epoch: 1592 \tTraining Loss: 0.400220\n",
            "Epoch: 1593 \tTraining Loss: 0.400233\n",
            "Epoch: 1594 \tTraining Loss: 0.400209\n",
            "Epoch: 1595 \tTraining Loss: 0.400147\n",
            "Epoch: 1596 \tTraining Loss: 0.400058\n",
            "Epoch: 1597 \tTraining Loss: 0.399964\n",
            "Epoch: 1598 \tTraining Loss: 0.399905\n",
            "Epoch: 1599 \tTraining Loss: 0.399874\n",
            "Epoch: 1600 \tTraining Loss: 0.399848\n",
            "Epoch: 1601 \tTraining Loss: 0.399813\n",
            "Epoch: 1602 \tTraining Loss: 0.399771\n",
            "Epoch: 1603 \tTraining Loss: 0.399740\n",
            "Epoch: 1604 \tTraining Loss: 0.399760\n",
            "Epoch: 1605 \tTraining Loss: 0.399775\n",
            "Epoch: 1606 \tTraining Loss: 0.399808\n",
            "Epoch: 1607 \tTraining Loss: 0.399633\n",
            "Epoch: 1608 \tTraining Loss: 0.399462\n",
            "Epoch: 1609 \tTraining Loss: 0.399352\n",
            "Epoch: 1610 \tTraining Loss: 0.399347\n",
            "Epoch: 1611 \tTraining Loss: 0.399425\n",
            "Epoch: 1612 \tTraining Loss: 0.399469\n",
            "Epoch: 1613 \tTraining Loss: 0.399443\n",
            "Epoch: 1614 \tTraining Loss: 0.399170\n",
            "Epoch: 1615 \tTraining Loss: 0.399089\n",
            "Epoch: 1616 \tTraining Loss: 0.399198\n",
            "Epoch: 1617 \tTraining Loss: 0.399181\n",
            "Epoch: 1618 \tTraining Loss: 0.399059\n",
            "Epoch: 1619 \tTraining Loss: 0.398867\n",
            "Epoch: 1620 \tTraining Loss: 0.398799\n",
            "Epoch: 1621 \tTraining Loss: 0.398825\n",
            "Epoch: 1622 \tTraining Loss: 0.398780\n",
            "Epoch: 1623 \tTraining Loss: 0.398675\n",
            "Epoch: 1624 \tTraining Loss: 0.398582\n",
            "Epoch: 1625 \tTraining Loss: 0.398555\n",
            "Epoch: 1626 \tTraining Loss: 0.398552\n",
            "Epoch: 1627 \tTraining Loss: 0.398500\n",
            "Epoch: 1628 \tTraining Loss: 0.398412\n",
            "Epoch: 1629 \tTraining Loss: 0.398332\n",
            "Epoch: 1630 \tTraining Loss: 0.398299\n",
            "Epoch: 1631 \tTraining Loss: 0.398282\n",
            "Epoch: 1632 \tTraining Loss: 0.398227\n",
            "Epoch: 1633 \tTraining Loss: 0.398148\n",
            "Epoch: 1634 \tTraining Loss: 0.398076\n",
            "Epoch: 1635 \tTraining Loss: 0.398035\n",
            "Epoch: 1636 \tTraining Loss: 0.398001\n",
            "Epoch: 1637 \tTraining Loss: 0.397946\n",
            "Epoch: 1638 \tTraining Loss: 0.397880\n",
            "Epoch: 1639 \tTraining Loss: 0.397822\n",
            "Epoch: 1640 \tTraining Loss: 0.397779\n",
            "Epoch: 1641 \tTraining Loss: 0.397738\n",
            "Epoch: 1642 \tTraining Loss: 0.397685\n",
            "Epoch: 1643 \tTraining Loss: 0.397625\n",
            "Epoch: 1644 \tTraining Loss: 0.397568\n",
            "Epoch: 1645 \tTraining Loss: 0.397522\n",
            "Epoch: 1646 \tTraining Loss: 0.397478\n",
            "Epoch: 1647 \tTraining Loss: 0.397428\n",
            "Epoch: 1648 \tTraining Loss: 0.397371\n",
            "Epoch: 1649 \tTraining Loss: 0.397315\n",
            "Epoch: 1650 \tTraining Loss: 0.397264\n",
            "Epoch: 1651 \tTraining Loss: 0.397216\n",
            "Epoch: 1652 \tTraining Loss: 0.397167\n",
            "Epoch: 1653 \tTraining Loss: 0.397115\n",
            "Epoch: 1654 \tTraining Loss: 0.397060\n",
            "Epoch: 1655 \tTraining Loss: 0.397008\n",
            "Epoch: 1656 \tTraining Loss: 0.396958\n",
            "Epoch: 1657 \tTraining Loss: 0.396909\n",
            "Epoch: 1658 \tTraining Loss: 0.396859\n",
            "Epoch: 1659 \tTraining Loss: 0.396807\n",
            "Epoch: 1660 \tTraining Loss: 0.396754\n",
            "Epoch: 1661 \tTraining Loss: 0.396703\n",
            "Epoch: 1662 \tTraining Loss: 0.396653\n",
            "Epoch: 1663 \tTraining Loss: 0.396603\n",
            "Epoch: 1664 \tTraining Loss: 0.396552\n",
            "Epoch: 1665 \tTraining Loss: 0.396501\n",
            "Epoch: 1666 \tTraining Loss: 0.396449\n",
            "Epoch: 1667 \tTraining Loss: 0.396398\n",
            "Epoch: 1668 \tTraining Loss: 0.396348\n",
            "Epoch: 1669 \tTraining Loss: 0.396297\n",
            "Epoch: 1670 \tTraining Loss: 0.396247\n",
            "Epoch: 1671 \tTraining Loss: 0.396195\n",
            "Epoch: 1672 \tTraining Loss: 0.396144\n",
            "Epoch: 1673 \tTraining Loss: 0.396093\n",
            "Epoch: 1674 \tTraining Loss: 0.396042\n",
            "Epoch: 1675 \tTraining Loss: 0.395991\n",
            "Epoch: 1676 \tTraining Loss: 0.395941\n",
            "Epoch: 1677 \tTraining Loss: 0.395890\n",
            "Epoch: 1678 \tTraining Loss: 0.395839\n",
            "Epoch: 1679 \tTraining Loss: 0.395788\n",
            "Epoch: 1680 \tTraining Loss: 0.395736\n",
            "Epoch: 1681 \tTraining Loss: 0.395685\n",
            "Epoch: 1682 \tTraining Loss: 0.395635\n",
            "Epoch: 1683 \tTraining Loss: 0.395584\n",
            "Epoch: 1684 \tTraining Loss: 0.395533\n",
            "Epoch: 1685 \tTraining Loss: 0.395482\n",
            "Epoch: 1686 \tTraining Loss: 0.395431\n",
            "Epoch: 1687 \tTraining Loss: 0.395380\n",
            "Epoch: 1688 \tTraining Loss: 0.395329\n",
            "Epoch: 1689 \tTraining Loss: 0.395278\n",
            "Epoch: 1690 \tTraining Loss: 0.395227\n",
            "Epoch: 1691 \tTraining Loss: 0.395176\n",
            "Epoch: 1692 \tTraining Loss: 0.395125\n",
            "Epoch: 1693 \tTraining Loss: 0.395074\n",
            "Epoch: 1694 \tTraining Loss: 0.395023\n",
            "Epoch: 1695 \tTraining Loss: 0.394972\n",
            "Epoch: 1696 \tTraining Loss: 0.394921\n",
            "Epoch: 1697 \tTraining Loss: 0.394870\n",
            "Epoch: 1698 \tTraining Loss: 0.394818\n",
            "Epoch: 1699 \tTraining Loss: 0.394767\n",
            "Epoch: 1700 \tTraining Loss: 0.394716\n",
            "Epoch: 1701 \tTraining Loss: 0.394665\n",
            "Epoch: 1702 \tTraining Loss: 0.394614\n",
            "Epoch: 1703 \tTraining Loss: 0.394563\n",
            "Epoch: 1704 \tTraining Loss: 0.394512\n",
            "Epoch: 1705 \tTraining Loss: 0.394461\n",
            "Epoch: 1706 \tTraining Loss: 0.394410\n",
            "Epoch: 1707 \tTraining Loss: 0.394358\n",
            "Epoch: 1708 \tTraining Loss: 0.394307\n",
            "Epoch: 1709 \tTraining Loss: 0.394256\n",
            "Epoch: 1710 \tTraining Loss: 0.394205\n",
            "Epoch: 1711 \tTraining Loss: 0.394154\n",
            "Epoch: 1712 \tTraining Loss: 0.394102\n",
            "Epoch: 1713 \tTraining Loss: 0.394051\n",
            "Epoch: 1714 \tTraining Loss: 0.394000\n",
            "Epoch: 1715 \tTraining Loss: 0.393948\n",
            "Epoch: 1716 \tTraining Loss: 0.393897\n",
            "Epoch: 1717 \tTraining Loss: 0.393846\n",
            "Epoch: 1718 \tTraining Loss: 0.393795\n",
            "Epoch: 1719 \tTraining Loss: 0.393743\n",
            "Epoch: 1720 \tTraining Loss: 0.393692\n",
            "Epoch: 1721 \tTraining Loss: 0.393641\n",
            "Epoch: 1722 \tTraining Loss: 0.393589\n",
            "Epoch: 1723 \tTraining Loss: 0.393538\n",
            "Epoch: 1724 \tTraining Loss: 0.393487\n",
            "Epoch: 1725 \tTraining Loss: 0.393435\n",
            "Epoch: 1726 \tTraining Loss: 0.393384\n",
            "Epoch: 1727 \tTraining Loss: 0.393333\n",
            "Epoch: 1728 \tTraining Loss: 0.393282\n",
            "Epoch: 1729 \tTraining Loss: 0.393232\n",
            "Epoch: 1730 \tTraining Loss: 0.393182\n",
            "Epoch: 1731 \tTraining Loss: 0.393134\n",
            "Epoch: 1732 \tTraining Loss: 0.393088\n",
            "Epoch: 1733 \tTraining Loss: 0.393046\n",
            "Epoch: 1734 \tTraining Loss: 0.393012\n",
            "Epoch: 1735 \tTraining Loss: 0.392981\n",
            "Epoch: 1736 \tTraining Loss: 0.392971\n",
            "Epoch: 1737 \tTraining Loss: 0.392941\n",
            "Epoch: 1738 \tTraining Loss: 0.392927\n",
            "Epoch: 1739 \tTraining Loss: 0.392836\n",
            "Epoch: 1740 \tTraining Loss: 0.392749\n",
            "Epoch: 1741 \tTraining Loss: 0.392642\n",
            "Epoch: 1742 \tTraining Loss: 0.392564\n",
            "Epoch: 1743 \tTraining Loss: 0.392510\n",
            "Epoch: 1744 \tTraining Loss: 0.392473\n",
            "Epoch: 1745 \tTraining Loss: 0.392453\n",
            "Epoch: 1746 \tTraining Loss: 0.392429\n",
            "Epoch: 1747 \tTraining Loss: 0.392406\n",
            "Epoch: 1748 \tTraining Loss: 0.392327\n",
            "Epoch: 1749 \tTraining Loss: 0.392244\n",
            "Epoch: 1750 \tTraining Loss: 0.392159\n",
            "Epoch: 1751 \tTraining Loss: 0.392095\n",
            "Epoch: 1752 \tTraining Loss: 0.392046\n",
            "Epoch: 1753 \tTraining Loss: 0.392007\n",
            "Epoch: 1754 \tTraining Loss: 0.391978\n",
            "Epoch: 1755 \tTraining Loss: 0.391938\n",
            "Epoch: 1756 \tTraining Loss: 0.391893\n",
            "Epoch: 1757 \tTraining Loss: 0.391821\n",
            "Epoch: 1758 \tTraining Loss: 0.391752\n",
            "Epoch: 1759 \tTraining Loss: 0.391686\n",
            "Epoch: 1760 \tTraining Loss: 0.391627\n",
            "Epoch: 1761 \tTraining Loss: 0.391574\n",
            "Epoch: 1762 \tTraining Loss: 0.391526\n",
            "Epoch: 1763 \tTraining Loss: 0.391481\n",
            "Epoch: 1764 \tTraining Loss: 0.391434\n",
            "Epoch: 1765 \tTraining Loss: 0.391387\n",
            "Epoch: 1766 \tTraining Loss: 0.391334\n",
            "Epoch: 1767 \tTraining Loss: 0.391282\n",
            "Epoch: 1768 \tTraining Loss: 0.391225\n",
            "Epoch: 1769 \tTraining Loss: 0.391170\n",
            "Epoch: 1770 \tTraining Loss: 0.391112\n",
            "Epoch: 1771 \tTraining Loss: 0.391056\n",
            "Epoch: 1772 \tTraining Loss: 0.391001\n",
            "Epoch: 1773 \tTraining Loss: 0.390946\n",
            "Epoch: 1774 \tTraining Loss: 0.390894\n",
            "Epoch: 1775 \tTraining Loss: 0.390842\n",
            "Epoch: 1776 \tTraining Loss: 0.390790\n",
            "Epoch: 1777 \tTraining Loss: 0.390739\n",
            "Epoch: 1778 \tTraining Loss: 0.390688\n",
            "Epoch: 1779 \tTraining Loss: 0.390637\n",
            "Epoch: 1780 \tTraining Loss: 0.390586\n",
            "Epoch: 1781 \tTraining Loss: 0.390535\n",
            "Epoch: 1782 \tTraining Loss: 0.390485\n",
            "Epoch: 1783 \tTraining Loss: 0.390434\n",
            "Epoch: 1784 \tTraining Loss: 0.390384\n",
            "Epoch: 1785 \tTraining Loss: 0.390332\n",
            "Epoch: 1786 \tTraining Loss: 0.390282\n",
            "Epoch: 1787 \tTraining Loss: 0.390231\n",
            "Epoch: 1788 \tTraining Loss: 0.390181\n",
            "Epoch: 1789 \tTraining Loss: 0.390130\n",
            "Epoch: 1790 \tTraining Loss: 0.390081\n",
            "Epoch: 1791 \tTraining Loss: 0.390028\n",
            "Epoch: 1792 \tTraining Loss: 0.389978\n",
            "Epoch: 1793 \tTraining Loss: 0.389924\n",
            "Epoch: 1794 \tTraining Loss: 0.389874\n",
            "Epoch: 1795 \tTraining Loss: 0.389820\n",
            "Epoch: 1796 \tTraining Loss: 0.389771\n",
            "Epoch: 1797 \tTraining Loss: 0.389718\n",
            "Epoch: 1798 \tTraining Loss: 0.389671\n",
            "Epoch: 1799 \tTraining Loss: 0.389622\n",
            "Epoch: 1800 \tTraining Loss: 0.389581\n",
            "Epoch: 1801 \tTraining Loss: 0.389541\n",
            "Epoch: 1802 \tTraining Loss: 0.389515\n",
            "Epoch: 1803 \tTraining Loss: 0.389495\n",
            "Epoch: 1804 \tTraining Loss: 0.389516\n",
            "Epoch: 1805 \tTraining Loss: 0.389566\n",
            "Epoch: 1806 \tTraining Loss: 0.389793\n",
            "Epoch: 1807 \tTraining Loss: 0.390353\n",
            "Epoch: 1808 \tTraining Loss: 0.393515\n",
            "Epoch: 1809 \tTraining Loss: 0.409915\n",
            "Epoch: 1810 \tTraining Loss: 0.411802\n",
            "Epoch: 1811 \tTraining Loss: 0.424221\n",
            "Epoch: 1812 \tTraining Loss: 0.394518\n",
            "Epoch: 1813 \tTraining Loss: 0.430661\n",
            "Epoch: 1814 \tTraining Loss: 0.446760\n",
            "Epoch: 1815 \tTraining Loss: 0.416531\n",
            "Epoch: 1816 \tTraining Loss: 0.426179\n",
            "Epoch: 1817 \tTraining Loss: 0.422369\n",
            "Epoch: 1818 \tTraining Loss: 0.419990\n",
            "Epoch: 1819 \tTraining Loss: 0.406090\n",
            "Epoch: 1820 \tTraining Loss: 0.408653\n",
            "Epoch: 1821 \tTraining Loss: 0.408459\n",
            "Epoch: 1822 \tTraining Loss: 0.402985\n",
            "Epoch: 1823 \tTraining Loss: 0.403443\n",
            "Epoch: 1824 \tTraining Loss: 0.411018\n",
            "Epoch: 1825 \tTraining Loss: 0.403610\n",
            "Epoch: 1826 \tTraining Loss: 0.401911\n",
            "Epoch: 1827 \tTraining Loss: 0.404888\n",
            "Epoch: 1828 \tTraining Loss: 0.400382\n",
            "Epoch: 1829 \tTraining Loss: 0.399183\n",
            "Epoch: 1830 \tTraining Loss: 0.398459\n",
            "Epoch: 1831 \tTraining Loss: 0.400302\n",
            "Epoch: 1832 \tTraining Loss: 0.397263\n",
            "Epoch: 1833 \tTraining Loss: 0.395181\n",
            "Epoch: 1834 \tTraining Loss: 0.395597\n",
            "Epoch: 1835 \tTraining Loss: 0.395898\n",
            "Epoch: 1836 \tTraining Loss: 0.394729\n",
            "Epoch: 1837 \tTraining Loss: 0.393906\n",
            "Epoch: 1838 \tTraining Loss: 0.393214\n",
            "Epoch: 1839 \tTraining Loss: 0.392022\n",
            "Epoch: 1840 \tTraining Loss: 0.392786\n",
            "Epoch: 1841 \tTraining Loss: 0.392140\n",
            "Epoch: 1842 \tTraining Loss: 0.391457\n",
            "Epoch: 1843 \tTraining Loss: 0.391375\n",
            "Epoch: 1844 \tTraining Loss: 0.391008\n",
            "Epoch: 1845 \tTraining Loss: 0.390916\n",
            "Epoch: 1846 \tTraining Loss: 0.390303\n",
            "Epoch: 1847 \tTraining Loss: 0.390211\n",
            "Epoch: 1848 \tTraining Loss: 0.389916\n",
            "Epoch: 1849 \tTraining Loss: 0.389684\n",
            "Epoch: 1850 \tTraining Loss: 0.389252\n",
            "Epoch: 1851 \tTraining Loss: 0.389282\n",
            "Epoch: 1852 \tTraining Loss: 0.389052\n",
            "Epoch: 1853 \tTraining Loss: 0.389024\n",
            "Epoch: 1854 \tTraining Loss: 0.388673\n",
            "Epoch: 1855 \tTraining Loss: 0.388552\n",
            "Epoch: 1856 \tTraining Loss: 0.388493\n",
            "Epoch: 1857 \tTraining Loss: 0.388441\n",
            "Epoch: 1858 \tTraining Loss: 0.388185\n",
            "Epoch: 1859 \tTraining Loss: 0.388041\n",
            "Epoch: 1860 \tTraining Loss: 0.388049\n",
            "Epoch: 1861 \tTraining Loss: 0.387927\n",
            "Epoch: 1862 \tTraining Loss: 0.387735\n",
            "Epoch: 1863 \tTraining Loss: 0.387707\n",
            "Epoch: 1864 \tTraining Loss: 0.387649\n",
            "Epoch: 1865 \tTraining Loss: 0.387574\n",
            "Epoch: 1866 \tTraining Loss: 0.387504\n",
            "Epoch: 1867 \tTraining Loss: 0.387383\n",
            "Epoch: 1868 \tTraining Loss: 0.387339\n",
            "Epoch: 1869 \tTraining Loss: 0.387284\n",
            "Epoch: 1870 \tTraining Loss: 0.387217\n",
            "Epoch: 1871 \tTraining Loss: 0.387126\n",
            "Epoch: 1872 \tTraining Loss: 0.387055\n",
            "Epoch: 1873 \tTraining Loss: 0.387012\n",
            "Epoch: 1874 \tTraining Loss: 0.386967\n",
            "Epoch: 1875 \tTraining Loss: 0.386872\n",
            "Epoch: 1876 \tTraining Loss: 0.386820\n",
            "Epoch: 1877 \tTraining Loss: 0.386790\n",
            "Epoch: 1878 \tTraining Loss: 0.386729\n",
            "Epoch: 1879 \tTraining Loss: 0.386661\n",
            "Epoch: 1880 \tTraining Loss: 0.386607\n",
            "Epoch: 1881 \tTraining Loss: 0.386561\n",
            "Epoch: 1882 \tTraining Loss: 0.386513\n",
            "Epoch: 1883 \tTraining Loss: 0.386456\n",
            "Epoch: 1884 \tTraining Loss: 0.386406\n",
            "Epoch: 1885 \tTraining Loss: 0.386364\n",
            "Epoch: 1886 \tTraining Loss: 0.386322\n",
            "Epoch: 1887 \tTraining Loss: 0.386270\n",
            "Epoch: 1888 \tTraining Loss: 0.386217\n",
            "Epoch: 1889 \tTraining Loss: 0.386171\n",
            "Epoch: 1890 \tTraining Loss: 0.386129\n",
            "Epoch: 1891 \tTraining Loss: 0.386082\n",
            "Epoch: 1892 \tTraining Loss: 0.386034\n",
            "Epoch: 1893 \tTraining Loss: 0.385989\n",
            "Epoch: 1894 \tTraining Loss: 0.385946\n",
            "Epoch: 1895 \tTraining Loss: 0.385903\n",
            "Epoch: 1896 \tTraining Loss: 0.385858\n",
            "Epoch: 1897 \tTraining Loss: 0.385813\n",
            "Epoch: 1898 \tTraining Loss: 0.385770\n",
            "Epoch: 1899 \tTraining Loss: 0.385727\n",
            "Epoch: 1900 \tTraining Loss: 0.385683\n",
            "Epoch: 1901 \tTraining Loss: 0.385637\n",
            "Epoch: 1902 \tTraining Loss: 0.385591\n",
            "Epoch: 1903 \tTraining Loss: 0.385539\n",
            "Epoch: 1904 \tTraining Loss: 0.385476\n",
            "Epoch: 1905 \tTraining Loss: 0.385429\n",
            "Epoch: 1906 \tTraining Loss: 0.385396\n",
            "Epoch: 1907 \tTraining Loss: 0.385335\n",
            "Epoch: 1908 \tTraining Loss: 0.385301\n",
            "Epoch: 1909 \tTraining Loss: 0.385264\n",
            "Epoch: 1910 \tTraining Loss: 0.385212\n",
            "Epoch: 1911 \tTraining Loss: 0.385175\n",
            "Epoch: 1912 \tTraining Loss: 0.385132\n",
            "Epoch: 1913 \tTraining Loss: 0.385083\n",
            "Epoch: 1914 \tTraining Loss: 0.385043\n",
            "Epoch: 1915 \tTraining Loss: 0.385000\n",
            "Epoch: 1916 \tTraining Loss: 0.384955\n",
            "Epoch: 1917 \tTraining Loss: 0.384916\n",
            "Epoch: 1918 \tTraining Loss: 0.384875\n",
            "Epoch: 1919 \tTraining Loss: 0.384831\n",
            "Epoch: 1920 \tTraining Loss: 0.384791\n",
            "Epoch: 1921 \tTraining Loss: 0.384750\n",
            "Epoch: 1922 \tTraining Loss: 0.384705\n",
            "Epoch: 1923 \tTraining Loss: 0.384663\n",
            "Epoch: 1924 \tTraining Loss: 0.384623\n",
            "Epoch: 1925 \tTraining Loss: 0.384580\n",
            "Epoch: 1926 \tTraining Loss: 0.384539\n",
            "Epoch: 1927 \tTraining Loss: 0.384499\n",
            "Epoch: 1928 \tTraining Loss: 0.384457\n",
            "Epoch: 1929 \tTraining Loss: 0.384416\n",
            "Epoch: 1930 \tTraining Loss: 0.384375\n",
            "Epoch: 1931 \tTraining Loss: 0.384333\n",
            "Epoch: 1932 \tTraining Loss: 0.384292\n",
            "Epoch: 1933 \tTraining Loss: 0.384251\n",
            "Epoch: 1934 \tTraining Loss: 0.384209\n",
            "Epoch: 1935 \tTraining Loss: 0.384168\n",
            "Epoch: 1936 \tTraining Loss: 0.384128\n",
            "Epoch: 1937 \tTraining Loss: 0.384087\n",
            "Epoch: 1938 \tTraining Loss: 0.384046\n",
            "Epoch: 1939 \tTraining Loss: 0.384005\n",
            "Epoch: 1940 \tTraining Loss: 0.383964\n",
            "Epoch: 1941 \tTraining Loss: 0.383923\n",
            "Epoch: 1942 \tTraining Loss: 0.383882\n",
            "Epoch: 1943 \tTraining Loss: 0.383841\n",
            "Epoch: 1944 \tTraining Loss: 0.383801\n",
            "Epoch: 1945 \tTraining Loss: 0.383760\n",
            "Epoch: 1946 \tTraining Loss: 0.383719\n",
            "Epoch: 1947 \tTraining Loss: 0.383679\n",
            "Epoch: 1948 \tTraining Loss: 0.383638\n",
            "Epoch: 1949 \tTraining Loss: 0.383597\n",
            "Epoch: 1950 \tTraining Loss: 0.383557\n",
            "Epoch: 1951 \tTraining Loss: 0.383516\n",
            "Epoch: 1952 \tTraining Loss: 0.383475\n",
            "Epoch: 1953 \tTraining Loss: 0.383435\n",
            "Epoch: 1954 \tTraining Loss: 0.383394\n",
            "Epoch: 1955 \tTraining Loss: 0.383354\n",
            "Epoch: 1956 \tTraining Loss: 0.383313\n",
            "Epoch: 1957 \tTraining Loss: 0.383273\n",
            "Epoch: 1958 \tTraining Loss: 0.383232\n",
            "Epoch: 1959 \tTraining Loss: 0.383192\n",
            "Epoch: 1960 \tTraining Loss: 0.383152\n",
            "Epoch: 1961 \tTraining Loss: 0.383111\n",
            "Epoch: 1962 \tTraining Loss: 0.383071\n",
            "Epoch: 1963 \tTraining Loss: 0.383030\n",
            "Epoch: 1964 \tTraining Loss: 0.382990\n",
            "Epoch: 1965 \tTraining Loss: 0.382950\n",
            "Epoch: 1966 \tTraining Loss: 0.382909\n",
            "Epoch: 1967 \tTraining Loss: 0.382869\n",
            "Epoch: 1968 \tTraining Loss: 0.382829\n",
            "Epoch: 1969 \tTraining Loss: 0.382788\n",
            "Epoch: 1970 \tTraining Loss: 0.382748\n",
            "Epoch: 1971 \tTraining Loss: 0.382708\n",
            "Epoch: 1972 \tTraining Loss: 0.382668\n",
            "Epoch: 1973 \tTraining Loss: 0.382627\n",
            "Epoch: 1974 \tTraining Loss: 0.382587\n",
            "Epoch: 1975 \tTraining Loss: 0.382547\n",
            "Epoch: 1976 \tTraining Loss: 0.382507\n",
            "Epoch: 1977 \tTraining Loss: 0.382467\n",
            "Epoch: 1978 \tTraining Loss: 0.382426\n",
            "Epoch: 1979 \tTraining Loss: 0.382386\n",
            "Epoch: 1980 \tTraining Loss: 0.382346\n",
            "Epoch: 1981 \tTraining Loss: 0.382306\n",
            "Epoch: 1982 \tTraining Loss: 0.382266\n",
            "Epoch: 1983 \tTraining Loss: 0.382226\n",
            "Epoch: 1984 \tTraining Loss: 0.382186\n",
            "Epoch: 1985 \tTraining Loss: 0.382146\n",
            "Epoch: 1986 \tTraining Loss: 0.382105\n",
            "Epoch: 1987 \tTraining Loss: 0.382065\n",
            "Epoch: 1988 \tTraining Loss: 0.382025\n",
            "Epoch: 1989 \tTraining Loss: 0.381985\n",
            "Epoch: 1990 \tTraining Loss: 0.381945\n",
            "Epoch: 1991 \tTraining Loss: 0.381905\n",
            "Epoch: 1992 \tTraining Loss: 0.381865\n",
            "Epoch: 1993 \tTraining Loss: 0.381825\n",
            "Epoch: 1994 \tTraining Loss: 0.381785\n",
            "Epoch: 1995 \tTraining Loss: 0.381745\n",
            "Epoch: 1996 \tTraining Loss: 0.381705\n",
            "Epoch: 1997 \tTraining Loss: 0.381665\n",
            "Epoch: 1998 \tTraining Loss: 0.381625\n",
            "Epoch: 1999 \tTraining Loss: 0.381585\n",
            "Epoch: 2000 \tTraining Loss: 0.381545\n",
            "Epoch: 2001 \tTraining Loss: 0.381505\n",
            "Epoch: 2002 \tTraining Loss: 0.381465\n",
            "Epoch: 2003 \tTraining Loss: 0.381426\n",
            "Epoch: 2004 \tTraining Loss: 0.381386\n",
            "Epoch: 2005 \tTraining Loss: 0.381346\n",
            "Epoch: 2006 \tTraining Loss: 0.381306\n",
            "Epoch: 2007 \tTraining Loss: 0.381266\n",
            "Epoch: 2008 \tTraining Loss: 0.381226\n",
            "Epoch: 2009 \tTraining Loss: 0.381186\n",
            "Epoch: 2010 \tTraining Loss: 0.381146\n",
            "Epoch: 2011 \tTraining Loss: 0.381106\n",
            "Epoch: 2012 \tTraining Loss: 0.381067\n",
            "Epoch: 2013 \tTraining Loss: 0.381027\n",
            "Epoch: 2014 \tTraining Loss: 0.380987\n",
            "Epoch: 2015 \tTraining Loss: 0.380947\n",
            "Epoch: 2016 \tTraining Loss: 0.380907\n",
            "Epoch: 2017 \tTraining Loss: 0.380868\n",
            "Epoch: 2018 \tTraining Loss: 0.380828\n",
            "Epoch: 2019 \tTraining Loss: 0.380788\n",
            "Epoch: 2020 \tTraining Loss: 0.380748\n",
            "Epoch: 2021 \tTraining Loss: 0.380708\n",
            "Epoch: 2022 \tTraining Loss: 0.380668\n",
            "Epoch: 2023 \tTraining Loss: 0.380629\n",
            "Epoch: 2024 \tTraining Loss: 0.380589\n",
            "Epoch: 2025 \tTraining Loss: 0.380549\n",
            "Epoch: 2026 \tTraining Loss: 0.380509\n",
            "Epoch: 2027 \tTraining Loss: 0.380470\n",
            "Epoch: 2028 \tTraining Loss: 0.380430\n",
            "Epoch: 2029 \tTraining Loss: 0.380390\n",
            "Epoch: 2030 \tTraining Loss: 0.380350\n",
            "Epoch: 2031 \tTraining Loss: 0.380311\n",
            "Epoch: 2032 \tTraining Loss: 0.380271\n",
            "Epoch: 2033 \tTraining Loss: 0.380231\n",
            "Epoch: 2034 \tTraining Loss: 0.380192\n",
            "Epoch: 2035 \tTraining Loss: 0.380152\n",
            "Epoch: 2036 \tTraining Loss: 0.380112\n",
            "Epoch: 2037 \tTraining Loss: 0.380072\n",
            "Epoch: 2038 \tTraining Loss: 0.380033\n",
            "Epoch: 2039 \tTraining Loss: 0.379993\n",
            "Epoch: 2040 \tTraining Loss: 0.379953\n",
            "Epoch: 2041 \tTraining Loss: 0.379914\n",
            "Epoch: 2042 \tTraining Loss: 0.379874\n",
            "Epoch: 2043 \tTraining Loss: 0.379834\n",
            "Epoch: 2044 \tTraining Loss: 0.379795\n",
            "Epoch: 2045 \tTraining Loss: 0.379755\n",
            "Epoch: 2046 \tTraining Loss: 0.379715\n",
            "Epoch: 2047 \tTraining Loss: 0.379676\n",
            "Epoch: 2048 \tTraining Loss: 0.379636\n",
            "Epoch: 2049 \tTraining Loss: 0.379596\n",
            "Epoch: 2050 \tTraining Loss: 0.379557\n",
            "Epoch: 2051 \tTraining Loss: 0.379517\n",
            "Epoch: 2052 \tTraining Loss: 0.379477\n",
            "Epoch: 2053 \tTraining Loss: 0.379438\n",
            "Epoch: 2054 \tTraining Loss: 0.379398\n",
            "Epoch: 2055 \tTraining Loss: 0.379359\n",
            "Epoch: 2056 \tTraining Loss: 0.379319\n",
            "Epoch: 2057 \tTraining Loss: 0.379279\n",
            "Epoch: 2058 \tTraining Loss: 0.379240\n",
            "Epoch: 2059 \tTraining Loss: 0.379200\n",
            "Epoch: 2060 \tTraining Loss: 0.379160\n",
            "Epoch: 2061 \tTraining Loss: 0.379121\n",
            "Epoch: 2062 \tTraining Loss: 0.379081\n",
            "Epoch: 2063 \tTraining Loss: 0.379042\n",
            "Epoch: 2064 \tTraining Loss: 0.379002\n",
            "Epoch: 2065 \tTraining Loss: 0.378963\n",
            "Epoch: 2066 \tTraining Loss: 0.378923\n",
            "Epoch: 2067 \tTraining Loss: 0.378883\n",
            "Epoch: 2068 \tTraining Loss: 0.378844\n",
            "Epoch: 2069 \tTraining Loss: 0.378804\n",
            "Epoch: 2070 \tTraining Loss: 0.378765\n",
            "Epoch: 2071 \tTraining Loss: 0.378725\n",
            "Epoch: 2072 \tTraining Loss: 0.378686\n",
            "Epoch: 2073 \tTraining Loss: 0.378646\n",
            "Epoch: 2074 \tTraining Loss: 0.378606\n",
            "Epoch: 2075 \tTraining Loss: 0.378567\n",
            "Epoch: 2076 \tTraining Loss: 0.378527\n",
            "Epoch: 2077 \tTraining Loss: 0.378488\n",
            "Epoch: 2078 \tTraining Loss: 0.378448\n",
            "Epoch: 2079 \tTraining Loss: 0.378409\n",
            "Epoch: 2080 \tTraining Loss: 0.378369\n",
            "Epoch: 2081 \tTraining Loss: 0.378330\n",
            "Epoch: 2082 \tTraining Loss: 0.378290\n",
            "Epoch: 2083 \tTraining Loss: 0.378250\n",
            "Epoch: 2084 \tTraining Loss: 0.378211\n",
            "Epoch: 2085 \tTraining Loss: 0.378171\n",
            "Epoch: 2086 \tTraining Loss: 0.378132\n",
            "Epoch: 2087 \tTraining Loss: 0.378092\n",
            "Epoch: 2088 \tTraining Loss: 0.378053\n",
            "Epoch: 2089 \tTraining Loss: 0.378013\n",
            "Epoch: 2090 \tTraining Loss: 0.377974\n",
            "Epoch: 2091 \tTraining Loss: 0.377934\n",
            "Epoch: 2092 \tTraining Loss: 0.377895\n",
            "Epoch: 2093 \tTraining Loss: 0.377855\n",
            "Epoch: 2094 \tTraining Loss: 0.377815\n",
            "Epoch: 2095 \tTraining Loss: 0.377776\n",
            "Epoch: 2096 \tTraining Loss: 0.377736\n",
            "Epoch: 2097 \tTraining Loss: 0.377697\n",
            "Epoch: 2098 \tTraining Loss: 0.377657\n",
            "Epoch: 2099 \tTraining Loss: 0.377618\n",
            "Epoch: 2100 \tTraining Loss: 0.377578\n",
            "Epoch: 2101 \tTraining Loss: 0.377539\n",
            "Epoch: 2102 \tTraining Loss: 0.377499\n",
            "Epoch: 2103 \tTraining Loss: 0.377460\n",
            "Epoch: 2104 \tTraining Loss: 0.377420\n",
            "Epoch: 2105 \tTraining Loss: 0.377381\n",
            "Epoch: 2106 \tTraining Loss: 0.377341\n",
            "Epoch: 2107 \tTraining Loss: 0.377301\n",
            "Epoch: 2108 \tTraining Loss: 0.377262\n",
            "Epoch: 2109 \tTraining Loss: 0.377222\n",
            "Epoch: 2110 \tTraining Loss: 0.377183\n",
            "Epoch: 2111 \tTraining Loss: 0.377143\n",
            "Epoch: 2112 \tTraining Loss: 0.377104\n",
            "Epoch: 2113 \tTraining Loss: 0.377064\n",
            "Epoch: 2114 \tTraining Loss: 0.377025\n",
            "Epoch: 2115 \tTraining Loss: 0.376985\n",
            "Epoch: 2116 \tTraining Loss: 0.376946\n",
            "Epoch: 2117 \tTraining Loss: 0.376906\n",
            "Epoch: 2118 \tTraining Loss: 0.376867\n",
            "Epoch: 2119 \tTraining Loss: 0.376827\n",
            "Epoch: 2120 \tTraining Loss: 0.376787\n",
            "Epoch: 2121 \tTraining Loss: 0.376748\n",
            "Epoch: 2122 \tTraining Loss: 0.376708\n",
            "Epoch: 2123 \tTraining Loss: 0.376669\n",
            "Epoch: 2124 \tTraining Loss: 0.376629\n",
            "Epoch: 2125 \tTraining Loss: 0.376590\n",
            "Epoch: 2126 \tTraining Loss: 0.376550\n",
            "Epoch: 2127 \tTraining Loss: 0.376510\n",
            "Epoch: 2128 \tTraining Loss: 0.376471\n",
            "Epoch: 2129 \tTraining Loss: 0.376431\n",
            "Epoch: 2130 \tTraining Loss: 0.376392\n",
            "Epoch: 2131 \tTraining Loss: 0.376352\n",
            "Epoch: 2132 \tTraining Loss: 0.376313\n",
            "Epoch: 2133 \tTraining Loss: 0.376273\n",
            "Epoch: 2134 \tTraining Loss: 0.376233\n",
            "Epoch: 2135 \tTraining Loss: 0.376194\n",
            "Epoch: 2136 \tTraining Loss: 0.376154\n",
            "Epoch: 2137 \tTraining Loss: 0.376115\n",
            "Epoch: 2138 \tTraining Loss: 0.376075\n",
            "Epoch: 2139 \tTraining Loss: 0.376035\n",
            "Epoch: 2140 \tTraining Loss: 0.375996\n",
            "Epoch: 2141 \tTraining Loss: 0.375956\n",
            "Epoch: 2142 \tTraining Loss: 0.375916\n",
            "Epoch: 2143 \tTraining Loss: 0.375877\n",
            "Epoch: 2144 \tTraining Loss: 0.375837\n",
            "Epoch: 2145 \tTraining Loss: 0.375798\n",
            "Epoch: 2146 \tTraining Loss: 0.375758\n",
            "Epoch: 2147 \tTraining Loss: 0.375718\n",
            "Epoch: 2148 \tTraining Loss: 0.375679\n",
            "Epoch: 2149 \tTraining Loss: 0.375639\n",
            "Epoch: 2150 \tTraining Loss: 0.375599\n",
            "Epoch: 2151 \tTraining Loss: 0.375560\n",
            "Epoch: 2152 \tTraining Loss: 0.375520\n",
            "Epoch: 2153 \tTraining Loss: 0.375480\n",
            "Epoch: 2154 \tTraining Loss: 0.375441\n",
            "Epoch: 2155 \tTraining Loss: 0.375401\n",
            "Epoch: 2156 \tTraining Loss: 0.375361\n",
            "Epoch: 2157 \tTraining Loss: 0.375322\n",
            "Epoch: 2158 \tTraining Loss: 0.375282\n",
            "Epoch: 2159 \tTraining Loss: 0.375242\n",
            "Epoch: 2160 \tTraining Loss: 0.375203\n",
            "Epoch: 2161 \tTraining Loss: 0.375163\n",
            "Epoch: 2162 \tTraining Loss: 0.375123\n",
            "Epoch: 2163 \tTraining Loss: 0.375083\n",
            "Epoch: 2164 \tTraining Loss: 0.375044\n",
            "Epoch: 2165 \tTraining Loss: 0.375004\n",
            "Epoch: 2166 \tTraining Loss: 0.374964\n",
            "Epoch: 2167 \tTraining Loss: 0.374925\n",
            "Epoch: 2168 \tTraining Loss: 0.374885\n",
            "Epoch: 2169 \tTraining Loss: 0.374845\n",
            "Epoch: 2170 \tTraining Loss: 0.374805\n",
            "Epoch: 2171 \tTraining Loss: 0.374766\n",
            "Epoch: 2172 \tTraining Loss: 0.374726\n",
            "Epoch: 2173 \tTraining Loss: 0.374686\n",
            "Epoch: 2174 \tTraining Loss: 0.374646\n",
            "Epoch: 2175 \tTraining Loss: 0.374607\n",
            "Epoch: 2176 \tTraining Loss: 0.374567\n",
            "Epoch: 2177 \tTraining Loss: 0.374527\n",
            "Epoch: 2178 \tTraining Loss: 0.374487\n",
            "Epoch: 2179 \tTraining Loss: 0.374447\n",
            "Epoch: 2180 \tTraining Loss: 0.374408\n",
            "Epoch: 2181 \tTraining Loss: 0.374368\n",
            "Epoch: 2182 \tTraining Loss: 0.374328\n",
            "Epoch: 2183 \tTraining Loss: 0.374288\n",
            "Epoch: 2184 \tTraining Loss: 0.374248\n",
            "Epoch: 2185 \tTraining Loss: 0.374209\n",
            "Epoch: 2186 \tTraining Loss: 0.374169\n",
            "Epoch: 2187 \tTraining Loss: 0.374129\n",
            "Epoch: 2188 \tTraining Loss: 0.374089\n",
            "Epoch: 2189 \tTraining Loss: 0.374049\n",
            "Epoch: 2190 \tTraining Loss: 0.374009\n",
            "Epoch: 2191 \tTraining Loss: 0.373970\n",
            "Epoch: 2192 \tTraining Loss: 0.373930\n",
            "Epoch: 2193 \tTraining Loss: 0.373890\n",
            "Epoch: 2194 \tTraining Loss: 0.373850\n",
            "Epoch: 2195 \tTraining Loss: 0.373810\n",
            "Epoch: 2196 \tTraining Loss: 0.373770\n",
            "Epoch: 2197 \tTraining Loss: 0.373731\n",
            "Epoch: 2198 \tTraining Loss: 0.373691\n",
            "Epoch: 2199 \tTraining Loss: 0.373651\n",
            "Epoch: 2200 \tTraining Loss: 0.373611\n",
            "Epoch: 2201 \tTraining Loss: 0.373571\n",
            "Epoch: 2202 \tTraining Loss: 0.373531\n",
            "Epoch: 2203 \tTraining Loss: 0.373491\n",
            "Epoch: 2204 \tTraining Loss: 0.373451\n",
            "Epoch: 2205 \tTraining Loss: 0.373412\n",
            "Epoch: 2206 \tTraining Loss: 0.373372\n",
            "Epoch: 2207 \tTraining Loss: 0.373332\n",
            "Epoch: 2208 \tTraining Loss: 0.373292\n",
            "Epoch: 2209 \tTraining Loss: 0.373252\n",
            "Epoch: 2210 \tTraining Loss: 0.373212\n",
            "Epoch: 2211 \tTraining Loss: 0.373172\n",
            "Epoch: 2212 \tTraining Loss: 0.373132\n",
            "Epoch: 2213 \tTraining Loss: 0.373092\n",
            "Epoch: 2214 \tTraining Loss: 0.373052\n",
            "Epoch: 2215 \tTraining Loss: 0.373013\n",
            "Epoch: 2216 \tTraining Loss: 0.372973\n",
            "Epoch: 2217 \tTraining Loss: 0.372933\n",
            "Epoch: 2218 \tTraining Loss: 0.372893\n",
            "Epoch: 2219 \tTraining Loss: 0.372853\n",
            "Epoch: 2220 \tTraining Loss: 0.372813\n",
            "Epoch: 2221 \tTraining Loss: 0.372773\n",
            "Epoch: 2222 \tTraining Loss: 0.372733\n",
            "Epoch: 2223 \tTraining Loss: 0.372693\n",
            "Epoch: 2224 \tTraining Loss: 0.372653\n",
            "Epoch: 2225 \tTraining Loss: 0.372613\n",
            "Epoch: 2226 \tTraining Loss: 0.372574\n",
            "Epoch: 2227 \tTraining Loss: 0.372534\n",
            "Epoch: 2228 \tTraining Loss: 0.372494\n",
            "Epoch: 2229 \tTraining Loss: 0.372454\n",
            "Epoch: 2230 \tTraining Loss: 0.372414\n",
            "Epoch: 2231 \tTraining Loss: 0.372374\n",
            "Epoch: 2232 \tTraining Loss: 0.372334\n",
            "Epoch: 2233 \tTraining Loss: 0.372294\n",
            "Epoch: 2234 \tTraining Loss: 0.372254\n",
            "Epoch: 2235 \tTraining Loss: 0.372214\n",
            "Epoch: 2236 \tTraining Loss: 0.372174\n",
            "Epoch: 2237 \tTraining Loss: 0.372135\n",
            "Epoch: 2238 \tTraining Loss: 0.372095\n",
            "Epoch: 2239 \tTraining Loss: 0.372055\n",
            "Epoch: 2240 \tTraining Loss: 0.372015\n",
            "Epoch: 2241 \tTraining Loss: 0.371975\n",
            "Epoch: 2242 \tTraining Loss: 0.371936\n",
            "Epoch: 2243 \tTraining Loss: 0.371897\n",
            "Epoch: 2244 \tTraining Loss: 0.371859\n",
            "Epoch: 2245 \tTraining Loss: 0.371824\n",
            "Epoch: 2246 \tTraining Loss: 0.371792\n",
            "Epoch: 2247 \tTraining Loss: 0.371772\n",
            "Epoch: 2248 \tTraining Loss: 0.371760\n",
            "Epoch: 2249 \tTraining Loss: 0.371791\n",
            "Epoch: 2250 \tTraining Loss: 0.371794\n",
            "Epoch: 2251 \tTraining Loss: 0.371869\n",
            "Epoch: 2252 \tTraining Loss: 0.371724\n",
            "Epoch: 2253 \tTraining Loss: 0.371613\n",
            "Epoch: 2254 \tTraining Loss: 0.371479\n",
            "Epoch: 2255 \tTraining Loss: 0.371423\n",
            "Epoch: 2256 \tTraining Loss: 0.371437\n",
            "Epoch: 2257 \tTraining Loss: 0.371493\n",
            "Epoch: 2258 \tTraining Loss: 0.371656\n",
            "Epoch: 2259 \tTraining Loss: 0.371517\n",
            "Epoch: 2260 \tTraining Loss: 0.371374\n",
            "Epoch: 2261 \tTraining Loss: 0.371208\n",
            "Epoch: 2262 \tTraining Loss: 0.371246\n",
            "Epoch: 2263 \tTraining Loss: 0.371481\n",
            "Epoch: 2264 \tTraining Loss: 0.371444\n",
            "Epoch: 2265 \tTraining Loss: 0.371471\n",
            "Epoch: 2266 \tTraining Loss: 0.371115\n",
            "Epoch: 2267 \tTraining Loss: 0.371111\n",
            "Epoch: 2268 \tTraining Loss: 0.371203\n",
            "Epoch: 2269 \tTraining Loss: 0.370956\n",
            "Epoch: 2270 \tTraining Loss: 0.370913\n",
            "Epoch: 2271 \tTraining Loss: 0.370971\n",
            "Epoch: 2272 \tTraining Loss: 0.370975\n",
            "Epoch: 2273 \tTraining Loss: 0.370922\n",
            "Epoch: 2274 \tTraining Loss: 0.370752\n",
            "Epoch: 2275 \tTraining Loss: 0.370796\n",
            "Epoch: 2276 \tTraining Loss: 0.370802\n",
            "Epoch: 2277 \tTraining Loss: 0.370642\n",
            "Epoch: 2278 \tTraining Loss: 0.370550\n",
            "Epoch: 2279 \tTraining Loss: 0.370526\n",
            "Epoch: 2280 \tTraining Loss: 0.370510\n",
            "Epoch: 2281 \tTraining Loss: 0.370496\n",
            "Epoch: 2282 \tTraining Loss: 0.370435\n",
            "Epoch: 2283 \tTraining Loss: 0.370357\n",
            "Epoch: 2284 \tTraining Loss: 0.370292\n",
            "Epoch: 2285 \tTraining Loss: 0.370293\n",
            "Epoch: 2286 \tTraining Loss: 0.370285\n",
            "Epoch: 2287 \tTraining Loss: 0.370185\n",
            "Epoch: 2288 \tTraining Loss: 0.370108\n",
            "Epoch: 2289 \tTraining Loss: 0.370097\n",
            "Epoch: 2290 \tTraining Loss: 0.370072\n",
            "Epoch: 2291 \tTraining Loss: 0.370013\n",
            "Epoch: 2292 \tTraining Loss: 0.369961\n",
            "Epoch: 2293 \tTraining Loss: 0.369920\n",
            "Epoch: 2294 \tTraining Loss: 0.369886\n",
            "Epoch: 2295 \tTraining Loss: 0.369849\n",
            "Epoch: 2296 \tTraining Loss: 0.369815\n",
            "Epoch: 2297 \tTraining Loss: 0.369762\n",
            "Epoch: 2298 \tTraining Loss: 0.369708\n",
            "Epoch: 2299 \tTraining Loss: 0.369678\n",
            "Epoch: 2300 \tTraining Loss: 0.369647\n",
            "Epoch: 2301 \tTraining Loss: 0.369600\n",
            "Epoch: 2302 \tTraining Loss: 0.369549\n",
            "Epoch: 2303 \tTraining Loss: 0.369511\n",
            "Epoch: 2304 \tTraining Loss: 0.369474\n",
            "Epoch: 2305 \tTraining Loss: 0.369434\n",
            "Epoch: 2306 \tTraining Loss: 0.369393\n",
            "Epoch: 2307 \tTraining Loss: 0.369351\n",
            "Epoch: 2308 \tTraining Loss: 0.369309\n",
            "Epoch: 2309 \tTraining Loss: 0.369268\n",
            "Epoch: 2310 \tTraining Loss: 0.369232\n",
            "Epoch: 2311 \tTraining Loss: 0.369194\n",
            "Epoch: 2312 \tTraining Loss: 0.369149\n",
            "Epoch: 2313 \tTraining Loss: 0.369106\n",
            "Epoch: 2314 \tTraining Loss: 0.369068\n",
            "Epoch: 2315 \tTraining Loss: 0.369030\n",
            "Epoch: 2316 \tTraining Loss: 0.368990\n",
            "Epoch: 2317 \tTraining Loss: 0.368948\n",
            "Epoch: 2318 \tTraining Loss: 0.368908\n",
            "Epoch: 2319 \tTraining Loss: 0.368867\n",
            "Epoch: 2320 \tTraining Loss: 0.368827\n",
            "Epoch: 2321 \tTraining Loss: 0.368788\n",
            "Epoch: 2322 \tTraining Loss: 0.368748\n",
            "Epoch: 2323 \tTraining Loss: 0.368707\n",
            "Epoch: 2324 \tTraining Loss: 0.368666\n",
            "Epoch: 2325 \tTraining Loss: 0.368626\n",
            "Epoch: 2326 \tTraining Loss: 0.368587\n",
            "Epoch: 2327 \tTraining Loss: 0.368547\n",
            "Epoch: 2328 \tTraining Loss: 0.368507\n",
            "Epoch: 2329 \tTraining Loss: 0.368466\n",
            "Epoch: 2330 \tTraining Loss: 0.368426\n",
            "Epoch: 2331 \tTraining Loss: 0.368386\n",
            "Epoch: 2332 \tTraining Loss: 0.368346\n",
            "Epoch: 2333 \tTraining Loss: 0.368306\n",
            "Epoch: 2334 \tTraining Loss: 0.368266\n",
            "Epoch: 2335 \tTraining Loss: 0.368226\n",
            "Epoch: 2336 \tTraining Loss: 0.368186\n",
            "Epoch: 2337 \tTraining Loss: 0.368146\n",
            "Epoch: 2338 \tTraining Loss: 0.368106\n",
            "Epoch: 2339 \tTraining Loss: 0.368066\n",
            "Epoch: 2340 \tTraining Loss: 0.368026\n",
            "Epoch: 2341 \tTraining Loss: 0.367985\n",
            "Epoch: 2342 \tTraining Loss: 0.367945\n",
            "Epoch: 2343 \tTraining Loss: 0.367905\n",
            "Epoch: 2344 \tTraining Loss: 0.367865\n",
            "Epoch: 2345 \tTraining Loss: 0.367825\n",
            "Epoch: 2346 \tTraining Loss: 0.367785\n",
            "Epoch: 2347 \tTraining Loss: 0.367745\n",
            "Epoch: 2348 \tTraining Loss: 0.367704\n",
            "Epoch: 2349 \tTraining Loss: 0.367664\n",
            "Epoch: 2350 \tTraining Loss: 0.367624\n",
            "Epoch: 2351 \tTraining Loss: 0.367584\n",
            "Epoch: 2352 \tTraining Loss: 0.367544\n",
            "Epoch: 2353 \tTraining Loss: 0.367503\n",
            "Epoch: 2354 \tTraining Loss: 0.367463\n",
            "Epoch: 2355 \tTraining Loss: 0.367423\n",
            "Epoch: 2356 \tTraining Loss: 0.367383\n",
            "Epoch: 2357 \tTraining Loss: 0.367343\n",
            "Epoch: 2358 \tTraining Loss: 0.367302\n",
            "Epoch: 2359 \tTraining Loss: 0.367262\n",
            "Epoch: 2360 \tTraining Loss: 0.367222\n",
            "Epoch: 2361 \tTraining Loss: 0.367182\n",
            "Epoch: 2362 \tTraining Loss: 0.367142\n",
            "Epoch: 2363 \tTraining Loss: 0.367101\n",
            "Epoch: 2364 \tTraining Loss: 0.367061\n",
            "Epoch: 2365 \tTraining Loss: 0.367021\n",
            "Epoch: 2366 \tTraining Loss: 0.366980\n",
            "Epoch: 2367 \tTraining Loss: 0.366940\n",
            "Epoch: 2368 \tTraining Loss: 0.366900\n",
            "Epoch: 2369 \tTraining Loss: 0.366860\n",
            "Epoch: 2370 \tTraining Loss: 0.366819\n",
            "Epoch: 2371 \tTraining Loss: 0.366779\n",
            "Epoch: 2372 \tTraining Loss: 0.366739\n",
            "Epoch: 2373 \tTraining Loss: 0.366699\n",
            "Epoch: 2374 \tTraining Loss: 0.366658\n",
            "Epoch: 2375 \tTraining Loss: 0.366618\n",
            "Epoch: 2376 \tTraining Loss: 0.366578\n",
            "Epoch: 2377 \tTraining Loss: 0.366537\n",
            "Epoch: 2378 \tTraining Loss: 0.366497\n",
            "Epoch: 2379 \tTraining Loss: 0.366457\n",
            "Epoch: 2380 \tTraining Loss: 0.366416\n",
            "Epoch: 2381 \tTraining Loss: 0.366376\n",
            "Epoch: 2382 \tTraining Loss: 0.366336\n",
            "Epoch: 2383 \tTraining Loss: 0.366295\n",
            "Epoch: 2384 \tTraining Loss: 0.366255\n",
            "Epoch: 2385 \tTraining Loss: 0.366214\n",
            "Epoch: 2386 \tTraining Loss: 0.366174\n",
            "Epoch: 2387 \tTraining Loss: 0.366134\n",
            "Epoch: 2388 \tTraining Loss: 0.366093\n",
            "Epoch: 2389 \tTraining Loss: 0.366053\n",
            "Epoch: 2390 \tTraining Loss: 0.366013\n",
            "Epoch: 2391 \tTraining Loss: 0.365972\n",
            "Epoch: 2392 \tTraining Loss: 0.365932\n",
            "Epoch: 2393 \tTraining Loss: 0.365891\n",
            "Epoch: 2394 \tTraining Loss: 0.365851\n",
            "Epoch: 2395 \tTraining Loss: 0.365811\n",
            "Epoch: 2396 \tTraining Loss: 0.365770\n",
            "Epoch: 2397 \tTraining Loss: 0.365730\n",
            "Epoch: 2398 \tTraining Loss: 0.365689\n",
            "Epoch: 2399 \tTraining Loss: 0.365649\n",
            "Epoch: 2400 \tTraining Loss: 0.365609\n",
            "Epoch: 2401 \tTraining Loss: 0.365568\n",
            "Epoch: 2402 \tTraining Loss: 0.365528\n",
            "Epoch: 2403 \tTraining Loss: 0.365488\n",
            "Epoch: 2404 \tTraining Loss: 0.365447\n",
            "Epoch: 2405 \tTraining Loss: 0.365407\n",
            "Epoch: 2406 \tTraining Loss: 0.365367\n",
            "Epoch: 2407 \tTraining Loss: 0.365327\n",
            "Epoch: 2408 \tTraining Loss: 0.365288\n",
            "Epoch: 2409 \tTraining Loss: 0.365249\n",
            "Epoch: 2410 \tTraining Loss: 0.365211\n",
            "Epoch: 2411 \tTraining Loss: 0.365176\n",
            "Epoch: 2412 \tTraining Loss: 0.365146\n",
            "Epoch: 2413 \tTraining Loss: 0.365124\n",
            "Epoch: 2414 \tTraining Loss: 0.365120\n",
            "Epoch: 2415 \tTraining Loss: 0.365150\n",
            "Epoch: 2416 \tTraining Loss: 0.365255\n",
            "Epoch: 2417 \tTraining Loss: 0.365528\n",
            "Epoch: 2418 \tTraining Loss: 0.366376\n",
            "Epoch: 2419 \tTraining Loss: 0.371409\n",
            "Epoch: 2420 \tTraining Loss: 0.402595\n",
            "Epoch: 2421 \tTraining Loss: 0.440925\n",
            "Epoch: 2422 \tTraining Loss: 0.501376\n",
            "Epoch: 2423 \tTraining Loss: 0.490385\n",
            "Epoch: 2424 \tTraining Loss: 0.408199\n",
            "Epoch: 2425 \tTraining Loss: 0.494748\n",
            "Epoch: 2426 \tTraining Loss: 0.413563\n",
            "Epoch: 2427 \tTraining Loss: 0.430456\n",
            "Epoch: 2428 \tTraining Loss: 0.450681\n",
            "Epoch: 2429 \tTraining Loss: 0.398621\n",
            "Epoch: 2430 \tTraining Loss: 0.418721\n",
            "Epoch: 2431 \tTraining Loss: 0.410698\n",
            "Epoch: 2432 \tTraining Loss: 0.403908\n",
            "Epoch: 2433 \tTraining Loss: 0.413426\n",
            "Epoch: 2434 \tTraining Loss: 0.401854\n",
            "Epoch: 2435 \tTraining Loss: 0.391203\n",
            "Epoch: 2436 \tTraining Loss: 0.397258\n",
            "Epoch: 2437 \tTraining Loss: 0.395558\n",
            "Epoch: 2438 \tTraining Loss: 0.396500\n",
            "Epoch: 2439 \tTraining Loss: 0.395959\n",
            "Epoch: 2440 \tTraining Loss: 0.384778\n",
            "Epoch: 2441 \tTraining Loss: 0.386638\n",
            "Epoch: 2442 \tTraining Loss: 0.386419\n",
            "Epoch: 2443 \tTraining Loss: 0.383598\n",
            "Epoch: 2444 \tTraining Loss: 0.382076\n",
            "Epoch: 2445 \tTraining Loss: 0.382056\n",
            "Epoch: 2446 \tTraining Loss: 0.377918\n",
            "Epoch: 2447 \tTraining Loss: 0.378854\n",
            "Epoch: 2448 \tTraining Loss: 0.376488\n",
            "Epoch: 2449 \tTraining Loss: 0.377973\n",
            "Epoch: 2450 \tTraining Loss: 0.374266\n",
            "Epoch: 2451 \tTraining Loss: 0.374542\n",
            "Epoch: 2452 \tTraining Loss: 0.371942\n",
            "Epoch: 2453 \tTraining Loss: 0.373400\n",
            "Epoch: 2454 \tTraining Loss: 0.371296\n",
            "Epoch: 2455 \tTraining Loss: 0.371001\n",
            "Epoch: 2456 \tTraining Loss: 0.370815\n",
            "Epoch: 2457 \tTraining Loss: 0.370187\n",
            "Epoch: 2458 \tTraining Loss: 0.369340\n",
            "Epoch: 2459 \tTraining Loss: 0.369120\n",
            "Epoch: 2460 \tTraining Loss: 0.369244\n",
            "Epoch: 2461 \tTraining Loss: 0.368117\n",
            "Epoch: 2462 \tTraining Loss: 0.367901\n",
            "Epoch: 2463 \tTraining Loss: 0.368074\n",
            "Epoch: 2464 \tTraining Loss: 0.367621\n",
            "Epoch: 2465 \tTraining Loss: 0.367260\n",
            "Epoch: 2466 \tTraining Loss: 0.366901\n",
            "Epoch: 2467 \tTraining Loss: 0.366773\n",
            "Epoch: 2468 \tTraining Loss: 0.366607\n",
            "Epoch: 2469 \tTraining Loss: 0.366087\n",
            "Epoch: 2470 \tTraining Loss: 0.366229\n",
            "Epoch: 2471 \tTraining Loss: 0.365989\n",
            "Epoch: 2472 \tTraining Loss: 0.365873\n",
            "Epoch: 2473 \tTraining Loss: 0.365545\n",
            "Epoch: 2474 \tTraining Loss: 0.365620\n",
            "Epoch: 2475 \tTraining Loss: 0.365359\n",
            "Epoch: 2476 \tTraining Loss: 0.365205\n",
            "Epoch: 2477 \tTraining Loss: 0.365251\n",
            "Epoch: 2478 \tTraining Loss: 0.364890\n",
            "Epoch: 2479 \tTraining Loss: 0.364879\n",
            "Epoch: 2480 \tTraining Loss: 0.364753\n",
            "Epoch: 2481 \tTraining Loss: 0.364720\n",
            "Epoch: 2482 \tTraining Loss: 0.364496\n",
            "Epoch: 2483 \tTraining Loss: 0.364498\n",
            "Epoch: 2484 \tTraining Loss: 0.364388\n",
            "Epoch: 2485 \tTraining Loss: 0.364332\n",
            "Epoch: 2486 \tTraining Loss: 0.364280\n",
            "Epoch: 2487 \tTraining Loss: 0.364162\n",
            "Epoch: 2488 \tTraining Loss: 0.364103\n",
            "Epoch: 2489 \tTraining Loss: 0.364016\n",
            "Epoch: 2490 \tTraining Loss: 0.363977\n",
            "Epoch: 2491 \tTraining Loss: 0.363877\n",
            "Epoch: 2492 \tTraining Loss: 0.363858\n",
            "Epoch: 2493 \tTraining Loss: 0.363778\n",
            "Epoch: 2494 \tTraining Loss: 0.363711\n",
            "Epoch: 2495 \tTraining Loss: 0.363684\n",
            "Epoch: 2496 \tTraining Loss: 0.363600\n",
            "Epoch: 2497 \tTraining Loss: 0.363554\n",
            "Epoch: 2498 \tTraining Loss: 0.363505\n",
            "Epoch: 2499 \tTraining Loss: 0.363454\n",
            "Epoch: 2500 \tTraining Loss: 0.363402\n",
            "Epoch: 2501 \tTraining Loss: 0.363365\n",
            "Epoch: 2502 \tTraining Loss: 0.363303\n",
            "Epoch: 2503 \tTraining Loss: 0.363252\n",
            "Epoch: 2504 \tTraining Loss: 0.363228\n",
            "Epoch: 2505 \tTraining Loss: 0.363168\n",
            "Epoch: 2506 \tTraining Loss: 0.363128\n",
            "Epoch: 2507 \tTraining Loss: 0.363090\n",
            "Epoch: 2508 \tTraining Loss: 0.363042\n",
            "Epoch: 2509 \tTraining Loss: 0.363002\n",
            "Epoch: 2510 \tTraining Loss: 0.362964\n",
            "Epoch: 2511 \tTraining Loss: 0.362923\n",
            "Epoch: 2512 \tTraining Loss: 0.362881\n",
            "Epoch: 2513 \tTraining Loss: 0.362848\n",
            "Epoch: 2514 \tTraining Loss: 0.362805\n",
            "Epoch: 2515 \tTraining Loss: 0.362772\n",
            "Epoch: 2516 \tTraining Loss: 0.362735\n",
            "Epoch: 2517 \tTraining Loss: 0.362699\n",
            "Epoch: 2518 \tTraining Loss: 0.362663\n",
            "Epoch: 2519 \tTraining Loss: 0.362630\n",
            "Epoch: 2520 \tTraining Loss: 0.362596\n",
            "Epoch: 2521 \tTraining Loss: 0.362560\n",
            "Epoch: 2522 \tTraining Loss: 0.362529\n",
            "Epoch: 2523 \tTraining Loss: 0.362494\n",
            "Epoch: 2524 \tTraining Loss: 0.362462\n",
            "Epoch: 2525 \tTraining Loss: 0.362430\n",
            "Epoch: 2526 \tTraining Loss: 0.362398\n",
            "Epoch: 2527 \tTraining Loss: 0.362365\n",
            "Epoch: 2528 \tTraining Loss: 0.362335\n",
            "Epoch: 2529 \tTraining Loss: 0.362302\n",
            "Epoch: 2530 \tTraining Loss: 0.362271\n",
            "Epoch: 2531 \tTraining Loss: 0.362239\n",
            "Epoch: 2532 \tTraining Loss: 0.362208\n",
            "Epoch: 2533 \tTraining Loss: 0.362177\n",
            "Epoch: 2534 \tTraining Loss: 0.362146\n",
            "Epoch: 2535 \tTraining Loss: 0.362115\n",
            "Epoch: 2536 \tTraining Loss: 0.362084\n",
            "Epoch: 2537 \tTraining Loss: 0.362054\n",
            "Epoch: 2538 \tTraining Loss: 0.362023\n",
            "Epoch: 2539 \tTraining Loss: 0.361992\n",
            "Epoch: 2540 \tTraining Loss: 0.361962\n",
            "Epoch: 2541 \tTraining Loss: 0.361931\n",
            "Epoch: 2542 \tTraining Loss: 0.361901\n",
            "Epoch: 2543 \tTraining Loss: 0.361871\n",
            "Epoch: 2544 \tTraining Loss: 0.361840\n",
            "Epoch: 2545 \tTraining Loss: 0.361810\n",
            "Epoch: 2546 \tTraining Loss: 0.361780\n",
            "Epoch: 2547 \tTraining Loss: 0.361750\n",
            "Epoch: 2548 \tTraining Loss: 0.361720\n",
            "Epoch: 2549 \tTraining Loss: 0.361690\n",
            "Epoch: 2550 \tTraining Loss: 0.361660\n",
            "Epoch: 2551 \tTraining Loss: 0.361630\n",
            "Epoch: 2552 \tTraining Loss: 0.361600\n",
            "Epoch: 2553 \tTraining Loss: 0.361570\n",
            "Epoch: 2554 \tTraining Loss: 0.361540\n",
            "Epoch: 2555 \tTraining Loss: 0.361510\n",
            "Epoch: 2556 \tTraining Loss: 0.361480\n",
            "Epoch: 2557 \tTraining Loss: 0.361450\n",
            "Epoch: 2558 \tTraining Loss: 0.361421\n",
            "Epoch: 2559 \tTraining Loss: 0.361391\n",
            "Epoch: 2560 \tTraining Loss: 0.361361\n",
            "Epoch: 2561 \tTraining Loss: 0.361331\n",
            "Epoch: 2562 \tTraining Loss: 0.361301\n",
            "Epoch: 2563 \tTraining Loss: 0.361272\n",
            "Epoch: 2564 \tTraining Loss: 0.361242\n",
            "Epoch: 2565 \tTraining Loss: 0.361212\n",
            "Epoch: 2566 \tTraining Loss: 0.361182\n",
            "Epoch: 2567 \tTraining Loss: 0.361152\n",
            "Epoch: 2568 \tTraining Loss: 0.361123\n",
            "Epoch: 2569 \tTraining Loss: 0.361093\n",
            "Epoch: 2570 \tTraining Loss: 0.361063\n",
            "Epoch: 2571 \tTraining Loss: 0.361033\n",
            "Epoch: 2572 \tTraining Loss: 0.361003\n",
            "Epoch: 2573 \tTraining Loss: 0.360974\n",
            "Epoch: 2574 \tTraining Loss: 0.360944\n",
            "Epoch: 2575 \tTraining Loss: 0.360914\n",
            "Epoch: 2576 \tTraining Loss: 0.360884\n",
            "Epoch: 2577 \tTraining Loss: 0.360854\n",
            "Epoch: 2578 \tTraining Loss: 0.360824\n",
            "Epoch: 2579 \tTraining Loss: 0.360795\n",
            "Epoch: 2580 \tTraining Loss: 0.360765\n",
            "Epoch: 2581 \tTraining Loss: 0.360735\n",
            "Epoch: 2582 \tTraining Loss: 0.360705\n",
            "Epoch: 2583 \tTraining Loss: 0.360675\n",
            "Epoch: 2584 \tTraining Loss: 0.360645\n",
            "Epoch: 2585 \tTraining Loss: 0.360616\n",
            "Epoch: 2586 \tTraining Loss: 0.360586\n",
            "Epoch: 2587 \tTraining Loss: 0.360556\n",
            "Epoch: 2588 \tTraining Loss: 0.360526\n",
            "Epoch: 2589 \tTraining Loss: 0.360496\n",
            "Epoch: 2590 \tTraining Loss: 0.360466\n",
            "Epoch: 2591 \tTraining Loss: 0.360437\n",
            "Epoch: 2592 \tTraining Loss: 0.360407\n",
            "Epoch: 2593 \tTraining Loss: 0.360377\n",
            "Epoch: 2594 \tTraining Loss: 0.360348\n",
            "Epoch: 2595 \tTraining Loss: 0.360318\n",
            "Epoch: 2596 \tTraining Loss: 0.360288\n",
            "Epoch: 2597 \tTraining Loss: 0.360258\n",
            "Epoch: 2598 \tTraining Loss: 0.360229\n",
            "Epoch: 2599 \tTraining Loss: 0.360199\n",
            "Epoch: 2600 \tTraining Loss: 0.360170\n",
            "Epoch: 2601 \tTraining Loss: 0.360140\n",
            "Epoch: 2602 \tTraining Loss: 0.360110\n",
            "Epoch: 2603 \tTraining Loss: 0.360081\n",
            "Epoch: 2604 \tTraining Loss: 0.360051\n",
            "Epoch: 2605 \tTraining Loss: 0.360022\n",
            "Epoch: 2606 \tTraining Loss: 0.359992\n",
            "Epoch: 2607 \tTraining Loss: 0.359963\n",
            "Epoch: 2608 \tTraining Loss: 0.359934\n",
            "Epoch: 2609 \tTraining Loss: 0.359904\n",
            "Epoch: 2610 \tTraining Loss: 0.359875\n",
            "Epoch: 2611 \tTraining Loss: 0.359846\n",
            "Epoch: 2612 \tTraining Loss: 0.359816\n",
            "Epoch: 2613 \tTraining Loss: 0.359787\n",
            "Epoch: 2614 \tTraining Loss: 0.359758\n",
            "Epoch: 2615 \tTraining Loss: 0.359729\n",
            "Epoch: 2616 \tTraining Loss: 0.359700\n",
            "Epoch: 2617 \tTraining Loss: 0.359671\n",
            "Epoch: 2618 \tTraining Loss: 0.359642\n",
            "Epoch: 2619 \tTraining Loss: 0.359613\n",
            "Epoch: 2620 \tTraining Loss: 0.359584\n",
            "Epoch: 2621 \tTraining Loss: 0.359555\n",
            "Epoch: 2622 \tTraining Loss: 0.359526\n",
            "Epoch: 2623 \tTraining Loss: 0.359497\n",
            "Epoch: 2624 \tTraining Loss: 0.359468\n",
            "Epoch: 2625 \tTraining Loss: 0.359439\n",
            "Epoch: 2626 \tTraining Loss: 0.359410\n",
            "Epoch: 2627 \tTraining Loss: 0.359382\n",
            "Epoch: 2628 \tTraining Loss: 0.359353\n",
            "Epoch: 2629 \tTraining Loss: 0.359324\n",
            "Epoch: 2630 \tTraining Loss: 0.359296\n",
            "Epoch: 2631 \tTraining Loss: 0.359267\n",
            "Epoch: 2632 \tTraining Loss: 0.359238\n",
            "Epoch: 2633 \tTraining Loss: 0.359210\n",
            "Epoch: 2634 \tTraining Loss: 0.359181\n",
            "Epoch: 2635 \tTraining Loss: 0.359153\n",
            "Epoch: 2636 \tTraining Loss: 0.359124\n",
            "Epoch: 2637 \tTraining Loss: 0.359096\n",
            "Epoch: 2638 \tTraining Loss: 0.359067\n",
            "Epoch: 2639 \tTraining Loss: 0.359039\n",
            "Epoch: 2640 \tTraining Loss: 0.359010\n",
            "Epoch: 2641 \tTraining Loss: 0.358982\n",
            "Epoch: 2642 \tTraining Loss: 0.358954\n",
            "Epoch: 2643 \tTraining Loss: 0.358925\n",
            "Epoch: 2644 \tTraining Loss: 0.358897\n",
            "Epoch: 2645 \tTraining Loss: 0.358869\n",
            "Epoch: 2646 \tTraining Loss: 0.358840\n",
            "Epoch: 2647 \tTraining Loss: 0.358812\n",
            "Epoch: 2648 \tTraining Loss: 0.358784\n",
            "Epoch: 2649 \tTraining Loss: 0.358756\n",
            "Epoch: 2650 \tTraining Loss: 0.358727\n",
            "Epoch: 2651 \tTraining Loss: 0.358699\n",
            "Epoch: 2652 \tTraining Loss: 0.358671\n",
            "Epoch: 2653 \tTraining Loss: 0.358643\n",
            "Epoch: 2654 \tTraining Loss: 0.358615\n",
            "Epoch: 2655 \tTraining Loss: 0.358587\n",
            "Epoch: 2656 \tTraining Loss: 0.358559\n",
            "Epoch: 2657 \tTraining Loss: 0.358531\n",
            "Epoch: 2658 \tTraining Loss: 0.358503\n",
            "Epoch: 2659 \tTraining Loss: 0.358475\n",
            "Epoch: 2660 \tTraining Loss: 0.358447\n",
            "Epoch: 2661 \tTraining Loss: 0.358419\n",
            "Epoch: 2662 \tTraining Loss: 0.358391\n",
            "Epoch: 2663 \tTraining Loss: 0.358363\n",
            "Epoch: 2664 \tTraining Loss: 0.358335\n",
            "Epoch: 2665 \tTraining Loss: 0.358307\n",
            "Epoch: 2666 \tTraining Loss: 0.358279\n",
            "Epoch: 2667 \tTraining Loss: 0.358251\n",
            "Epoch: 2668 \tTraining Loss: 0.358223\n",
            "Epoch: 2669 \tTraining Loss: 0.358195\n",
            "Epoch: 2670 \tTraining Loss: 0.358167\n",
            "Epoch: 2671 \tTraining Loss: 0.358139\n",
            "Epoch: 2672 \tTraining Loss: 0.358112\n",
            "Epoch: 2673 \tTraining Loss: 0.358084\n",
            "Epoch: 2674 \tTraining Loss: 0.358056\n",
            "Epoch: 2675 \tTraining Loss: 0.358028\n",
            "Epoch: 2676 \tTraining Loss: 0.358000\n",
            "Epoch: 2677 \tTraining Loss: 0.357972\n",
            "Epoch: 2678 \tTraining Loss: 0.357944\n",
            "Epoch: 2679 \tTraining Loss: 0.357917\n",
            "Epoch: 2680 \tTraining Loss: 0.357889\n",
            "Epoch: 2681 \tTraining Loss: 0.357861\n",
            "Epoch: 2682 \tTraining Loss: 0.357832\n",
            "Epoch: 2683 \tTraining Loss: 0.357804\n",
            "Epoch: 2684 \tTraining Loss: 0.357775\n",
            "Epoch: 2685 \tTraining Loss: 0.357747\n",
            "Epoch: 2686 \tTraining Loss: 0.357718\n",
            "Epoch: 2687 \tTraining Loss: 0.357690\n",
            "Epoch: 2688 \tTraining Loss: 0.357662\n",
            "Epoch: 2689 \tTraining Loss: 0.357635\n",
            "Epoch: 2690 \tTraining Loss: 0.357606\n",
            "Epoch: 2691 \tTraining Loss: 0.357578\n",
            "Epoch: 2692 \tTraining Loss: 0.357550\n",
            "Epoch: 2693 \tTraining Loss: 0.357522\n",
            "Epoch: 2694 \tTraining Loss: 0.357493\n",
            "Epoch: 2695 \tTraining Loss: 0.357465\n",
            "Epoch: 2696 \tTraining Loss: 0.357437\n",
            "Epoch: 2697 \tTraining Loss: 0.357408\n",
            "Epoch: 2698 \tTraining Loss: 0.357380\n",
            "Epoch: 2699 \tTraining Loss: 0.357352\n",
            "Epoch: 2700 \tTraining Loss: 0.357324\n",
            "Epoch: 2701 \tTraining Loss: 0.357296\n",
            "Epoch: 2702 \tTraining Loss: 0.357268\n",
            "Epoch: 2703 \tTraining Loss: 0.357240\n",
            "Epoch: 2704 \tTraining Loss: 0.357211\n",
            "Epoch: 2705 \tTraining Loss: 0.357183\n",
            "Epoch: 2706 \tTraining Loss: 0.357155\n",
            "Epoch: 2707 \tTraining Loss: 0.357127\n",
            "Epoch: 2708 \tTraining Loss: 0.357098\n",
            "Epoch: 2709 \tTraining Loss: 0.357070\n",
            "Epoch: 2710 \tTraining Loss: 0.357042\n",
            "Epoch: 2711 \tTraining Loss: 0.357014\n",
            "Epoch: 2712 \tTraining Loss: 0.356986\n",
            "Epoch: 2713 \tTraining Loss: 0.356957\n",
            "Epoch: 2714 \tTraining Loss: 0.356929\n",
            "Epoch: 2715 \tTraining Loss: 0.356901\n",
            "Epoch: 2716 \tTraining Loss: 0.356873\n",
            "Epoch: 2717 \tTraining Loss: 0.356844\n",
            "Epoch: 2718 \tTraining Loss: 0.356816\n",
            "Epoch: 2719 \tTraining Loss: 0.356788\n",
            "Epoch: 2720 \tTraining Loss: 0.356759\n",
            "Epoch: 2721 \tTraining Loss: 0.356732\n",
            "Epoch: 2722 \tTraining Loss: 0.356704\n",
            "Epoch: 2723 \tTraining Loss: 0.356677\n",
            "Epoch: 2724 \tTraining Loss: 0.356648\n",
            "Epoch: 2725 \tTraining Loss: 0.356618\n",
            "Epoch: 2726 \tTraining Loss: 0.356590\n",
            "Epoch: 2727 \tTraining Loss: 0.356562\n",
            "Epoch: 2728 \tTraining Loss: 0.356534\n",
            "Epoch: 2729 \tTraining Loss: 0.356505\n",
            "Epoch: 2730 \tTraining Loss: 0.356477\n",
            "Epoch: 2731 \tTraining Loss: 0.356449\n",
            "Epoch: 2732 \tTraining Loss: 0.356421\n",
            "Epoch: 2733 \tTraining Loss: 0.356392\n",
            "Epoch: 2734 \tTraining Loss: 0.356363\n",
            "Epoch: 2735 \tTraining Loss: 0.356336\n",
            "Epoch: 2736 \tTraining Loss: 0.356308\n",
            "Epoch: 2737 \tTraining Loss: 0.356279\n",
            "Epoch: 2738 \tTraining Loss: 0.356250\n",
            "Epoch: 2739 \tTraining Loss: 0.356222\n",
            "Epoch: 2740 \tTraining Loss: 0.356194\n",
            "Epoch: 2741 \tTraining Loss: 0.356166\n",
            "Epoch: 2742 \tTraining Loss: 0.356138\n",
            "Epoch: 2743 \tTraining Loss: 0.356111\n",
            "Epoch: 2744 \tTraining Loss: 0.356083\n",
            "Epoch: 2745 \tTraining Loss: 0.356055\n",
            "Epoch: 2746 \tTraining Loss: 0.356026\n",
            "Epoch: 2747 \tTraining Loss: 0.355997\n",
            "Epoch: 2748 \tTraining Loss: 0.355968\n",
            "Epoch: 2749 \tTraining Loss: 0.355939\n",
            "Epoch: 2750 \tTraining Loss: 0.355911\n",
            "Epoch: 2751 \tTraining Loss: 0.355883\n",
            "Epoch: 2752 \tTraining Loss: 0.355855\n",
            "Epoch: 2753 \tTraining Loss: 0.355826\n",
            "Epoch: 2754 \tTraining Loss: 0.355797\n",
            "Epoch: 2755 \tTraining Loss: 0.355768\n",
            "Epoch: 2756 \tTraining Loss: 0.355740\n",
            "Epoch: 2757 \tTraining Loss: 0.355712\n",
            "Epoch: 2758 \tTraining Loss: 0.355683\n",
            "Epoch: 2759 \tTraining Loss: 0.355654\n",
            "Epoch: 2760 \tTraining Loss: 0.355625\n",
            "Epoch: 2761 \tTraining Loss: 0.355597\n",
            "Epoch: 2762 \tTraining Loss: 0.355569\n",
            "Epoch: 2763 \tTraining Loss: 0.355541\n",
            "Epoch: 2764 \tTraining Loss: 0.355512\n",
            "Epoch: 2765 \tTraining Loss: 0.355484\n",
            "Epoch: 2766 \tTraining Loss: 0.355457\n",
            "Epoch: 2767 \tTraining Loss: 0.355429\n",
            "Epoch: 2768 \tTraining Loss: 0.355401\n",
            "Epoch: 2769 \tTraining Loss: 0.355372\n",
            "Epoch: 2770 \tTraining Loss: 0.355341\n",
            "Epoch: 2771 \tTraining Loss: 0.355311\n",
            "Epoch: 2772 \tTraining Loss: 0.355281\n",
            "Epoch: 2773 \tTraining Loss: 0.355253\n",
            "Epoch: 2774 \tTraining Loss: 0.355225\n",
            "Epoch: 2775 \tTraining Loss: 0.355197\n",
            "Epoch: 2776 \tTraining Loss: 0.355168\n",
            "Epoch: 2777 \tTraining Loss: 0.355138\n",
            "Epoch: 2778 \tTraining Loss: 0.355108\n",
            "Epoch: 2779 \tTraining Loss: 0.355079\n",
            "Epoch: 2780 \tTraining Loss: 0.355049\n",
            "Epoch: 2781 \tTraining Loss: 0.355021\n",
            "Epoch: 2782 \tTraining Loss: 0.354992\n",
            "Epoch: 2783 \tTraining Loss: 0.354963\n",
            "Epoch: 2784 \tTraining Loss: 0.354935\n",
            "Epoch: 2785 \tTraining Loss: 0.354905\n",
            "Epoch: 2786 \tTraining Loss: 0.354876\n",
            "Epoch: 2787 \tTraining Loss: 0.354846\n",
            "Epoch: 2788 \tTraining Loss: 0.354817\n",
            "Epoch: 2789 \tTraining Loss: 0.354788\n",
            "Epoch: 2790 \tTraining Loss: 0.354759\n",
            "Epoch: 2791 \tTraining Loss: 0.354730\n",
            "Epoch: 2792 \tTraining Loss: 0.354701\n",
            "Epoch: 2793 \tTraining Loss: 0.354672\n",
            "Epoch: 2794 \tTraining Loss: 0.354644\n",
            "Epoch: 2795 \tTraining Loss: 0.354615\n",
            "Epoch: 2796 \tTraining Loss: 0.354586\n",
            "Epoch: 2797 \tTraining Loss: 0.354557\n",
            "Epoch: 2798 \tTraining Loss: 0.354529\n",
            "Epoch: 2799 \tTraining Loss: 0.354500\n",
            "Epoch: 2800 \tTraining Loss: 0.354471\n",
            "Epoch: 2801 \tTraining Loss: 0.354442\n",
            "Epoch: 2802 \tTraining Loss: 0.354413\n",
            "Epoch: 2803 \tTraining Loss: 0.354385\n",
            "Epoch: 2804 \tTraining Loss: 0.354356\n",
            "Epoch: 2805 \tTraining Loss: 0.354327\n",
            "Epoch: 2806 \tTraining Loss: 0.354298\n",
            "Epoch: 2807 \tTraining Loss: 0.354270\n",
            "Epoch: 2808 \tTraining Loss: 0.354241\n",
            "Epoch: 2809 \tTraining Loss: 0.354213\n",
            "Epoch: 2810 \tTraining Loss: 0.354184\n",
            "Epoch: 2811 \tTraining Loss: 0.354156\n",
            "Epoch: 2812 \tTraining Loss: 0.354128\n",
            "Epoch: 2813 \tTraining Loss: 0.354099\n",
            "Epoch: 2814 \tTraining Loss: 0.354071\n",
            "Epoch: 2815 \tTraining Loss: 0.354043\n",
            "Epoch: 2816 \tTraining Loss: 0.354015\n",
            "Epoch: 2817 \tTraining Loss: 0.353987\n",
            "Epoch: 2818 \tTraining Loss: 0.353959\n",
            "Epoch: 2819 \tTraining Loss: 0.353933\n",
            "Epoch: 2820 \tTraining Loss: 0.353907\n",
            "Epoch: 2821 \tTraining Loss: 0.353882\n",
            "Epoch: 2822 \tTraining Loss: 0.353857\n",
            "Epoch: 2823 \tTraining Loss: 0.353829\n",
            "Epoch: 2824 \tTraining Loss: 0.353798\n",
            "Epoch: 2825 \tTraining Loss: 0.353766\n",
            "Epoch: 2826 \tTraining Loss: 0.353735\n",
            "Epoch: 2827 \tTraining Loss: 0.353704\n",
            "Epoch: 2828 \tTraining Loss: 0.353676\n",
            "Epoch: 2829 \tTraining Loss: 0.353650\n",
            "Epoch: 2830 \tTraining Loss: 0.353623\n",
            "Epoch: 2831 \tTraining Loss: 0.353595\n",
            "Epoch: 2832 \tTraining Loss: 0.353565\n",
            "Epoch: 2833 \tTraining Loss: 0.353536\n",
            "Epoch: 2834 \tTraining Loss: 0.353508\n",
            "Epoch: 2835 \tTraining Loss: 0.353480\n",
            "Epoch: 2836 \tTraining Loss: 0.353451\n",
            "Epoch: 2837 \tTraining Loss: 0.353423\n",
            "Epoch: 2838 \tTraining Loss: 0.353395\n",
            "Epoch: 2839 \tTraining Loss: 0.353366\n",
            "Epoch: 2840 \tTraining Loss: 0.353338\n",
            "Epoch: 2841 \tTraining Loss: 0.353309\n",
            "Epoch: 2842 \tTraining Loss: 0.353281\n",
            "Epoch: 2843 \tTraining Loss: 0.353253\n",
            "Epoch: 2844 \tTraining Loss: 0.353226\n",
            "Epoch: 2845 \tTraining Loss: 0.353198\n",
            "Epoch: 2846 \tTraining Loss: 0.353169\n",
            "Epoch: 2847 \tTraining Loss: 0.353141\n",
            "Epoch: 2848 \tTraining Loss: 0.353112\n",
            "Epoch: 2849 \tTraining Loss: 0.353084\n",
            "Epoch: 2850 \tTraining Loss: 0.353056\n",
            "Epoch: 2851 \tTraining Loss: 0.353027\n",
            "Epoch: 2852 \tTraining Loss: 0.352999\n",
            "Epoch: 2853 \tTraining Loss: 0.352971\n",
            "Epoch: 2854 \tTraining Loss: 0.352944\n",
            "Epoch: 2855 \tTraining Loss: 0.352915\n",
            "Epoch: 2856 \tTraining Loss: 0.352887\n",
            "Epoch: 2857 \tTraining Loss: 0.352859\n",
            "Epoch: 2858 \tTraining Loss: 0.352831\n",
            "Epoch: 2859 \tTraining Loss: 0.352803\n",
            "Epoch: 2860 \tTraining Loss: 0.352775\n",
            "Epoch: 2861 \tTraining Loss: 0.352747\n",
            "Epoch: 2862 \tTraining Loss: 0.352719\n",
            "Epoch: 2863 \tTraining Loss: 0.352690\n",
            "Epoch: 2864 \tTraining Loss: 0.352662\n",
            "Epoch: 2865 \tTraining Loss: 0.352634\n",
            "Epoch: 2866 \tTraining Loss: 0.352606\n",
            "Epoch: 2867 \tTraining Loss: 0.352578\n",
            "Epoch: 2868 \tTraining Loss: 0.352550\n",
            "Epoch: 2869 \tTraining Loss: 0.352522\n",
            "Epoch: 2870 \tTraining Loss: 0.352495\n",
            "Epoch: 2871 \tTraining Loss: 0.352467\n",
            "Epoch: 2872 \tTraining Loss: 0.352440\n",
            "Epoch: 2873 \tTraining Loss: 0.352414\n",
            "Epoch: 2874 \tTraining Loss: 0.352391\n",
            "Epoch: 2875 \tTraining Loss: 0.352370\n",
            "Epoch: 2876 \tTraining Loss: 0.352352\n",
            "Epoch: 2877 \tTraining Loss: 0.352335\n",
            "Epoch: 2878 \tTraining Loss: 0.352314\n",
            "Epoch: 2879 \tTraining Loss: 0.352290\n",
            "Epoch: 2880 \tTraining Loss: 0.352257\n",
            "Epoch: 2881 \tTraining Loss: 0.352232\n",
            "Epoch: 2882 \tTraining Loss: 0.352207\n",
            "Epoch: 2883 \tTraining Loss: 0.352208\n",
            "Epoch: 2884 \tTraining Loss: 0.352186\n",
            "Epoch: 2885 \tTraining Loss: 0.352177\n",
            "Epoch: 2886 \tTraining Loss: 0.352118\n",
            "Epoch: 2887 \tTraining Loss: 0.352073\n",
            "Epoch: 2888 \tTraining Loss: 0.352018\n",
            "Epoch: 2889 \tTraining Loss: 0.351978\n",
            "Epoch: 2890 \tTraining Loss: 0.351945\n",
            "Epoch: 2891 \tTraining Loss: 0.351917\n",
            "Epoch: 2892 \tTraining Loss: 0.351889\n",
            "Epoch: 2893 \tTraining Loss: 0.351859\n",
            "Epoch: 2894 \tTraining Loss: 0.351833\n",
            "Epoch: 2895 \tTraining Loss: 0.351809\n",
            "Epoch: 2896 \tTraining Loss: 0.351791\n",
            "Epoch: 2897 \tTraining Loss: 0.351771\n",
            "Epoch: 2898 \tTraining Loss: 0.351753\n",
            "Epoch: 2899 \tTraining Loss: 0.351728\n",
            "Epoch: 2900 \tTraining Loss: 0.351711\n",
            "Epoch: 2901 \tTraining Loss: 0.351680\n",
            "Epoch: 2902 \tTraining Loss: 0.351661\n",
            "Epoch: 2903 \tTraining Loss: 0.351621\n",
            "Epoch: 2904 \tTraining Loss: 0.351590\n",
            "Epoch: 2905 \tTraining Loss: 0.351545\n",
            "Epoch: 2906 \tTraining Loss: 0.351507\n",
            "Epoch: 2907 \tTraining Loss: 0.351469\n",
            "Epoch: 2908 \tTraining Loss: 0.351436\n",
            "Epoch: 2909 \tTraining Loss: 0.351406\n",
            "Epoch: 2910 \tTraining Loss: 0.351377\n",
            "Epoch: 2911 \tTraining Loss: 0.351349\n",
            "Epoch: 2912 \tTraining Loss: 0.351323\n",
            "Epoch: 2913 \tTraining Loss: 0.351296\n",
            "Epoch: 2914 \tTraining Loss: 0.351270\n",
            "Epoch: 2915 \tTraining Loss: 0.351245\n",
            "Epoch: 2916 \tTraining Loss: 0.351219\n",
            "Epoch: 2917 \tTraining Loss: 0.351196\n",
            "Epoch: 2918 \tTraining Loss: 0.351173\n",
            "Epoch: 2919 \tTraining Loss: 0.351153\n",
            "Epoch: 2920 \tTraining Loss: 0.351130\n",
            "Epoch: 2921 \tTraining Loss: 0.351113\n",
            "Epoch: 2922 \tTraining Loss: 0.351088\n",
            "Epoch: 2923 \tTraining Loss: 0.351072\n",
            "Epoch: 2924 \tTraining Loss: 0.351042\n",
            "Epoch: 2925 \tTraining Loss: 0.351023\n",
            "Epoch: 2926 \tTraining Loss: 0.350986\n",
            "Epoch: 2927 \tTraining Loss: 0.350960\n",
            "Epoch: 2928 \tTraining Loss: 0.350919\n",
            "Epoch: 2929 \tTraining Loss: 0.350886\n",
            "Epoch: 2930 \tTraining Loss: 0.350846\n",
            "Epoch: 2931 \tTraining Loss: 0.350812\n",
            "Epoch: 2932 \tTraining Loss: 0.350777\n",
            "Epoch: 2933 \tTraining Loss: 0.350745\n",
            "Epoch: 2934 \tTraining Loss: 0.350713\n",
            "Epoch: 2935 \tTraining Loss: 0.350683\n",
            "Epoch: 2936 \tTraining Loss: 0.350654\n",
            "Epoch: 2937 \tTraining Loss: 0.350625\n",
            "Epoch: 2938 \tTraining Loss: 0.350596\n",
            "Epoch: 2939 \tTraining Loss: 0.350568\n",
            "Epoch: 2940 \tTraining Loss: 0.350540\n",
            "Epoch: 2941 \tTraining Loss: 0.350512\n",
            "Epoch: 2942 \tTraining Loss: 0.350484\n",
            "Epoch: 2943 \tTraining Loss: 0.350456\n",
            "Epoch: 2944 \tTraining Loss: 0.350428\n",
            "Epoch: 2945 \tTraining Loss: 0.350400\n",
            "Epoch: 2946 \tTraining Loss: 0.350372\n",
            "Epoch: 2947 \tTraining Loss: 0.350344\n",
            "Epoch: 2948 \tTraining Loss: 0.350316\n",
            "Epoch: 2949 \tTraining Loss: 0.350288\n",
            "Epoch: 2950 \tTraining Loss: 0.350260\n",
            "Epoch: 2951 \tTraining Loss: 0.350232\n",
            "Epoch: 2952 \tTraining Loss: 0.350204\n",
            "Epoch: 2953 \tTraining Loss: 0.350177\n",
            "Epoch: 2954 \tTraining Loss: 0.350149\n",
            "Epoch: 2955 \tTraining Loss: 0.350122\n",
            "Epoch: 2956 \tTraining Loss: 0.350095\n",
            "Epoch: 2957 \tTraining Loss: 0.350070\n",
            "Epoch: 2958 \tTraining Loss: 0.350046\n",
            "Epoch: 2959 \tTraining Loss: 0.350026\n",
            "Epoch: 2960 \tTraining Loss: 0.350010\n",
            "Epoch: 2961 \tTraining Loss: 0.350008\n",
            "Epoch: 2962 \tTraining Loss: 0.350013\n",
            "Epoch: 2963 \tTraining Loss: 0.350059\n",
            "Epoch: 2964 \tTraining Loss: 0.350089\n",
            "Epoch: 2965 \tTraining Loss: 0.350196\n",
            "Epoch: 2966 \tTraining Loss: 0.350163\n",
            "Epoch: 2967 \tTraining Loss: 0.350162\n",
            "Epoch: 2968 \tTraining Loss: 0.350009\n",
            "Epoch: 2969 \tTraining Loss: 0.349863\n",
            "Epoch: 2970 \tTraining Loss: 0.349736\n",
            "Epoch: 2971 \tTraining Loss: 0.349684\n",
            "Epoch: 2972 \tTraining Loss: 0.349711\n",
            "Epoch: 2973 \tTraining Loss: 0.349773\n",
            "Epoch: 2974 \tTraining Loss: 0.349862\n",
            "Epoch: 2975 \tTraining Loss: 0.349815\n",
            "Epoch: 2976 \tTraining Loss: 0.349750\n",
            "Epoch: 2977 \tTraining Loss: 0.349578\n",
            "Epoch: 2978 \tTraining Loss: 0.349483\n",
            "Epoch: 2979 \tTraining Loss: 0.349481\n",
            "Epoch: 2980 \tTraining Loss: 0.349539\n",
            "Epoch: 2981 \tTraining Loss: 0.349652\n",
            "Epoch: 2982 \tTraining Loss: 0.349639\n",
            "Epoch: 2983 \tTraining Loss: 0.349664\n",
            "Epoch: 2984 \tTraining Loss: 0.349417\n",
            "Epoch: 2985 \tTraining Loss: 0.349310\n",
            "Epoch: 2986 \tTraining Loss: 0.349363\n",
            "Epoch: 2987 \tTraining Loss: 0.349413\n",
            "Epoch: 2988 \tTraining Loss: 0.349509\n",
            "Epoch: 2989 \tTraining Loss: 0.349357\n",
            "Epoch: 2990 \tTraining Loss: 0.349250\n",
            "Epoch: 2991 \tTraining Loss: 0.349158\n",
            "Epoch: 2992 \tTraining Loss: 0.349135\n",
            "Epoch: 2993 \tTraining Loss: 0.349145\n",
            "Epoch: 2994 \tTraining Loss: 0.349107\n",
            "Epoch: 2995 \tTraining Loss: 0.349075\n",
            "Epoch: 2996 \tTraining Loss: 0.349033\n",
            "Epoch: 2997 \tTraining Loss: 0.349001\n",
            "Epoch: 2998 \tTraining Loss: 0.348969\n",
            "Epoch: 2999 \tTraining Loss: 0.348932\n",
            "Epoch: 3000 \tTraining Loss: 0.348896\n",
            "Epoch: 3001 \tTraining Loss: 0.348858\n",
            "Epoch: 3002 \tTraining Loss: 0.348834\n",
            "Epoch: 3003 \tTraining Loss: 0.348817\n",
            "Epoch: 3004 \tTraining Loss: 0.348798\n",
            "Epoch: 3005 \tTraining Loss: 0.348767\n",
            "Epoch: 3006 \tTraining Loss: 0.348724\n",
            "Epoch: 3007 \tTraining Loss: 0.348681\n",
            "Epoch: 3008 \tTraining Loss: 0.348647\n",
            "Epoch: 3009 \tTraining Loss: 0.348624\n",
            "Epoch: 3010 \tTraining Loss: 0.348606\n",
            "Epoch: 3011 \tTraining Loss: 0.348586\n",
            "Epoch: 3012 \tTraining Loss: 0.348557\n",
            "Epoch: 3013 \tTraining Loss: 0.348522\n",
            "Epoch: 3014 \tTraining Loss: 0.348485\n",
            "Epoch: 3015 \tTraining Loss: 0.348450\n",
            "Epoch: 3016 \tTraining Loss: 0.348421\n",
            "Epoch: 3017 \tTraining Loss: 0.348396\n",
            "Epoch: 3018 \tTraining Loss: 0.348372\n",
            "Epoch: 3019 \tTraining Loss: 0.348346\n",
            "Epoch: 3020 \tTraining Loss: 0.348318\n",
            "Epoch: 3021 \tTraining Loss: 0.348288\n",
            "Epoch: 3022 \tTraining Loss: 0.348259\n",
            "Epoch: 3023 \tTraining Loss: 0.348230\n",
            "Epoch: 3024 \tTraining Loss: 0.348202\n",
            "Epoch: 3025 \tTraining Loss: 0.348173\n",
            "Epoch: 3026 \tTraining Loss: 0.348144\n",
            "Epoch: 3027 \tTraining Loss: 0.348114\n",
            "Epoch: 3028 \tTraining Loss: 0.348084\n",
            "Epoch: 3029 \tTraining Loss: 0.348055\n",
            "Epoch: 3030 \tTraining Loss: 0.348027\n",
            "Epoch: 3031 \tTraining Loss: 0.347999\n",
            "Epoch: 3032 \tTraining Loss: 0.347972\n",
            "Epoch: 3033 \tTraining Loss: 0.347945\n",
            "Epoch: 3034 \tTraining Loss: 0.347918\n",
            "Epoch: 3035 \tTraining Loss: 0.347890\n",
            "Epoch: 3036 \tTraining Loss: 0.347862\n",
            "Epoch: 3037 \tTraining Loss: 0.347834\n",
            "Epoch: 3038 \tTraining Loss: 0.347806\n",
            "Epoch: 3039 \tTraining Loss: 0.347780\n",
            "Epoch: 3040 \tTraining Loss: 0.347753\n",
            "Epoch: 3041 \tTraining Loss: 0.347728\n",
            "Epoch: 3042 \tTraining Loss: 0.347704\n",
            "Epoch: 3043 \tTraining Loss: 0.347682\n",
            "Epoch: 3044 \tTraining Loss: 0.347662\n",
            "Epoch: 3045 \tTraining Loss: 0.347647\n",
            "Epoch: 3046 \tTraining Loss: 0.347638\n",
            "Epoch: 3047 \tTraining Loss: 0.347643\n",
            "Epoch: 3048 \tTraining Loss: 0.347658\n",
            "Epoch: 3049 \tTraining Loss: 0.347711\n",
            "Epoch: 3050 \tTraining Loss: 0.347780\n",
            "Epoch: 3051 \tTraining Loss: 0.347935\n",
            "Epoch: 3052 \tTraining Loss: 0.348072\n",
            "Epoch: 3053 \tTraining Loss: 0.348346\n",
            "Epoch: 3054 \tTraining Loss: 0.348398\n",
            "Epoch: 3055 \tTraining Loss: 0.348483\n",
            "Epoch: 3056 \tTraining Loss: 0.348051\n",
            "Epoch: 3057 \tTraining Loss: 0.347639\n",
            "Epoch: 3058 \tTraining Loss: 0.347301\n",
            "Epoch: 3059 \tTraining Loss: 0.347312\n",
            "Epoch: 3060 \tTraining Loss: 0.347525\n",
            "Epoch: 3061 \tTraining Loss: 0.347647\n",
            "Epoch: 3062 \tTraining Loss: 0.347644\n",
            "Epoch: 3063 \tTraining Loss: 0.347425\n",
            "Epoch: 3064 \tTraining Loss: 0.347305\n",
            "Epoch: 3065 \tTraining Loss: 0.347176\n",
            "Epoch: 3066 \tTraining Loss: 0.347177\n",
            "Epoch: 3067 \tTraining Loss: 0.347204\n",
            "Epoch: 3068 \tTraining Loss: 0.347196\n",
            "Epoch: 3069 \tTraining Loss: 0.347216\n",
            "Epoch: 3070 \tTraining Loss: 0.347111\n",
            "Epoch: 3071 \tTraining Loss: 0.347093\n",
            "Epoch: 3072 \tTraining Loss: 0.346942\n",
            "Epoch: 3073 \tTraining Loss: 0.346883\n",
            "Epoch: 3074 \tTraining Loss: 0.346910\n",
            "Epoch: 3075 \tTraining Loss: 0.346918\n",
            "Epoch: 3076 \tTraining Loss: 0.346918\n",
            "Epoch: 3077 \tTraining Loss: 0.346780\n",
            "Epoch: 3078 \tTraining Loss: 0.346694\n",
            "Epoch: 3079 \tTraining Loss: 0.346667\n",
            "Epoch: 3080 \tTraining Loss: 0.346685\n",
            "Epoch: 3081 \tTraining Loss: 0.346712\n",
            "Epoch: 3082 \tTraining Loss: 0.346650\n",
            "Epoch: 3083 \tTraining Loss: 0.346581\n",
            "Epoch: 3084 \tTraining Loss: 0.346518\n",
            "Epoch: 3085 \tTraining Loss: 0.346497\n",
            "Epoch: 3086 \tTraining Loss: 0.346502\n",
            "Epoch: 3087 \tTraining Loss: 0.346484\n",
            "Epoch: 3088 \tTraining Loss: 0.346459\n",
            "Epoch: 3089 \tTraining Loss: 0.346404\n",
            "Epoch: 3090 \tTraining Loss: 0.346361\n",
            "Epoch: 3091 \tTraining Loss: 0.346332\n",
            "Epoch: 3092 \tTraining Loss: 0.346310\n",
            "Epoch: 3093 \tTraining Loss: 0.346283\n",
            "Epoch: 3094 \tTraining Loss: 0.346245\n",
            "Epoch: 3095 \tTraining Loss: 0.346209\n",
            "Epoch: 3096 \tTraining Loss: 0.346180\n",
            "Epoch: 3097 \tTraining Loss: 0.346159\n",
            "Epoch: 3098 \tTraining Loss: 0.346137\n",
            "Epoch: 3099 \tTraining Loss: 0.346109\n",
            "Epoch: 3100 \tTraining Loss: 0.346074\n",
            "Epoch: 3101 \tTraining Loss: 0.346036\n",
            "Epoch: 3102 \tTraining Loss: 0.346001\n",
            "Epoch: 3103 \tTraining Loss: 0.345972\n",
            "Epoch: 3104 \tTraining Loss: 0.345946\n",
            "Epoch: 3105 \tTraining Loss: 0.345921\n",
            "Epoch: 3106 \tTraining Loss: 0.345893\n",
            "Epoch: 3107 \tTraining Loss: 0.345863\n",
            "Epoch: 3108 \tTraining Loss: 0.345833\n",
            "Epoch: 3109 \tTraining Loss: 0.345804\n",
            "Epoch: 3110 \tTraining Loss: 0.345777\n",
            "Epoch: 3111 \tTraining Loss: 0.345750\n",
            "Epoch: 3112 \tTraining Loss: 0.345723\n",
            "Epoch: 3113 \tTraining Loss: 0.345694\n",
            "Epoch: 3114 \tTraining Loss: 0.345664\n",
            "Epoch: 3115 \tTraining Loss: 0.345633\n",
            "Epoch: 3116 \tTraining Loss: 0.345604\n",
            "Epoch: 3117 \tTraining Loss: 0.345576\n",
            "Epoch: 3118 \tTraining Loss: 0.345549\n",
            "Epoch: 3119 \tTraining Loss: 0.345523\n",
            "Epoch: 3120 \tTraining Loss: 0.345497\n",
            "Epoch: 3121 \tTraining Loss: 0.345472\n",
            "Epoch: 3122 \tTraining Loss: 0.345450\n",
            "Epoch: 3123 \tTraining Loss: 0.345430\n",
            "Epoch: 3124 \tTraining Loss: 0.345419\n",
            "Epoch: 3125 \tTraining Loss: 0.345415\n",
            "Epoch: 3126 \tTraining Loss: 0.345433\n",
            "Epoch: 3127 \tTraining Loss: 0.345467\n",
            "Epoch: 3128 \tTraining Loss: 0.345567\n",
            "Epoch: 3129 \tTraining Loss: 0.345697\n",
            "Epoch: 3130 \tTraining Loss: 0.346026\n",
            "Epoch: 3131 \tTraining Loss: 0.346359\n",
            "Epoch: 3132 \tTraining Loss: 0.347157\n",
            "Epoch: 3133 \tTraining Loss: 0.347487\n",
            "Epoch: 3134 \tTraining Loss: 0.347944\n",
            "Epoch: 3135 \tTraining Loss: 0.346665\n",
            "Epoch: 3136 \tTraining Loss: 0.345532\n",
            "Epoch: 3137 \tTraining Loss: 0.345175\n",
            "Epoch: 3138 \tTraining Loss: 0.345729\n",
            "Epoch: 3139 \tTraining Loss: 0.346414\n",
            "Epoch: 3140 \tTraining Loss: 0.346147\n",
            "Epoch: 3141 \tTraining Loss: 0.345539\n",
            "Epoch: 3142 \tTraining Loss: 0.345134\n",
            "Epoch: 3143 \tTraining Loss: 0.345463\n",
            "Epoch: 3144 \tTraining Loss: 0.345556\n",
            "Epoch: 3145 \tTraining Loss: 0.345307\n",
            "Epoch: 3146 \tTraining Loss: 0.345136\n",
            "Epoch: 3147 \tTraining Loss: 0.344965\n",
            "Epoch: 3148 \tTraining Loss: 0.345073\n",
            "Epoch: 3149 \tTraining Loss: 0.345067\n",
            "Epoch: 3150 \tTraining Loss: 0.344953\n",
            "Epoch: 3151 \tTraining Loss: 0.344927\n",
            "Epoch: 3152 \tTraining Loss: 0.344787\n",
            "Epoch: 3153 \tTraining Loss: 0.344768\n",
            "Epoch: 3154 \tTraining Loss: 0.344695\n",
            "Epoch: 3155 \tTraining Loss: 0.344661\n",
            "Epoch: 3156 \tTraining Loss: 0.344711\n",
            "Epoch: 3157 \tTraining Loss: 0.344530\n",
            "Epoch: 3158 \tTraining Loss: 0.344446\n",
            "Epoch: 3159 \tTraining Loss: 0.344435\n",
            "Epoch: 3160 \tTraining Loss: 0.344451\n",
            "Epoch: 3161 \tTraining Loss: 0.344486\n",
            "Epoch: 3162 \tTraining Loss: 0.344321\n",
            "Epoch: 3163 \tTraining Loss: 0.344243\n",
            "Epoch: 3164 \tTraining Loss: 0.344254\n",
            "Epoch: 3165 \tTraining Loss: 0.344262\n",
            "Epoch: 3166 \tTraining Loss: 0.344243\n",
            "Epoch: 3167 \tTraining Loss: 0.344121\n",
            "Epoch: 3168 \tTraining Loss: 0.344074\n",
            "Epoch: 3169 \tTraining Loss: 0.344081\n",
            "Epoch: 3170 \tTraining Loss: 0.344074\n",
            "Epoch: 3171 \tTraining Loss: 0.344045\n",
            "Epoch: 3172 \tTraining Loss: 0.343965\n",
            "Epoch: 3173 \tTraining Loss: 0.343927\n",
            "Epoch: 3174 \tTraining Loss: 0.343915\n",
            "Epoch: 3175 \tTraining Loss: 0.343895\n",
            "Epoch: 3176 \tTraining Loss: 0.343861\n",
            "Epoch: 3177 \tTraining Loss: 0.343812\n",
            "Epoch: 3178 \tTraining Loss: 0.343782\n",
            "Epoch: 3179 \tTraining Loss: 0.343756\n",
            "Epoch: 3180 \tTraining Loss: 0.343728\n",
            "Epoch: 3181 \tTraining Loss: 0.343693\n",
            "Epoch: 3182 \tTraining Loss: 0.343656\n",
            "Epoch: 3183 \tTraining Loss: 0.343630\n",
            "Epoch: 3184 \tTraining Loss: 0.343605\n",
            "Epoch: 3185 \tTraining Loss: 0.343576\n",
            "Epoch: 3186 \tTraining Loss: 0.343539\n",
            "Epoch: 3187 \tTraining Loss: 0.343502\n",
            "Epoch: 3188 \tTraining Loss: 0.343472\n",
            "Epoch: 3189 \tTraining Loss: 0.343447\n",
            "Epoch: 3190 \tTraining Loss: 0.343422\n",
            "Epoch: 3191 \tTraining Loss: 0.343391\n",
            "Epoch: 3192 \tTraining Loss: 0.343356\n",
            "Epoch: 3193 \tTraining Loss: 0.343323\n",
            "Epoch: 3194 \tTraining Loss: 0.343293\n",
            "Epoch: 3195 \tTraining Loss: 0.343265\n",
            "Epoch: 3196 \tTraining Loss: 0.343236\n",
            "Epoch: 3197 \tTraining Loss: 0.343205\n",
            "Epoch: 3198 \tTraining Loss: 0.343174\n",
            "Epoch: 3199 \tTraining Loss: 0.343144\n",
            "Epoch: 3200 \tTraining Loss: 0.343115\n",
            "Epoch: 3201 \tTraining Loss: 0.343085\n",
            "Epoch: 3202 \tTraining Loss: 0.343054\n",
            "Epoch: 3203 \tTraining Loss: 0.343022\n",
            "Epoch: 3204 \tTraining Loss: 0.342991\n",
            "Epoch: 3205 \tTraining Loss: 0.342961\n",
            "Epoch: 3206 \tTraining Loss: 0.342932\n",
            "Epoch: 3207 \tTraining Loss: 0.342902\n",
            "Epoch: 3208 \tTraining Loss: 0.342872\n",
            "Epoch: 3209 \tTraining Loss: 0.342841\n",
            "Epoch: 3210 \tTraining Loss: 0.342811\n",
            "Epoch: 3211 \tTraining Loss: 0.342780\n",
            "Epoch: 3212 \tTraining Loss: 0.342751\n",
            "Epoch: 3213 \tTraining Loss: 0.342720\n",
            "Epoch: 3214 \tTraining Loss: 0.342690\n",
            "Epoch: 3215 \tTraining Loss: 0.342659\n",
            "Epoch: 3216 \tTraining Loss: 0.342629\n",
            "Epoch: 3217 \tTraining Loss: 0.342598\n",
            "Epoch: 3218 \tTraining Loss: 0.342568\n",
            "Epoch: 3219 \tTraining Loss: 0.342537\n",
            "Epoch: 3220 \tTraining Loss: 0.342507\n",
            "Epoch: 3221 \tTraining Loss: 0.342477\n",
            "Epoch: 3222 \tTraining Loss: 0.342447\n",
            "Epoch: 3223 \tTraining Loss: 0.342417\n",
            "Epoch: 3224 \tTraining Loss: 0.342388\n",
            "Epoch: 3225 \tTraining Loss: 0.342360\n",
            "Epoch: 3226 \tTraining Loss: 0.342334\n",
            "Epoch: 3227 \tTraining Loss: 0.342310\n",
            "Epoch: 3228 \tTraining Loss: 0.342291\n",
            "Epoch: 3229 \tTraining Loss: 0.342280\n",
            "Epoch: 3230 \tTraining Loss: 0.342287\n",
            "Epoch: 3231 \tTraining Loss: 0.342315\n",
            "Epoch: 3232 \tTraining Loss: 0.342410\n",
            "Epoch: 3233 \tTraining Loss: 0.342564\n",
            "Epoch: 3234 \tTraining Loss: 0.342970\n",
            "Epoch: 3235 \tTraining Loss: 0.343493\n",
            "Epoch: 3236 \tTraining Loss: 0.344882\n",
            "Epoch: 3237 \tTraining Loss: 0.346009\n",
            "Epoch: 3238 \tTraining Loss: 0.348014\n",
            "Epoch: 3239 \tTraining Loss: 0.346759\n",
            "Epoch: 3240 \tTraining Loss: 0.344569\n",
            "Epoch: 3241 \tTraining Loss: 0.343609\n",
            "Epoch: 3242 \tTraining Loss: 0.343079\n",
            "Epoch: 3243 \tTraining Loss: 0.344675\n",
            "Epoch: 3244 \tTraining Loss: 0.347387\n",
            "Epoch: 3245 \tTraining Loss: 0.353591\n",
            "Epoch: 3246 \tTraining Loss: 0.421786\n",
            "Epoch: 3247 \tTraining Loss: 0.359345\n",
            "Epoch: 3248 \tTraining Loss: 0.419453\n",
            "Epoch: 3249 \tTraining Loss: 0.501459\n",
            "Epoch: 3250 \tTraining Loss: 0.460463\n",
            "Epoch: 3251 \tTraining Loss: 0.431863\n",
            "Epoch: 3252 \tTraining Loss: 0.464414\n",
            "Epoch: 3253 \tTraining Loss: 0.440549\n",
            "Epoch: 3254 \tTraining Loss: 0.420182\n",
            "Epoch: 3255 \tTraining Loss: 0.415117\n",
            "Epoch: 3256 \tTraining Loss: 0.414431\n",
            "Epoch: 3257 \tTraining Loss: 0.390832\n",
            "Epoch: 3258 \tTraining Loss: 0.380178\n",
            "Epoch: 3259 \tTraining Loss: 0.395176\n",
            "Epoch: 3260 \tTraining Loss: 0.392816\n",
            "Epoch: 3261 \tTraining Loss: 0.382639\n",
            "Epoch: 3262 \tTraining Loss: 0.387149\n",
            "Epoch: 3263 \tTraining Loss: 0.373560\n",
            "Epoch: 3264 \tTraining Loss: 0.374120\n",
            "Epoch: 3265 \tTraining Loss: 0.363382\n",
            "Epoch: 3266 \tTraining Loss: 0.379593\n",
            "Epoch: 3267 \tTraining Loss: 0.371506\n",
            "Epoch: 3268 \tTraining Loss: 0.366154\n",
            "Epoch: 3269 \tTraining Loss: 0.362519\n",
            "Epoch: 3270 \tTraining Loss: 0.360456\n",
            "Epoch: 3271 \tTraining Loss: 0.359146\n",
            "Epoch: 3272 \tTraining Loss: 0.362089\n",
            "Epoch: 3273 \tTraining Loss: 0.359333\n",
            "Epoch: 3274 \tTraining Loss: 0.356527\n",
            "Epoch: 3275 \tTraining Loss: 0.355640\n",
            "Epoch: 3276 \tTraining Loss: 0.353108\n",
            "Epoch: 3277 \tTraining Loss: 0.353896\n",
            "Epoch: 3278 \tTraining Loss: 0.350214\n",
            "Epoch: 3279 \tTraining Loss: 0.350026\n",
            "Epoch: 3280 \tTraining Loss: 0.350323\n",
            "Epoch: 3281 \tTraining Loss: 0.350199\n",
            "Epoch: 3282 \tTraining Loss: 0.349510\n",
            "Epoch: 3283 \tTraining Loss: 0.348410\n",
            "Epoch: 3284 \tTraining Loss: 0.347163\n",
            "Epoch: 3285 \tTraining Loss: 0.347558\n",
            "Epoch: 3286 \tTraining Loss: 0.347247\n",
            "Epoch: 3287 \tTraining Loss: 0.346006\n",
            "Epoch: 3288 \tTraining Loss: 0.345795\n",
            "Epoch: 3289 \tTraining Loss: 0.345746\n",
            "Epoch: 3290 \tTraining Loss: 0.345662\n",
            "Epoch: 3291 \tTraining Loss: 0.345143\n",
            "Epoch: 3292 \tTraining Loss: 0.344510\n",
            "Epoch: 3293 \tTraining Loss: 0.344620\n",
            "Epoch: 3294 \tTraining Loss: 0.344287\n",
            "Epoch: 3295 \tTraining Loss: 0.343971\n",
            "Epoch: 3296 \tTraining Loss: 0.343966\n",
            "Epoch: 3297 \tTraining Loss: 0.343739\n",
            "Epoch: 3298 \tTraining Loss: 0.343757\n",
            "Epoch: 3299 \tTraining Loss: 0.343426\n",
            "Epoch: 3300 \tTraining Loss: 0.343139\n",
            "Epoch: 3301 \tTraining Loss: 0.343088\n",
            "Epoch: 3302 \tTraining Loss: 0.342965\n",
            "Epoch: 3303 \tTraining Loss: 0.342907\n",
            "Epoch: 3304 \tTraining Loss: 0.342701\n",
            "Epoch: 3305 \tTraining Loss: 0.342632\n",
            "Epoch: 3306 \tTraining Loss: 0.342596\n",
            "Epoch: 3307 \tTraining Loss: 0.342375\n",
            "Epoch: 3308 \tTraining Loss: 0.342211\n",
            "Epoch: 3309 \tTraining Loss: 0.342169\n",
            "Epoch: 3310 \tTraining Loss: 0.342089\n",
            "Epoch: 3311 \tTraining Loss: 0.341988\n",
            "Epoch: 3312 \tTraining Loss: 0.341910\n",
            "Epoch: 3313 \tTraining Loss: 0.341855\n",
            "Epoch: 3314 \tTraining Loss: 0.341827\n",
            "Epoch: 3315 \tTraining Loss: 0.341724\n",
            "Epoch: 3316 \tTraining Loss: 0.341667\n",
            "Epoch: 3317 \tTraining Loss: 0.341591\n",
            "Epoch: 3318 \tTraining Loss: 0.341567\n",
            "Epoch: 3319 \tTraining Loss: 0.341508\n",
            "Epoch: 3320 \tTraining Loss: 0.341412\n",
            "Epoch: 3321 \tTraining Loss: 0.341375\n",
            "Epoch: 3322 \tTraining Loss: 0.341320\n",
            "Epoch: 3323 \tTraining Loss: 0.341270\n",
            "Epoch: 3324 \tTraining Loss: 0.341206\n",
            "Epoch: 3325 \tTraining Loss: 0.341163\n",
            "Epoch: 3326 \tTraining Loss: 0.341133\n",
            "Epoch: 3327 \tTraining Loss: 0.341084\n",
            "Epoch: 3328 \tTraining Loss: 0.341028\n",
            "Epoch: 3329 \tTraining Loss: 0.340973\n",
            "Epoch: 3330 \tTraining Loss: 0.340938\n",
            "Epoch: 3331 \tTraining Loss: 0.340914\n",
            "Epoch: 3332 \tTraining Loss: 0.340875\n",
            "Epoch: 3333 \tTraining Loss: 0.340831\n",
            "Epoch: 3334 \tTraining Loss: 0.340796\n",
            "Epoch: 3335 \tTraining Loss: 0.340757\n",
            "Epoch: 3336 \tTraining Loss: 0.340727\n",
            "Epoch: 3337 \tTraining Loss: 0.340690\n",
            "Epoch: 3338 \tTraining Loss: 0.340659\n",
            "Epoch: 3339 \tTraining Loss: 0.340624\n",
            "Epoch: 3340 \tTraining Loss: 0.340590\n",
            "Epoch: 3341 \tTraining Loss: 0.340556\n",
            "Epoch: 3342 \tTraining Loss: 0.340521\n",
            "Epoch: 3343 \tTraining Loss: 0.340491\n",
            "Epoch: 3344 \tTraining Loss: 0.340458\n",
            "Epoch: 3345 \tTraining Loss: 0.340428\n",
            "Epoch: 3346 \tTraining Loss: 0.340398\n",
            "Epoch: 3347 \tTraining Loss: 0.340367\n",
            "Epoch: 3348 \tTraining Loss: 0.340334\n",
            "Epoch: 3349 \tTraining Loss: 0.340303\n",
            "Epoch: 3350 \tTraining Loss: 0.340273\n",
            "Epoch: 3351 \tTraining Loss: 0.340244\n",
            "Epoch: 3352 \tTraining Loss: 0.340214\n",
            "Epoch: 3353 \tTraining Loss: 0.340184\n",
            "Epoch: 3354 \tTraining Loss: 0.340153\n",
            "Epoch: 3355 \tTraining Loss: 0.340123\n",
            "Epoch: 3356 \tTraining Loss: 0.340092\n",
            "Epoch: 3357 \tTraining Loss: 0.340061\n",
            "Epoch: 3358 \tTraining Loss: 0.340031\n",
            "Epoch: 3359 \tTraining Loss: 0.340001\n",
            "Epoch: 3360 \tTraining Loss: 0.339970\n",
            "Epoch: 3361 \tTraining Loss: 0.339940\n",
            "Epoch: 3362 \tTraining Loss: 0.339910\n",
            "Epoch: 3363 \tTraining Loss: 0.339879\n",
            "Epoch: 3364 \tTraining Loss: 0.339849\n",
            "Epoch: 3365 \tTraining Loss: 0.339819\n",
            "Epoch: 3366 \tTraining Loss: 0.339790\n",
            "Epoch: 3367 \tTraining Loss: 0.339761\n",
            "Epoch: 3368 \tTraining Loss: 0.339733\n",
            "Epoch: 3369 \tTraining Loss: 0.339705\n",
            "Epoch: 3370 \tTraining Loss: 0.339678\n",
            "Epoch: 3371 \tTraining Loss: 0.339651\n",
            "Epoch: 3372 \tTraining Loss: 0.339623\n",
            "Epoch: 3373 \tTraining Loss: 0.339595\n",
            "Epoch: 3374 \tTraining Loss: 0.339568\n",
            "Epoch: 3375 \tTraining Loss: 0.339540\n",
            "Epoch: 3376 \tTraining Loss: 0.339512\n",
            "Epoch: 3377 \tTraining Loss: 0.339485\n",
            "Epoch: 3378 \tTraining Loss: 0.339457\n",
            "Epoch: 3379 \tTraining Loss: 0.339430\n",
            "Epoch: 3380 \tTraining Loss: 0.339403\n",
            "Epoch: 3381 \tTraining Loss: 0.339376\n",
            "Epoch: 3382 \tTraining Loss: 0.339350\n",
            "Epoch: 3383 \tTraining Loss: 0.339323\n",
            "Epoch: 3384 \tTraining Loss: 0.339297\n",
            "Epoch: 3385 \tTraining Loss: 0.339271\n",
            "Epoch: 3386 \tTraining Loss: 0.339245\n",
            "Epoch: 3387 \tTraining Loss: 0.339220\n",
            "Epoch: 3388 \tTraining Loss: 0.339194\n",
            "Epoch: 3389 \tTraining Loss: 0.339168\n",
            "Epoch: 3390 \tTraining Loss: 0.339143\n",
            "Epoch: 3391 \tTraining Loss: 0.339117\n",
            "Epoch: 3392 \tTraining Loss: 0.339091\n",
            "Epoch: 3393 \tTraining Loss: 0.339065\n",
            "Epoch: 3394 \tTraining Loss: 0.339039\n",
            "Epoch: 3395 \tTraining Loss: 0.339014\n",
            "Epoch: 3396 \tTraining Loss: 0.338988\n",
            "Epoch: 3397 \tTraining Loss: 0.338963\n",
            "Epoch: 3398 \tTraining Loss: 0.338938\n",
            "Epoch: 3399 \tTraining Loss: 0.338912\n",
            "Epoch: 3400 \tTraining Loss: 0.338887\n",
            "Epoch: 3401 \tTraining Loss: 0.338862\n",
            "Epoch: 3402 \tTraining Loss: 0.338837\n",
            "Epoch: 3403 \tTraining Loss: 0.338812\n",
            "Epoch: 3404 \tTraining Loss: 0.338786\n",
            "Epoch: 3405 \tTraining Loss: 0.338761\n",
            "Epoch: 3406 \tTraining Loss: 0.338736\n",
            "Epoch: 3407 \tTraining Loss: 0.338711\n",
            "Epoch: 3408 \tTraining Loss: 0.338686\n",
            "Epoch: 3409 \tTraining Loss: 0.338661\n",
            "Epoch: 3410 \tTraining Loss: 0.338635\n",
            "Epoch: 3411 \tTraining Loss: 0.338610\n",
            "Epoch: 3412 \tTraining Loss: 0.338585\n",
            "Epoch: 3413 \tTraining Loss: 0.338560\n",
            "Epoch: 3414 \tTraining Loss: 0.338535\n",
            "Epoch: 3415 \tTraining Loss: 0.338510\n",
            "Epoch: 3416 \tTraining Loss: 0.338485\n",
            "Epoch: 3417 \tTraining Loss: 0.338460\n",
            "Epoch: 3418 \tTraining Loss: 0.338434\n",
            "Epoch: 3419 \tTraining Loss: 0.338409\n",
            "Epoch: 3420 \tTraining Loss: 0.338384\n",
            "Epoch: 3421 \tTraining Loss: 0.338359\n",
            "Epoch: 3422 \tTraining Loss: 0.338334\n",
            "Epoch: 3423 \tTraining Loss: 0.338309\n",
            "Epoch: 3424 \tTraining Loss: 0.338284\n",
            "Epoch: 3425 \tTraining Loss: 0.338259\n",
            "Epoch: 3426 \tTraining Loss: 0.338234\n",
            "Epoch: 3427 \tTraining Loss: 0.338209\n",
            "Epoch: 3428 \tTraining Loss: 0.338184\n",
            "Epoch: 3429 \tTraining Loss: 0.338160\n",
            "Epoch: 3430 \tTraining Loss: 0.338135\n",
            "Epoch: 3431 \tTraining Loss: 0.338112\n",
            "Epoch: 3432 \tTraining Loss: 0.338090\n",
            "Epoch: 3433 \tTraining Loss: 0.338073\n",
            "Epoch: 3434 \tTraining Loss: 0.338058\n",
            "Epoch: 3435 \tTraining Loss: 0.338058\n",
            "Epoch: 3436 \tTraining Loss: 0.338060\n",
            "Epoch: 3437 \tTraining Loss: 0.338113\n",
            "Epoch: 3438 \tTraining Loss: 0.338116\n",
            "Epoch: 3439 \tTraining Loss: 0.338218\n",
            "Epoch: 3440 \tTraining Loss: 0.338092\n",
            "Epoch: 3441 \tTraining Loss: 0.338038\n",
            "Epoch: 3442 \tTraining Loss: 0.337891\n",
            "Epoch: 3443 \tTraining Loss: 0.337813\n",
            "Epoch: 3444 \tTraining Loss: 0.337793\n",
            "Epoch: 3445 \tTraining Loss: 0.337820\n",
            "Epoch: 3446 \tTraining Loss: 0.337909\n",
            "Epoch: 3447 \tTraining Loss: 0.337940\n",
            "Epoch: 3448 \tTraining Loss: 0.338090\n",
            "Epoch: 3449 \tTraining Loss: 0.337856\n",
            "Epoch: 3450 \tTraining Loss: 0.337707\n",
            "Epoch: 3451 \tTraining Loss: 0.337624\n",
            "Epoch: 3452 \tTraining Loss: 0.337667\n",
            "Epoch: 3453 \tTraining Loss: 0.337795\n",
            "Epoch: 3454 \tTraining Loss: 0.337757\n",
            "Epoch: 3455 \tTraining Loss: 0.337785\n",
            "Epoch: 3456 \tTraining Loss: 0.337595\n",
            "Epoch: 3457 \tTraining Loss: 0.337487\n",
            "Epoch: 3458 \tTraining Loss: 0.337443\n",
            "Epoch: 3459 \tTraining Loss: 0.337461\n",
            "Epoch: 3460 \tTraining Loss: 0.337501\n",
            "Epoch: 3461 \tTraining Loss: 0.337457\n",
            "Epoch: 3462 \tTraining Loss: 0.337408\n",
            "Epoch: 3463 \tTraining Loss: 0.337334\n",
            "Epoch: 3464 \tTraining Loss: 0.337291\n",
            "Epoch: 3465 \tTraining Loss: 0.337277\n",
            "Epoch: 3466 \tTraining Loss: 0.337276\n",
            "Epoch: 3467 \tTraining Loss: 0.337278\n",
            "Epoch: 3468 \tTraining Loss: 0.337243\n",
            "Epoch: 3469 \tTraining Loss: 0.337203\n",
            "Epoch: 3470 \tTraining Loss: 0.337153\n",
            "Epoch: 3471 \tTraining Loss: 0.337120\n",
            "Epoch: 3472 \tTraining Loss: 0.337103\n",
            "Epoch: 3473 \tTraining Loss: 0.337093\n",
            "Epoch: 3474 \tTraining Loss: 0.337084\n",
            "Epoch: 3475 \tTraining Loss: 0.337056\n",
            "Epoch: 3476 \tTraining Loss: 0.337027\n",
            "Epoch: 3477 \tTraining Loss: 0.336987\n",
            "Epoch: 3478 \tTraining Loss: 0.336953\n",
            "Epoch: 3479 \tTraining Loss: 0.336926\n",
            "Epoch: 3480 \tTraining Loss: 0.336906\n",
            "Epoch: 3481 \tTraining Loss: 0.336890\n",
            "Epoch: 3482 \tTraining Loss: 0.336870\n",
            "Epoch: 3483 \tTraining Loss: 0.336848\n",
            "Epoch: 3484 \tTraining Loss: 0.336819\n",
            "Epoch: 3485 \tTraining Loss: 0.336790\n",
            "Epoch: 3486 \tTraining Loss: 0.336760\n",
            "Epoch: 3487 \tTraining Loss: 0.336733\n",
            "Epoch: 3488 \tTraining Loss: 0.336709\n",
            "Epoch: 3489 \tTraining Loss: 0.336688\n",
            "Epoch: 3490 \tTraining Loss: 0.336667\n",
            "Epoch: 3491 \tTraining Loss: 0.336644\n",
            "Epoch: 3492 \tTraining Loss: 0.336621\n",
            "Epoch: 3493 \tTraining Loss: 0.336595\n",
            "Epoch: 3494 \tTraining Loss: 0.336569\n",
            "Epoch: 3495 \tTraining Loss: 0.336543\n",
            "Epoch: 3496 \tTraining Loss: 0.336518\n",
            "Epoch: 3497 \tTraining Loss: 0.336493\n",
            "Epoch: 3498 \tTraining Loss: 0.336470\n",
            "Epoch: 3499 \tTraining Loss: 0.336447\n",
            "Epoch: 3500 \tTraining Loss: 0.336424\n",
            "Epoch: 3501 \tTraining Loss: 0.336401\n",
            "Epoch: 3502 \tTraining Loss: 0.336377\n",
            "Epoch: 3503 \tTraining Loss: 0.336354\n",
            "Epoch: 3504 \tTraining Loss: 0.336329\n",
            "Epoch: 3505 \tTraining Loss: 0.336305\n",
            "Epoch: 3506 \tTraining Loss: 0.336281\n",
            "Epoch: 3507 \tTraining Loss: 0.336256\n",
            "Epoch: 3508 \tTraining Loss: 0.336232\n",
            "Epoch: 3509 \tTraining Loss: 0.336208\n",
            "Epoch: 3510 \tTraining Loss: 0.336184\n",
            "Epoch: 3511 \tTraining Loss: 0.336161\n",
            "Epoch: 3512 \tTraining Loss: 0.336137\n",
            "Epoch: 3513 \tTraining Loss: 0.336114\n",
            "Epoch: 3514 \tTraining Loss: 0.336090\n",
            "Epoch: 3515 \tTraining Loss: 0.336067\n",
            "Epoch: 3516 \tTraining Loss: 0.336043\n",
            "Epoch: 3517 \tTraining Loss: 0.336020\n",
            "Epoch: 3518 \tTraining Loss: 0.335996\n",
            "Epoch: 3519 \tTraining Loss: 0.335973\n",
            "Epoch: 3520 \tTraining Loss: 0.335949\n",
            "Epoch: 3521 \tTraining Loss: 0.335926\n",
            "Epoch: 3522 \tTraining Loss: 0.335902\n",
            "Epoch: 3523 \tTraining Loss: 0.335879\n",
            "Epoch: 3524 \tTraining Loss: 0.335855\n",
            "Epoch: 3525 \tTraining Loss: 0.335832\n",
            "Epoch: 3526 \tTraining Loss: 0.335809\n",
            "Epoch: 3527 \tTraining Loss: 0.335786\n",
            "Epoch: 3528 \tTraining Loss: 0.335763\n",
            "Epoch: 3529 \tTraining Loss: 0.335740\n",
            "Epoch: 3530 \tTraining Loss: 0.335718\n",
            "Epoch: 3531 \tTraining Loss: 0.335695\n",
            "Epoch: 3532 \tTraining Loss: 0.335674\n",
            "Epoch: 3533 \tTraining Loss: 0.335653\n",
            "Epoch: 3534 \tTraining Loss: 0.335634\n",
            "Epoch: 3535 \tTraining Loss: 0.335615\n",
            "Epoch: 3536 \tTraining Loss: 0.335601\n",
            "Epoch: 3537 \tTraining Loss: 0.335584\n",
            "Epoch: 3538 \tTraining Loss: 0.335578\n",
            "Epoch: 3539 \tTraining Loss: 0.335561\n",
            "Epoch: 3540 \tTraining Loss: 0.335564\n",
            "Epoch: 3541 \tTraining Loss: 0.335539\n",
            "Epoch: 3542 \tTraining Loss: 0.335541\n",
            "Epoch: 3543 \tTraining Loss: 0.335496\n",
            "Epoch: 3544 \tTraining Loss: 0.335476\n",
            "Epoch: 3545 \tTraining Loss: 0.335421\n",
            "Epoch: 3546 \tTraining Loss: 0.335383\n",
            "Epoch: 3547 \tTraining Loss: 0.335338\n",
            "Epoch: 3548 \tTraining Loss: 0.335304\n",
            "Epoch: 3549 \tTraining Loss: 0.335272\n",
            "Epoch: 3550 \tTraining Loss: 0.335245\n",
            "Epoch: 3551 \tTraining Loss: 0.335221\n",
            "Epoch: 3552 \tTraining Loss: 0.335198\n",
            "Epoch: 3553 \tTraining Loss: 0.335176\n",
            "Epoch: 3554 \tTraining Loss: 0.335156\n",
            "Epoch: 3555 \tTraining Loss: 0.335136\n",
            "Epoch: 3556 \tTraining Loss: 0.335117\n",
            "Epoch: 3557 \tTraining Loss: 0.335101\n",
            "Epoch: 3558 \tTraining Loss: 0.335084\n",
            "Epoch: 3559 \tTraining Loss: 0.335074\n",
            "Epoch: 3560 \tTraining Loss: 0.335057\n",
            "Epoch: 3561 \tTraining Loss: 0.335056\n",
            "Epoch: 3562 \tTraining Loss: 0.335036\n",
            "Epoch: 3563 \tTraining Loss: 0.335038\n",
            "Epoch: 3564 \tTraining Loss: 0.335002\n",
            "Epoch: 3565 \tTraining Loss: 0.334988\n",
            "Epoch: 3566 \tTraining Loss: 0.334937\n",
            "Epoch: 3567 \tTraining Loss: 0.334903\n",
            "Epoch: 3568 \tTraining Loss: 0.334856\n",
            "Epoch: 3569 \tTraining Loss: 0.334821\n",
            "Epoch: 3570 \tTraining Loss: 0.334787\n",
            "Epoch: 3571 \tTraining Loss: 0.334758\n",
            "Epoch: 3572 \tTraining Loss: 0.334732\n",
            "Epoch: 3573 \tTraining Loss: 0.334708\n",
            "Epoch: 3574 \tTraining Loss: 0.334686\n",
            "Epoch: 3575 \tTraining Loss: 0.334664\n",
            "Epoch: 3576 \tTraining Loss: 0.334644\n",
            "Epoch: 3577 \tTraining Loss: 0.334623\n",
            "Epoch: 3578 \tTraining Loss: 0.334605\n",
            "Epoch: 3579 \tTraining Loss: 0.334586\n",
            "Epoch: 3580 \tTraining Loss: 0.334571\n",
            "Epoch: 3581 \tTraining Loss: 0.334554\n",
            "Epoch: 3582 \tTraining Loss: 0.334547\n",
            "Epoch: 3583 \tTraining Loss: 0.334531\n",
            "Epoch: 3584 \tTraining Loss: 0.334532\n",
            "Epoch: 3585 \tTraining Loss: 0.334508\n",
            "Epoch: 3586 \tTraining Loss: 0.334507\n",
            "Epoch: 3587 \tTraining Loss: 0.334465\n",
            "Epoch: 3588 \tTraining Loss: 0.334444\n",
            "Epoch: 3589 \tTraining Loss: 0.334393\n",
            "Epoch: 3590 \tTraining Loss: 0.334359\n",
            "Epoch: 3591 \tTraining Loss: 0.334316\n",
            "Epoch: 3592 \tTraining Loss: 0.334284\n",
            "Epoch: 3593 \tTraining Loss: 0.334252\n",
            "Epoch: 3594 \tTraining Loss: 0.334225\n",
            "Epoch: 3595 \tTraining Loss: 0.334199\n",
            "Epoch: 3596 \tTraining Loss: 0.334175\n",
            "Epoch: 3597 \tTraining Loss: 0.334152\n",
            "Epoch: 3598 \tTraining Loss: 0.334130\n",
            "Epoch: 3599 \tTraining Loss: 0.334108\n",
            "Epoch: 3600 \tTraining Loss: 0.334086\n",
            "Epoch: 3601 \tTraining Loss: 0.334066\n",
            "Epoch: 3602 \tTraining Loss: 0.334046\n",
            "Epoch: 3603 \tTraining Loss: 0.334028\n",
            "Epoch: 3604 \tTraining Loss: 0.334010\n",
            "Epoch: 3605 \tTraining Loss: 0.333998\n",
            "Epoch: 3606 \tTraining Loss: 0.333983\n",
            "Epoch: 3607 \tTraining Loss: 0.333981\n",
            "Epoch: 3608 \tTraining Loss: 0.333965\n",
            "Epoch: 3609 \tTraining Loss: 0.333971\n",
            "Epoch: 3610 \tTraining Loss: 0.333942\n",
            "Epoch: 3611 \tTraining Loss: 0.333941\n",
            "Epoch: 3612 \tTraining Loss: 0.333892\n",
            "Epoch: 3613 \tTraining Loss: 0.333867\n",
            "Epoch: 3614 \tTraining Loss: 0.333813\n",
            "Epoch: 3615 \tTraining Loss: 0.333776\n",
            "Epoch: 3616 \tTraining Loss: 0.333734\n",
            "Epoch: 3617 \tTraining Loss: 0.333701\n",
            "Epoch: 3618 \tTraining Loss: 0.333671\n",
            "Epoch: 3619 \tTraining Loss: 0.333645\n",
            "Epoch: 3620 \tTraining Loss: 0.333620\n",
            "Epoch: 3621 \tTraining Loss: 0.333597\n",
            "Epoch: 3622 \tTraining Loss: 0.333576\n",
            "Epoch: 3623 \tTraining Loss: 0.333555\n",
            "Epoch: 3624 \tTraining Loss: 0.333535\n",
            "Epoch: 3625 \tTraining Loss: 0.333516\n",
            "Epoch: 3626 \tTraining Loss: 0.333499\n",
            "Epoch: 3627 \tTraining Loss: 0.333481\n",
            "Epoch: 3628 \tTraining Loss: 0.333470\n",
            "Epoch: 3629 \tTraining Loss: 0.333454\n",
            "Epoch: 3630 \tTraining Loss: 0.333452\n",
            "Epoch: 3631 \tTraining Loss: 0.333435\n",
            "Epoch: 3632 \tTraining Loss: 0.333439\n",
            "Epoch: 3633 \tTraining Loss: 0.333408\n",
            "Epoch: 3634 \tTraining Loss: 0.333403\n",
            "Epoch: 3635 \tTraining Loss: 0.333353\n",
            "Epoch: 3636 \tTraining Loss: 0.333325\n",
            "Epoch: 3637 \tTraining Loss: 0.333273\n",
            "Epoch: 3638 \tTraining Loss: 0.333237\n",
            "Epoch: 3639 \tTraining Loss: 0.333198\n",
            "Epoch: 3640 \tTraining Loss: 0.333167\n",
            "Epoch: 3641 \tTraining Loss: 0.333139\n",
            "Epoch: 3642 \tTraining Loss: 0.333113\n",
            "Epoch: 3643 \tTraining Loss: 0.333089\n",
            "Epoch: 3644 \tTraining Loss: 0.333067\n",
            "Epoch: 3645 \tTraining Loss: 0.333045\n",
            "Epoch: 3646 \tTraining Loss: 0.333024\n",
            "Epoch: 3647 \tTraining Loss: 0.333004\n",
            "Epoch: 3648 \tTraining Loss: 0.332984\n",
            "Epoch: 3649 \tTraining Loss: 0.332967\n",
            "Epoch: 3650 \tTraining Loss: 0.332949\n",
            "Epoch: 3651 \tTraining Loss: 0.332937\n",
            "Epoch: 3652 \tTraining Loss: 0.332921\n",
            "Epoch: 3653 \tTraining Loss: 0.332918\n",
            "Epoch: 3654 \tTraining Loss: 0.332902\n",
            "Epoch: 3655 \tTraining Loss: 0.332907\n",
            "Epoch: 3656 \tTraining Loss: 0.332879\n",
            "Epoch: 3657 \tTraining Loss: 0.332877\n",
            "Epoch: 3658 \tTraining Loss: 0.332829\n",
            "Epoch: 3659 \tTraining Loss: 0.332804\n",
            "Epoch: 3660 \tTraining Loss: 0.332752\n",
            "Epoch: 3661 \tTraining Loss: 0.332715\n",
            "Epoch: 3662 \tTraining Loss: 0.332674\n",
            "Epoch: 3663 \tTraining Loss: 0.332642\n",
            "Epoch: 3664 \tTraining Loss: 0.332611\n",
            "Epoch: 3665 \tTraining Loss: 0.332584\n",
            "Epoch: 3666 \tTraining Loss: 0.332559\n",
            "Epoch: 3667 \tTraining Loss: 0.332536\n",
            "Epoch: 3668 \tTraining Loss: 0.332513\n",
            "Epoch: 3669 \tTraining Loss: 0.332491\n",
            "Epoch: 3670 \tTraining Loss: 0.332469\n",
            "Epoch: 3671 \tTraining Loss: 0.332448\n",
            "Epoch: 3672 \tTraining Loss: 0.332428\n",
            "Epoch: 3673 \tTraining Loss: 0.332408\n",
            "Epoch: 3674 \tTraining Loss: 0.332392\n",
            "Epoch: 3675 \tTraining Loss: 0.332374\n",
            "Epoch: 3676 \tTraining Loss: 0.332364\n",
            "Epoch: 3677 \tTraining Loss: 0.332350\n",
            "Epoch: 3678 \tTraining Loss: 0.332351\n",
            "Epoch: 3679 \tTraining Loss: 0.332336\n",
            "Epoch: 3680 \tTraining Loss: 0.332345\n",
            "Epoch: 3681 \tTraining Loss: 0.332316\n",
            "Epoch: 3682 \tTraining Loss: 0.332317\n",
            "Epoch: 3683 \tTraining Loss: 0.332267\n",
            "Epoch: 3684 \tTraining Loss: 0.332244\n",
            "Epoch: 3685 \tTraining Loss: 0.332189\n",
            "Epoch: 3686 \tTraining Loss: 0.332152\n",
            "Epoch: 3687 \tTraining Loss: 0.332107\n",
            "Epoch: 3688 \tTraining Loss: 0.332073\n",
            "Epoch: 3689 \tTraining Loss: 0.332040\n",
            "Epoch: 3690 \tTraining Loss: 0.332011\n",
            "Epoch: 3691 \tTraining Loss: 0.331984\n",
            "Epoch: 3692 \tTraining Loss: 0.331960\n",
            "Epoch: 3693 \tTraining Loss: 0.331936\n",
            "Epoch: 3694 \tTraining Loss: 0.331913\n",
            "Epoch: 3695 \tTraining Loss: 0.331891\n",
            "Epoch: 3696 \tTraining Loss: 0.331869\n",
            "Epoch: 3697 \tTraining Loss: 0.331847\n",
            "Epoch: 3698 \tTraining Loss: 0.331825\n",
            "Epoch: 3699 \tTraining Loss: 0.331805\n",
            "Epoch: 3700 \tTraining Loss: 0.331786\n",
            "Epoch: 3701 \tTraining Loss: 0.331770\n",
            "Epoch: 3702 \tTraining Loss: 0.331754\n",
            "Epoch: 3703 \tTraining Loss: 0.331746\n",
            "Epoch: 3704 \tTraining Loss: 0.331734\n",
            "Epoch: 3705 \tTraining Loss: 0.331741\n",
            "Epoch: 3706 \tTraining Loss: 0.331729\n",
            "Epoch: 3707 \tTraining Loss: 0.331749\n",
            "Epoch: 3708 \tTraining Loss: 0.331724\n",
            "Epoch: 3709 \tTraining Loss: 0.331741\n",
            "Epoch: 3710 \tTraining Loss: 0.331699\n",
            "Epoch: 3711 \tTraining Loss: 0.331700\n",
            "Epoch: 3712 \tTraining Loss: 0.331661\n",
            "Epoch: 3713 \tTraining Loss: 0.331660\n",
            "Epoch: 3714 \tTraining Loss: 0.331634\n",
            "Epoch: 3715 \tTraining Loss: 0.331641\n",
            "Epoch: 3716 \tTraining Loss: 0.331615\n",
            "Epoch: 3717 \tTraining Loss: 0.331625\n",
            "Epoch: 3718 \tTraining Loss: 0.331592\n",
            "Epoch: 3719 \tTraining Loss: 0.331603\n",
            "Epoch: 3720 \tTraining Loss: 0.331563\n",
            "Epoch: 3721 \tTraining Loss: 0.331575\n",
            "Epoch: 3722 \tTraining Loss: 0.331510\n",
            "Epoch: 3723 \tTraining Loss: 0.331493\n",
            "Epoch: 3724 \tTraining Loss: 0.331400\n",
            "Epoch: 3725 \tTraining Loss: 0.331345\n",
            "Epoch: 3726 \tTraining Loss: 0.331266\n",
            "Epoch: 3727 \tTraining Loss: 0.331211\n",
            "Epoch: 3728 \tTraining Loss: 0.331161\n",
            "Epoch: 3729 \tTraining Loss: 0.331122\n",
            "Epoch: 3730 \tTraining Loss: 0.331089\n",
            "Epoch: 3731 \tTraining Loss: 0.331062\n",
            "Epoch: 3732 \tTraining Loss: 0.331040\n",
            "Epoch: 3733 \tTraining Loss: 0.331022\n",
            "Epoch: 3734 \tTraining Loss: 0.331007\n",
            "Epoch: 3735 \tTraining Loss: 0.330994\n",
            "Epoch: 3736 \tTraining Loss: 0.330985\n",
            "Epoch: 3737 \tTraining Loss: 0.330974\n",
            "Epoch: 3738 \tTraining Loss: 0.330973\n",
            "Epoch: 3739 \tTraining Loss: 0.330968\n",
            "Epoch: 3740 \tTraining Loss: 0.330981\n",
            "Epoch: 3741 \tTraining Loss: 0.330984\n",
            "Epoch: 3742 \tTraining Loss: 0.331024\n",
            "Epoch: 3743 \tTraining Loss: 0.331034\n",
            "Epoch: 3744 \tTraining Loss: 0.331110\n",
            "Epoch: 3745 \tTraining Loss: 0.331119\n",
            "Epoch: 3746 \tTraining Loss: 0.331237\n",
            "Epoch: 3747 \tTraining Loss: 0.331243\n",
            "Epoch: 3748 \tTraining Loss: 0.331423\n",
            "Epoch: 3749 \tTraining Loss: 0.331407\n",
            "Epoch: 3750 \tTraining Loss: 0.331631\n",
            "Epoch: 3751 \tTraining Loss: 0.331494\n",
            "Epoch: 3752 \tTraining Loss: 0.331584\n",
            "Epoch: 3753 \tTraining Loss: 0.331234\n",
            "Epoch: 3754 \tTraining Loss: 0.331029\n",
            "Epoch: 3755 \tTraining Loss: 0.330708\n",
            "Epoch: 3756 \tTraining Loss: 0.330573\n",
            "Epoch: 3757 \tTraining Loss: 0.330556\n",
            "Epoch: 3758 \tTraining Loss: 0.330612\n",
            "Epoch: 3759 \tTraining Loss: 0.330686\n",
            "Epoch: 3760 \tTraining Loss: 0.330669\n",
            "Epoch: 3761 \tTraining Loss: 0.330667\n",
            "Epoch: 3762 \tTraining Loss: 0.330579\n",
            "Epoch: 3763 \tTraining Loss: 0.330570\n",
            "Epoch: 3764 \tTraining Loss: 0.330429\n",
            "Epoch: 3765 \tTraining Loss: 0.330352\n",
            "Epoch: 3766 \tTraining Loss: 0.330306\n",
            "Epoch: 3767 \tTraining Loss: 0.330317\n",
            "Epoch: 3768 \tTraining Loss: 0.330374\n",
            "Epoch: 3769 \tTraining Loss: 0.330370\n",
            "Epoch: 3770 \tTraining Loss: 0.330389\n",
            "Epoch: 3771 \tTraining Loss: 0.330260\n",
            "Epoch: 3772 \tTraining Loss: 0.330171\n",
            "Epoch: 3773 \tTraining Loss: 0.330105\n",
            "Epoch: 3774 \tTraining Loss: 0.330092\n",
            "Epoch: 3775 \tTraining Loss: 0.330111\n",
            "Epoch: 3776 \tTraining Loss: 0.330111\n",
            "Epoch: 3777 \tTraining Loss: 0.330107\n",
            "Epoch: 3778 \tTraining Loss: 0.330052\n",
            "Epoch: 3779 \tTraining Loss: 0.330007\n",
            "Epoch: 3780 \tTraining Loss: 0.329966\n",
            "Epoch: 3781 \tTraining Loss: 0.329938\n",
            "Epoch: 3782 \tTraining Loss: 0.329916\n",
            "Epoch: 3783 \tTraining Loss: 0.329893\n",
            "Epoch: 3784 \tTraining Loss: 0.329870\n",
            "Epoch: 3785 \tTraining Loss: 0.329842\n",
            "Epoch: 3786 \tTraining Loss: 0.329819\n",
            "Epoch: 3787 \tTraining Loss: 0.329799\n",
            "Epoch: 3788 \tTraining Loss: 0.329782\n",
            "Epoch: 3789 \tTraining Loss: 0.329763\n",
            "Epoch: 3790 \tTraining Loss: 0.329742\n",
            "Epoch: 3791 \tTraining Loss: 0.329713\n",
            "Epoch: 3792 \tTraining Loss: 0.329682\n",
            "Epoch: 3793 \tTraining Loss: 0.329651\n",
            "Epoch: 3794 \tTraining Loss: 0.329622\n",
            "Epoch: 3795 \tTraining Loss: 0.329597\n",
            "Epoch: 3796 \tTraining Loss: 0.329574\n",
            "Epoch: 3797 \tTraining Loss: 0.329553\n",
            "Epoch: 3798 \tTraining Loss: 0.329531\n",
            "Epoch: 3799 \tTraining Loss: 0.329509\n",
            "Epoch: 3800 \tTraining Loss: 0.329485\n",
            "Epoch: 3801 \tTraining Loss: 0.329462\n",
            "Epoch: 3802 \tTraining Loss: 0.329439\n",
            "Epoch: 3803 \tTraining Loss: 0.329417\n",
            "Epoch: 3804 \tTraining Loss: 0.329396\n",
            "Epoch: 3805 \tTraining Loss: 0.329377\n",
            "Epoch: 3806 \tTraining Loss: 0.329357\n",
            "Epoch: 3807 \tTraining Loss: 0.329338\n",
            "Epoch: 3808 \tTraining Loss: 0.329319\n",
            "Epoch: 3809 \tTraining Loss: 0.329303\n",
            "Epoch: 3810 \tTraining Loss: 0.329288\n",
            "Epoch: 3811 \tTraining Loss: 0.329280\n",
            "Epoch: 3812 \tTraining Loss: 0.329275\n",
            "Epoch: 3813 \tTraining Loss: 0.329288\n",
            "Epoch: 3814 \tTraining Loss: 0.329307\n",
            "Epoch: 3815 \tTraining Loss: 0.329374\n",
            "Epoch: 3816 \tTraining Loss: 0.329449\n",
            "Epoch: 3817 \tTraining Loss: 0.329671\n",
            "Epoch: 3818 \tTraining Loss: 0.329885\n",
            "Epoch: 3819 \tTraining Loss: 0.330596\n",
            "Epoch: 3820 \tTraining Loss: 0.331161\n",
            "Epoch: 3821 \tTraining Loss: 0.333478\n",
            "Epoch: 3822 \tTraining Loss: 0.334280\n",
            "Epoch: 3823 \tTraining Loss: 0.337779\n",
            "Epoch: 3824 \tTraining Loss: 0.332515\n",
            "Epoch: 3825 \tTraining Loss: 0.329572\n",
            "Epoch: 3826 \tTraining Loss: 0.330076\n",
            "Epoch: 3827 \tTraining Loss: 0.331116\n",
            "Epoch: 3828 \tTraining Loss: 0.331244\n",
            "Epoch: 3829 \tTraining Loss: 0.329663\n",
            "Epoch: 3830 \tTraining Loss: 0.330303\n",
            "Epoch: 3831 \tTraining Loss: 0.330904\n",
            "Epoch: 3832 \tTraining Loss: 0.329614\n",
            "Epoch: 3833 \tTraining Loss: 0.329589\n",
            "Epoch: 3834 \tTraining Loss: 0.329978\n",
            "Epoch: 3835 \tTraining Loss: 0.329445\n",
            "Epoch: 3836 \tTraining Loss: 0.329005\n",
            "Epoch: 3837 \tTraining Loss: 0.329323\n",
            "Epoch: 3838 \tTraining Loss: 0.329637\n",
            "Epoch: 3839 \tTraining Loss: 0.328945\n",
            "Epoch: 3840 \tTraining Loss: 0.328858\n",
            "Epoch: 3841 \tTraining Loss: 0.329462\n",
            "Epoch: 3842 \tTraining Loss: 0.328995\n",
            "Epoch: 3843 \tTraining Loss: 0.328639\n",
            "Epoch: 3844 \tTraining Loss: 0.328842\n",
            "Epoch: 3845 \tTraining Loss: 0.328967\n",
            "Epoch: 3846 \tTraining Loss: 0.328617\n",
            "Epoch: 3847 \tTraining Loss: 0.328573\n",
            "Epoch: 3848 \tTraining Loss: 0.328742\n",
            "Epoch: 3849 \tTraining Loss: 0.328699\n",
            "Epoch: 3850 \tTraining Loss: 0.328507\n",
            "Epoch: 3851 \tTraining Loss: 0.328531\n",
            "Epoch: 3852 \tTraining Loss: 0.328514\n",
            "Epoch: 3853 \tTraining Loss: 0.328402\n",
            "Epoch: 3854 \tTraining Loss: 0.328416\n",
            "Epoch: 3855 \tTraining Loss: 0.328420\n",
            "Epoch: 3856 \tTraining Loss: 0.328332\n",
            "Epoch: 3857 \tTraining Loss: 0.328259\n",
            "Epoch: 3858 \tTraining Loss: 0.328306\n",
            "Epoch: 3859 \tTraining Loss: 0.328307\n",
            "Epoch: 3860 \tTraining Loss: 0.328207\n",
            "Epoch: 3861 \tTraining Loss: 0.328153\n",
            "Epoch: 3862 \tTraining Loss: 0.328182\n",
            "Epoch: 3863 \tTraining Loss: 0.328154\n",
            "Epoch: 3864 \tTraining Loss: 0.328102\n",
            "Epoch: 3865 \tTraining Loss: 0.328075\n",
            "Epoch: 3866 \tTraining Loss: 0.328060\n",
            "Epoch: 3867 \tTraining Loss: 0.328029\n",
            "Epoch: 3868 \tTraining Loss: 0.327999\n",
            "Epoch: 3869 \tTraining Loss: 0.327990\n",
            "Epoch: 3870 \tTraining Loss: 0.327960\n",
            "Epoch: 3871 \tTraining Loss: 0.327920\n",
            "Epoch: 3872 \tTraining Loss: 0.327898\n",
            "Epoch: 3873 \tTraining Loss: 0.327890\n",
            "Epoch: 3874 \tTraining Loss: 0.327866\n",
            "Epoch: 3875 \tTraining Loss: 0.327828\n",
            "Epoch: 3876 \tTraining Loss: 0.327801\n",
            "Epoch: 3877 \tTraining Loss: 0.327788\n",
            "Epoch: 3878 \tTraining Loss: 0.327767\n",
            "Epoch: 3879 \tTraining Loss: 0.327738\n",
            "Epoch: 3880 \tTraining Loss: 0.327712\n",
            "Epoch: 3881 \tTraining Loss: 0.327692\n",
            "Epoch: 3882 \tTraining Loss: 0.327669\n",
            "Epoch: 3883 \tTraining Loss: 0.327644\n",
            "Epoch: 3884 \tTraining Loss: 0.327622\n",
            "Epoch: 3885 \tTraining Loss: 0.327601\n",
            "Epoch: 3886 \tTraining Loss: 0.327576\n",
            "Epoch: 3887 \tTraining Loss: 0.327550\n",
            "Epoch: 3888 \tTraining Loss: 0.327528\n",
            "Epoch: 3889 \tTraining Loss: 0.327509\n",
            "Epoch: 3890 \tTraining Loss: 0.327486\n",
            "Epoch: 3891 \tTraining Loss: 0.327460\n",
            "Epoch: 3892 \tTraining Loss: 0.327436\n",
            "Epoch: 3893 \tTraining Loss: 0.327415\n",
            "Epoch: 3894 \tTraining Loss: 0.327394\n",
            "Epoch: 3895 \tTraining Loss: 0.327370\n",
            "Epoch: 3896 \tTraining Loss: 0.327347\n",
            "Epoch: 3897 \tTraining Loss: 0.327324\n",
            "Epoch: 3898 \tTraining Loss: 0.327302\n",
            "Epoch: 3899 \tTraining Loss: 0.327279\n",
            "Epoch: 3900 \tTraining Loss: 0.327256\n",
            "Epoch: 3901 \tTraining Loss: 0.327233\n",
            "Epoch: 3902 \tTraining Loss: 0.327211\n",
            "Epoch: 3903 \tTraining Loss: 0.327188\n",
            "Epoch: 3904 \tTraining Loss: 0.327165\n",
            "Epoch: 3905 \tTraining Loss: 0.327142\n",
            "Epoch: 3906 \tTraining Loss: 0.327120\n",
            "Epoch: 3907 \tTraining Loss: 0.327098\n",
            "Epoch: 3908 \tTraining Loss: 0.327075\n",
            "Epoch: 3909 \tTraining Loss: 0.327052\n",
            "Epoch: 3910 \tTraining Loss: 0.327029\n",
            "Epoch: 3911 \tTraining Loss: 0.327007\n",
            "Epoch: 3912 \tTraining Loss: 0.326984\n",
            "Epoch: 3913 \tTraining Loss: 0.326962\n",
            "Epoch: 3914 \tTraining Loss: 0.326939\n",
            "Epoch: 3915 \tTraining Loss: 0.326916\n",
            "Epoch: 3916 \tTraining Loss: 0.326894\n",
            "Epoch: 3917 \tTraining Loss: 0.326871\n",
            "Epoch: 3918 \tTraining Loss: 0.326848\n",
            "Epoch: 3919 \tTraining Loss: 0.326826\n",
            "Epoch: 3920 \tTraining Loss: 0.326803\n",
            "Epoch: 3921 \tTraining Loss: 0.326780\n",
            "Epoch: 3922 \tTraining Loss: 0.326758\n",
            "Epoch: 3923 \tTraining Loss: 0.326735\n",
            "Epoch: 3924 \tTraining Loss: 0.326712\n",
            "Epoch: 3925 \tTraining Loss: 0.326690\n",
            "Epoch: 3926 \tTraining Loss: 0.326667\n",
            "Epoch: 3927 \tTraining Loss: 0.326644\n",
            "Epoch: 3928 \tTraining Loss: 0.326622\n",
            "Epoch: 3929 \tTraining Loss: 0.326599\n",
            "Epoch: 3930 \tTraining Loss: 0.326576\n",
            "Epoch: 3931 \tTraining Loss: 0.326554\n",
            "Epoch: 3932 \tTraining Loss: 0.326531\n",
            "Epoch: 3933 \tTraining Loss: 0.326508\n",
            "Epoch: 3934 \tTraining Loss: 0.326485\n",
            "Epoch: 3935 \tTraining Loss: 0.326463\n",
            "Epoch: 3936 \tTraining Loss: 0.326440\n",
            "Epoch: 3937 \tTraining Loss: 0.326417\n",
            "Epoch: 3938 \tTraining Loss: 0.326395\n",
            "Epoch: 3939 \tTraining Loss: 0.326372\n",
            "Epoch: 3940 \tTraining Loss: 0.326349\n",
            "Epoch: 3941 \tTraining Loss: 0.326326\n",
            "Epoch: 3944 \tTraining Loss: 0.326258\n",
            "Epoch: 3945 \tTraining Loss: 0.326236\n",
            "Epoch: 3946 \tTraining Loss: 0.326213\n",
            "Epoch: 3947 \tTraining Loss: 0.326190\n",
            "Epoch: 3948 \tTraining Loss: 0.326167\n",
            "Epoch: 3949 \tTraining Loss: 0.326145\n",
            "Epoch: 3950 \tTraining Loss: 0.326122\n",
            "Epoch: 3951 \tTraining Loss: 0.326099\n",
            "Epoch: 3952 \tTraining Loss: 0.326076\n",
            "Epoch: 3953 \tTraining Loss: 0.326054\n",
            "Epoch: 3954 \tTraining Loss: 0.326031\n",
            "Epoch: 3955 \tTraining Loss: 0.326008\n",
            "Epoch: 3956 \tTraining Loss: 0.325985\n",
            "Epoch: 3957 \tTraining Loss: 0.325962\n",
            "Epoch: 3958 \tTraining Loss: 0.325940\n",
            "Epoch: 3959 \tTraining Loss: 0.325917\n",
            "Epoch: 3960 \tTraining Loss: 0.325894\n",
            "Epoch: 3961 \tTraining Loss: 0.325871\n",
            "Epoch: 3962 \tTraining Loss: 0.325848\n",
            "Epoch: 3963 \tTraining Loss: 0.325826\n",
            "Epoch: 3964 \tTraining Loss: 0.325803\n",
            "Epoch: 3965 \tTraining Loss: 0.325780\n",
            "Epoch: 3966 \tTraining Loss: 0.325757\n",
            "Epoch: 3967 \tTraining Loss: 0.325734\n",
            "Epoch: 3968 \tTraining Loss: 0.325711\n",
            "Epoch: 3969 \tTraining Loss: 0.325689\n",
            "Epoch: 3970 \tTraining Loss: 0.325666\n",
            "Epoch: 3971 \tTraining Loss: 0.325643\n",
            "Epoch: 3972 \tTraining Loss: 0.325620\n",
            "Epoch: 3973 \tTraining Loss: 0.325597\n",
            "Epoch: 3974 \tTraining Loss: 0.325574\n",
            "Epoch: 3975 \tTraining Loss: 0.325552\n",
            "Epoch: 3976 \tTraining Loss: 0.325529\n",
            "Epoch: 3977 \tTraining Loss: 0.325506\n",
            "Epoch: 3978 \tTraining Loss: 0.325483\n",
            "Epoch: 3979 \tTraining Loss: 0.325460\n",
            "Epoch: 3980 \tTraining Loss: 0.325437\n",
            "Epoch: 3981 \tTraining Loss: 0.325414\n",
            "Epoch: 3982 \tTraining Loss: 0.325391\n",
            "Epoch: 3983 \tTraining Loss: 0.325368\n",
            "Epoch: 3984 \tTraining Loss: 0.325346\n",
            "Epoch: 3985 \tTraining Loss: 0.325323\n",
            "Epoch: 3986 \tTraining Loss: 0.325300\n",
            "Epoch: 3987 \tTraining Loss: 0.325277\n",
            "Epoch: 3988 \tTraining Loss: 0.325254\n",
            "Epoch: 3989 \tTraining Loss: 0.325231\n",
            "Epoch: 3990 \tTraining Loss: 0.325208\n",
            "Epoch: 3991 \tTraining Loss: 0.325185\n",
            "Epoch: 3992 \tTraining Loss: 0.325162\n",
            "Epoch: 3993 \tTraining Loss: 0.325139\n",
            "Epoch: 3994 \tTraining Loss: 0.325116\n",
            "Epoch: 3995 \tTraining Loss: 0.325093\n",
            "Epoch: 3996 \tTraining Loss: 0.325071\n",
            "Epoch: 3997 \tTraining Loss: 0.325047\n",
            "Epoch: 3998 \tTraining Loss: 0.325025\n",
            "Epoch: 3999 \tTraining Loss: 0.325002\n",
            "Epoch: 4000 \tTraining Loss: 0.324979\n",
            "Epoch: 4001 \tTraining Loss: 0.324956\n",
            "Epoch: 4002 \tTraining Loss: 0.324933\n",
            "Epoch: 4003 \tTraining Loss: 0.324910\n",
            "Epoch: 4004 \tTraining Loss: 0.324888\n",
            "Epoch: 4005 \tTraining Loss: 0.324866\n",
            "Epoch: 4006 \tTraining Loss: 0.324844\n",
            "Epoch: 4007 \tTraining Loss: 0.324824\n",
            "Epoch: 4008 \tTraining Loss: 0.324806\n",
            "Epoch: 4009 \tTraining Loss: 0.324793\n",
            "Epoch: 4010 \tTraining Loss: 0.324786\n",
            "Epoch: 4011 \tTraining Loss: 0.324801\n",
            "Epoch: 4012 \tTraining Loss: 0.324825\n",
            "Epoch: 4013 \tTraining Loss: 0.324924\n",
            "Epoch: 4014 \tTraining Loss: 0.324994\n",
            "Epoch: 4015 \tTraining Loss: 0.325266\n",
            "Epoch: 4016 \tTraining Loss: 0.325321\n",
            "Epoch: 4017 \tTraining Loss: 0.325808\n",
            "Epoch: 4018 \tTraining Loss: 0.326239\n",
            "Epoch: 4019 \tTraining Loss: 0.328367\n",
            "Epoch: 4020 \tTraining Loss: 0.331298\n",
            "Epoch: 4021 \tTraining Loss: 0.343062\n",
            "Epoch: 4022 \tTraining Loss: 0.341754\n",
            "Epoch: 4023 \tTraining Loss: 0.339790\n",
            "Epoch: 4024 \tTraining Loss: 0.338672\n",
            "Epoch: 4025 \tTraining Loss: 0.342146\n",
            "Epoch: 4026 \tTraining Loss: 0.348757\n",
            "Epoch: 4027 \tTraining Loss: 0.340867\n",
            "Epoch: 4028 \tTraining Loss: 0.347293\n",
            "Epoch: 4029 \tTraining Loss: 0.342420\n",
            "Epoch: 4030 \tTraining Loss: 0.337329\n",
            "Epoch: 4031 \tTraining Loss: 0.339400\n",
            "Epoch: 4032 \tTraining Loss: 0.334490\n",
            "Epoch: 4033 \tTraining Loss: 0.336946\n",
            "Epoch: 4034 \tTraining Loss: 0.337104\n",
            "Epoch: 4035 \tTraining Loss: 0.337400\n",
            "Epoch: 4036 \tTraining Loss: 0.345675\n",
            "Epoch: 4037 \tTraining Loss: 0.336709\n",
            "Epoch: 4038 \tTraining Loss: 0.333894\n",
            "Epoch: 4039 \tTraining Loss: 0.332939\n",
            "Epoch: 4040 \tTraining Loss: 0.336786\n",
            "Epoch: 4041 \tTraining Loss: 0.333408\n",
            "Epoch: 4042 \tTraining Loss: 0.330130\n",
            "Epoch: 4043 \tTraining Loss: 0.333229\n",
            "Epoch: 4044 \tTraining Loss: 0.329481\n",
            "Epoch: 4045 \tTraining Loss: 0.329509\n",
            "Epoch: 4046 \tTraining Loss: 0.330684\n",
            "Epoch: 4047 \tTraining Loss: 0.328755\n",
            "Epoch: 4048 \tTraining Loss: 0.328110\n",
            "Epoch: 4049 \tTraining Loss: 0.328551\n",
            "Epoch: 4050 \tTraining Loss: 0.328054\n",
            "Epoch: 4051 \tTraining Loss: 0.327242\n",
            "Epoch: 4052 \tTraining Loss: 0.327863\n",
            "Epoch: 4053 \tTraining Loss: 0.326264\n",
            "Epoch: 4054 \tTraining Loss: 0.326809\n",
            "Epoch: 4055 \tTraining Loss: 0.326226\n",
            "Epoch: 4056 \tTraining Loss: 0.326134\n",
            "Epoch: 4057 \tTraining Loss: 0.325779\n",
            "Epoch: 4058 \tTraining Loss: 0.325900\n",
            "Epoch: 4059 \tTraining Loss: 0.325303\n",
            "Epoch: 4060 \tTraining Loss: 0.325465\n",
            "Epoch: 4061 \tTraining Loss: 0.324811\n",
            "Epoch: 4062 \tTraining Loss: 0.325421\n",
            "Epoch: 4063 \tTraining Loss: 0.324852\n",
            "Epoch: 4064 \tTraining Loss: 0.324763\n",
            "Epoch: 4065 \tTraining Loss: 0.324608\n",
            "Epoch: 4066 \tTraining Loss: 0.324641\n",
            "Epoch: 4067 \tTraining Loss: 0.324459\n",
            "Epoch: 4068 \tTraining Loss: 0.324495\n",
            "Epoch: 4069 \tTraining Loss: 0.324221\n",
            "Epoch: 4070 \tTraining Loss: 0.324301\n",
            "Epoch: 4071 \tTraining Loss: 0.324127\n",
            "Epoch: 4072 \tTraining Loss: 0.324123\n",
            "Epoch: 4073 \tTraining Loss: 0.324055\n",
            "Epoch: 4074 \tTraining Loss: 0.323974\n",
            "Epoch: 4075 \tTraining Loss: 0.323936\n",
            "Epoch: 4076 \tTraining Loss: 0.323875\n",
            "Epoch: 4077 \tTraining Loss: 0.323851\n",
            "Epoch: 4078 \tTraining Loss: 0.323779\n",
            "Epoch: 4079 \tTraining Loss: 0.323731\n",
            "Epoch: 4080 \tTraining Loss: 0.323751\n",
            "Epoch: 4081 \tTraining Loss: 0.323672\n",
            "Epoch: 4082 \tTraining Loss: 0.323586\n",
            "Epoch: 4083 \tTraining Loss: 0.323622\n",
            "Epoch: 4084 \tTraining Loss: 0.323528\n",
            "Epoch: 4085 \tTraining Loss: 0.323527\n",
            "Epoch: 4086 \tTraining Loss: 0.323479\n",
            "Epoch: 4087 \tTraining Loss: 0.323460\n",
            "Epoch: 4088 \tTraining Loss: 0.323411\n",
            "Epoch: 4089 \tTraining Loss: 0.323391\n",
            "Epoch: 4090 \tTraining Loss: 0.323370\n",
            "Epoch: 4091 \tTraining Loss: 0.323324\n",
            "Epoch: 4092 \tTraining Loss: 0.323297\n",
            "Epoch: 4093 \tTraining Loss: 0.323281\n",
            "Epoch: 4094 \tTraining Loss: 0.323246\n",
            "Epoch: 4095 \tTraining Loss: 0.323217\n",
            "Epoch: 4096 \tTraining Loss: 0.323194\n",
            "Epoch: 4097 \tTraining Loss: 0.323167\n",
            "Epoch: 4098 \tTraining Loss: 0.323146\n",
            "Epoch: 4099 \tTraining Loss: 0.323109\n",
            "Epoch: 4100 \tTraining Loss: 0.323095\n",
            "Epoch: 4101 \tTraining Loss: 0.323064\n",
            "Epoch: 4102 \tTraining Loss: 0.323043\n",
            "Epoch: 4103 \tTraining Loss: 0.323020\n",
            "Epoch: 4104 \tTraining Loss: 0.322995\n",
            "Epoch: 4105 \tTraining Loss: 0.322975\n",
            "Epoch: 4106 \tTraining Loss: 0.322947\n",
            "Epoch: 4107 \tTraining Loss: 0.322928\n",
            "Epoch: 4108 \tTraining Loss: 0.322905\n",
            "Epoch: 4109 \tTraining Loss: 0.322881\n",
            "Epoch: 4110 \tTraining Loss: 0.322859\n",
            "Epoch: 4111 \tTraining Loss: 0.322838\n",
            "Epoch: 4112 \tTraining Loss: 0.322815\n",
            "Epoch: 4113 \tTraining Loss: 0.322792\n",
            "Epoch: 4114 \tTraining Loss: 0.322772\n",
            "Epoch: 4115 \tTraining Loss: 0.322750\n",
            "Epoch: 4116 \tTraining Loss: 0.322727\n",
            "Epoch: 4117 \tTraining Loss: 0.322706\n",
            "Epoch: 4118 \tTraining Loss: 0.322685\n",
            "Epoch: 4119 \tTraining Loss: 0.322663\n",
            "Epoch: 4120 \tTraining Loss: 0.322643\n",
            "Epoch: 4121 \tTraining Loss: 0.322621\n",
            "Epoch: 4122 \tTraining Loss: 0.322600\n",
            "Epoch: 4123 \tTraining Loss: 0.322579\n",
            "Epoch: 4124 \tTraining Loss: 0.322557\n",
            "Epoch: 4125 \tTraining Loss: 0.322537\n",
            "Epoch: 4126 \tTraining Loss: 0.322515\n",
            "Epoch: 4127 \tTraining Loss: 0.322495\n",
            "Epoch: 4128 \tTraining Loss: 0.322474\n",
            "Epoch: 4129 \tTraining Loss: 0.322453\n",
            "Epoch: 4130 \tTraining Loss: 0.322432\n",
            "Epoch: 4131 \tTraining Loss: 0.322411\n",
            "Epoch: 4132 \tTraining Loss: 0.322391\n",
            "Epoch: 4133 \tTraining Loss: 0.322370\n",
            "Epoch: 4134 \tTraining Loss: 0.322349\n",
            "Epoch: 4135 \tTraining Loss: 0.322328\n",
            "Epoch: 4136 \tTraining Loss: 0.322308\n",
            "Epoch: 4137 \tTraining Loss: 0.322287\n",
            "Epoch: 4138 \tTraining Loss: 0.322266\n",
            "Epoch: 4139 \tTraining Loss: 0.322246\n",
            "Epoch: 4140 \tTraining Loss: 0.322225\n",
            "Epoch: 4141 \tTraining Loss: 0.322205\n",
            "Epoch: 4142 \tTraining Loss: 0.322184\n",
            "Epoch: 4143 \tTraining Loss: 0.322164\n",
            "Epoch: 4144 \tTraining Loss: 0.322143\n",
            "Epoch: 4145 \tTraining Loss: 0.322123\n",
            "Epoch: 4146 \tTraining Loss: 0.322102\n",
            "Epoch: 4147 \tTraining Loss: 0.322082\n",
            "Epoch: 4148 \tTraining Loss: 0.322062\n",
            "Epoch: 4149 \tTraining Loss: 0.322041\n",
            "Epoch: 4150 \tTraining Loss: 0.322021\n",
            "Epoch: 4151 \tTraining Loss: 0.322000\n",
            "Epoch: 4152 \tTraining Loss: 0.321980\n",
            "Epoch: 4153 \tTraining Loss: 0.321959\n",
            "Epoch: 4154 \tTraining Loss: 0.321939\n",
            "Epoch: 4155 \tTraining Loss: 0.321918\n",
            "Epoch: 4156 \tTraining Loss: 0.321898\n",
            "Epoch: 4157 \tTraining Loss: 0.321877\n",
            "Epoch: 4158 \tTraining Loss: 0.321857\n",
            "Epoch: 4159 \tTraining Loss: 0.321836\n",
            "Epoch: 4160 \tTraining Loss: 0.321816\n",
            "Epoch: 4161 \tTraining Loss: 0.321795\n",
            "Epoch: 4162 \tTraining Loss: 0.321775\n",
            "Epoch: 4163 \tTraining Loss: 0.321754\n",
            "Epoch: 4164 \tTraining Loss: 0.321733\n",
            "Epoch: 4165 \tTraining Loss: 0.321713\n",
            "Epoch: 4166 \tTraining Loss: 0.321692\n",
            "Epoch: 4167 \tTraining Loss: 0.321671\n",
            "Epoch: 4168 \tTraining Loss: 0.321650\n",
            "Epoch: 4169 \tTraining Loss: 0.321628\n",
            "Epoch: 4170 \tTraining Loss: 0.321607\n",
            "Epoch: 4171 \tTraining Loss: 0.321585\n",
            "Epoch: 4172 \tTraining Loss: 0.321564\n",
            "Epoch: 4173 \tTraining Loss: 0.321541\n",
            "Epoch: 4174 \tTraining Loss: 0.321519\n",
            "Epoch: 4175 \tTraining Loss: 0.321496\n",
            "Epoch: 4176 \tTraining Loss: 0.321473\n",
            "Epoch: 4177 \tTraining Loss: 0.321450\n",
            "Epoch: 4178 \tTraining Loss: 0.321427\n",
            "Epoch: 4179 \tTraining Loss: 0.321406\n",
            "Epoch: 4180 \tTraining Loss: 0.321386\n",
            "Epoch: 4181 \tTraining Loss: 0.321367\n",
            "Epoch: 4182 \tTraining Loss: 0.321348\n",
            "Epoch: 4183 \tTraining Loss: 0.321329\n",
            "Epoch: 4184 \tTraining Loss: 0.321309\n",
            "Epoch: 4185 \tTraining Loss: 0.321288\n",
            "Epoch: 4186 \tTraining Loss: 0.321267\n",
            "Epoch: 4187 \tTraining Loss: 0.321246\n",
            "Epoch: 4188 \tTraining Loss: 0.321226\n",
            "Epoch: 4189 \tTraining Loss: 0.321206\n",
            "Epoch: 4190 \tTraining Loss: 0.321185\n",
            "Epoch: 4191 \tTraining Loss: 0.321165\n",
            "Epoch: 4192 \tTraining Loss: 0.321145\n",
            "Epoch: 4193 \tTraining Loss: 0.321125\n",
            "Epoch: 4194 \tTraining Loss: 0.321105\n",
            "Epoch: 4195 \tTraining Loss: 0.321085\n",
            "Epoch: 4196 \tTraining Loss: 0.321065\n",
            "Epoch: 4197 \tTraining Loss: 0.321045\n",
            "Epoch: 4198 \tTraining Loss: 0.321025\n",
            "Epoch: 4199 \tTraining Loss: 0.321005\n",
            "Epoch: 4200 \tTraining Loss: 0.320985\n",
            "Epoch: 4201 \tTraining Loss: 0.320965\n",
            "Epoch: 4202 \tTraining Loss: 0.320945\n",
            "Epoch: 4203 \tTraining Loss: 0.320925\n",
            "Epoch: 4204 \tTraining Loss: 0.320905\n",
            "Epoch: 4205 \tTraining Loss: 0.320885\n",
            "Epoch: 4206 \tTraining Loss: 0.320865\n",
            "Epoch: 4207 \tTraining Loss: 0.320845\n",
            "Epoch: 4208 \tTraining Loss: 0.320825\n",
            "Epoch: 4209 \tTraining Loss: 0.320805\n",
            "Epoch: 4210 \tTraining Loss: 0.320785\n",
            "Epoch: 4211 \tTraining Loss: 0.320765\n",
            "Epoch: 4212 \tTraining Loss: 0.320745\n",
            "Epoch: 4213 \tTraining Loss: 0.320725\n",
            "Epoch: 4214 \tTraining Loss: 0.320705\n",
            "Epoch: 4215 \tTraining Loss: 0.320685\n",
            "Epoch: 4216 \tTraining Loss: 0.320665\n",
            "Epoch: 4217 \tTraining Loss: 0.320645\n",
            "Epoch: 4218 \tTraining Loss: 0.320625\n",
            "Epoch: 4219 \tTraining Loss: 0.320605\n",
            "Epoch: 4220 \tTraining Loss: 0.320585\n",
            "Epoch: 4221 \tTraining Loss: 0.320565\n",
            "Epoch: 4222 \tTraining Loss: 0.320545\n",
            "Epoch: 4223 \tTraining Loss: 0.320525\n",
            "Epoch: 4224 \tTraining Loss: 0.320505\n",
            "Epoch: 4225 \tTraining Loss: 0.320485\n",
            "Epoch: 4226 \tTraining Loss: 0.320465\n",
            "Epoch: 4227 \tTraining Loss: 0.320445\n",
            "Epoch: 4228 \tTraining Loss: 0.320425\n",
            "Epoch: 4229 \tTraining Loss: 0.320406\n",
            "Epoch: 4230 \tTraining Loss: 0.320386\n",
            "Epoch: 4231 \tTraining Loss: 0.320365\n",
            "Epoch: 4232 \tTraining Loss: 0.320345\n",
            "Epoch: 4233 \tTraining Loss: 0.320325\n",
            "Epoch: 4234 \tTraining Loss: 0.320305\n",
            "Epoch: 4235 \tTraining Loss: 0.320285\n",
            "Epoch: 4236 \tTraining Loss: 0.320265\n",
            "Epoch: 4237 \tTraining Loss: 0.320245\n",
            "Epoch: 4238 \tTraining Loss: 0.320225\n",
            "Epoch: 4239 \tTraining Loss: 0.320204\n",
            "Epoch: 4240 \tTraining Loss: 0.320184\n",
            "Epoch: 4241 \tTraining Loss: 0.320163\n",
            "Epoch: 4242 \tTraining Loss: 0.320142\n",
            "Epoch: 4243 \tTraining Loss: 0.320120\n",
            "Epoch: 4244 \tTraining Loss: 0.320097\n",
            "Epoch: 4245 \tTraining Loss: 0.320072\n",
            "Epoch: 4246 \tTraining Loss: 0.320046\n",
            "Epoch: 4247 \tTraining Loss: 0.320018\n",
            "Epoch: 4248 \tTraining Loss: 0.319994\n",
            "Epoch: 4249 \tTraining Loss: 0.319973\n",
            "Epoch: 4250 \tTraining Loss: 0.319955\n",
            "Epoch: 4251 \tTraining Loss: 0.319941\n",
            "Epoch: 4252 \tTraining Loss: 0.319923\n",
            "Epoch: 4253 \tTraining Loss: 0.319900\n",
            "Epoch: 4254 \tTraining Loss: 0.319876\n",
            "Epoch: 4255 \tTraining Loss: 0.319854\n",
            "Epoch: 4256 \tTraining Loss: 0.319833\n",
            "Epoch: 4257 \tTraining Loss: 0.319814\n",
            "Epoch: 4258 \tTraining Loss: 0.319795\n",
            "Epoch: 4259 \tTraining Loss: 0.319776\n",
            "Epoch: 4260 \tTraining Loss: 0.319757\n",
            "Epoch: 4261 \tTraining Loss: 0.319737\n",
            "Epoch: 4262 \tTraining Loss: 0.319716\n",
            "Epoch: 4263 \tTraining Loss: 0.319695\n",
            "Epoch: 4264 \tTraining Loss: 0.319674\n",
            "Epoch: 4265 \tTraining Loss: 0.319653\n",
            "Epoch: 4266 \tTraining Loss: 0.319633\n",
            "Epoch: 4267 \tTraining Loss: 0.319613\n",
            "Epoch: 4268 \tTraining Loss: 0.319594\n",
            "Epoch: 4269 \tTraining Loss: 0.319574\n",
            "Epoch: 4270 \tTraining Loss: 0.319554\n",
            "Epoch: 4271 \tTraining Loss: 0.319534\n",
            "Epoch: 4272 \tTraining Loss: 0.319513\n",
            "Epoch: 4273 \tTraining Loss: 0.319493\n",
            "Epoch: 4274 \tTraining Loss: 0.319473\n",
            "Epoch: 4275 \tTraining Loss: 0.319453\n",
            "Epoch: 4276 \tTraining Loss: 0.319433\n",
            "Epoch: 4277 \tTraining Loss: 0.319413\n",
            "Epoch: 4278 \tTraining Loss: 0.319393\n",
            "Epoch: 4279 \tTraining Loss: 0.319373\n",
            "Epoch: 4280 \tTraining Loss: 0.319353\n",
            "Epoch: 4281 \tTraining Loss: 0.319333\n",
            "Epoch: 4282 \tTraining Loss: 0.319313\n",
            "Epoch: 4283 \tTraining Loss: 0.319293\n",
            "Epoch: 4284 \tTraining Loss: 0.319273\n",
            "Epoch: 4285 \tTraining Loss: 0.319253\n",
            "Epoch: 4286 \tTraining Loss: 0.319233\n",
            "Epoch: 4287 \tTraining Loss: 0.319213\n",
            "Epoch: 4288 \tTraining Loss: 0.319193\n",
            "Epoch: 4289 \tTraining Loss: 0.319173\n",
            "Epoch: 4290 \tTraining Loss: 0.319153\n",
            "Epoch: 4291 \tTraining Loss: 0.319133\n",
            "Epoch: 4292 \tTraining Loss: 0.319113\n",
            "Epoch: 4293 \tTraining Loss: 0.319093\n",
            "Epoch: 4294 \tTraining Loss: 0.319073\n",
            "Epoch: 4295 \tTraining Loss: 0.319053\n",
            "Epoch: 4296 \tTraining Loss: 0.319032\n",
            "Epoch: 4297 \tTraining Loss: 0.319012\n",
            "Epoch: 4298 \tTraining Loss: 0.318992\n",
            "Epoch: 4299 \tTraining Loss: 0.318972\n",
            "Epoch: 4300 \tTraining Loss: 0.318952\n",
            "Epoch: 4301 \tTraining Loss: 0.318932\n",
            "Epoch: 4302 \tTraining Loss: 0.318912\n",
            "Epoch: 4303 \tTraining Loss: 0.318892\n",
            "Epoch: 4304 \tTraining Loss: 0.318872\n",
            "Epoch: 4305 \tTraining Loss: 0.318852\n",
            "Epoch: 4306 \tTraining Loss: 0.318832\n",
            "Epoch: 4307 \tTraining Loss: 0.318812\n",
            "Epoch: 4308 \tTraining Loss: 0.318792\n",
            "Epoch: 4309 \tTraining Loss: 0.318772\n",
            "Epoch: 4310 \tTraining Loss: 0.318751\n",
            "Epoch: 4311 \tTraining Loss: 0.318731\n",
            "Epoch: 4312 \tTraining Loss: 0.318711\n",
            "Epoch: 4313 \tTraining Loss: 0.318691\n",
            "Epoch: 4314 \tTraining Loss: 0.318671\n",
            "Epoch: 4315 \tTraining Loss: 0.318651\n",
            "Epoch: 4316 \tTraining Loss: 0.318631\n",
            "Epoch: 4317 \tTraining Loss: 0.318611\n",
            "Epoch: 4318 \tTraining Loss: 0.318591\n",
            "Epoch: 4319 \tTraining Loss: 0.318571\n",
            "Epoch: 4320 \tTraining Loss: 0.318550\n",
            "Epoch: 4321 \tTraining Loss: 0.318530\n",
            "Epoch: 4322 \tTraining Loss: 0.318510\n",
            "Epoch: 4323 \tTraining Loss: 0.318490\n",
            "Epoch: 4324 \tTraining Loss: 0.318470\n",
            "Epoch: 4325 \tTraining Loss: 0.318450\n",
            "Epoch: 4326 \tTraining Loss: 0.318430\n",
            "Epoch: 4327 \tTraining Loss: 0.318410\n",
            "Epoch: 4328 \tTraining Loss: 0.318389\n",
            "Epoch: 4329 \tTraining Loss: 0.318369\n",
            "Epoch: 4330 \tTraining Loss: 0.318349\n",
            "Epoch: 4331 \tTraining Loss: 0.318329\n",
            "Epoch: 4332 \tTraining Loss: 0.318309\n",
            "Epoch: 4333 \tTraining Loss: 0.318289\n",
            "Epoch: 4334 \tTraining Loss: 0.318268\n",
            "Epoch: 4335 \tTraining Loss: 0.318248\n",
            "Epoch: 4336 \tTraining Loss: 0.318228\n",
            "Epoch: 4337 \tTraining Loss: 0.318208\n",
            "Epoch: 4338 \tTraining Loss: 0.318188\n",
            "Epoch: 4339 \tTraining Loss: 0.318168\n",
            "Epoch: 4340 \tTraining Loss: 0.318147\n",
            "Epoch: 4341 \tTraining Loss: 0.318127\n",
            "Epoch: 4342 \tTraining Loss: 0.318107\n",
            "Epoch: 4343 \tTraining Loss: 0.318087\n",
            "Epoch: 4344 \tTraining Loss: 0.318067\n",
            "Epoch: 4345 \tTraining Loss: 0.318046\n",
            "Epoch: 4346 \tTraining Loss: 0.318026\n",
            "Epoch: 4347 \tTraining Loss: 0.318006\n",
            "Epoch: 4348 \tTraining Loss: 0.317986\n",
            "Epoch: 4349 \tTraining Loss: 0.317966\n",
            "Epoch: 4350 \tTraining Loss: 0.317945\n",
            "Epoch: 4351 \tTraining Loss: 0.317925\n",
            "Epoch: 4352 \tTraining Loss: 0.317905\n",
            "Epoch: 4353 \tTraining Loss: 0.317885\n",
            "Epoch: 4354 \tTraining Loss: 0.317864\n",
            "Epoch: 4355 \tTraining Loss: 0.317844\n",
            "Epoch: 4356 \tTraining Loss: 0.317824\n",
            "Epoch: 4357 \tTraining Loss: 0.317804\n",
            "Epoch: 4358 \tTraining Loss: 0.317784\n",
            "Epoch: 4359 \tTraining Loss: 0.317764\n",
            "Epoch: 4360 \tTraining Loss: 0.317745\n",
            "Epoch: 4361 \tTraining Loss: 0.317726\n",
            "Epoch: 4362 \tTraining Loss: 0.317710\n",
            "Epoch: 4363 \tTraining Loss: 0.317699\n",
            "Epoch: 4364 \tTraining Loss: 0.317695\n",
            "Epoch: 4365 \tTraining Loss: 0.317720\n",
            "Epoch: 4366 \tTraining Loss: 0.317759\n",
            "Epoch: 4367 \tTraining Loss: 0.317923\n",
            "Epoch: 4368 \tTraining Loss: 0.317955\n",
            "Epoch: 4369 \tTraining Loss: 0.318241\n",
            "Epoch: 4370 \tTraining Loss: 0.317855\n",
            "Epoch: 4371 \tTraining Loss: 0.317666\n",
            "Epoch: 4372 \tTraining Loss: 0.317521\n",
            "Epoch: 4373 \tTraining Loss: 0.317525\n",
            "Epoch: 4374 \tTraining Loss: 0.317666\n",
            "Epoch: 4375 \tTraining Loss: 0.317786\n",
            "Epoch: 4376 \tTraining Loss: 0.318156\n",
            "Epoch: 4377 \tTraining Loss: 0.317773\n",
            "Epoch: 4378 \tTraining Loss: 0.317606\n",
            "Epoch: 4379 \tTraining Loss: 0.317397\n",
            "Epoch: 4380 \tTraining Loss: 0.317380\n",
            "Epoch: 4381 \tTraining Loss: 0.317503\n",
            "Epoch: 4382 \tTraining Loss: 0.317534\n",
            "Epoch: 4383 \tTraining Loss: 0.317589\n",
            "Epoch: 4384 \tTraining Loss: 0.317403\n",
            "Epoch: 4385 \tTraining Loss: 0.317294\n",
            "Epoch: 4386 \tTraining Loss: 0.317222\n",
            "Epoch: 4387 \tTraining Loss: 0.317215\n",
            "Epoch: 4388 \tTraining Loss: 0.317252\n",
            "Epoch: 4389 \tTraining Loss: 0.317268\n",
            "Epoch: 4390 \tTraining Loss: 0.317287\n",
            "Epoch: 4391 \tTraining Loss: 0.317203\n",
            "Epoch: 4392 \tTraining Loss: 0.317136\n",
            "Epoch: 4393 \tTraining Loss: 0.317080\n",
            "Epoch: 4394 \tTraining Loss: 0.317063\n",
            "Epoch: 4395 \tTraining Loss: 0.317075\n",
            "Epoch: 4396 \tTraining Loss: 0.317083\n",
            "Epoch: 4397 \tTraining Loss: 0.317099\n",
            "Epoch: 4398 \tTraining Loss: 0.317057\n",
            "Epoch: 4399 \tTraining Loss: 0.317021\n",
            "Epoch: 4400 \tTraining Loss: 0.316961\n",
            "Epoch: 4401 \tTraining Loss: 0.316918\n",
            "Epoch: 4402 \tTraining Loss: 0.316896\n",
            "Epoch: 4403 \tTraining Loss: 0.316890\n",
            "Epoch: 4404 \tTraining Loss: 0.316891\n",
            "Epoch: 4405 \tTraining Loss: 0.316879\n",
            "Epoch: 4406 \tTraining Loss: 0.316866\n",
            "Epoch: 4407 \tTraining Loss: 0.316830\n",
            "Epoch: 4408 \tTraining Loss: 0.316796\n",
            "Epoch: 4409 \tTraining Loss: 0.316760\n",
            "Epoch: 4410 \tTraining Loss: 0.316731\n",
            "Epoch: 4411 \tTraining Loss: 0.316711\n",
            "Epoch: 4412 \tTraining Loss: 0.316697\n",
            "Epoch: 4413 \tTraining Loss: 0.316685\n",
            "Epoch: 4414 \tTraining Loss: 0.316669\n",
            "Epoch: 4415 \tTraining Loss: 0.316651\n",
            "Epoch: 4416 \tTraining Loss: 0.316627\n",
            "Epoch: 4417 \tTraining Loss: 0.316602\n",
            "Epoch: 4418 \tTraining Loss: 0.316575\n",
            "Epoch: 4419 \tTraining Loss: 0.316549\n",
            "Epoch: 4420 \tTraining Loss: 0.316527\n",
            "Epoch: 4421 \tTraining Loss: 0.316506\n",
            "Epoch: 4422 \tTraining Loss: 0.316487\n",
            "Epoch: 4423 \tTraining Loss: 0.316469\n",
            "Epoch: 4424 \tTraining Loss: 0.316452\n",
            "Epoch: 4425 \tTraining Loss: 0.316432\n",
            "Epoch: 4426 \tTraining Loss: 0.316412\n",
            "Epoch: 4427 \tTraining Loss: 0.316391\n",
            "Epoch: 4428 \tTraining Loss: 0.316369\n",
            "Epoch: 4429 \tTraining Loss: 0.316346\n",
            "Epoch: 4430 \tTraining Loss: 0.316324\n",
            "Epoch: 4431 \tTraining Loss: 0.316302\n",
            "Epoch: 4432 \tTraining Loss: 0.316281\n",
            "Epoch: 4433 \tTraining Loss: 0.316260\n",
            "Epoch: 4434 \tTraining Loss: 0.316240\n",
            "Epoch: 4435 \tTraining Loss: 0.316220\n",
            "Epoch: 4436 \tTraining Loss: 0.316201\n",
            "Epoch: 4437 \tTraining Loss: 0.316181\n",
            "Epoch: 4438 \tTraining Loss: 0.316161\n",
            "Epoch: 4439 \tTraining Loss: 0.316141\n",
            "Epoch: 4440 \tTraining Loss: 0.316121\n",
            "Epoch: 4441 \tTraining Loss: 0.316100\n",
            "Epoch: 4442 \tTraining Loss: 0.316079\n",
            "Epoch: 4443 \tTraining Loss: 0.316059\n",
            "Epoch: 4444 \tTraining Loss: 0.316038\n",
            "Epoch: 4445 \tTraining Loss: 0.316017\n",
            "Epoch: 4446 \tTraining Loss: 0.315996\n",
            "Epoch: 4447 \tTraining Loss: 0.315975\n",
            "Epoch: 4448 \tTraining Loss: 0.315955\n",
            "Epoch: 4449 \tTraining Loss: 0.315934\n",
            "Epoch: 4450 \tTraining Loss: 0.315913\n",
            "Epoch: 4451 \tTraining Loss: 0.315893\n",
            "Epoch: 4452 \tTraining Loss: 0.315872\n",
            "Epoch: 4453 \tTraining Loss: 0.315851\n",
            "Epoch: 4454 \tTraining Loss: 0.315831\n",
            "Epoch: 4455 \tTraining Loss: 0.315810\n",
            "Epoch: 4456 \tTraining Loss: 0.315790\n",
            "Epoch: 4457 \tTraining Loss: 0.315769\n",
            "Epoch: 4458 \tTraining Loss: 0.315749\n",
            "Epoch: 4459 \tTraining Loss: 0.315729\n",
            "Epoch: 4460 \tTraining Loss: 0.315709\n",
            "Epoch: 4461 \tTraining Loss: 0.315689\n",
            "Epoch: 4462 \tTraining Loss: 0.315669\n",
            "Epoch: 4463 \tTraining Loss: 0.315650\n",
            "Epoch: 4464 \tTraining Loss: 0.315631\n",
            "Epoch: 4465 \tTraining Loss: 0.315613\n",
            "Epoch: 4466 \tTraining Loss: 0.315595\n",
            "Epoch: 4467 \tTraining Loss: 0.315581\n",
            "Epoch: 4468 \tTraining Loss: 0.315567\n",
            "Epoch: 4469 \tTraining Loss: 0.315560\n",
            "Epoch: 4470 \tTraining Loss: 0.315551\n",
            "Epoch: 4471 \tTraining Loss: 0.315558\n",
            "Epoch: 4472 \tTraining Loss: 0.315553\n",
            "Epoch: 4473 \tTraining Loss: 0.315580\n",
            "Epoch: 4474 \tTraining Loss: 0.315564\n",
            "Epoch: 4475 \tTraining Loss: 0.315597\n",
            "Epoch: 4476 \tTraining Loss: 0.315550\n",
            "Epoch: 4477 \tTraining Loss: 0.315551\n",
            "Epoch: 4478 \tTraining Loss: 0.315485\n",
            "Epoch: 4479 \tTraining Loss: 0.315453\n",
            "Epoch: 4480 \tTraining Loss: 0.315394\n",
            "Epoch: 4481 \tTraining Loss: 0.315355\n",
            "Epoch: 4482 \tTraining Loss: 0.315308\n",
            "Epoch: 4483 \tTraining Loss: 0.315273\n",
            "Epoch: 4484 \tTraining Loss: 0.315238\n",
            "Epoch: 4485 \tTraining Loss: 0.315212\n",
            "Epoch: 4486 \tTraining Loss: 0.315188\n",
            "Epoch: 4487 \tTraining Loss: 0.315169\n",
            "Epoch: 4488 \tTraining Loss: 0.315147\n",
            "Epoch: 4489 \tTraining Loss: 0.315127\n",
            "Epoch: 4490 \tTraining Loss: 0.315103\n",
            "Epoch: 4491 \tTraining Loss: 0.315080\n",
            "Epoch: 4492 \tTraining Loss: 0.315058\n",
            "Epoch: 4493 \tTraining Loss: 0.315038\n",
            "Epoch: 4494 \tTraining Loss: 0.315020\n",
            "Epoch: 4495 \tTraining Loss: 0.315004\n",
            "Epoch: 4496 \tTraining Loss: 0.314990\n",
            "Epoch: 4497 \tTraining Loss: 0.314980\n",
            "Epoch: 4498 \tTraining Loss: 0.314970\n",
            "Epoch: 4499 \tTraining Loss: 0.314972\n",
            "Epoch: 4500 \tTraining Loss: 0.314971\n",
            "Epoch: 4501 \tTraining Loss: 0.314997\n",
            "Epoch: 4502 \tTraining Loss: 0.315004\n",
            "Epoch: 4503 \tTraining Loss: 0.315067\n",
            "Epoch: 4504 \tTraining Loss: 0.315065\n",
            "Epoch: 4505 \tTraining Loss: 0.315152\n",
            "Epoch: 4506 \tTraining Loss: 0.315127\n",
            "Epoch: 4507 \tTraining Loss: 0.315219\n",
            "Epoch: 4508 \tTraining Loss: 0.315215\n",
            "Epoch: 4509 \tTraining Loss: 0.315374\n",
            "Epoch: 4510 \tTraining Loss: 0.315435\n",
            "Epoch: 4511 \tTraining Loss: 0.315787\n",
            "Epoch: 4512 \tTraining Loss: 0.315970\n",
            "Epoch: 4513 \tTraining Loss: 0.316867\n",
            "Epoch: 4514 \tTraining Loss: 0.317462\n",
            "Epoch: 4515 \tTraining Loss: 0.320448\n",
            "Epoch: 4516 \tTraining Loss: 0.322695\n",
            "Epoch: 4517 \tTraining Loss: 0.331905\n",
            "Epoch: 4518 \tTraining Loss: 0.328017\n",
            "Epoch: 4519 \tTraining Loss: 0.325019\n",
            "Epoch: 4520 \tTraining Loss: 0.317188\n",
            "Epoch: 4521 \tTraining Loss: 0.325285\n",
            "Epoch: 4522 \tTraining Loss: 0.337799\n",
            "Epoch: 4523 \tTraining Loss: 0.329554\n",
            "Epoch: 4524 \tTraining Loss: 0.364455\n",
            "Epoch: 4525 \tTraining Loss: 0.493232\n",
            "Epoch: 4526 \tTraining Loss: 0.447111\n",
            "Epoch: 4527 \tTraining Loss: 0.418345\n",
            "Epoch: 4528 \tTraining Loss: 0.408244\n",
            "Epoch: 4529 \tTraining Loss: 0.422991\n",
            "Epoch: 4530 \tTraining Loss: 0.405918\n",
            "Epoch: 4531 \tTraining Loss: 0.369042\n",
            "Epoch: 4532 \tTraining Loss: 0.382104\n",
            "Epoch: 4533 \tTraining Loss: 0.374501\n",
            "Epoch: 4534 \tTraining Loss: 0.380561\n",
            "Epoch: 4535 \tTraining Loss: 0.371784\n",
            "Epoch: 4536 \tTraining Loss: 0.365599\n",
            "Epoch: 4537 \tTraining Loss: 0.364394\n",
            "Epoch: 4538 \tTraining Loss: 0.355559\n",
            "Epoch: 4539 \tTraining Loss: 0.356390\n",
            "Epoch: 4540 \tTraining Loss: 0.360535\n",
            "Epoch: 4541 \tTraining Loss: 0.347741\n",
            "Epoch: 4542 \tTraining Loss: 0.350725\n",
            "Epoch: 4543 \tTraining Loss: 0.347151\n",
            "Epoch: 4544 \tTraining Loss: 0.340315\n",
            "Epoch: 4545 \tTraining Loss: 0.333968\n",
            "Epoch: 4546 \tTraining Loss: 0.340312\n",
            "Epoch: 4547 \tTraining Loss: 0.339907\n",
            "Epoch: 4548 \tTraining Loss: 0.334807\n",
            "Epoch: 4549 \tTraining Loss: 0.336387\n",
            "Epoch: 4550 \tTraining Loss: 0.330340\n",
            "Epoch: 4551 \tTraining Loss: 0.331077\n",
            "Epoch: 4552 \tTraining Loss: 0.331172\n",
            "Epoch: 4553 \tTraining Loss: 0.326244\n",
            "Epoch: 4554 \tTraining Loss: 0.327713\n",
            "Epoch: 4555 \tTraining Loss: 0.322162\n",
            "Epoch: 4556 \tTraining Loss: 0.325680\n",
            "Epoch: 4557 \tTraining Loss: 0.323578\n",
            "Epoch: 4558 \tTraining Loss: 0.323766\n",
            "Epoch: 4559 \tTraining Loss: 0.320336\n",
            "Epoch: 4560 \tTraining Loss: 0.322069\n",
            "Epoch: 4561 \tTraining Loss: 0.320764\n",
            "Epoch: 4562 \tTraining Loss: 0.320246\n",
            "Epoch: 4563 \tTraining Loss: 0.319942\n",
            "Epoch: 4564 \tTraining Loss: 0.319197\n",
            "Epoch: 4565 \tTraining Loss: 0.319038\n",
            "Epoch: 4566 \tTraining Loss: 0.318055\n",
            "Epoch: 4567 \tTraining Loss: 0.317612\n",
            "Epoch: 4568 \tTraining Loss: 0.317242\n",
            "Epoch: 4569 \tTraining Loss: 0.317314\n",
            "Epoch: 4570 \tTraining Loss: 0.317084\n",
            "Epoch: 4571 \tTraining Loss: 0.317066\n",
            "Epoch: 4572 \tTraining Loss: 0.316655\n",
            "Epoch: 4573 \tTraining Loss: 0.316307\n",
            "Epoch: 4574 \tTraining Loss: 0.316095\n",
            "Epoch: 4575 \tTraining Loss: 0.315777\n",
            "Epoch: 4576 \tTraining Loss: 0.315637\n",
            "Epoch: 4577 \tTraining Loss: 0.315788\n",
            "Epoch: 4578 \tTraining Loss: 0.315613\n",
            "Epoch: 4579 \tTraining Loss: 0.315561\n",
            "Epoch: 4580 \tTraining Loss: 0.315397\n",
            "Epoch: 4581 \tTraining Loss: 0.315321\n",
            "Epoch: 4582 \tTraining Loss: 0.315200\n",
            "Epoch: 4583 \tTraining Loss: 0.315045\n",
            "Epoch: 4584 \tTraining Loss: 0.315021\n",
            "Epoch: 4585 \tTraining Loss: 0.315022\n",
            "Epoch: 4586 \tTraining Loss: 0.314905\n",
            "Epoch: 4587 \tTraining Loss: 0.314870\n",
            "Epoch: 4588 \tTraining Loss: 0.314769\n",
            "Epoch: 4589 \tTraining Loss: 0.314715\n",
            "Epoch: 4590 \tTraining Loss: 0.314656\n",
            "Epoch: 4591 \tTraining Loss: 0.314593\n",
            "Epoch: 4592 \tTraining Loss: 0.314556\n",
            "Epoch: 4593 \tTraining Loss: 0.314526\n",
            "Epoch: 4594 \tTraining Loss: 0.314453\n",
            "Epoch: 4595 \tTraining Loss: 0.314432\n",
            "Epoch: 4596 \tTraining Loss: 0.314396\n",
            "Epoch: 4597 \tTraining Loss: 0.314353\n",
            "Epoch: 4598 \tTraining Loss: 0.314335\n",
            "Epoch: 4599 \tTraining Loss: 0.314283\n",
            "Epoch: 4600 \tTraining Loss: 0.314257\n",
            "Epoch: 4601 \tTraining Loss: 0.314231\n",
            "Epoch: 4602 \tTraining Loss: 0.314194\n",
            "Epoch: 4603 \tTraining Loss: 0.314160\n",
            "Epoch: 4604 \tTraining Loss: 0.314132\n",
            "Epoch: 4605 \tTraining Loss: 0.314098\n",
            "Epoch: 4606 \tTraining Loss: 0.314083\n",
            "Epoch: 4607 \tTraining Loss: 0.314055\n",
            "Epoch: 4608 \tTraining Loss: 0.314026\n",
            "Epoch: 4609 \tTraining Loss: 0.314003\n",
            "Epoch: 4610 \tTraining Loss: 0.313972\n",
            "Epoch: 4611 \tTraining Loss: 0.313949\n",
            "Epoch: 4612 \tTraining Loss: 0.313930\n",
            "Epoch: 4613 \tTraining Loss: 0.313908\n",
            "Epoch: 4614 \tTraining Loss: 0.313883\n",
            "Epoch: 4615 \tTraining Loss: 0.313861\n",
            "Epoch: 4616 \tTraining Loss: 0.313837\n",
            "Epoch: 4617 \tTraining Loss: 0.313816\n",
            "Epoch: 4618 \tTraining Loss: 0.313791\n",
            "Epoch: 4619 \tTraining Loss: 0.313769\n",
            "Epoch: 4620 \tTraining Loss: 0.313750\n",
            "Epoch: 4621 \tTraining Loss: 0.313728\n",
            "Epoch: 4622 \tTraining Loss: 0.313708\n",
            "Epoch: 4623 \tTraining Loss: 0.313688\n",
            "Epoch: 4624 \tTraining Loss: 0.313668\n",
            "Epoch: 4625 \tTraining Loss: 0.313649\n",
            "Epoch: 4626 \tTraining Loss: 0.313630\n",
            "Epoch: 4627 \tTraining Loss: 0.313609\n",
            "Epoch: 4628 \tTraining Loss: 0.313590\n",
            "Epoch: 4629 \tTraining Loss: 0.313570\n",
            "Epoch: 4630 \tTraining Loss: 0.313551\n",
            "Epoch: 4631 \tTraining Loss: 0.313532\n",
            "Epoch: 4632 \tTraining Loss: 0.313512\n",
            "Epoch: 4633 \tTraining Loss: 0.313493\n",
            "Epoch: 4634 \tTraining Loss: 0.313475\n",
            "Epoch: 4635 \tTraining Loss: 0.313456\n",
            "Epoch: 4636 \tTraining Loss: 0.313437\n",
            "Epoch: 4637 \tTraining Loss: 0.313419\n",
            "Epoch: 4638 \tTraining Loss: 0.313400\n",
            "Epoch: 4639 \tTraining Loss: 0.313382\n",
            "Epoch: 4640 \tTraining Loss: 0.313364\n",
            "Epoch: 4641 \tTraining Loss: 0.313345\n",
            "Epoch: 4642 \tTraining Loss: 0.313327\n",
            "Epoch: 4643 \tTraining Loss: 0.313309\n",
            "Epoch: 4644 \tTraining Loss: 0.313291\n",
            "Epoch: 4645 \tTraining Loss: 0.313273\n",
            "Epoch: 4646 \tTraining Loss: 0.313255\n",
            "Epoch: 4647 \tTraining Loss: 0.313237\n",
            "Epoch: 4648 \tTraining Loss: 0.313219\n",
            "Epoch: 4649 \tTraining Loss: 0.313201\n",
            "Epoch: 4650 \tTraining Loss: 0.313182\n",
            "Epoch: 4651 \tTraining Loss: 0.313164\n",
            "Epoch: 4652 \tTraining Loss: 0.313146\n",
            "Epoch: 4653 \tTraining Loss: 0.313128\n",
            "Epoch: 4654 \tTraining Loss: 0.313109\n",
            "Epoch: 4655 \tTraining Loss: 0.313090\n",
            "Epoch: 4656 \tTraining Loss: 0.313069\n",
            "Epoch: 4657 \tTraining Loss: 0.313048\n",
            "Epoch: 4658 \tTraining Loss: 0.313025\n",
            "Epoch: 4659 \tTraining Loss: 0.313000\n",
            "Epoch: 4660 \tTraining Loss: 0.312977\n",
            "Epoch: 4661 \tTraining Loss: 0.312955\n",
            "Epoch: 4662 \tTraining Loss: 0.312937\n",
            "Epoch: 4663 \tTraining Loss: 0.312924\n",
            "Epoch: 4664 \tTraining Loss: 0.312912\n",
            "Epoch: 4665 \tTraining Loss: 0.312896\n",
            "Epoch: 4666 \tTraining Loss: 0.312874\n",
            "Epoch: 4667 \tTraining Loss: 0.312853\n",
            "Epoch: 4668 \tTraining Loss: 0.312834\n",
            "Epoch: 4669 \tTraining Loss: 0.312818\n",
            "Epoch: 4670 \tTraining Loss: 0.312802\n",
            "Epoch: 4671 \tTraining Loss: 0.312786\n",
            "Epoch: 4672 \tTraining Loss: 0.312770\n",
            "Epoch: 4673 \tTraining Loss: 0.312753\n",
            "Epoch: 4674 \tTraining Loss: 0.312736\n",
            "Epoch: 4675 \tTraining Loss: 0.312718\n",
            "Epoch: 4676 \tTraining Loss: 0.312700\n",
            "Epoch: 4677 \tTraining Loss: 0.312682\n",
            "Epoch: 4678 \tTraining Loss: 0.312664\n",
            "Epoch: 4679 \tTraining Loss: 0.312647\n",
            "Epoch: 4680 \tTraining Loss: 0.312631\n",
            "Epoch: 4681 \tTraining Loss: 0.312615\n",
            "Epoch: 4682 \tTraining Loss: 0.312598\n",
            "Epoch: 4683 \tTraining Loss: 0.312581\n",
            "Epoch: 4684 \tTraining Loss: 0.312564\n",
            "Epoch: 4685 \tTraining Loss: 0.312546\n",
            "Epoch: 4686 \tTraining Loss: 0.312529\n",
            "Epoch: 4687 \tTraining Loss: 0.312513\n",
            "Epoch: 4688 \tTraining Loss: 0.312496\n",
            "Epoch: 4689 \tTraining Loss: 0.312479\n",
            "Epoch: 4690 \tTraining Loss: 0.312463\n",
            "Epoch: 4691 \tTraining Loss: 0.312446\n",
            "Epoch: 4692 \tTraining Loss: 0.312429\n",
            "Epoch: 4693 \tTraining Loss: 0.312412\n",
            "Epoch: 4694 \tTraining Loss: 0.312395\n",
            "Epoch: 4695 \tTraining Loss: 0.312378\n",
            "Epoch: 4696 \tTraining Loss: 0.312361\n",
            "Epoch: 4697 \tTraining Loss: 0.312344\n",
            "Epoch: 4698 \tTraining Loss: 0.312328\n",
            "Epoch: 4699 \tTraining Loss: 0.312311\n",
            "Epoch: 4700 \tTraining Loss: 0.312294\n",
            "Epoch: 4701 \tTraining Loss: 0.312277\n",
            "Epoch: 4702 \tTraining Loss: 0.312260\n",
            "Epoch: 4703 \tTraining Loss: 0.312243\n",
            "Epoch: 4704 \tTraining Loss: 0.312226\n",
            "Epoch: 4705 \tTraining Loss: 0.312209\n",
            "Epoch: 4706 \tTraining Loss: 0.312192\n",
            "Epoch: 4707 \tTraining Loss: 0.312175\n",
            "Epoch: 4708 \tTraining Loss: 0.312158\n",
            "Epoch: 4709 \tTraining Loss: 0.312141\n",
            "Epoch: 4710 \tTraining Loss: 0.312124\n",
            "Epoch: 4711 \tTraining Loss: 0.312107\n",
            "Epoch: 4712 \tTraining Loss: 0.312090\n",
            "Epoch: 4713 \tTraining Loss: 0.312073\n",
            "Epoch: 4714 \tTraining Loss: 0.312056\n",
            "Epoch: 4715 \tTraining Loss: 0.312039\n",
            "Epoch: 4716 \tTraining Loss: 0.312023\n",
            "Epoch: 4717 \tTraining Loss: 0.312006\n",
            "Epoch: 4718 \tTraining Loss: 0.311989\n",
            "Epoch: 4719 \tTraining Loss: 0.311972\n",
            "Epoch: 4720 \tTraining Loss: 0.311956\n",
            "Epoch: 4721 \tTraining Loss: 0.311939\n",
            "Epoch: 4722 \tTraining Loss: 0.311923\n",
            "Epoch: 4723 \tTraining Loss: 0.311906\n",
            "Epoch: 4724 \tTraining Loss: 0.311889\n",
            "Epoch: 4725 \tTraining Loss: 0.311873\n",
            "Epoch: 4726 \tTraining Loss: 0.311856\n",
            "Epoch: 4727 \tTraining Loss: 0.311840\n",
            "Epoch: 4728 \tTraining Loss: 0.311823\n",
            "Epoch: 4729 \tTraining Loss: 0.311806\n",
            "Epoch: 4730 \tTraining Loss: 0.311790\n",
            "Epoch: 4731 \tTraining Loss: 0.311773\n",
            "Epoch: 4732 \tTraining Loss: 0.311756\n",
            "Epoch: 4733 \tTraining Loss: 0.311739\n",
            "Epoch: 4734 \tTraining Loss: 0.311722\n",
            "Epoch: 4735 \tTraining Loss: 0.311705\n",
            "Epoch: 4736 \tTraining Loss: 0.311687\n",
            "Epoch: 4737 \tTraining Loss: 0.311670\n",
            "Epoch: 4738 \tTraining Loss: 0.311652\n",
            "Epoch: 4739 \tTraining Loss: 0.311633\n",
            "Epoch: 4740 \tTraining Loss: 0.311615\n",
            "Epoch: 4741 \tTraining Loss: 0.311596\n",
            "Epoch: 4742 \tTraining Loss: 0.311577\n",
            "Epoch: 4743 \tTraining Loss: 0.311558\n",
            "Epoch: 4744 \tTraining Loss: 0.311540\n",
            "Epoch: 4745 \tTraining Loss: 0.311522\n",
            "Epoch: 4746 \tTraining Loss: 0.311504\n",
            "Epoch: 4747 \tTraining Loss: 0.311487\n",
            "Epoch: 4748 \tTraining Loss: 0.311469\n",
            "Epoch: 4749 \tTraining Loss: 0.311453\n",
            "Epoch: 4750 \tTraining Loss: 0.311436\n",
            "Epoch: 4751 \tTraining Loss: 0.311419\n",
            "Epoch: 4752 \tTraining Loss: 0.311401\n",
            "Epoch: 4753 \tTraining Loss: 0.311384\n",
            "Epoch: 4754 \tTraining Loss: 0.311366\n",
            "Epoch: 4755 \tTraining Loss: 0.311349\n",
            "Epoch: 4756 \tTraining Loss: 0.311331\n",
            "Epoch: 4757 \tTraining Loss: 0.311313\n",
            "Epoch: 4758 \tTraining Loss: 0.311296\n",
            "Epoch: 4759 \tTraining Loss: 0.311279\n",
            "Epoch: 4760 \tTraining Loss: 0.311264\n",
            "Epoch: 4761 \tTraining Loss: 0.311248\n",
            "Epoch: 4762 \tTraining Loss: 0.311232\n",
            "Epoch: 4763 \tTraining Loss: 0.311215\n",
            "Epoch: 4764 \tTraining Loss: 0.311198\n",
            "Epoch: 4765 \tTraining Loss: 0.311181\n",
            "Epoch: 4766 \tTraining Loss: 0.311165\n",
            "Epoch: 4767 \tTraining Loss: 0.311148\n",
            "Epoch: 4768 \tTraining Loss: 0.311132\n",
            "Epoch: 4769 \tTraining Loss: 0.311116\n",
            "Epoch: 4770 \tTraining Loss: 0.311100\n",
            "Epoch: 4771 \tTraining Loss: 0.311084\n",
            "Epoch: 4772 \tTraining Loss: 0.311068\n",
            "Epoch: 4773 \tTraining Loss: 0.311052\n",
            "Epoch: 4774 \tTraining Loss: 0.311035\n",
            "Epoch: 4775 \tTraining Loss: 0.311019\n",
            "Epoch: 4776 \tTraining Loss: 0.311003\n",
            "Epoch: 4777 \tTraining Loss: 0.310987\n",
            "Epoch: 4778 \tTraining Loss: 0.310970\n",
            "Epoch: 4779 \tTraining Loss: 0.310954\n",
            "Epoch: 4780 \tTraining Loss: 0.310938\n",
            "Epoch: 4781 \tTraining Loss: 0.310922\n",
            "Epoch: 4782 \tTraining Loss: 0.310906\n",
            "Epoch: 4783 \tTraining Loss: 0.310890\n",
            "Epoch: 4784 \tTraining Loss: 0.310874\n",
            "Epoch: 4785 \tTraining Loss: 0.310858\n",
            "Epoch: 4786 \tTraining Loss: 0.310841\n",
            "Epoch: 4787 \tTraining Loss: 0.310825\n",
            "Epoch: 4788 \tTraining Loss: 0.310809\n",
            "Epoch: 4789 \tTraining Loss: 0.310793\n",
            "Epoch: 4790 \tTraining Loss: 0.310777\n",
            "Epoch: 4791 \tTraining Loss: 0.310761\n",
            "Epoch: 4792 \tTraining Loss: 0.310745\n",
            "Epoch: 4793 \tTraining Loss: 0.310729\n",
            "Epoch: 4794 \tTraining Loss: 0.310713\n",
            "Epoch: 4795 \tTraining Loss: 0.310697\n",
            "Epoch: 4796 \tTraining Loss: 0.310680\n",
            "Epoch: 4797 \tTraining Loss: 0.310664\n",
            "Epoch: 4798 \tTraining Loss: 0.310648\n",
            "Epoch: 4799 \tTraining Loss: 0.310632\n",
            "Epoch: 4800 \tTraining Loss: 0.310616\n",
            "Epoch: 4801 \tTraining Loss: 0.310600\n",
            "Epoch: 4802 \tTraining Loss: 0.310584\n",
            "Epoch: 4803 \tTraining Loss: 0.310568\n",
            "Epoch: 4804 \tTraining Loss: 0.310552\n",
            "Epoch: 4805 \tTraining Loss: 0.310536\n",
            "Epoch: 4806 \tTraining Loss: 0.310519\n",
            "Epoch: 4807 \tTraining Loss: 0.310503\n",
            "Epoch: 4808 \tTraining Loss: 0.310487\n",
            "Epoch: 4809 \tTraining Loss: 0.310471\n",
            "Epoch: 4810 \tTraining Loss: 0.310455\n",
            "Epoch: 4811 \tTraining Loss: 0.310439\n",
            "Epoch: 4812 \tTraining Loss: 0.310423\n",
            "Epoch: 4813 \tTraining Loss: 0.310407\n",
            "Epoch: 4814 \tTraining Loss: 0.310391\n",
            "Epoch: 4815 \tTraining Loss: 0.310375\n",
            "Epoch: 4816 \tTraining Loss: 0.310358\n",
            "Epoch: 4817 \tTraining Loss: 0.310342\n",
            "Epoch: 4818 \tTraining Loss: 0.310326\n",
            "Epoch: 4819 \tTraining Loss: 0.310310\n",
            "Epoch: 4820 \tTraining Loss: 0.310294\n",
            "Epoch: 4821 \tTraining Loss: 0.310278\n",
            "Epoch: 4822 \tTraining Loss: 0.310262\n",
            "Epoch: 4823 \tTraining Loss: 0.310246\n",
            "Epoch: 4824 \tTraining Loss: 0.310230\n",
            "Epoch: 4825 \tTraining Loss: 0.310214\n",
            "Epoch: 4826 \tTraining Loss: 0.310197\n",
            "Epoch: 4827 \tTraining Loss: 0.310181\n",
            "Epoch: 4828 \tTraining Loss: 0.310165\n",
            "Epoch: 4829 \tTraining Loss: 0.310149\n",
            "Epoch: 4830 \tTraining Loss: 0.310133\n",
            "Epoch: 4831 \tTraining Loss: 0.310117\n",
            "Epoch: 4832 \tTraining Loss: 0.310101\n",
            "Epoch: 4833 \tTraining Loss: 0.310085\n",
            "Epoch: 4834 \tTraining Loss: 0.310069\n",
            "Epoch: 4835 \tTraining Loss: 0.310052\n",
            "Epoch: 4836 \tTraining Loss: 0.310036\n",
            "Epoch: 4837 \tTraining Loss: 0.310020\n",
            "Epoch: 4838 \tTraining Loss: 0.310004\n",
            "Epoch: 4839 \tTraining Loss: 0.309988\n",
            "Epoch: 4840 \tTraining Loss: 0.309972\n",
            "Epoch: 4841 \tTraining Loss: 0.309956\n",
            "Epoch: 4842 \tTraining Loss: 0.309940\n",
            "Epoch: 4843 \tTraining Loss: 0.309923\n",
            "Epoch: 4844 \tTraining Loss: 0.309907\n",
            "Epoch: 4845 \tTraining Loss: 0.309891\n",
            "Epoch: 4846 \tTraining Loss: 0.309875\n",
            "Epoch: 4847 \tTraining Loss: 0.309859\n",
            "Epoch: 4848 \tTraining Loss: 0.309843\n",
            "Epoch: 4849 \tTraining Loss: 0.309826\n",
            "Epoch: 4850 \tTraining Loss: 0.309810\n",
            "Epoch: 4851 \tTraining Loss: 0.309794\n",
            "Epoch: 4852 \tTraining Loss: 0.309778\n",
            "Epoch: 4853 \tTraining Loss: 0.309762\n",
            "Epoch: 4854 \tTraining Loss: 0.309746\n",
            "Epoch: 4855 \tTraining Loss: 0.309729\n",
            "Epoch: 4856 \tTraining Loss: 0.309713\n",
            "Epoch: 4857 \tTraining Loss: 0.309697\n",
            "Epoch: 4858 \tTraining Loss: 0.309681\n",
            "Epoch: 4859 \tTraining Loss: 0.309664\n",
            "Epoch: 4860 \tTraining Loss: 0.309648\n",
            "Epoch: 4861 \tTraining Loss: 0.309632\n",
            "Epoch: 4862 \tTraining Loss: 0.309616\n",
            "Epoch: 4863 \tTraining Loss: 0.309599\n",
            "Epoch: 4864 \tTraining Loss: 0.309583\n",
            "Epoch: 4865 \tTraining Loss: 0.309567\n",
            "Epoch: 4866 \tTraining Loss: 0.309550\n",
            "Epoch: 4867 \tTraining Loss: 0.309534\n",
            "Epoch: 4868 \tTraining Loss: 0.309518\n",
            "Epoch: 4869 \tTraining Loss: 0.309501\n",
            "Epoch: 4870 \tTraining Loss: 0.309485\n",
            "Epoch: 4871 \tTraining Loss: 0.309468\n",
            "Epoch: 4872 \tTraining Loss: 0.309452\n",
            "Epoch: 4873 \tTraining Loss: 0.309435\n",
            "Epoch: 4874 \tTraining Loss: 0.309418\n",
            "Epoch: 4875 \tTraining Loss: 0.309402\n",
            "Epoch: 4876 \tTraining Loss: 0.309385\n",
            "Epoch: 4877 \tTraining Loss: 0.309368\n",
            "Epoch: 4878 \tTraining Loss: 0.309351\n",
            "Epoch: 4879 \tTraining Loss: 0.309334\n",
            "Epoch: 4880 \tTraining Loss: 0.309318\n",
            "Epoch: 4881 \tTraining Loss: 0.309301\n",
            "Epoch: 4882 \tTraining Loss: 0.309284\n",
            "Epoch: 4883 \tTraining Loss: 0.309268\n",
            "Epoch: 4884 \tTraining Loss: 0.309252\n",
            "Epoch: 4885 \tTraining Loss: 0.309236\n",
            "Epoch: 4886 \tTraining Loss: 0.309220\n",
            "Epoch: 4887 \tTraining Loss: 0.309204\n",
            "Epoch: 4888 \tTraining Loss: 0.309188\n",
            "Epoch: 4889 \tTraining Loss: 0.309172\n",
            "Epoch: 4890 \tTraining Loss: 0.309156\n",
            "Epoch: 4891 \tTraining Loss: 0.309139\n",
            "Epoch: 4892 \tTraining Loss: 0.309123\n",
            "Epoch: 4893 \tTraining Loss: 0.309107\n",
            "Epoch: 4894 \tTraining Loss: 0.309090\n",
            "Epoch: 4895 \tTraining Loss: 0.309074\n",
            "Epoch: 4896 \tTraining Loss: 0.309058\n",
            "Epoch: 4897 \tTraining Loss: 0.309042\n",
            "Epoch: 4898 \tTraining Loss: 0.309025\n",
            "Epoch: 4899 \tTraining Loss: 0.309009\n",
            "Epoch: 4900 \tTraining Loss: 0.308993\n",
            "Epoch: 4901 \tTraining Loss: 0.308977\n",
            "Epoch: 4902 \tTraining Loss: 0.308961\n",
            "Epoch: 4903 \tTraining Loss: 0.308945\n",
            "Epoch: 4904 \tTraining Loss: 0.308928\n",
            "Epoch: 4905 \tTraining Loss: 0.308912\n",
            "Epoch: 4906 \tTraining Loss: 0.308896\n",
            "Epoch: 4907 \tTraining Loss: 0.308880\n",
            "Epoch: 4908 \tTraining Loss: 0.308863\n",
            "Epoch: 4909 \tTraining Loss: 0.308847\n",
            "Epoch: 4910 \tTraining Loss: 0.308831\n",
            "Epoch: 4911 \tTraining Loss: 0.308815\n",
            "Epoch: 4912 \tTraining Loss: 0.308799\n",
            "Epoch: 4913 \tTraining Loss: 0.308782\n",
            "Epoch: 4914 \tTraining Loss: 0.308766\n",
            "Epoch: 4915 \tTraining Loss: 0.308750\n",
            "Epoch: 4916 \tTraining Loss: 0.308734\n",
            "Epoch: 4917 \tTraining Loss: 0.308718\n",
            "Epoch: 4918 \tTraining Loss: 0.308701\n",
            "Epoch: 4919 \tTraining Loss: 0.308685\n",
            "Epoch: 4920 \tTraining Loss: 0.308669\n",
            "Epoch: 4921 \tTraining Loss: 0.308653\n",
            "Epoch: 4922 \tTraining Loss: 0.308636\n",
            "Epoch: 4923 \tTraining Loss: 0.308620\n",
            "Epoch: 4924 \tTraining Loss: 0.308604\n",
            "Epoch: 4925 \tTraining Loss: 0.308588\n",
            "Epoch: 4926 \tTraining Loss: 0.308571\n",
            "Epoch: 4927 \tTraining Loss: 0.308555\n",
            "Epoch: 4928 \tTraining Loss: 0.308539\n",
            "Epoch: 4929 \tTraining Loss: 0.308523\n",
            "Epoch: 4930 \tTraining Loss: 0.308506\n",
            "Epoch: 4931 \tTraining Loss: 0.308490\n",
            "Epoch: 4932 \tTraining Loss: 0.308474\n",
            "Epoch: 4933 \tTraining Loss: 0.308458\n",
            "Epoch: 4934 \tTraining Loss: 0.308442\n",
            "Epoch: 4935 \tTraining Loss: 0.308425\n",
            "Epoch: 4936 \tTraining Loss: 0.308409\n",
            "Epoch: 4937 \tTraining Loss: 0.308393\n",
            "Epoch: 4938 \tTraining Loss: 0.308376\n",
            "Epoch: 4939 \tTraining Loss: 0.308360\n",
            "Epoch: 4940 \tTraining Loss: 0.308344\n",
            "Epoch: 4941 \tTraining Loss: 0.308328\n",
            "Epoch: 4942 \tTraining Loss: 0.308311\n",
            "Epoch: 4943 \tTraining Loss: 0.308295\n",
            "Epoch: 4944 \tTraining Loss: 0.308279\n",
            "Epoch: 4945 \tTraining Loss: 0.308263\n",
            "Epoch: 4946 \tTraining Loss: 0.308246\n",
            "Epoch: 4947 \tTraining Loss: 0.308230\n",
            "Epoch: 4948 \tTraining Loss: 0.308214\n",
            "Epoch: 4949 \tTraining Loss: 0.308197\n",
            "Epoch: 4950 \tTraining Loss: 0.308181\n",
            "Epoch: 4951 \tTraining Loss: 0.308165\n",
            "Epoch: 4952 \tTraining Loss: 0.308149\n",
            "Epoch: 4953 \tTraining Loss: 0.308132\n",
            "Epoch: 4954 \tTraining Loss: 0.308116\n",
            "Epoch: 4955 \tTraining Loss: 0.308100\n",
            "Epoch: 4956 \tTraining Loss: 0.308083\n",
            "Epoch: 4957 \tTraining Loss: 0.308067\n",
            "Epoch: 4958 \tTraining Loss: 0.308051\n",
            "Epoch: 4959 \tTraining Loss: 0.308034\n",
            "Epoch: 4960 \tTraining Loss: 0.308018\n",
            "Epoch: 4961 \tTraining Loss: 0.308002\n",
            "Epoch: 4962 \tTraining Loss: 0.307985\n",
            "Epoch: 4963 \tTraining Loss: 0.307969\n",
            "Epoch: 4964 \tTraining Loss: 0.307953\n",
            "Epoch: 4965 \tTraining Loss: 0.307936\n",
            "Epoch: 4966 \tTraining Loss: 0.307920\n",
            "Epoch: 4967 \tTraining Loss: 0.307904\n",
            "Epoch: 4968 \tTraining Loss: 0.307887\n",
            "Epoch: 4969 \tTraining Loss: 0.307871\n",
            "Epoch: 4970 \tTraining Loss: 0.307855\n",
            "Epoch: 4971 \tTraining Loss: 0.307838\n",
            "Epoch: 4972 \tTraining Loss: 0.307822\n",
            "Epoch: 4973 \tTraining Loss: 0.307806\n",
            "Epoch: 4974 \tTraining Loss: 0.307789\n",
            "Epoch: 4975 \tTraining Loss: 0.307773\n",
            "Epoch: 4976 \tTraining Loss: 0.307757\n",
            "Epoch: 4977 \tTraining Loss: 0.307740\n",
            "Epoch: 4978 \tTraining Loss: 0.307724\n",
            "Epoch: 4979 \tTraining Loss: 0.307708\n",
            "Epoch: 4980 \tTraining Loss: 0.307691\n",
            "Epoch: 4981 \tTraining Loss: 0.307675\n",
            "Epoch: 4982 \tTraining Loss: 0.307658\n",
            "Epoch: 4983 \tTraining Loss: 0.307642\n",
            "Epoch: 4984 \tTraining Loss: 0.307626\n",
            "Epoch: 4985 \tTraining Loss: 0.307609\n",
            "Epoch: 4986 \tTraining Loss: 0.307593\n",
            "Epoch: 4987 \tTraining Loss: 0.307577\n",
            "Epoch: 4988 \tTraining Loss: 0.307560\n",
            "Epoch: 4989 \tTraining Loss: 0.307544\n",
            "Epoch: 4990 \tTraining Loss: 0.307527\n",
            "Epoch: 4991 \tTraining Loss: 0.307511\n",
            "Epoch: 4992 \tTraining Loss: 0.307495\n",
            "Epoch: 4993 \tTraining Loss: 0.307478\n",
            "Epoch: 4994 \tTraining Loss: 0.307462\n",
            "Epoch: 4995 \tTraining Loss: 0.307445\n",
            "Epoch: 4996 \tTraining Loss: 0.307429\n",
            "Epoch: 4997 \tTraining Loss: 0.307412\n",
            "Epoch: 4998 \tTraining Loss: 0.307396\n",
            "Epoch: 4999 \tTraining Loss: 0.307380\n",
            "Epoch: 5000 \tTraining Loss: 0.307363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Test"
      ],
      "metadata": {
        "id": "kf-zaFw918io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test.float())\n",
        "    predicted_labels = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "predicted_labels = np.where(predicted_labels == 0 ,-1,1)\n",
        "y_test = np.where(y_test == 0 ,-1,1)\n",
        "\n",
        "print(predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmhd6hJR18io",
        "outputId": "7516d455-0dba-49c6-a673-4341e2a10c10"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1\n",
            " -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1  1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            "  1 -1 -1  1 -1  1  1 -1 -1  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1\n",
            "  1  1  1 -1 -1 -1 -1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1\n",
            " -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1  1 -1  1  1  1\n",
            "  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
            " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# a = np.array(*predicted_labels)\n",
        "test_accuracy = accuracy_score(y_test, predicted_labels)\n",
        "print(f'Test Accuracy: {100 * test_accuracy:.2f}%')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
        "import seaborn as sns\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QziSLBQo18io",
        "outputId": "7b51b7e5-1ef0-4059-aa34-f99f23a3402a"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 48.88%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly detection using autoencoder"
      ],
      "metadata": {
        "id": "Qn_R9VlU7w4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Data"
      ],
      "metadata": {
        "id": "d4cgb1qrFGTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = op.join('/content/drive/My Drive/','DL','DL_HW05','USD.csv')  # Path to the Data folder\n",
        "USD = pd.read_csv(dir)\n",
        "X = np.array(USD['last price recorded on the day'])"
      ],
      "metadata": {
        "id": "IxBxGr7A-QXs"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Define Model"
      ],
      "metadata": {
        "id": "ouvM7ItZFJ0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Sigmoid()  # Sigmoid activation for reconstruction in the range [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BTv9J71UFQXd"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Split data and adjust layers"
      ],
      "metadata": {
        "id": "9QlqtcXtF7Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42,shuffle=False)\n",
        "train_data = torch.from_numpy(X_train).float()\n",
        "\n",
        "# Define and train the autoencoder\n",
        "input_size = 1\n",
        "hidden_size = 4\n",
        "model = Autoencoder(input_size, hidden_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "mCe-UET2GDcF"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train"
      ],
      "metadata": {
        "id": "EJ1JsHKHGTvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=100\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader:\n",
        "        data = data.float().view(-1, 1)  # Reshape for 1D input\n",
        "        reconstructions = model(data)\n",
        "        loss = criterion(reconstructions, data)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1,\n",
        "        loss.item()\n",
        "        ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjGztFFCGV9X",
        "outputId": "7df6d268-591a-47c1-ea9d-8724cd3bca05"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 1144763520.000000\n",
            "Epoch: 2 \tTraining Loss: 1034111616.000000\n",
            "Epoch: 3 \tTraining Loss: 1211212928.000000\n",
            "Epoch: 4 \tTraining Loss: 1130698240.000000\n",
            "Epoch: 5 \tTraining Loss: 1094131328.000000\n",
            "Epoch: 6 \tTraining Loss: 1719138816.000000\n",
            "Epoch: 7 \tTraining Loss: 1167155072.000000\n",
            "Epoch: 8 \tTraining Loss: 1454562560.000000\n",
            "Epoch: 9 \tTraining Loss: 1264586496.000000\n",
            "Epoch: 10 \tTraining Loss: 1614127872.000000\n",
            "Epoch: 11 \tTraining Loss: 992885184.000000\n",
            "Epoch: 12 \tTraining Loss: 1101694080.000000\n",
            "Epoch: 13 \tTraining Loss: 1021535808.000000\n",
            "Epoch: 14 \tTraining Loss: 1174750336.000000\n",
            "Epoch: 15 \tTraining Loss: 1125910144.000000\n",
            "Epoch: 16 \tTraining Loss: 1405753472.000000\n",
            "Epoch: 17 \tTraining Loss: 1188805120.000000\n",
            "Epoch: 18 \tTraining Loss: 1365015296.000000\n",
            "Epoch: 19 \tTraining Loss: 1345600128.000000\n",
            "Epoch: 20 \tTraining Loss: 1077338368.000000\n",
            "Epoch: 21 \tTraining Loss: 1334825600.000000\n",
            "Epoch: 22 \tTraining Loss: 1141294208.000000\n",
            "Epoch: 23 \tTraining Loss: 2737394688.000000\n",
            "Epoch: 24 \tTraining Loss: 1112735488.000000\n",
            "Epoch: 25 \tTraining Loss: 1089094016.000000\n",
            "Epoch: 26 \tTraining Loss: 1230864896.000000\n",
            "Epoch: 27 \tTraining Loss: 1136382336.000000\n",
            "Epoch: 28 \tTraining Loss: 1031568320.000000\n",
            "Epoch: 29 \tTraining Loss: 1311681408.000000\n",
            "Epoch: 30 \tTraining Loss: 1219517568.000000\n",
            "Epoch: 31 \tTraining Loss: 1197249792.000000\n",
            "Epoch: 32 \tTraining Loss: 1060996352.000000\n",
            "Epoch: 33 \tTraining Loss: 1045675648.000000\n",
            "Epoch: 34 \tTraining Loss: 1218047872.000000\n",
            "Epoch: 35 \tTraining Loss: 1172704768.000000\n",
            "Epoch: 36 \tTraining Loss: 1631019136.000000\n",
            "Epoch: 37 \tTraining Loss: 1048173952.000000\n",
            "Epoch: 38 \tTraining Loss: 1249322112.000000\n",
            "Epoch: 39 \tTraining Loss: 1720941824.000000\n",
            "Epoch: 40 \tTraining Loss: 1182955776.000000\n",
            "Epoch: 41 \tTraining Loss: 1268618752.000000\n",
            "Epoch: 42 \tTraining Loss: 1169002624.000000\n",
            "Epoch: 43 \tTraining Loss: 1174675712.000000\n",
            "Epoch: 44 \tTraining Loss: 1501459328.000000\n",
            "Epoch: 45 \tTraining Loss: 1415976960.000000\n",
            "Epoch: 46 \tTraining Loss: 1260591232.000000\n",
            "Epoch: 47 \tTraining Loss: 2128803712.000000\n",
            "Epoch: 48 \tTraining Loss: 1585770880.000000\n",
            "Epoch: 49 \tTraining Loss: 1681581696.000000\n",
            "Epoch: 50 \tTraining Loss: 1072020672.000000\n",
            "Epoch: 51 \tTraining Loss: 1720589952.000000\n",
            "Epoch: 52 \tTraining Loss: 1700380800.000000\n",
            "Epoch: 53 \tTraining Loss: 1154592256.000000\n",
            "Epoch: 54 \tTraining Loss: 1603035136.000000\n",
            "Epoch: 55 \tTraining Loss: 1038129088.000000\n",
            "Epoch: 56 \tTraining Loss: 1400088192.000000\n",
            "Epoch: 57 \tTraining Loss: 1132427776.000000\n",
            "Epoch: 58 \tTraining Loss: 1215800704.000000\n",
            "Epoch: 59 \tTraining Loss: 1498948736.000000\n",
            "Epoch: 60 \tTraining Loss: 1161129600.000000\n",
            "Epoch: 61 \tTraining Loss: 1177842176.000000\n",
            "Epoch: 62 \tTraining Loss: 985878464.000000\n",
            "Epoch: 63 \tTraining Loss: 1449936384.000000\n",
            "Epoch: 64 \tTraining Loss: 1532014080.000000\n",
            "Epoch: 65 \tTraining Loss: 1300794112.000000\n",
            "Epoch: 66 \tTraining Loss: 1263670784.000000\n",
            "Epoch: 67 \tTraining Loss: 941908992.000000\n",
            "Epoch: 68 \tTraining Loss: 1463276416.000000\n",
            "Epoch: 69 \tTraining Loss: 1273551232.000000\n",
            "Epoch: 70 \tTraining Loss: 987117952.000000\n",
            "Epoch: 71 \tTraining Loss: 1336780672.000000\n",
            "Epoch: 72 \tTraining Loss: 1115052800.000000\n",
            "Epoch: 73 \tTraining Loss: 1031877568.000000\n",
            "Epoch: 74 \tTraining Loss: 1375531776.000000\n",
            "Epoch: 75 \tTraining Loss: 1302755584.000000\n",
            "Epoch: 76 \tTraining Loss: 1398452096.000000\n",
            "Epoch: 77 \tTraining Loss: 1193307008.000000\n",
            "Epoch: 78 \tTraining Loss: 1457172736.000000\n",
            "Epoch: 79 \tTraining Loss: 1242207104.000000\n",
            "Epoch: 80 \tTraining Loss: 1256547072.000000\n",
            "Epoch: 81 \tTraining Loss: 1097992576.000000\n",
            "Epoch: 82 \tTraining Loss: 1149768192.000000\n",
            "Epoch: 83 \tTraining Loss: 986573056.000000\n",
            "Epoch: 84 \tTraining Loss: 1054001536.000000\n",
            "Epoch: 85 \tTraining Loss: 1379870592.000000\n",
            "Epoch: 86 \tTraining Loss: 1784920064.000000\n",
            "Epoch: 87 \tTraining Loss: 1307618688.000000\n",
            "Epoch: 88 \tTraining Loss: 941431360.000000\n",
            "Epoch: 89 \tTraining Loss: 1323078272.000000\n",
            "Epoch: 90 \tTraining Loss: 1814958976.000000\n",
            "Epoch: 91 \tTraining Loss: 1462613120.000000\n",
            "Epoch: 92 \tTraining Loss: 1318212608.000000\n",
            "Epoch: 93 \tTraining Loss: 1147886080.000000\n",
            "Epoch: 94 \tTraining Loss: 1164634624.000000\n",
            "Epoch: 95 \tTraining Loss: 941057024.000000\n",
            "Epoch: 96 \tTraining Loss: 917869760.000000\n",
            "Epoch: 97 \tTraining Loss: 1149906176.000000\n",
            "Epoch: 98 \tTraining Loss: 1372038912.000000\n",
            "Epoch: 99 \tTraining Loss: 1140137472.000000\n",
            "Epoch: 100 \tTraining Loss: 939066112.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Anomaly detection"
      ],
      "metadata": {
        "id": "IY6lS3acHFbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_anomalies(model, data, threshold):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        reconstructions = model(data.float().view(-1, 1))  # Reshape for 1D input\n",
        "        mse_loss = nn.MSELoss(reduction='none')(reconstructions, data.float().view(-1, 1))\n",
        "        mse_loss = mse_loss.mean(dim=1)\n",
        "    anomalies = mse_loss > threshold\n",
        "    return anomalies.numpy()\n",
        "\n",
        "threshold = 0.1\n",
        "test_data = torch.from_numpy(X_test).float()\n",
        "anomalies = detect_anomalies(model, test_data, threshold)\n",
        "\n",
        "print(\"Anomalies:\", anomalies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJQqy9Dk8DTN",
        "outputId": "66fc5c67-b80f-455f-d72c-1c5f7e99d4a5"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomalies: [ True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True  True  True  True  True\n",
            "  True  True  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "# Plot training data\n",
        "plt.plot(range(len(train_data)), train_data, label='Training Data', color='blue')\n",
        "\n",
        "# Mark anomalies in red\n",
        "anomalous_indices = np.where(anomalies)[0]\n",
        "plt.scatter(anomalous_indices + len(train_data), test_data[anomalous_indices].numpy(), label='Anomalous Data', color='red')\n",
        "\n",
        "plt.title('Autoencoder: Training Data and Test Data')\n",
        "plt.xlabel('Data Index')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "39VHbB81Cipk",
        "outputId": "20c76a7d-d376-4127-fccb-dcd241bb991a"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WDdJFcJpglpw",
        "4qPS_VyOsgRJ",
        "lWu1IljJ9tjS",
        "vFcB5UXF9yzv",
        "xoEI9n-BMJg8",
        "EwAS7rX4MjIW",
        "HMCHvwUVM7zF",
        "L2IRXvtoRpZP",
        "nT14eQRNR80e",
        "MROj3PHKSHAP",
        "zJVkNSqcd09r",
        "LUacQMhNd09s",
        "V9IZaKTdd09t",
        "PRJHOvbAd09t",
        "V-byM0CffWtO",
        "PyrowQNRyo3O",
        "geJlMRc8yo3P",
        "P3EgEE_byo3Q",
        "D7nugXpeyo3Q",
        "beOLOze0yKFe",
        "yrRo434K1I4z",
        "VD1sojrw13_O",
        "Qn_R9VlU7w4F",
        "d4cgb1qrFGTz",
        "ouvM7ItZFJ0_",
        "9QlqtcXtF7Fj",
        "EJ1JsHKHGTvC",
        "IY6lS3acHFbN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}