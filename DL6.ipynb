{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdipourasl/Deep-Learning-1402/blob/main/DL6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQPNR08rglpt"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<h1>Deep Learning Project #6<h1>\n",
        "Amin Abdipour 401133011</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDdJFcJpglpw"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "OCzkV-6xVs6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72c69faa-47e2-488c-ffea-1663a3ffe1a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import os\n",
        "import os.path as op\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# from PIL import Image\n",
        "# from torch.utils.data import random_split, DataLoader\n",
        "# from sklearn.metrics import accuracy_score, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model"
      ],
      "metadata": {
        "id": "XcJfjkSNtVgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L2 Normalization"
      ],
      "metadata": {
        "id": "8onofJm33dyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)"
      ],
      "metadata": {
        "id": "iBq_c1-L3gvj"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectral Normalization"
      ],
      "metadata": {
        "id": "lCO_FzB53mM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)"
      ],
      "metadata": {
        "id": "mZSAicl53tb7"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Up Conv"
      ],
      "metadata": {
        "id": "Etd3tjki3v1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upconv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, spectral_norm=False):\n",
        "    \"\"\"Creates a upsample-and-convolution layer, with optional batch normalization.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    if stride>1:\n",
        "        layers.append(nn.Upsample(scale_factor=stride))\n",
        "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)\n",
        "    if spectral_norm:\n",
        "        layers.append(SpectralNorm(conv_layer))\n",
        "    else:\n",
        "        layers.append(conv_layer)\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def conv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, init_zero_weights=False, spectral_norm=False):\n",
        "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "    if init_zero_weights:\n",
        "        conv_layer.weight.data = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n",
        "\n",
        "    if spectral_norm:\n",
        "        layers.append(SpectralNorm(conv_layer))\n",
        "    else:\n",
        "        layers.append(conv_layer)\n",
        "\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "GDO6vjxw31M9"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet Block"
      ],
      "metadata": {
        "id": "icoHHD2232CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, conv_dim):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_layer = conv(in_channels=conv_dim, out_channels=conv_dim, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.conv_layer(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "of0ZX8S44BL2"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "1xarJzsv4GMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCGenerator(nn.Module):\n",
        "    def __init__(self, noise_size=100, conv_dim=32, spectral_norm=False):\n",
        "        super(DCGenerator, self).__init__()\n",
        "\n",
        "        self.conv_dim = conv_dim\n",
        "\n",
        "        # Linear layer and BatchNorm for initial input\n",
        "        self.linear_bn = nn.Conv2d(in_channels=noise_size, out_channels=32*4*4*4, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "\n",
        "        # Upsampling and convolution layers\n",
        "        self.upconv1 = upconv(conv_dim*4, conv_dim*2, kernel_size=5, stride=2, padding=2, spectral_norm=spectral_norm)\n",
        "        self.upconv2 = upconv(conv_dim*2, conv_dim, kernel_size=5, stride=2, padding=2, spectral_norm=spectral_norm)\n",
        "        self.upconv3 = upconv(conv_dim, 3, kernel_size=5, stride=2, padding=2, batch_norm=False, spectral_norm=spectral_norm)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"Generates an image given a sample of random noise.\n",
        "\n",
        "            Input\n",
        "            -----\n",
        "                z: BS x noise_size x 1 x 1   -->  BSx100x1x1 (during training)\n",
        "\n",
        "            Output\n",
        "            ------\n",
        "                out: BS x channels x image_width x image_height  -->  BSx3x32x32 (during training)\n",
        "        \"\"\"\n",
        "        batch_size = z.size(0)\n",
        "\n",
        "        out = F.relu(self.linear_bn(z)).view(-1, self.conv_dim*4, 4, 4)    # BS x 128 x 4 x 4\n",
        "        out = F.relu(self.upconv1(out))  # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.upconv2(out))  # BS x 32 x 16 x 16\n",
        "        out = F.tanh(self.upconv3(out))  # BS x 3 x 32 x 32\n",
        "\n",
        "        out_size = out.size()\n",
        "        if out_size != torch.Size([batch_size, 3, 32, 32]):\n",
        "            raise ValueError(\"expect {} x 3 x 32 x 32, but get {}\".format(batch_size, out_size))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "wn1qlrJ55A1K"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "cXDcXsbF5FVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, conv_dim=64, spectral_norm=False):\n",
        "        super(DCDiscriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = conv(in_channels=3, out_channels=conv_dim, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
        "        self.conv2 = conv(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
        "        self.conv3 = conv(in_channels=conv_dim*2, out_channels=conv_dim*4, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
        "        self.conv4 = conv(in_channels=conv_dim*4, out_channels=1, kernel_size=5, stride=2, padding=1, batch_norm=False, spectral_norm=spectral_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        out = F.relu(self.conv1(x))    # BS x 64 x 16 x 16\n",
        "        out = F.relu(self.conv2(out))    # BS x 64 x 8 x 8\n",
        "        out = F.relu(self.conv3(out))    # BS x 64 x 4 x 4\n",
        "\n",
        "        out = self.conv4(out).squeeze()\n",
        "        out_size = out.size()\n",
        "        if out_size != torch.Size([batch_size,]):\n",
        "            raise ValueError(\"expect {} x 1, but get {}\".format(batch_size, out_size))\n",
        "        return out"
      ],
      "metadata": {
        "id": "eA_bUG-u5PtM"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8o-_kDKGBXbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "RaFnMVuaBYlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# from model import DCDiscriminator, DCGenerator\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def to_var(tensor, cuda=True):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def to_data(x):\n",
        "    \"\"\"Converts variable to numpy.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cpu()\n",
        "    return x.data.numpy()\n",
        "\n",
        "\n",
        "def create_dir(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def gan_checkpoint(iteration, G, D, opts):\n",
        "    \"\"\"Saves the parameters of the generator G and discriminator D.\n",
        "    \"\"\"\n",
        "    G_path = os.path.join(opts.checkpoint_dir, f'G_{iteration}.pkl')\n",
        "    D_path = os.path.join(opts.checkpoint_dir, f'D_{iteration}.pkl')\n",
        "    torch.save(G.state_dict(), G_path)\n",
        "    torch.save(D.state_dict(), D_path)\n",
        "\n",
        "def load_checkpoint(opts, iteration):\n",
        "    \"\"\"Loads the generator and discriminator models from checkpoints.\n",
        "    \"\"\"\n",
        "    G_path = os.path.join(opts.load, f'G_{iteration}.pkl')\n",
        "    D_path = os.path.join(opts.load, f'D_{iteration}.pkl')\n",
        "\n",
        "    G = DCGenerator(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim, spectral_norm=opts.spectral_norm)\n",
        "    D = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
        "\n",
        "    G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
        "    D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        G.cuda()\n",
        "        D.cuda()\n",
        "        print('Models moved to GPU.')\n",
        "\n",
        "    return G, D\n",
        "\n",
        "\n",
        "def gan_save_samples(G, fixed_noise, iteration, opts):\n",
        "    generated_images = G(fixed_noise)\n",
        "    generated_images = to_data(generated_images)\n",
        "\n",
        "    # Save images in sample dir\n",
        "    #print(\"generated_images.shape: \", generated_images.shape)\n",
        "\n",
        "    # We have a numpy array named 'image_array' with shape (100, 3, 32, 32)\n",
        "    output_directory = op.join('/content/drive/My Drive/','DL','DL_HW06')\n",
        "    # Loop through the array and save each image\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        # Get the RGB image from the array\n",
        "        rgb_image = (generated_images[i] * 255).astype(np.uint8).transpose(1, 2, 0)  # Scale to 0-255 and transpose to (32, 32, 3)\n",
        "\n",
        "        # Create a PIL Image from the numpy array\n",
        "        pil_image = Image.fromarray(rgb_image)\n",
        "\n",
        "        # Save the image\n",
        "        save_path = os.path.join(output_directory,f'epoch{iteration}', f'image_{i+1}.png')\n",
        "        pil_image.save(save_path)\n",
        "\n",
        "    print(\"Images saved successfully in the directory:\", output_directory)\n",
        "\n",
        "def create_model(opts):\n",
        "    \"\"\"Builds the generators and discriminators.\n",
        "    \"\"\"\n",
        "    ### GAN\n",
        "    G = DCGenerator(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim, spectral_norm=opts.spectral_norm)\n",
        "    D = DCDiscriminator(conv_dim=opts.d_conv_dim, spectral_norm=opts.spectral_norm)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        G.cuda()\n",
        "        D.cuda()\n",
        "        print('Models moved to GPU.')\n",
        "    return G, D\n",
        "\n",
        "def sample_noise(batch_size, dim):\n",
        "    \"\"\"\n",
        "    Generate a PyTorch Tensor of uniform random noise.\n",
        "\n",
        "    Input:\n",
        "    - batch_size: Integer giving the batch size of noise to generate.\n",
        "    - dim: Integer giving the dimension of noise to generate.\n",
        "\n",
        "    Output:\n",
        "    - A PyTorch Tensor of shape (batch_size, dim, 1, 1) containing uniform\n",
        "      random noise in the range (-1, 1).\n",
        "    \"\"\"\n",
        "    return to_var(torch.rand(batch_size, dim) * 2 - 1).unsqueeze(2).unsqueeze(3)\n"
      ],
      "metadata": {
        "id": "-c6Tqu8lBanQ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dataset.py"
      ],
      "metadata": {
        "id": "C7wpA7uxCHhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def get_emoji_loader(train_path, test_path, batch_size, image_size):\n",
        "    transform = transforms.Compose([\n",
        "                    transforms.Resize(image_size),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                ])\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
        "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
        "\n",
        "    train_dloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    test_dloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    return train_dloader, test_dloader"
      ],
      "metadata": {
        "id": "M_EvQzSrCLYq"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "Tbw24US9-Ofa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "# from utils import create_dir, create_model, sample_noise, to_var, gan_save_samples, gan_checkpoint\n",
        "# from dataset import get_emoji_loader\n",
        "\n",
        "\n",
        "def gan_training_loop(dataloader, test_dataloader, opts):\n",
        "\n",
        "    G, D = create_model(opts)\n",
        "\n",
        "    g_params = G.parameters()  # Get generator parameters\n",
        "    d_params = D.parameters()  # Get discriminator parameters\n",
        "\n",
        "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
        "    d_optimizer = optim.Adam(d_params, opts.lr * 2., [opts.beta1, opts.beta2])\n",
        "\n",
        "    train_iter = iter(dataloader)\n",
        "\n",
        "    test_iter = iter(test_dataloader)\n",
        "\n",
        "    # Get some fixed data from domains X for sampling. These are images that are held\n",
        "    # constant throughout training, that allow us to inspect the model's performance.\n",
        "    fixed_noise = sample_noise(100, opts.noise_size)  # # 100 x noise_size x 1 x 1\n",
        "\n",
        "    iter_per_epoch = len(train_iter)\n",
        "    total_train_iters = opts.train_iters\n",
        "\n",
        "    losses = {\"iteration\": [], \"D_fake_loss\": [], \"D_real_loss\": [], \"G_loss\": []}\n",
        "\n",
        "\n",
        "    try:\n",
        "        for iteration in range(1, opts.train_iters + 1):\n",
        "\n",
        "            # Reset data_iter for each epoch\n",
        "            if iteration % iter_per_epoch == 0:\n",
        "                train_iter = iter(dataloader)\n",
        "\n",
        "            batch = next(iter(train_iter))\n",
        "            real_images, real_labels = batch\n",
        "            real_images, real_labels = to_var(real_images), to_var(real_labels).long().squeeze()\n",
        "\n",
        "            # ones = Variable(torch.Tensor(real_images.shape[0]).float().cuda().fill_(1.0), requires_grad=False)\n",
        "\n",
        "            for d_i in range(opts.d_train_iters):\n",
        "                d_optimizer.zero_grad()\n",
        "\n",
        "                # 1. Compute the discriminator loss on real images\n",
        "                real_logits = D(real_images)\n",
        "                D_real_loss = F.binary_cross_entropy_with_logits(real_logits, torch.ones_like(real_logits))\n",
        "\n",
        "                # 2. Sample noise\n",
        "                noise = sample_noise(real_images.size(0), opts.noise_size)\n",
        "\n",
        "                # 3. Generate fake images from the noise\n",
        "                fake_images = G(noise)\n",
        "\n",
        "                # 4. Compute the discriminator loss on the fake images\n",
        "                fake_logits = D(fake_images.detach())  # Detach to avoid computing gradients through the generator\n",
        "                D_fake_loss = F.binary_cross_entropy_with_logits(fake_logits, torch.zeros_like(fake_logits))\n",
        "\n",
        "                # 5. Compute the total discriminator loss\n",
        "                D_total_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "                D_total_loss.backward()\n",
        "                d_optimizer.step()\n",
        "\n",
        "            # TRAIN THE GENERATOR\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            # 1. Sample noise\n",
        "            noise = sample_noise(real_images.size(0), opts.noise_size)\n",
        "\n",
        "            # 2. Generate fake images from the noise\n",
        "            fake_images = G(noise)\n",
        "\n",
        "            # 3. Compute the generator loss\n",
        "            fake_logits = D(fake_images)\n",
        "            G_loss = F.binary_cross_entropy_with_logits(fake_logits, torch.ones_like(fake_logits))\n",
        "\n",
        "            G_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "\n",
        "            # Print the log info\n",
        "            if iteration % opts.log_step == 0:\n",
        "                losses['iteration'].append(iteration)\n",
        "                losses['D_real_loss'].append(D_real_loss.item())\n",
        "                losses['D_fake_loss'].append(D_fake_loss.item())\n",
        "                losses['G_loss'].append(G_loss.item())\n",
        "                print('Iteration [{:4d}/{:4d}] | D_real_loss: {:6.4f} | D_fake_loss: {:6.4f} | G_loss: {:6.4f}'.format(\n",
        "                    iteration, total_train_iters, D_real_loss.item(), D_fake_loss.item(), G_loss.item()))\n",
        "\n",
        "            # Save the generated samples\n",
        "            if iteration % opts.sample_every == 0:\n",
        "                gan_save_samples(G, fixed_noise, iteration, opts)\n",
        "\n",
        "            # Save the model parameters\n",
        "            if iteration % opts.checkpoint_every == 0:\n",
        "                gan_checkpoint(iteration, G, D, opts)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(losses['iteration'], losses['D_real_loss'], label='D_real')\n",
        "    plt.plot(losses['iteration'], losses['D_fake_loss'], label='D_fake')\n",
        "    plt.plot(losses['iteration'], losses['G_loss'], label='G')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(opts.sample_dir, 'losses.png'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "\n",
        "    dataloader_X, test_dataloader_X = get_emoji_loader(opts.train_path, opts.test_path, opts.batch_size, opts.image_size)\n",
        "    import random\n",
        "        # Set the random seed manually for reproducibility.\n",
        "    torch.manual_seed(opts.fix_seed)\n",
        "    random.seed(opts.fix_seed)\n",
        "    np.random.seed(opts.fix_seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(opts.fix_seed)\n",
        "\n",
        "    create_dir(opts.checkpoint_dir)\n",
        "    create_dir(opts.sample_dir)\n",
        "\n",
        "    gan_training_loop(dataloader_X, test_dataloader_X, opts)\n",
        "\n",
        "\n",
        "    create_dir(opts.checkpoint_dir)\n",
        "    create_dir(opts.sample_dir)\n",
        "\n",
        "    gan_training_loop(dataloader_X, test_dataloader_X, opts)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "slY0IlBl-RIz"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load config"
      ],
      "metadata": {
        "id": "tvu8TjhqEhvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "dir = op.join('/content/drive/My Drive/','DL','DL_HW06','config.yaml')  # Path to the Data folder\n",
        "class Dict_Object:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(dir, 'r') as file:\n",
        "        args = yaml.safe_load(file)\n",
        "    train_path = op.join('/content/drive/My Drive/','DL','DL_HW06', 'Apple')\n",
        "    test_path = op.join('/content/drive/My Drive/','DL','DL_HW06', 'Test_Apple')\n",
        "    # Update\n",
        "    args['train_path'] = train_path\n",
        "    args['test_path'] = test_path\n",
        "\n",
        "    args_o = Dict_Object(**args)\n",
        "    train(args_o)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoK3jM_2Cbgt",
        "outputId": "e21ed937-216f-449f-c7ae-322e0306a51f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models moved to GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exiting early from training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f84214e7490>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models moved to GPU.\n",
            "Exiting early from training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f84214e7490>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}